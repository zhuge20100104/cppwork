{"cells":[{"cell_type":"code","execution_count":0,"id":"20191206-055651_1675997189","metadata":{},"outputs":[],"source":["\n# path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_category.v1/fact/granularity=daily/month=2018-02/device_id=2001/store_id=143445/\"\n# spark.read.parquet(path).filter(\"app_id=100 and date='2018-02-28'\").show()\n\n\n# path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v2/fact/granularity=monthly/date=2015-12-31/\"\n# spark.read.parquet(path).printSchema()\n# collect_list = spark.read.parquet(path).filter(\"device_code='android-phone' and app_id='20600000272436' and country_code='US'\").collect()\n# print collect_list\n# print len(collect_list[0])\n# for record in collect_list[0]:\n#     print record\n\nraw_path_1=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v2/fact/granularity=monthly/date=2013-01-31/\"\nspark.read.parquet(raw_path_1).printSchema()\n\nspark.read.parquet(raw_path_1).filter(\"app_id='20600000000014' and country_code='AU' and device_code='android-phone'\").show()\n\n\n# path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v1/fact/granularity=monthly/month=2018-02/device_id=1001/store_id=1/\"\n# spark.read.parquet(path).printSchema()\n# spark.read.parquet(path).filter(\"app_id='20600003808593' and seg_app_id='20600000011090'\").orderBy('kpi').show()\n"]},{"cell_type":"code","execution_count":0,"id":"20191204-072631_868317230","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n-- SELECT date\n-- FROM plproxy.execute_select_nestloop(\\$proxy\\$\n--\tselect date from au.app_monthly_2002_143445_201801\n-- \\$proxy\\$) t (date DATE) limit 3;\n\n-- SELECT device_id,store_id,date,app_id,kpi,estimate\n-- FROM plproxy.execute_select_nestloop(\\$proxy\\$\n-- select device_id,store_id,date,app_id,kpi,estimate from mu.app_monthly_2002_0_201601\n--\\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT) where app_id in (SELECT app_id\n-- FROM plproxy.execute_select_nestloop(\\$proxy\\$\n--select app_id from mu.app_monthly_2002_0_201601\n--\\$proxy\\$) t (app_id BIGINT) limit 1 ) order by kpi  ;\n\nSELECT device_id,store_id,date,app_id,kpi,estimate FROM plproxy.execute_select_nestloop(\\$proxy\\$\nselect device_id,store_id,date,app_id,kpi,estimate from mu.app_monthly_2002_143447_201501\\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT) where app_id in (SELECT app_id FROM plproxy.execute_select_nestloop(\\$proxy\\$select app_id from mu.app_monthly_2002_143447_201501\\$proxy\\$) t (app_id BIGINT) limit 1 ) order by kpi  ;\n\n\n-- SELECT device_id,store_id,date,app_id,kpi,estimate,category_id,rank\n-- FROM plproxy.execute_select_nestloop(\\$proxy\\$\n-- select device_id,store_id,date,app_id,kpi,estimate, category_id,rank from mu.category_daily_2001_143445_201802\n-- \\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT, category_id INT, rank INT) where app_id=100 and date='2018-02-28' and device_id='2001' limit 20;\n\n\n-- SELECT device_id,store_id,date,app_id,kpi,estimate,seg_app_id\n--  FROM plproxy.execute_select_nestloop(\\$proxy\\$select device_id,store_id,date,app_id,kpi,estimate,seg_app_id from au.app_monthly_1001_1_201802\n-- \\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT, seg_app_id BIGINT) where app_id='20600003808593' and seg_app_id='20600000011090' order by kpi asc limit 30;\n\nEOF\n\n143447 2002 2015-01"]},{"cell_type":"code","execution_count":0,"id":"20191216-034833_1881797511","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\n\nkpi_mapping={1:\"est_average_active_users\", 2: \"est_average_session_per_user\", 3:\"est_average_session_duration\", 4:\"est_install_penetration\", 5:\"est_average_active_days\", 6:\"est_percentage_active_days\", 7:\"est_average_bytes_per_user\" , 8:\"est_average_time_per_user\", 9:\"est_usage_penetration\", 10:\"est_open_rate\",11:\"est_total_time\", 12:\"est_share_of_category_time\",14:\"est_total_sessions\", 15:\"est_share_of_category_session\", 17:\"est_average_bytes_per_session\", 18:\"est_share_of_category_bytes\", 20:\"est_percent_of_wifi_total\", 21:\"est_mb_per_second\", 22:\"est_panel_size\", 23:\"est_installs\", 24:\"est_average_active_users_country_share\", 25:\"est_installs_country_share\", 26:\"est_audience_index\", 27:\"est_audience_percentage\",28:\"est_cross_product_affinity\"}\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return {\"2001\":\"ios-phone\",\"2002\":\"ios-tablet\"}\n    else:\n        return {\"1001\":\"android-phone\" ,\"1002\":\"android-tablet\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)  \n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db,current  ))\n        current += relativedelta(months=1)  \n    return result\n\n\ndef get_path_date_list(granularity):\n    # df_date = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v1/fact/granularity={}/\".format(granularity)).select('date').dropDuplicates()\n    # collect_date= df_date.collect()\n    collect_date=[Row(date=u'2014-02-22'), Row(date=u'2014-12-13'), Row(date=u'2019-08-31'), Row(date=u'2016-08-20'), Row(date=u'2016-05-21'), Row(date=u'2015-02-28'), Row(date=u'2013-12-28'), Row(date=u'2014-11-01'), Row(date=u'2015-08-15'), Row(date=u'2016-01-16'), Row(date=u'2018-12-22'), Row(date=u'2016-07-23'), Row(date=u'2014-02-08'), Row(date=u'2017-03-18'), Row(date=u'2019-06-08'), Row(date=u'2019-07-13'), Row(date=u'2019-01-26'), Row(date=u'2016-04-09'), Row(date=u'2016-07-30'), Row(date=u'2018-07-21'), Row(date=u'2015-02-07'), Row(date=u'2013-10-05'), Row(date=u'2018-09-15'), Row(date=u'2013-02-09'), Row(date=u'2015-10-10'), Row(date=u'2018-08-04'), Row(date=u'2019-06-15'), Row(date=u'2018-07-28'), Row(date=u'2018-12-15'), Row(date=u'2019-06-22'), Row(date=u'2016-01-30'), Row(date=u'2014-02-15'), Row(date=u'2014-10-18'), Row(date=u'2014-06-28'), Row(date=u'2014-05-24'), Row(date=u'2017-09-23'), Row(date=u'2015-01-10'), Row(date=u'2016-01-23'), Row(date=u'2016-10-15'), Row(date=u'2014-09-27'), Row(date=u'2015-03-28'), Row(date=u'2013-06-15'), Row(date=u'2018-06-09'), Row(date=u'2018-04-28'), Row(date=u'2019-01-05'), Row(date=u'2017-06-24'), Row(date=u'2014-09-20'), Row(date=u'2017-12-02'), Row(date=u'2013-04-06'), Row(date=u'2013-08-03'), Row(date=u'2014-04-05'), Row(date=u'2013-10-26'), Row(date=u'2019-11-16'), Row(date=u'2016-10-08'), Row(date=u'2019-01-12'), Row(date=u'2018-09-29'), Row(date=u'2017-07-29'), Row(date=u'2019-04-06'), Row(date=u'2018-02-17'), Row(date=u'2019-10-12'), Row(date=u'2016-10-01'), Row(date=u'2014-05-10'), Row(date=u'2018-06-30'), Row(date=u'2018-12-01'), Row(date=u'2013-09-28'), Row(date=u'2017-06-03'), Row(date=u'2013-07-27'), Row(date=u'2016-11-19'), Row(date=u'2013-11-16'), Row(date=u'2015-05-02'), Row(date=u'2017-02-11'), Row(date=u'2013-07-06'), Row(date=u'2015-01-24'), Row(date=u'2013-03-23'), Row(date=u'2019-09-21'), Row(date=u'2016-12-31'), Row(date=u'2018-04-14'), Row(date=u'2018-07-07'), Row(date=u'2018-08-18'), Row(date=u'2019-10-05'), Row(date=u'2018-11-17'), Row(date=u'2017-02-04'), Row(date=u'2017-12-09'), Row(date=u'2015-11-28'), Row(date=u'2014-04-12'), Row(date=u'2013-12-14'), Row(date=u'2017-08-26'), Row(date=u'2018-06-23'), Row(date=u'2018-01-20'), Row(date=u'2016-05-14'), Row(date=u'2018-10-13'), Row(date=u'2015-02-14'), Row(date=u'2015-07-11'), Row(date=u'2015-07-18'), Row(date=u'2014-07-26'), Row(date=u'2014-11-08'), Row(date=u'2015-05-30'), Row(date=u'2018-03-03'), Row(date=u'2014-10-25'), Row(date=u'2016-02-06'), Row(date=u'2017-02-18'), Row(date=u'2015-09-19'), Row(date=u'2015-04-11'), Row(date=u'2013-04-13'), Row(date=u'2016-11-26'), Row(date=u'2015-04-04'), Row(date=u'2016-12-24'), Row(date=u'2013-12-07'), Row(date=u'2016-11-12'), Row(date=u'2015-10-24'), Row(date=u'2015-05-09'), Row(date=u'2017-03-04'), Row(date=u'2018-12-29'), Row(date=u'2019-03-23'), Row(date=u'2014-12-20'), Row(date=u'2016-03-12'), Row(date=u'2017-02-25'), Row(date=u'2019-03-30'), Row(date=u'2014-01-18'), Row(date=u'2013-07-20'), Row(date=u'2019-06-29'), Row(date=u'2017-03-25'), Row(date=u'2018-07-14'), Row(date=u'2016-03-05'), Row(date=u'2017-05-13'), Row(date=u'2013-02-02'), Row(date=u'2014-08-09'), Row(date=u'2019-05-04'), Row(date=u'2013-06-08'), Row(date=u'2019-02-23'), Row(date=u'2015-07-25'), Row(date=u'2016-08-06'), Row(date=u'2018-01-06'), Row(date=u'2016-05-28'), Row(date=u'2017-11-25'), Row(date=u'2018-05-19'), Row(date=u'2014-10-11'), Row(date=u'2014-03-22'), Row(date=u'2013-06-22'), Row(date=u'2014-08-30'), Row(date=u'2016-01-09'), Row(date=u'2013-01-12'), Row(date=u'2014-08-23'), Row(date=u'2015-08-29'), Row(date=u'2019-03-16'), Row(date=u'2015-08-01'), Row(date=u'2016-08-13'), Row(date=u'2018-11-10'), Row(date=u'2015-01-31'), Row(date=u'2013-11-09'), Row(date=u'2014-05-17'), Row(date=u'2017-09-16'), Row(date=u'2013-07-13'), Row(date=u'2014-03-01'), Row(date=u'2018-10-06'), Row(date=u'2017-11-04'), Row(date=u'2018-11-24'), Row(date=u'2013-09-14'), Row(date=u'2013-10-19'), Row(date=u'2013-05-18'), Row(date=u'2016-09-17'), Row(date=u'2017-04-29'), Row(date=u'2014-01-25'), Row(date=u'2018-02-03'), Row(date=u'2017-05-27'), Row(date=u'2013-03-09'), Row(date=u'2019-10-19'), Row(date=u'2019-05-25'), Row(date=u'2013-02-23'), Row(date=u'2018-02-10'), Row(date=u'2015-03-21'), Row(date=u'2017-10-21'), Row(date=u'2014-06-07'), Row(date=u'2013-09-07'), Row(date=u'2019-04-20'), Row(date=u'2014-01-04'), Row(date=u'2019-08-17'), Row(date=u'2013-06-29'), Row(date=u'2016-07-09'), Row(date=u'2017-01-14'), Row(date=u'2019-08-10'), Row(date=u'2018-03-17'), Row(date=u'2017-04-22'), Row(date=u'2014-02-01'), Row(date=u'2017-09-09'), Row(date=u'2017-03-11'), Row(date=u'2014-11-29'), Row(date=u'2019-02-02'), Row(date=u'2019-05-11'), Row(date=u'2017-07-08'), Row(date=u'2014-06-21'), Row(date=u'2018-01-27'), Row(date=u'2015-12-26'), Row(date=u'2013-09-21'), Row(date=u'2015-10-17'), Row(date=u'2013-05-04'), Row(date=u'2015-12-05'), Row(date=u'2014-04-26'), Row(date=u'2018-03-31'), Row(date=u'2017-04-08'), Row(date=u'2015-10-31'), Row(date=u'2013-08-17'), Row(date=u'2019-07-06'), Row(date=u'2017-11-18'), Row(date=u'2019-11-02'), Row(date=u'2013-05-25'), Row(date=u'2019-09-14'), Row(date=u'2013-04-20'), Row(date=u'2013-03-30'), Row(date=u'2013-11-02'), Row(date=u'2015-11-07'), Row(date=u'2017-08-05'), Row(date=u'2013-05-11'), Row(date=u'2015-02-21'), Row(date=u'2013-03-02'), Row(date=u'2017-01-28'), Row(date=u'2013-08-10'), Row(date=u'2015-08-08'), Row(date=u'2017-01-21'), Row(date=u'2018-06-16'), Row(date=u'2014-12-06'), Row(date=u'2017-07-22'), Row(date=u'2017-07-15'), Row(date=u'2014-07-05'), Row(date=u'2016-02-27'), Row(date=u'2018-01-13'), Row(date=u'2015-09-26'), Row(date=u'2018-03-24'), Row(date=u'2014-05-31'), Row(date=u'2018-09-22'), Row(date=u'2016-09-24'), Row(date=u'2019-07-27'), Row(date=u'2013-03-16'), Row(date=u'2017-10-07'), Row(date=u'2015-06-27'), Row(date=u'2014-11-15'), Row(date=u'2013-04-27'), Row(date=u'2016-02-20'), Row(date=u'2013-06-01'), Row(date=u'2019-03-09'), Row(date=u'2015-12-19'), Row(date=u'2016-07-02'), Row(date=u'2016-12-03'), Row(date=u'2017-12-16'), Row(date=u'2017-05-06'), Row(date=u'2019-11-23'), Row(date=u'2016-02-13'), Row(date=u'2018-06-02'), Row(date=u'2017-01-07'), Row(date=u'2016-12-10'), Row(date=u'2017-12-23'), Row(date=u'2015-07-04'), Row(date=u'2016-04-16'), Row(date=u'2014-06-14'), Row(date=u'2017-09-30'), Row(date=u'2013-11-23'), Row(date=u'2017-09-02'), Row(date=u'2016-03-19'), Row(date=u'2014-04-19'), Row(date=u'2015-03-14'), Row(date=u'2017-12-30'), Row(date=u'2015-01-17'), Row(date=u'2019-09-28'), Row(date=u'2016-10-22'), Row(date=u'2014-11-22'), Row(date=u'2014-08-02'), Row(date=u'2017-08-19'), Row(date=u'2014-09-06'), Row(date=u'2019-04-27'), Row(date=u'2015-06-13'), Row(date=u'2015-08-22'), Row(date=u'2018-10-27'), Row(date=u'2018-09-08'), Row(date=u'2015-10-03'), Row(date=u'2019-07-20'), Row(date=u'2014-10-04'), Row(date=u'2018-08-11'), Row(date=u'2016-06-25'), Row(date=u'2018-11-03'), Row(date=u'2013-08-31'), Row(date=u'2018-10-20'), Row(date=u'2016-08-27'), Row(date=u'2019-08-24'), Row(date=u'2019-11-09'), Row(date=u'2017-06-17'), Row(date=u'2014-07-19'), Row(date=u'2014-03-29'), Row(date=u'2017-10-14'), Row(date=u'2016-04-02'), Row(date=u'2016-12-17'), Row(date=u'2017-08-12'), Row(date=u'2014-01-11'), Row(date=u'2018-02-24'), Row(date=u'2016-01-02'), Row(date=u'2018-05-05'), Row(date=u'2015-05-16'), Row(date=u'2019-01-19'), Row(date=u'2018-05-12'), Row(date=u'2017-11-11'), Row(date=u'2016-06-04'), Row(date=u'2019-02-16'), Row(date=u'2016-09-10'), Row(date=u'2013-11-30'), Row(date=u'2016-03-26'), Row(date=u'2019-06-01'), Row(date=u'2019-05-18'), Row(date=u'2017-07-01'), Row(date=u'2017-05-20'), Row(date=u'2013-12-21'), Row(date=u'2018-09-01'), Row(date=u'2015-09-05'), Row(date=u'2019-04-13'), Row(date=u'2017-04-01'), Row(date=u'2017-04-15'), Row(date=u'2013-01-26'), Row(date=u'2013-08-24'), Row(date=u'2014-09-13'), Row(date=u'2019-10-26'), Row(date=u'2016-11-05'), Row(date=u'2013-01-19'), Row(date=u'2019-03-02'), Row(date=u'2014-05-03'), Row(date=u'2014-03-15'), Row(date=u'2019-09-07'), Row(date=u'2015-04-25'), Row(date=u'2016-07-16'), Row(date=u'2016-04-30'), Row(date=u'2015-04-18'), Row(date=u'2019-02-09'), Row(date=u'2015-11-21'), Row(date=u'2015-09-12'), Row(date=u'2016-06-18'), Row(date=u'2019-08-03'), Row(date=u'2014-08-16'), Row(date=u'2015-01-03'), Row(date=u'2015-12-12'), Row(date=u'2014-12-27'), Row(date=u'2018-04-07'), Row(date=u'2018-05-26'), Row(date=u'2016-06-11'), Row(date=u'2013-02-16'), Row(date=u'2015-11-14'), Row(date=u'2016-04-23'), Row(date=u'2018-04-21'), Row(date=u'2016-10-29'), Row(date=u'2015-03-07'), Row(date=u'2018-03-10'), Row(date=u'2015-06-20'), Row(date=u'2018-08-25'), Row(date=u'2014-07-12'), Row(date=u'2014-03-08'), Row(date=u'2015-06-06'), Row(date=u'2013-10-12'), Row(date=u'2016-05-07'), Row(date=u'2017-06-10'), Row(date=u'2016-09-03'), Row(date=u'2018-12-08'), Row(date=u'2017-10-28'), Row(date=u'2015-05-23')]\n\n    print collect_date\n    date_list = [(x[0][:7],x[0]) for x in collect_date]\n\n    return date_list\n    \n    \nimport traceback\ndef check_au_app_data_count(store_id_list, device_id_list, _granularity, date_list):\n    t = unittest.TestCase('run')\n    for id,country_code in store_id_list.items():\n        for device,device_code in device_id_list.items():\n            for m in date_list:\n                raw_count_with_KPI=''\n                # print id, device, m[0] , m[1]\n                try:\n                    raw_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={raw_device_id}/store_id={raw_store_id}/\"\n                    unified_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.segmented-by-app-used.v1/fact/granularity={unified_granularity}/date={unified_date}/\"\n                    raw_path_parse=raw_path.format(raw_device_id=device,raw_store_id=id, raw_month=m[0], raw_granularity=_granularity)\n                    raw_count_with_KPI=spark.read.parquet(raw_path_parse).filter(\"date={}\".format(m[1])).select(\"kpi\",\"app_id\",\"seg_app_id\").distinct().groupBy(\"kpi\").agg(count(\"kpi\")).collect()\n                    # print raw_count_with_KPI\n                except AnalysisException as e: \n                    pass\n                    # print e\n                    break\n                    # traceback.print_exc()\n                for row in raw_count_with_KPI:\n                    # print 'row _ test', row[\"kpi\"], row[\"count(kpi)\"]\n                    unified_path_parse=unified_path.format(unified_date=m[1], unified_granularity=_granularity)\n                    unified_count= spark.read.parquet(unified_path_parse).filter(\"device_code='{}' and country_code='{}'\".format(device_code,country_code)).filter(\"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                    # print 'unified count' , unified_count\n                    t.assertEqual(row[\"count(kpi)\"], unified_count, \" raw: {} ~ unified data: {},device:{},  store_id:{} , month: {}, KPI {}\".format(row[\"count(kpi)\"], unified_count, device, id , m, row[\"kpi\"]))\ngraularity_list=[\"weekly\"]\nfor graularity in graularity_list:\n    check_au_app_data_count(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),graularity, get_path_date_list(graularity))\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200107-023157_1819515638","metadata":{},"outputs":[],"source":["\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v1/fact/granularity=weekly/month=2014-02/device_id=1001/store_id=1/\").filter(\"kpi=12 and date='2014-02-22'\").count()\n\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.segmented-by-app-used.v1/fact/granularity=weekly/date=2014-02-22\").filter(\"device_code='android-phone' and country_code='AU' and est_share_of_category_time is not null\").count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200107-023721_2085721225","metadata":{},"outputs":[],"source":["\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v1/fact/granularity=weekly/month=2014-02/device_id=1001/store_id=1/\").filter(\"kpi=12 and date='2014-02-22'\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200107-023246_497465412","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v1/fact/granularity=monthly/month=2014-02/device_id=1001/store_id=1/\n \n \n \n"]},{"cell_type":"code","execution_count":0,"id":"20191207-131148_1436427172","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom aaintdatapipeline.application.app_qa.db_check_v1.constants import pg_settings\nfrom aaintdatapipeline.core.urn import Urn\nfrom aaintdatapipeline.application.app_qa.conf import settings\nfrom aaintdatapipeline.core.loader.plproxy import build_db_settings\nfrom aaplproxy.connection import ClusterConnection\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport psycopg2\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\n# IOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES'}\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return ['2001','2002']\n    else:\n        return ['1001','1002']\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db ))\n        current += relativedelta(months=1)    \n    return result\n    \n    \ndef pg_settings(urn, sql, granularity, schema):\n    result = {}\n    db_settings = build_db_settings(urn, schema)\n    connection = ClusterConnection(db_settings)\n    master_runner = connection.master_runner\n    rows, _ = master_runner.select(sql)\n    result[granularity] = rows\n    return result\n\n\nurn = Urn(\n    namespace='app-qa.db-validation.v1',\n    owner='app_qa'\n)\n\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={device}/store_id={sid}/\"\n\n\nimport traceback\ndef check_au_app_dump_data(store_id_list, device_id_list, _granularity):\n    t = unittest.TestCase('run')\n    for id in store_id_list:\n        for device in device_id_list:\n            for m in get_month_list():\n                # print id, device, m[0] \n                try:\n                    weekly_basic = \"\"\"\n                        SELECT app_id_count FROM plproxy.execute_select_nestloop($$select count(app_id) from au.app_{granularity}_{device_id}_{store_id}_{month} $$) t (app_id_count BIGINT)\n                    \"\"\".format(device_id=device,store_id=id, month=m[1], granularity=_granularity)\n                    granularity_ = _granularity.capitalize()\n                    result = pg_settings(urn, weekly_basic, granularity_,'usage')\n                    l = [int(x[0]) for x in result[granularity_]]\n                    if sum(l)==0:\n                        pass\n                        # print 'the table is empty  - device:{}  store_id:{} month:{}'.format(device, id, m[0])\n                    else:\n                        dump_data_path=path.format(device=device, raw_month=m[0], sid=id, raw_granularity=_granularity)\n                        dump_count=spark.read.parquet(dump_data_path).count()\n                        t.assertEqual(dump_count, sum(l), \"{}:{} ~ raw: {} ~ dumped data: {},device:{},  store_id:{} , month: {}\".format(granularity_, m[0], sum(l), dump_count, device, id , m))\n                except psycopg2.ProgrammingError : \n                    pass\n                    # traceback.print_exc()\n                    # print(e)\n                    #  print 'the table is not existed - granularity {}, device:{}  store_id:{} month:{}'.format(granularity_, device, id, m[0])\n                \n\n# check_au_app_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'monthly')\n# check_au_app_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'monthly')\n# check_au_app_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'weekly')\ncheck_au_app_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'weekly')\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191211-072056_768960845","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom aaintdatapipeline.application.app_qa.db_check_v1.constants import pg_settings\nfrom aaintdatapipeline.core.urn import Urn\nfrom aaintdatapipeline.application.app_qa.conf import settings\nfrom aaintdatapipeline.core.loader.plproxy import build_db_settings\nfrom aaplproxy.connection import ClusterConnection\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport psycopg2\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return ['2001','2002']\n    else:\n        return ['1001','1002']\n   # return ['1001','1002','2001','2002']\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db ))\n        current += relativedelta(months=1)    \n    return result\n    \n    \ndef pg_settings(urn, sql, granularity, schema):\n    result = {}\n    db_settings = build_db_settings(urn, schema)\n    connection = ClusterConnection(db_settings)\n    master_runner = connection.master_runner\n    rows, _ = master_runner.select(sql)\n    result[granularity] = rows\n    return result\n\n\nurn = Urn(\n    namespace='app-qa.db-validation.v1',\n    owner='app_qa'\n)\n\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v1/fact/granularity=monthly/month={raw_month}/device_id={device}/store_id={sid}/\"\n\n\nimport traceback\ndef check_mobile_web_basic_date(store_id_list, device_id_list):\n    t = unittest.TestCase('run')\n    for id in store_id_list:\n        for device in device_id_list:\n            for m in get_month_list():\n                print id, device, m[0] \n                try:\n                    weekly_basic = \"\"\"\n                        SELECT app_id_count FROM plproxy.execute_select_nestloop($$select count(app_id) from mu.app_monthly_{device_id}_{store_id}_{month} $$) t (app_id_count BIGINT)\n                    \"\"\".format(device_id=device,store_id=id, month=m[1])\n                    granularity_ = 'Monthly'\n                    result = pg_settings(urn, weekly_basic, granularity_,'usage')\n                    l = [int(x[0]) for x in result[\"Monthly\"]]\n                    if sum(l)==0:\n                        print 'the table is empty  - device:{}  store_id:{} month:{}'.format(device, id, m[0])\n                    else:\n                        dump_data_path=path.format(device=device, raw_month=m[0], sid=id)\n                        dump_count=spark.read.parquet(dump_data_path).count()\n                        t.assertEqual(dump_count, sum(l), \"month:{} ~ raw: {} ~ dumped data: {} \".format(m[0], dump_count, sum(l)))\n                except psycopg2.ProgrammingError : \n                    # traceback.print_exc()\n                    # print(e)\n                    print 'the table is not existed - device:{}  store_id:{} month:{}'.format(device, id, m[0])\n                \n\n\ncheck_mobile_web_basic_date(IOS_COUNTRY_ID_CODES, get_device_list('ios'))\n#check_mobile_web_basic_date(ANDROID_COUNTRY_ID_CODES, get_device_list('android'))\n\n#  > /tmp/count_mu.txt"]},{"cell_type":"code","execution_count":0,"id":"20191207-080620_2118055088","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom aaintdatapipeline.application.app_qa.db_check_v1.constants import pg_settings\nfrom aaintdatapipeline.core.urn import Urn\nfrom aaintdatapipeline.application.app_qa.conf import settings\nfrom aaintdatapipeline.core.loader.plproxy import build_db_settings\nfrom aaplproxy.connection import ClusterConnection\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport psycopg2\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return ['2001','2002']\n    else:\n        return ['1001','1002']\n   # return ['1001','1002','2001','2002']\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db ))\n        current += relativedelta(months=1)    \n    return result\n    \n    \ndef pg_settings(urn, sql, granularity, schema):\n    result = {}\n    db_settings = build_db_settings(urn, schema)\n    connection = ClusterConnection(db_settings)\n    master_runner = connection.master_runner\n    rows, _ = master_runner.select(sql)\n    result[granularity] = rows\n    return result\n\n\nurn = Urn(\n    namespace='app-qa.db-validation.v1',\n    owner='app_qa'\n)\n\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={device}/store_id={sid}/\"\n\n\nimport traceback\ndef check_mobile_web_basic_date(store_id_list, device_id_list, _granularity):\n    t = unittest.TestCase('run')\n    for id in store_id_list:\n        for device in device_id_list:\n            for m in get_month_list():\n                # print id, device, m[0] \n                try:\n                    weekly_basic = \"\"\"\n                        SELECT app_id_count FROM plproxy.execute_select_nestloop($$select count(app_id) from mu.app_{granularity}_{device_id}_{store_id}_{month} $$) t (app_id_count BIGINT)\n                    \"\"\".format(device_id=device,store_id=id, month=m[1], granularity=_granularity)\n                    granularity_ = _granularity.capitalize()\n                    result = pg_settings(urn, weekly_basic, granularity_,'usage')\n                    l = [int(x[0]) for x in result[granularity_]]\n                    if sum(l)==0:\n                        pass\n                        # print 'the table is empty  - device:{}  store_id:{} month:{}'.format(device, id, m[0])\n                    else:\n                        dump_data_path=path.format(device=device, raw_month=m[0], sid=id, raw_granularity=_granularity)\n                        dump_count=spark.read.parquet(dump_data_path).count()\n                        t.assertEqual(dump_count, sum(l), \"{}:{} ~ raw: {} ~ dumped data: {} \".format(granularity_, m[0], dump_count, sum(l)))\n                except psycopg2.ProgrammingError : \n                    pass\n                    # traceback.print_exc()\n                    # print(e)\n                    # print 'the table is not existed - granularity {}, device:{}  store_id:{} month:{}'.format(granularity_, device, id, m[0])\n                \n\n\ncheck_mobile_web_basic_date(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'weekly')\ncheck_mobile_web_basic_date(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'weekly')\ncheck_mobile_web_basic_date(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'daily')\ncheck_mobile_web_basic_date(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'daily')\nprint 'pass'\n\n#  > /tmp/count_mu.txt"]},{"cell_type":"code","execution_count":0,"id":"20191210-030259_1892143835","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom aaintdatapipeline.application.app_qa.db_check_v1.constants import pg_settings\nfrom aaintdatapipeline.core.urn import Urn\nfrom aaintdatapipeline.application.app_qa.conf import settings\nfrom aaintdatapipeline.core.loader.plproxy import build_db_settings\nfrom aaplproxy.connection import ClusterConnection\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport psycopg2\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return ['2001','2002']\n    else:\n        return ['1001','1002']\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db ))\n        current += relativedelta(months=1)    \n    return result\n    \n    \ndef pg_settings(urn, sql, granularity, schema):\n    result = {}\n    db_settings = build_db_settings(urn, schema)\n    connection = ClusterConnection(db_settings)\n    master_runner = connection.master_runner\n    rows, _ = master_runner.select(sql)\n    result[granularity] = rows\n    return result\n\n\nurn = Urn(\n    namespace='app-qa.db-validation.v1',\n    owner='app_qa'\n)\n\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_category.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={device}/store_id={sid}/\"\n\n\nimport traceback\ndef check_mu_category_dump_data(store_id_list, device_id_list, _granularity):\n    t = unittest.TestCase('run')\n    for id in store_id_list:\n        for device in device_id_list:\n            for m in get_month_list():\n                # print id, device, m[0] \n                try:\n                    weekly_basic = \"\"\"\n                        SELECT app_id_count FROM plproxy.execute_select_nestloop($$select count(app_id) from mu.category_{granularity}_{device_id}_{store_id}_{month} $$) t (app_id_count BIGINT)\n                    \"\"\".format(device_id=device,store_id=id, month=m[1], granularity=_granularity)\n                    granularity_ = _granularity.capitalize()\n                    result = pg_settings(urn, weekly_basic, granularity_,'usage')\n                    l = [int(x[0]) for x in result[granularity_]]\n                    if sum(l)==0:\n                        pass\n                        # print 'the table is empty  - device:{}  store_id:{} month:{}'.format(device, id, m[0])\n                    else:\n                        dump_data_path=path.format(device=device, raw_month=m[0], sid=id, raw_granularity=_granularity)\n                        dump_count=spark.read.parquet(dump_data_path).count()\n                        t.assertEqual(dump_count, sum(l), \"{}:{} ~ raw: {} ~ dumped data: {} \".format(granularity_, m[0], dump_count, sum(l)))\n                except psycopg2.ProgrammingError : \n                    pass\n                    # traceback.print_exc()\n                    # print(e)\n                    # print 'the table is not existed - granularity {}, device:{}  store_id:{} month:{}'.format(granularity_, device, id, m[0])\n                \n\ncheck_mu_category_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'monthly')\ncheck_mu_category_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'monthly')\ncheck_mu_category_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'weekly')\ncheck_mu_category_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'weekly')\ncheck_mu_category_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'daily')\ncheck_mu_category_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'daily')\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191209-035143_1878238899","metadata":{},"outputs":[],"source":["\nfrom dateutil.relativedelta import relativedelta\nimport datetime\n# date1='2019-01-01'\n# date2='2019-01-01'\n# date_list=[]\n# start = datetime.datetime.strptime(date1, '%Y-%m-%d')\n# end = datetime.datetime.strptime(date2, '%Y-%m-%d')\n# step = datetime.timedelta(month=1)\n# while start <= end:\n#         date_list.append(str(start.date()))\n#         # print start.date()\n#         start += step\n# print date_list\n# print \"','\".join(date_list)\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)  \n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db,current  ))\n        current += relativedelta(months=1)  \n    return result\n\n\n# result = []\n# today = datetime.date(2019, 11, 1) \n# current = datetime.date(2013, 1, 1)    \n\n# while current <= today:\n#     month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n#     month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n#     # print month_data_raw , month_data_leg_db\n#     result.append((month_data_raw, month_data_leg_db ))\n#     current += relativedelta(months=1)\n\n# print result\n# for x in result:\n#     print x[0],x[1]\n    \nprint get_month_list()\nfor x in get_month_list():\n    print last_day_of_month(x[2])\n\n\n# IOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\n# for x in IOS_COUNTRY_ID_CODES:\n#     print x"]},{"cell_type":"code","execution_count":0,"id":"20191206-060529_1406728754","metadata":{},"outputs":[],"source":["%python\nfrom itertools import islice\n\ncount=0\nwith open('/tmp/count_mu.txt') as f:\n    for line in islice(f, 2, 10):\n        count=count+int(line)\nprint count\n"]},{"cell_type":"code","execution_count":0,"id":"20191206-060425_723826812","metadata":{},"outputs":[],"source":["%%sh\nre='[0-9]+$'\n\ncount=0\nwhile read p; do\n  echo \"$p\"\n  if ! [[ $p = $re ]] ; then\n    echo $p\n    count=`expr $count + $p`\n  fi\ndone </tmp/count_mu.txt\n\necho $count\n"]},{"cell_type":"code","execution_count":0,"id":"20191216-064147_2058538341","metadata":{},"outputs":[],"source":["\nkpi_mapping={1:\"est_average_active_users\", 2: \"est_average_session_per_user\", 3:\"est_average_session_duration\", 4:\"est_install_penetration\", 5:\"est_average_active_days\", 6:\"est_percentage_active_days\", 7:\"est_average_bytes_per_user\" , 8:\"est_average_time_per_user\", 9:\"est_usage_penetration\", 10:\"est_open_rate\", 12:\"est_share_of_category_time\", 15:\"est_share_of_category_session\", 17:\"est_average_bytes_per_session\", 18:\"est_share_of_category_bytes\", 20:\"est_percent_of_wifi_total\", 21:\"est_mb_per_second\", 22:\"est_panel_size\", 23:\"est_installs\", 24:\"est_average_active_users_country_share\", 25:\"est_installs_country_share\"}\n\nkpi_mapping[1]"]},{"cell_type":"code","execution_count":0,"id":"20191216-054333_806571330","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v1/fact/granularity=monthly/month=2013-01/\n\n \n\n "]},{"cell_type":"code","execution_count":0,"id":"20191216-054339_535386776","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\nfrom pyspark.storagelevel import StorageLevel\nKPI_LIST = ['est_average_active_users', 'est_average_session_per_user', 'est_average_session_duration',\n            'est_install_penetration', 'est_average_active_days', 'est_percentage_active_days',\n            'est_average_bytes_per_user', 'est_average_time_per_user', 'est_usage_penetration', 'est_open_rate',\n            'est_share_of_category_time', 'est_share_of_category_session', 'est_average_bytes_per_session',\n            'est_share_of_category_bytes', 'est_percent_of_wifi_total', 'est_mb_per_second', 'est_installs',\n            'est_average_active_users_country_share', 'est_installs_country_share']\nEXCLUDE_KPI_ID = [11, 13, 14, 16, 19, 22, 26, 27, 28]\nbase_path_dump = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v1/fact/granularity={granularity}/month={month_str}/\"\nbase_path_transformed = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v1/fact/granularity={granularity}/date={date}/\"\nGRANULARIY_LIST = [\"daily\"]\ndef check_ouput(granularity, date):\n    month_str = date[0:7]\n    path_dump = base_path_dump.format(**{'month_str': month_str, 'granularity': granularity})\n    path_transformed = base_path_transformed.format(**{'date': date, 'granularity': granularity})\n    df_dump = spark.read.parquet(path_dump)\n    df_dump = (\n        df_dump\n            .where(\"date='{}'\".format(date))\n    )\n    dump_count = df_dump.select(\"app_id\").dropDuplicates().count()\n    df_dump = (\n        df_dump\n            .filter(~df_dump.kpi.isin(EXCLUDE_KPI_ID))\n            .groupBy(\"app_id\")\n            .agg(F.col(\"app_id\"), F.sum(\"estimate\"))\n            .select('app_id', 'sum(estimate)')\n            .withColumnRenamed('sum(estimate)', 'raw_sum_est')\n    )\n    df_transformed = spark.read.parquet(path_transformed)\n    df_transformed = df_transformed.na.fill(0)\n    df_transformed = df_transformed.withColumn('sum_kpi', sum([df_transformed[col] for col in KPI_LIST]))\n    df_transformed = (\n        df_transformed\n            .groupBy(\"app_id\")\n            .agg(F.col(\"app_id\"), F.sum(\"sum_kpi\"))\n            .select('app_id', 'sum(sum_kpi)')\n            .withColumnRenamed('sum(sum_kpi)', 'trans_sum_est')\n            .withColumnRenamed('app_id', 'trans_app_id')\n    )\n    df_transformed.persist(StorageLevel.MEMORY_AND_DISK)\n    transformed_count = df_transformed.count()\n    df_join = (\n        df_dump\n            .join(df_transformed, df_dump['app_id'] == df_transformed['trans_app_id'], \"inner\")\n    )\n    df_join = (\n        df_join\n            .withColumn('minus', F.abs(df_join['trans_sum_est'] - df_join['raw_sum_est']))\n            .where('minus > 0.001')\n    )\n    count_check_str = '{}_{}, dump_count:{},transformed_count:{},minus:{}'.format(granularity, date, dump_count,\n                                                                                  transformed_count,\n                                                                                  dump_count - transformed_count)\n    write_to_file('/tmp/transform_qa.log', count_check_str)\n    if df_join.count() > 0:\n        df_join = (\n            df_join\n                .withColumn('granularity', F.lit(granularity))\n                .withColumn('date', F.lit(date))\n                .select('app_id', 'granularity', 'date', 'trans_sum_est', 'raw_sum_est', 'minus')\n        )\n        df_join.toPandas().to_csv('/tmp/transform_qa.csv'.format(), mode='a', header=False)\ndef write_to_file(filename, str):\n    with open(filename, 'a') as file_object:\n        file_object.write(str + \"\\n\")\nfor granularity in GRANULARIY_LIST:\n    df_date = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v1/fact/granularity={}/\".format(granularity)).select('date').dropDuplicates()\n    DATE_LIST =[str(row.date) for row in df_date.collect()]\n    for date in DATE_LIST:\n        check_ouput(granularity, date)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}