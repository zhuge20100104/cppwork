{"cells":[{"cell_type":"code","execution_count":0,"id":"20200915-012653_1878785171","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://aardvark-prod-pdx-mdm-to-int/audience_demographics/version=1.0.0/range_type=MONTH/date=2018-01-31/\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200915-012837_1163010610","metadata":{},"outputs":[],"source":["\nseg_by_demographics_audience=\"s3://aardvark-prod-pdx-mdm-to-int/audience_demographics/version=1.0.0/range_type=MONTH/date=2018-01-31/\"\ndf=spark.read.option(\"basePath\", \"s3://aardvark-prod-pdx-mdm-to-int/audience_demographics/version=1.0.0\").parquet(seg_by_demographics_audience)\n# df.groupBy('device_id').count().show()\n# df.groupBy('kpi').count().show()\n# df.show(10,False)\ndf.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200915-013311_1189756915","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom aadatapipelinecore.core.fs.driver import unified\nfrom aadatapipelinecore.core.fs.driver.unified import UnifiedDTO\nfrom aadatapipelinecore.core.log import logger\nfrom aadatapipelinecore.core.pipeline import schema, type_\nfrom aadatapipelinecore.core.urn import Urn\nfrom pyspark.sql.functions import lit, coalesce,upper\nfrom aadatapipelinecore.core.utils.identifier import atomic_id\ndef main(spark, params):\n    start = datetime.datetime.strptime(params.get(\"start\"), \"%Y-%m-%d\")\n    end = datetime.datetime.strptime(params.get(\"end\"), \"%Y-%m-%d\")\n    granularities = params.get(\"granularity\", \"daily\").split(\",\")\n    original_namespace = params.get(\"original_namespace\")\n    target_namespace = params.get(\"target_namespace\")\n    urn = Urn(\n        namespace=target_namespace,\n        deduplicate=True,\n        identifier=atomic_id()\n    )\n    urn.save_mode = \"overwrite\"\n    urn.bypass_data_monitor = True\n    urn.coalesce = 1\n    urn.manipulation = \"insert\"\n    urn.event = \"transform\"\n    d = end\n    logger.info(granularities)\n    while d > start:\n        d_str = d.strftime(\"%Y-%m\")\n        for granularity in granularities:\n            logger.info(\"for granularity: {}\".format(granularity))\n            logger.info(\"reading granularity = {} and d_str = {} from {} \".format(\n                granularity, d_str, original_namespace))\n            try:\n                usage_df = (\n                    spark\n                        .read\n                        .option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-usage/unified\"\n                                            \"/{}/fact/\".format(original_namespace))\n                        .parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/{}/fact/\"\n                                 \"granularity={}/date={}*\".format(original_namespace, granularity, d_str))\n                )\n            except Exception as e:\n                logger.error(e.message)\n                logger.error(\"date may not exist: {}-{}\".format(granularity, d_str))\n                continue\n\n            usage_df=(\n                usage_df\n                    .withColumn('country_code', upper(usage_df['country_code']))\n                    .withColumnRenamed('app_id', 'product_key')\n                    .withColumnRenamed('city_id', 'city_key')\n                    .withColumn('est_average_session_duration_milliseconds', usage_df['est_average_time_per_user'] / usage_df['est_average_session_per_user'])\n                    .withColumnRenamed('est_average_time_per_user_milliseconds', 'est_average_time_per_user')\n                    .withColumnRenamed('est_active_users', 'est_average_active_users')\n                    .withColumn('est_average_bytes_per_session', usage_df['est_average_bytes_per_user'] / usage_df['est_average_session_per_user'])\n                    .withColumn('est_total_time', usage_df['est_average_active_users'] * usage_df['est_average_time_per_user'])\n            )\n\n            usage_df.show()\n            d -= datetime.timedelta(days=31)\n            \n            usage_df.createOrReplaceTempView(\"df\")\n            all_df = usage_df.drop(\"_identifier\")\n            all_df.createOrReplaceTempView(\"df\")\n            all_df=spark.sql(\"\"\"\n                SELECT product_key,city_key,est_affinity,est_average_active_users,est_average_bytes_per_session,est_average_bytes_per_user,est_average_session_duration,est_average_session_per_user,est_average_time_per_user,est_total_time,est_usage_penetration,est_average_session_duration_milliseconds,\n                CASE WHEN granularity='m' THEN 'monthly' \n                    WHEN granularity='w' THEN 'weekly' END AS granularity,\n                    date,\n                    device_code,\n                    country_code\n                FROM df \n                WHERE est_average_active_users is not NULL\n            \"\"\")\n            all_df.createOrReplaceTempView(\"df\")\n            all_df.show()\n\nparams_list={\n    'start':'2020-8-30',\n    'end':'2020-8-31',\n    'granularity':'m',\n    'original_namespace':'usage.city-level.v3',\n    'target_namespace':'aa.usage_city.v6'\n    \n}\n\nmain(spark,params=params_list)"]},{"cell_type":"code","execution_count":0,"id":"20200915-025733_635920373","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom aadatapipelinecore.core.fs.driver import unified\nfrom aadatapipelinecore.core.fs.driver.unified import UnifiedDTO\nfrom aadatapipelinecore.core.log import logger\nfrom aadatapipelinecore.core.pipeline import schema, type_\nfrom aadatapipelinecore.core.urn import Urn\nfrom pyspark.sql.functions import lit, coalesce,upper\nfrom aadatapipelinecore.core.utils.identifier import atomic_id\ndef main(spark, params):\n    start = datetime.datetime.strptime(params.get(\"start\"), \"%Y-%m-%d\")\n    end = datetime.datetime.strptime(params.get(\"end\"), \"%Y-%m-%d\")\n    granularities = params.get(\"granularity\", \"daily\").split(\",\")\n    original_namespace_age_gender = params.get(\"original_namespace_age_gender\")\n    original_namespace_audience = params.get(\"original_namespace_audience\")\n    target_namespace = params.get(\"target_namespace\")\n    urn = Urn(\n        namespace=target_namespace,\n        deduplicate=True,\n        identifier=atomic_id()\n    )\n    urn.save_mode = \"overwrite\"\n    urn.bypass_data_monitor = True\n    urn.coalesce = 1\n    urn.manipulation = \"insert\"\n    urn.event = \"transform\"\n    d = end\n    logger.info(granularities)\n    while d > start:\n        d_str = d.strftime(\"%Y-%m\")\n        for granularity in granularities:\n            logger.info(\"for granularity: {}\".format(granularity))\n            logger.info(\"reading granularity = {} and d_str = {} from {} \".format(\n                granularity, d_str, original_namespace_age_gender))\n            logger.info(\"reading granularity = {} and d_str = {} from {} \".format(\n                granularity, d_str, original_namespace_audience))\n            try:\n                usage_df_age_gender = (\n                    spark\n                        .read\n                        .option(\"basePath\",\"s3://b2c-prod-data-pipeline-unified-usage/unified/{}/fact/\".format(original_namespace_age_gender))\n                        .parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/{}/fact/granularity={}/month={}\".format(original_namespace_age_gender, granularity, d_str))\n                )\n                usage_df_audience = (\n                    spark\n                        .read\n                        .option(\"basePath\", \"s3a://b2c-prod-data-pipeline-unified-usage\"\n                                            \"/unified/{}/fact/\".format(original_namespace_audience))\n                        .parquet(\"s3a://b2c-prod-data-pipeline-unified-usage/unified/{}/fact/\"\n                                 \"granularity={}/month={}\".format(original_namespace_audience, granularity, d_str))\n                )\n            except Exception as e:\n                logger.error(e.message)\n                logger.error(\"date may not exist: {}-{}\".format(granularity, d_str))\n                continue\n\n            usage_df_age_gender.show()\n            \n            usage_df_age_gender = usage_df_age_gender.groupby(usage_df_age_gender.device_id, usage_df_age_gender.store_id, usage_df_age_gender.date, usage_df_age_gender.app_id,usage_df_age_gender.gender,usage_df_age_gender.age).pivot(\"kpi\").max(\"estimate\")\n\n            usage_df_age_gender = (\n                usage_df_age_gender\n                .withColumnRenamed('gender', 'gender_key')\n                .withColumnRenamed('age', 'age_key')\n                .withColumnRenamed('app_id', 'product_key')\n                .withColumnRenamed('5', 'est_average_active_days')\n                .withColumnRenamed('3', 'est_average_session_duration_milliseconds')\n                .withColumnRenamed('2', 'est_average_session_per_user')\n                .withColumnRenamed('8', 'est_average_time_per_user_milliseconds')\n                .withColumnRenamed('1', 'est_active_users')\n                .withColumnRenamed('4', 'est_install_penetration')\n                .withColumnRenamed('17', 'est_average_bytes_per_session')\n                .withColumnRenamed('7', 'est_average_bytes_per_user')\n                .withColumnRenamed('20', 'est_percentage_of_bytes_wifi_in_total')\n                .withColumnRenamed('10', 'est_open_rate')\n                .withColumnRenamed('6', 'est_percentage_of_active_days_in_granularity')\n                .withColumnRenamed('9', 'est_usage_penetration')\n                .withColumnRenamed('23', 'est_install_base')\n            )\n            usage_df_age_gender = usage_df_age_gender.drop(\"21\") \n            usage_df_age_gender.createOrReplaceTempView(\"df_ag\")\n            usage_df_age_gender = spark.sql(\"\"\"\n                SELECT \n                df_ag.*,\n                CASE WHEN device_id='1001' THEN \"android-phone\" \n                    WHEN device_id='1002' THEN \"android-tablet\" END AS device_code\n                FROM df_ag   \n            \"\"\")\n            \n            usage_df_age_gender.show()\n            usage_df_age_gender.createOrReplaceTempView(\"df_ag\")\n            usage_df_age_gender_v2 = spark.sql(\"\"\"\n                SELECT df_ag.*,\n                    est_active_users/est_usage_penetration AS est_population,\n                    CAST(NULL AS DOUBLE) AS est_share_of_bytes_product_in_main_category,\n                    est_active_users*est_average_time_per_user_milliseconds/60 AS est_total_time_minutes,\n                    CAST(NULL AS DOUBLE) AS est_share_of_time_product_in_main_category,\n                    est_active_users*est_average_session_per_user AS est_total_session_count,\n                    CAST(NULL AS DOUBLE) AS est_share_of_session_product_in_main_category\n                   \n                FROM df_ag \n            \"\"\")\n            usage_df_age_gender_v2.createOrReplaceTempView(\"df_ag\")\n            usage_df_age_gender_v2.show()\n\n            usage_df_audience.show()\n            usage_df_audience = usage_df_audience.groupby(usage_df_audience.device_id, usage_df_audience.store_id, usage_df_audience.date, usage_df_audience.app_id,usage_df_audience.gender,usage_df_audience.age).pivot(\"kpi\").max(\"estimate\")\n            usage_df_audience = (\n                usage_df_audience\n                .withColumnRenamed('gender', 'gender_key')\n                .withColumnRenamed('age', 'age_key')\n                .withColumnRenamed('app_id', 'product_key')\n                .withColumnRenamed('26', 'est_audience_index')\n                .withColumnRenamed('27', 'est_audience_percentage')\n                \n            ) \n            \n            usage_df_audience.createOrReplaceTempView(\"df_au\")\n            usage_df_audience = spark.sql(\"\"\"\n                SELECT \n                df_au.*,\n                CASE WHEN device_id='1001' THEN \"android-phone\" \n                    WHEN device_id='1002' THEN \"android-tablet\" END AS device_code\n                FROM df_au   \n            \"\"\")\n            usage_df_audience.show()\n            usage_df_audience.createOrReplaceTempView(\"df_au\")\n            join_df=spark.sql(\"\"\"\n                SELECT ag.*,\n                    au.est_audience_index,\n                    au.est_audience_percentage\n                FROM df_ag AS ag,df_au AS au\n                WHERE ag.product_key=au.product_key\n                AND ag.gender_key=au.gender_key\n                AND ag.age_key=au.age_key\n            \"\"\")\n            join_df.createOrReplaceTempView(\"df\")\n            join_df.show()\n            d -= datetime.timedelta(days=31)\n            \n            \n            \nparams_list={\n    'start':'2020-8-30',\n    'end':'2020-8-31',\n    'granularity':'monthly',\n    'original_namespace_age_gender':'app-tech.usage.legacy-ag_app.v2',\n    'original_namespace_audience':'app-tech.usage.legacy-ad_app.v2',\n    'target_namespace':'aa.usage_demographics.v6'\n    \n}\n\nmain(spark,params=params_list)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200915-013445_2051807514","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_APP_CROSS_DOMAIN_METRICS/version=v1.0.0/granularity=MONTH/date=2019-01-31/platform=2/metric_name=CR/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200915-024130_1230879926","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.city.v6/fact/granularity=monthly/date=2020-01-31/\")\ndf.count()\n#df.groupBy('est_average_bytes_per_user').count().show()\n#df.show(10,False)\nurl=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.city.v6/fact/granularity=monthly/date=2020-01-31/\"\nspark.read.format(\"delta\").load(url).count()"]},{"cell_type":"code","execution_count":0,"id":"20200924-065057_215903127","metadata":{},"outputs":[],"source":["\ndf = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.city-level.v3/fact\").parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.city-level.v3/fact/granularity=m/date=2018-02-28/device_code=ap/country_code=us/part-00000-3f9dd3f5-f9fd-4b6a-a715-a44c1b48c339.c000.snappy.parquet\")\ndf.createOrReplaceTempView(\"table1\")\n\nusage_df_age_gender = spark.sql(\"\"\"\n                SELECT \n                CASE WHEN granularity='m' THEN 1 END AS gran_value\n                FROM table1\n                \"\"\")\nusage_df_age_gender.show()\n#df.count()\n#df.show(10,False)\n#df.groupBy('metric_name').count().show()"]},{"cell_type":"code","execution_count":0,"id":"20200915-025058_1299593961","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/"]},{"cell_type":"code","execution_count":0,"id":"20200915-025428_1708144","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}