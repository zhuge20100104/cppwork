{"cells":[{"cell_type":"code","execution_count":0,"id":"20191224-033314_1922433663","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\n\n\n\nkpi_mapping={1:\"est_average_active_users\", 2: \"est_average_session_per_user\", 3:\"est_average_session_duration\", 4:\"est_install_penetration\", 5:\"est_average_active_days\", 6:\"est_percentage_active_days\", 7:\"est_average_bytes_per_user\" , 8:\"est_average_time_per_user\", 9:\"est_usage_penetration\", 10:\"est_open_rate\",11:\"est_total_time\", 12:\"est_share_of_category_time\",14:\"est_total_sessions\", 15:\"est_share_of_category_session\", 17:\"est_average_bytes_per_session\", 18:\"est_share_of_category_bytes\", 20:\"est_percent_of_wifi_total\", 21:\"est_mb_per_second\", 22:\"est_panel_size\", 23:\"est_installs\", 24:\"est_average_active_users_country_share\", 25:\"est_installs_country_share\", 26:\"est_audience_index\", 27:\"est_audience_percentage\"}\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return {\"2001\":\"ios-phone\",\"2002\":\"ios-tablet\"}\n    else:\n        return {\"1001\":\"android-phone\" ,\"1002\":\"android-tablet\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)  \n    return next_month - datetime.timedelta(days=next_month.day)\n\n\n# def get_month_list():\n#     result = []\n#     today = datetime.date(2019, 10, 1) \n#     current = datetime.date(2013, 1, 1)    \n#     while current <= today:\n#         month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n#         month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n#         result.append((month_data_raw, month_data_leg_db,current  ))\n#         current += relativedelta(months=1)  \n#     return result\n\n\n# def get_path_date_list(granularity):\n#     df_date = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={}/\".format(granularity)).select('date').dropDuplicates()\n#     collect_date= df_date.collect()\n#     # collect_date=[Row(date=u'2019-12-31'), Row(date=u'2019-08-31'), Row(date=u'2015-02-28'), Row(date=u'2017-04-30'), Row(date=u'2014-08-31'), Row(date=u'2017-10-31'), Row(date=u'2018-10-31'), Row(date=u'2018-05-31'), Row(date=u'2017-08-31'), Row(date=u'2013-09-30'), Row(date=u'2016-09-30'), Row(date=u'2014-06-30'), Row(date=u'2013-07-31'), Row(date=u'2018-06-30'), Row(date=u'2014-07-31'), Row(date=u'2016-12-31'), Row(date=u'2014-09-30'), Row(date=u'2017-05-31'), Row(date=u'2019-06-30'), Row(date=u'2013-05-31'), Row(date=u'2015-03-31'), Row(date=u'2015-11-30'), Row(date=u'2019-10-31'), Row(date=u'2017-06-30'), Row(date=u'2019-02-28'), Row(date=u'2015-06-30'), Row(date=u'2018-04-30'), Row(date=u'2016-05-31'), Row(date=u'2013-03-31'), Row(date=u'2016-03-31'), Row(date=u'2019-09-30'), Row(date=u'2015-12-31'), Row(date=u'2015-01-31'), Row(date=u'2013-01-31'), Row(date=u'2014-02-28'), Row(date=u'2019-03-31'), Row(date=u'2016-08-31'), Row(date=u'2018-02-28'), Row(date=u'2013-06-30'), Row(date=u'2016-07-31'), Row(date=u'2015-10-31'), Row(date=u'2018-03-31'), Row(date=u'2014-01-31'), Row(date=u'2018-09-30'), Row(date=u'2017-07-31'), Row(date=u'2019-04-30'), Row(date=u'2014-05-31'), Row(date=u'2019-01-31'), Row(date=u'2018-08-31'), Row(date=u'2014-04-30'), Row(date=u'2016-01-31'), Row(date=u'2017-12-31'), Row(date=u'2019-05-31'), Row(date=u'2017-09-30'), Row(date=u'2018-11-30'), Row(date=u'2018-01-31'), Row(date=u'2016-06-30'), Row(date=u'2015-04-30'), Row(date=u'2015-05-31'), Row(date=u'2018-07-31'), Row(date=u'2016-02-29'), Row(date=u'2015-09-30'), Row(date=u'2013-12-31'), Row(date=u'2014-12-31'), Row(date=u'2013-08-31'), Row(date=u'2013-04-30'), Row(date=u'2019-07-31'), Row(date=u'2013-02-28'), Row(date=u'2017-01-31'), Row(date=u'2017-11-30'), Row(date=u'2013-11-30'), Row(date=u'2013-10-31'), Row(date=u'2017-02-28'), Row(date=u'2016-11-30'), Row(date=u'2016-04-30'), Row(date=u'2014-03-31'), Row(date=u'2014-11-30'), Row(date=u'2015-07-31'), Row(date=u'2017-03-31'), Row(date=u'2014-10-31'), Row(date=u'2016-10-31'), Row(date=u'2015-08-31')]\n#     date_list = [(x[0][:7],x[0]) for x in collect_date]\n#     print date_list\n#     return date_list\n\n    \nimport traceback\ndef check_mu_app_transform_count(store_id_list, device_id_list, _granularity, date_list):\n    t = unittest.TestCase('run')\n    for id,country_code in store_id_list.items():\n        for device,device_code in device_id_list.items():\n            for m in date_list:\n                raw_count_with_KPI=''\n                # print id, device, m[0] , m[1]\n                try:\n                    if datetime.datetime.strptime(m[1],\"%Y-%m-%d\").date() > datetime.date(2019,10,31):\n                        # print 'too large break'\n                        continue\n\n                    raw_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={raw_device_id}/store_id={raw_store_id}/\"\n                    unified_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity={unified_granularity}/date={unified_date}/\"\n                    raw_path_parse=raw_path.format(raw_device_id=device,raw_store_id=id, raw_month=m[0], raw_granularity=_granularity)\n                    raw_count_with_KPI=spark.read.parquet(raw_path_parse).filter(\"date='{}'\".format(m[1])).select(\"kpi\",\"app_id\").distinct().groupBy(\"kpi\").agg(count(\"kpi\")).collect()\n                    # print raw_count_with_KPI\n                except AnalysisException as e: \n                    break\n                    # traceback.print_exc()\n                # print 'raw count', raw_count\n                for row in raw_count_with_KPI:\n                    # print 'row _ test', row[\"kpi\"], row[\"count(kpi)\"]\n                    unified_path_parse=unified_path.format(unified_date=m[1], unified_granularity=_granularity)\n                    unified_count= spark.read.parquet(unified_path_parse).filter(\"device_code='{}' and country_code='{}'\".format(device_code,country_code)).filter(\"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                    # print 'unified count' , unified_count\n                    t.assertEqual(row[\"count(kpi)\"], unified_count, \" raw: {} ~ unified data: {},device:{},  store_id:{} , month: {}, KPI {}\".format(row[\"count(kpi)\"], unified_count, device, id , m, row[\"kpi\"]))\n\ngraularity_list=[\"daily\"]\nfor graularity in graularity_list:\n    print graularity\n    granularity_date_list=get_path_date_list(graularity)\n    # check_mu_app_transform_count(IOS_COUNTRY_ID_CODES, get_device_list('ios'),graularity, granularity_date_list)\n    check_mu_app_transform_count(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),graularity, granularity_date_list)\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200126-101056_514606843","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nfrom aadatapipelinecore.core.log import logger\n\n\nkpi_mapping={1:\"est_average_active_users\", 2: \"est_average_session_per_user\", 3:\"est_average_session_duration\", 4:\"est_install_penetration\", 5:\"est_average_active_days\", 6:\"est_percentage_active_days\", 7:\"est_average_bytes_per_user\" , 8:\"est_average_time_per_user\", 9:\"est_usage_penetration\", 10:\"est_open_rate\",11:\"est_total_time\", 12:\"est_share_of_category_time\",14:\"est_total_sessions\", 15:\"est_share_of_category_session\", 17:\"est_average_bytes_per_session\", 18:\"est_share_of_category_bytes\", 20:\"est_percent_of_wifi_total\", 21:\"est_mb_per_second\", 22:\"est_panel_size\", 23:\"est_installs\", 24:\"est_average_active_users_country_share\", 25:\"est_installs_country_share\", 26:\"est_audience_index\", 27:\"est_audience_percentage\"}\n\ncountry_code_list = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW', 143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW' }\n\n\n# IOS_COUNTRY_ID_CODES={}\n\ndevice_mapping = {\"2001\":\"ios-phone\",\"2002\":\"ios-tablet\",\"1001\":\"android-phone\" ,\"1002\":\"android-tablet\"}\n\n\n\nraw_path_granularity = \"s3://b2c-prod-data-pipeline-unified-usage/\"\n\n    \ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-usage'))\nusage_path_month_list = s3_bucket_list.all(prefix=\"unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\".format(raw_granularity=\"daily\"), depth_is_1=True)\n\n\nunified_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity={unified_granularity}/date={unified_date}/\"\n\n\ndef check_mu_app_transform_count_new():\n    for month in usage_path_month_list:\n        device_list = s3_bucket_list.all(prefix=month, depth_is_1=True)\n        for device in device_list[:2]:\n            raw_parse_path_list = s3_bucket_list.all(prefix=device, depth_is_1=True)\n            for raw_path in raw_parse_path_list[:2]:\n                print raw_path\n                device_code = device_mapping[raw_path.split(\"/\")[5].split(\"=\")[1]]\n                country_code = country_code_list[int(raw_path.split(\"/\")[6].split(\"=\")[1])]\n                raw_count_with_KPI=spark.read.parquet(raw_path_granularity+raw_path).select(\"date\",\"kpi\",\"app_id\").distinct().groupBy(\"kpi\",\"date\").agg(count(\"kpi\")).collect()\n                for singe_record in raw_count_with_KPI:\n                    unified_path_parse=unified_path.format(unified_date=singe_record[\"date\"], unified_granularity=\"daily\")\n                    unified_count= spark.read.parquet(unified_path_parse).filter(\"device_code='{}' and country_code='{}'\".format(device_code,country_code)).filter(\"{} is not null\".format(kpi_mapping[singe_record[\"kpi\"]])).select(kpi_mapping[singe_record[\"kpi\"]]).count()\n                    if singe_record[\"count(kpi)\"] == unified_count:\n                       logger.info(\"the count is equal , country is: {} , kpi is: {}, date is : {}, raw_count is {}, unified_count is {}\".format(country_code,singe_record[\"kpi\"],singe_record[\"date\"],singe_record[\"count(kpi)\"] ,unified_count  ))\n                    elif singe_record[\"count(kpi)\"] != unified_count :\n                        logger.info(\"the count is not equal!!! country is: {} , kpi is: {}, date is : {}, raw_count is {}, unified_count is {}\".format(country_code,singe_record[\"kpi\"],singe_record[\"date\"] ,singe_record[\"count(kpi)\"] ,unified_count ))\n\ncheck_mu_app_transform_count_new()\n"]},{"cell_type":"code","execution_count":0,"id":"20191224-033725_1343317640","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2015-12/device_id=1001/store_id=1/\n  | head -3\n \n                           "]},{"cell_type":"code","execution_count":0,"id":"20200113-071711_1255643520","metadata":{},"outputs":[],"source":["\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=weekly/month=2014-02/device_id=1001/store_id=1/\").filter(\"kpi=12 and date='2014-02-22'\").show()\n\n\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2015-12/device_id=1001/store_id=1/\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200126-095831_1089209747","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-usage'))\nusage_path_month_list = s3_bucket_list.all(prefix=\"unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\".format(raw_granularity=\"daily\"), depth_is_1=True)\n\nfor month in usage_path_month_list:\n    device_list = s3_bucket_list.all(prefix=month, depth_is_1=True)\n    for device in device_list:\n        print s3_bucket_list.all(prefix=device, depth_is_1=True)"]},{"cell_type":"code","execution_count":0,"id":"20200126-100502_479870242","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}