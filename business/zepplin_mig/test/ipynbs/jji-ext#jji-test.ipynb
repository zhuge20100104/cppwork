{"cells":[{"cell_type":"code","execution_count":0,"id":"20200701-091623_770704576","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(\n    spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"jji-application\"\n)\n"]},{"cell_type":"code","execution_count":0,"id":"20200701-092743_625549350","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200701-091028_719418182","metadata":{},"outputs":[],"source":["\nfrom applications.db_check_v1.common.db_check_utils import etl_skip\n"]},{"cell_type":"code","execution_count":0,"id":"20200701-091246_744578053","metadata":{},"outputs":[],"source":["\nfrom aaplproxy.connection import ClusterConnection"]},{"cell_type":"code","execution_count":0,"id":"20200701-092724_1391449998","metadata":{},"outputs":[],"source":["\nimport random\nimport datetime\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config, etl_skip\nfrom applications.db_check_v1.common.constants import query\nfrom applications.db_check_v1.common.db_check_utils import query_df\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.utils import string_to_datetime, datetime_to_string\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql import functions\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import LongType\nCITUS_USAGE_HOSTS = [('10.2.10.254', 5432)]\nCITUS_USAGE_NAME = 'aa_store_db'\nCITUS_USAGE_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nCITUS_USAGE_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\nDEVICE_CODE_MAPPING = {\n    1: {'1': 'android-phone', '2': 'android-tablet'},\n    2: {'1': 'ios-phone', '2': 'ios-tablet'}}\n\nGRANULARITY_IN_RAW_PATH_MAPPING = {\n    \"daily\": \"DAY\",\n    \"weekly\": \"WEEK\",\n    \"monthly\": \"MONTH\"\n}\n\nCITUS_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=CITUS_USAGE_NAME,\n        user=CITUS_USAGE_ACCESS_ID,\n        host=CITUS_USAGE_HOSTS[0][0],\n        password=CITUS_USAGE_SECRET_KEY,\n        port=CITUS_USAGE_HOSTS[0][1]\n    )\n)\n\n\nclass UsageRoutineRawData(object):\n    \"\"\"\n    Get data from Data Foundation\n    \"\"\"\n    _raw_s3_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={granularity}/date={date}'\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, granularity, date):\n        \"\"\"\n        :return: Raw data from DF team\n        :rtype: pyspark.sql.DataFrame\n        \"\"\"\n        raw_data = self.spark.read.parquet(self._raw_s3_path.format(\n            granularity=GRANULARITY_IN_RAW_PATH_MAPPING[granularity], date=date))\n        return raw_data\n\n\nclass TestUsageRoutineRawCompleteness(PipelineTest):\n    table_name = 'usage.usage_basic_kpi_fact_v6'\n    db_name = 'usage'\n\n    def setUp(self):\n        # super(PipelineTest, self).setUp()\n        self.check_date = None\n        self.granularity = None\n\n    def check_routine_raw_completeness(self):\n        date_list = [self.check_date]\n\n        if self.granularity == 'daily':\n            date = string_to_datetime(self.check_date)\n            weekly_day_nums = 7\n            date_list = [datetime_to_string(date - datetime.timedelta(days=x)) for x in range(weekly_day_nums)]\n\n        for date in date_list:\n            routine_df = UsageRoutineRawData(self.spark).get(self.granularity, date)\n            routine_count = routine_df.count()\n\n            citus_db_count = self.get_citus_db_count(date)\n            print routine_count, citus_db_count\n\n            self.assertEqual(routine_count, citus_db_count[0][0],\n                             msg=\"fount count mismatch when compare usage routine raw and citus db. \"\n                                 \"granularity is {}, date is {}, raw count is:{}, citus db count is:{}\".format(\n                                    self.granularity, date, routine_count, citus_db_count[0][0]))\n\n    def get_citus_db_count(self, date):\n        sql = \"\"\"select count(1) as cnt from {table_name} where date='{date}' and granularity='{granularity}';\n        \"\"\".format(table_name=self.table_name, date=date, granularity=self.granularity)\n        result = query(CITUS_DSN, sql)\n        return result\n\n\nclass TestUsageRoutineRawCompletenessDaily(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"0 12 * * 5\", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"daily\"\n\n    def test_routine_raw_completeness_daily(self):\n        self.check_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawCompletenessWeekly(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"0 12 * * 5\", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"weekly\"\n\n    @etl_skip()\n    def test_routine_raw_completeness_weekly(self):\n        self.check_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawCompletenessMonthly(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"0 12 6 * * \", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"monthly\"\n\n    @etl_skip()\n    def test_routine_raw_completeness_monthly(self):\n        self.check_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawAccuracy(PipelineTest):\n    table_name = 'usage.usage_basic_kpi_fact_v6'\n    raw_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={granularity}/date={date}'\n    db_name = 'usage'\n\n    def setUp(self):\n        super(PipelineTest, self).setUp()\n        self.check_date = None\n        self.granularity = None\n\n    def check_routine_raw_accuracy(self):\n        routine_df = UsageRoutineRawData(self.spark).get(self.granularity, self.check_date)\n        unified_v1 = (\n            routine_df\n            .withColumn('device_code', functions.UserDefinedFunction(\n                lambda x, y: DEVICE_CODE_MAPPING[x][y])(routine_df['platform'], routine_df['device_type']))\n            .withColumnRenamed('country', 'country_code')\n            .withColumn('app_id', routine_df['app_id'].cast(LongType()))\n            .withColumnRenamed('AU', 'est_average_active_users')\n            .withColumnRenamed('AFU', 'est_average_session_per_user')\n            .withColumnRenamed('ADU', 'est_average_session_duration')\n            .withColumnRenamed('IP', 'est_install_penetration')\n            .withColumnRenamed('AAD', 'est_average_active_days')\n            .withColumnRenamed('PAD', 'est_percentage_active_days')\n            .withColumnRenamed('MBPU', 'est_average_bytes_per_user')\n            .withColumnRenamed('ATU', 'est_average_time_per_user')\n            .withColumnRenamed('UP', 'est_usage_penetration')\n            .withColumnRenamed('OR', 'est_open_rate')\n            .withColumnRenamed('MBPS', 'est_average_bytes_per_session')\n            .withColumnRenamed('MBWFT', 'est_percent_of_wifi_total')\n            .withColumnRenamed('MBS', 'est_mb_per_second')\n            .withColumnRenamed('IS', 'est_installs')\n            .withColumnRenamed('SOU', 'est_average_active_users_country_share')\n            .withColumnRenamed('SOI', 'est_installs_country_share')\n            .withColumn('est_share_of_category_time', lit(None).cast(DoubleType()))\n            .withColumn('est_share_of_category_session', lit(None).cast(DoubleType()))\n            .withColumn('est_share_of_category_bytes', lit(None).cast(DoubleType()))\n            .withColumn('est_panel_size', lit(None).cast(DoubleType()))\n            .drop('device_type')\n            .drop('platform')\n        )\n\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] *\n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population',\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v3 = self.spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v3 = unified_v3.na.fill(0)\n\n        result = self.get_citus_db_df()\n        citus_db_df = self.spark.createDataFrame(result)\n        citus_db_df = citus_db_df.drop('date').drop('granularity').drop('_disable_idx_4_query')\n\n        diff_count1 = citus_db_df.select(unified_v3.columns).subtract(unified_v3).count()\n        diff_count2 = unified_v3.select(citus_db_df.columns).subtract(citus_db_df).count()\n        self.assertTrue(diff_count1 == 0 and diff_count2 == 0,\n                        msg=\"fount mismatch when compare usage routine raw and citus db.granularity is {}, \"\n                            \"date is {}, diff_count1 is:{}, diff_count2 is:{}\".format(\n                                self.granularity, self.check_date, diff_count1, diff_count2))\n\n    def get_citus_db_df(self):\n        sql = \"\"\"select * from {table_name} where date='{date}' and granularity='{granularity}';\n        \"\"\".format(table_name=self.table_name, date=self.check_date, granularity=self.granularity)\n        result = query_df(CITUS_DSN, sql)\n        return result\n\n\nclass TestUsageRoutineRawAccuracyDaily(TestUsageRoutineRawAccuracy):\n\n    trigger_date_config = (\"0 12 * * 5\", random.randint(6, 12))\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"daily\"\n\n    @etl_skip()\n    def test_routine_raw_accuracy_daily(self):\n        self.check_routine_raw_accuracy()\n\n\nclass TestUsageRoutineRawAccuracyWeekly(TestUsageRoutineRawAccuracy):\n    trigger_date_config = (\"0 12 * * 5\", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"weekly\"\n\n    @etl_skip()\n    def test_routine_raw_accuracy_weekly(self):\n        self.check_routine_raw_accuracy()\n\n\nclass TestUsageRoutineRawAccuracyMonthly(TestUsageRoutineRawAccuracy):\n    trigger_date_config = (\"0 12 6 * * \", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"monthly\"\n\n    @etl_skip()\n    def test_routine_raw_accuracy_monthly(self):\n        self.check_routine_raw_accuracy()\n\n\nsuite = unittest.TestSuite()\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessDaily))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessWeekly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessMonthly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyDaily))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyWeekly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyMonthly))\nrunner = unittest.TextTestRunner()\nrunner.run(suite)\n"]},{"cell_type":"code","execution_count":0,"id":"20200703-045000_1328037038","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=usage;\nselect count(1) from usage_basic_kpi_fact_v6 where granularity='monthly' and date='2020-06-30';\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200703-044949_622488380","metadata":{},"outputs":[],"source":["\nimport random\nimport datetime\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config, etl_skip\nfrom applications.db_check_v1.common.constants import query\nfrom applications.db_check_v1.common.db_check_utils import query_df\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.utils import string_to_datetime, datetime_to_string\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql import functions\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import LongType\nCITUS_USAGE_HOSTS = [('10.2.10.254', 5432)]\nCITUS_USAGE_NAME = 'aa_store_db'\nCITUS_USAGE_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nCITUS_USAGE_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\nDEVICE_CODE_MAPPING = {\n    1: {'1': 'android-phone', '2': 'android-tablet'},\n    2: {'1': 'ios-phone', '2': 'ios-tablet'}}\n\nGRANULARITY_IN_RAW_PATH_MAPPING = {\n    \"daily\": \"DAY\",\n    \"weekly\": \"WEEK\",\n    \"monthly\": \"MONTH\"\n}\n\nCITUS_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=CITUS_USAGE_NAME,\n        user=CITUS_USAGE_ACCESS_ID,\n        host=CITUS_USAGE_HOSTS[0][0],\n        password=CITUS_USAGE_SECRET_KEY,\n        port=CITUS_USAGE_HOSTS[0][1]\n    )\n)\n\n\nclass UsageRoutineRawData(object):\n    \"\"\"\n    Get data from Data Foundation\n    \"\"\"\n    _raw_s3_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={granularity}/date={date}'\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, granularity, date):\n        \"\"\"\n        :return: Raw data from DF team\n        :rtype: pyspark.sql.DataFrame\n        \"\"\"\n        raw_data = self.spark.read.parquet(self._raw_s3_path.format(\n            granularity=GRANULARITY_IN_RAW_PATH_MAPPING[granularity], date=date))\n        return raw_data\n\n\nclass TestUsageRoutineRawCompleteness(PipelineTest):\n    table_name = 'usage.usage_basic_kpi_fact_v6'\n    db_name = 'usage'\n\n    def setUp(self):\n        # super(PipelineTest, self).setUp()\n        self.check_date = None\n        self.granularity = None\n\n    def check_routine_raw_completeness(self):\n        date_list = [self.check_date]\n\n        if self.granularity == 'daily':\n            date = string_to_datetime(self.check_date)\n            weekly_day_nums = 7\n            date_list = [datetime_to_string(date - datetime.timedelta(days=x)) for x in range(weekly_day_nums)]\n\n        for date in date_list:\n            routine_df = UsageRoutineRawData(self.spark).get(self.granularity, date)\n            routine_count = routine_df.count()\n\n            citus_db_count = self.get_citus_db_count(date)\n            print routine_count, citus_db_count\n\n            self.assertEqual(routine_count, citus_db_count[0][0],\n                             msg=\"fount count mismatch when compare usage routine raw and citus db. \"\n                                 \"granularity is {}, date is {}, raw count is:{}, citus db count is:{}\".format(\n                                    self.granularity, date, routine_count, citus_db_count[0][0]))\n\n    def get_citus_db_count(self, date):\n        sql = \"\"\"select count(1) as cnt from {table_name} where date='{date}' and granularity='{granularity}';\n        \"\"\".format(table_name=self.table_name, date=date, granularity=self.granularity)\n        result = query(CITUS_DSN, sql)\n        return result\n\n\nclass TestUsageRoutineRawCompletenessDaily(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"0 12 * * 5\", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"daily\"\n\n    def test_routine_raw_completeness_daily(self):\n        self.check_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawCompletenessWeekly(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"0 12 * * 5\", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"weekly\"\n\n    @etl_skip()\n    def test_routine_raw_completeness_weekly(self):\n        self.check_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawCompletenessMonthly(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"0 12 6 * * \", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"monthly\"\n\n    @etl_skip()\n    def test_routine_raw_completeness_monthly(self):\n        self.check_routine_raw_completeness()\n\n\nsuite = unittest.TestSuite()\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessDaily))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessWeekly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessMonthly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyDaily))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyWeekly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyMonthly))\nrunner = unittest.TextTestRunner()\nrunner.run(suite)"]},{"cell_type":"code","execution_count":0,"id":"20200702-080025_1477846044","metadata":{},"outputs":[],"source":["\nimport datetime\nimport croniter\nimport random\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\nfrom applications.db_check_v1.common.utils import string_to_datetime, datetime_to_string\ndef _get_pre_date_from_refresh_routing_config(config):\n    schedule = config[0]\n    # here use UTC now\n    cron = croniter.croniter(schedule, datetime.datetime.utcnow())\n    date = cron.get_prev(datetime.datetime)\n    return date\nprint datetime.datetime.utcnow()\ntrigger_date_config = (\"0 12 * * 5\", 6)\npre_etl_date = _get_pre_date_from_refresh_routing_config(trigger_date_config)\ncheck_date = _get_date_from_refresh_routing_config(trigger_date_config)\nprint pre_etl_date, check_date\ndelta_days=1\nskipped_condition = datetime.datetime.utcnow() - pre_etl_date > datetime.timedelta(days=delta_days)\nprint skipped_condition\n"]},{"cell_type":"code","execution_count":0,"id":"20200702-073443_2035510573","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\nfrom applications.db_check_v1.common.constants import query, citus_settings\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.types import LongType\nfrom applications.db_check_v1.common.db_check_utils import query_df, etl_skip\nfrom pyspark.sql import functions\nfrom pyspark.sql.functions import lit, coalesce\nfrom applications.db_check_v1.common.table_common_info import urn\n\n\nclass TestUsageRoutineRawCompleteness(unittest.TestCase):\n    table_name = 'usage.usage_basic_kpi_fact_v6'\n    raw_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={granularity}/date={date}'\n    db_name = 'usage'\n\n    def setUp(self):\n        self.check_date = None\n        self.granularity = None  # granularity type is list, like ['DAY', 'daily']\n\n    def test_routine_raw_completeness(self):\n        routine_df = spark.read.parquet(self.raw_path.format(\n            granularity=self.granularity[0], date=self.check_date))\n        routine_count = routine_df.count()\n\n        citus_db_count = self.get_citus_db_count()\n\n        self.assertEqual(routine_count, citus_db_count[0][0],\n                         msg=\"fount count mismatch when compare usage routine raw and citus db. \"\n                             \"granularity is {}, date is {}, raw count is:{}, citus db count is:{}\".format(\n                             self.granularity[1], self.check_date, routine_count, citus_db_count[0][0]))\n\n    def get_citus_db_count(self):\n        sql = \"\"\"select count(1) as cnt from {table_name} where date='{date}' and granularity='{granularity}';\n        \"\"\".format(table_name=self.table_name, date=self.check_date, granularity=self.granularity[1])\n        result = query(citus_settings(self.db_name), sql)\n        return result\n\n\nclass TestUsageRoutineRawCompletenessDaily(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = '2020-06-14'\n    # check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = ['DAY', 'daily']\n\n    def test_routine_raw_completeness_daily(self):\n        self.test_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawCompletenessWeekly(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = ['WEEK', 'weekly']\n\n    def test_routine_raw_completeness_weekly(self):\n        self.test_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawCompletenessMonthly(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = ['MONTH', 'monthly']\n\n    def test_routine_raw_completeness_monthly(self):\n        self.test_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawAccuracy(unittest.TestCase):\n    table_name = 'usage.usage_basic_kpi_fact_v6'\n    raw_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={granularity}/date={date}'\n    db_name = 'usage'\n    device_code_mapping = {\n            1: {'1': 'android-phone', '2': 'android-tablet'},\n            2: {'1': 'ios-phone', '2': 'ios-tablet'}}\n\n    def setUp(self):\n        self.check_date = None\n        self.granularity = None  # granularity type is list, like ['DAY', 'daily']\n\n    def test_routine_raw_accuracy(self):\n        print self.device_code_mapping\n        routine_df = spark.read.parquet(self.raw_path.format(\n            granularity=self.granularity[0], date=self.check_date))\n        unified_v1 = (\n            routine_df\n                .withColumn('device_code', functions.UserDefinedFunction(\n                lambda x, y: self.device_code_mapping[x][y])(routine_df['platform'], routine_df['device_type']))\n                .withColumnRenamed('country', 'country_code')\n                .withColumn('app_id', routine_df['app_id'].cast(LongType()))\n                .withColumnRenamed('AU', 'est_average_active_users')\n                .withColumnRenamed('AFU', 'est_average_session_per_user')\n                .withColumnRenamed('ADU', 'est_average_session_duration')\n                .withColumnRenamed('IP', 'est_install_penetration')\n                .withColumnRenamed('AAD', 'est_average_active_days')\n                .withColumnRenamed('PAD', 'est_percentage_active_days')\n                .withColumnRenamed('MBPU', 'est_average_bytes_per_user')\n                .withColumnRenamed('ATU', 'est_average_time_per_user')\n                .withColumnRenamed('UP', 'est_usage_penetration')\n                .withColumnRenamed('OR', 'est_open_rate')\n                .withColumnRenamed('MBPS', 'est_average_bytes_per_session')\n                .withColumnRenamed('MBWFT', 'est_percent_of_wifi_total')\n                .withColumnRenamed('MBS', 'est_mb_per_second')\n                .withColumnRenamed('IS', 'est_installs')\n                .withColumnRenamed('SOU', 'est_average_active_users_country_share')\n                .withColumnRenamed('SOI', 'est_installs_country_share')\n                .withColumn('est_share_of_category_time', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_category_session', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_category_bytes', lit(None).cast(DoubleType()))\n                .withColumn('est_panel_size', lit(None).cast(DoubleType()))\n                .drop('device_type')\n                .drop('platform')\n        )\n\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] *\n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population',\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v3 = spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v3 = unified_v3.na.fill(0)\n\n        result = self.get_citus_db_df()\n        citus_db_df = spark.createDataFrame(result)\n        citus_db_df = citus_db_df.drop('date').drop('granularity').drop('_disable_idx_4_query')\n\n        diff_count1 = citus_db_df.select(unified_v3.columns).subtract(unified_v3).count()\n        diff_count2 = unified_v3.select(citus_db_df.columns).subtract(citus_db_df).count()\n        print diff_count1, diff_count2\n        self.assertTrue(diff_count1 == 0 and diff_count2 == 0,\n                        msg=\"fount mismatch when compare usage routine raw and citus db.granularity is daily, \"\n                            \"date is {}, diff_count1 is:{}, diff_count2 is:{}\".format(\n                                self.check_date, diff_count1, diff_count2))\n\n    def get_citus_db_df(self):\n        sql = \"\"\"select * from {table_name} where date='{date}' and granularity='{granularity}';\n        \"\"\".format(table_name=self.table_name, date=self.check_date, granularity=self.granularity[1])\n        result = query_df(citus_settings(self.db_name), sql)\n        return result\n\n\nclass TestUsageRoutineRawAccuracyDaily(TestUsageRoutineRawAccuracy):\n\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = '2020-06-14'\n    # check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = ['DAY', 'daily']\n\n    def test_routine_raw_accuracy_daily(self):\n        self.test_routine_raw_accuracy()\n\n\nclass TestUsageRoutineRawAccuracyWeekly(TestUsageRoutineRawAccuracy):\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = ['WEEK', 'weekly']\n\n    def test_routine_raw_accuracy_weekly(self):\n        self.test_routine_raw_accuracy()\n\n\nclass TestUsageRoutineRawAccuracyMonthly(TestUsageRoutineRawAccuracy):\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = ['MONTH', 'monthly']\n\n    def test_routine_raw_accuracy_monthly(self):\n        self.test_routine_raw_accuracy()\n\nsuite = unittest.TestSuite()\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyDaily))\nrunner = unittest.TextTestRunner()\nrunner.run(suite)\n"]},{"cell_type":"code","execution_count":0,"id":"20200701-094333_1720154581","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\nfrom applications.db_check_v1.common.constants import query, citus_settings\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY\nfrom applications.db_check_v1.common.table_common_info import urn\n\n\nplproxy_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\n\nclass TestUsageRoutinePlproxyCompleteness(unittest.TestCase):\n    citus_table_name = 'usage.usage_basic_kpi_fact_v6'\n    db_name = 'usage'\n    plproxy_table_name = 'mu.app_{granularity}'\n\n    def setUp(self):\n        self.check_date = None\n        self.granularity = None  # granularity type is list, like ['DAY', 'daily']\n\n    def test_routine_plproxy_completeness(self):\n        plproxy_db_count = self.get_plproxy_db_count()\n        citus_db_count = self.get_citus_db_count()\n\n        self.assertEqual(plproxy_db_count[0][0], citus_db_count[0][0],\n                         msg=\"fount count mismatch when compare usage routine plproxy and citus db. \"\n                             \"granularity is {}, date is {}, plproxy count is:{}, citus db count is:{}\".format(\n                             self.granularity[1], self.check_date, plproxy_db_count, citus_db_count[0][0]))\n\n    def get_citus_db_count(self):\n        sql = \"\"\"select count(1) as cnt from {table_name} where date='{date}' and granularity='{granularity}';\n        \"\"\".format(table_name=self.citus_table_name, date=self.check_date, granularity=self.granularity[1])\n        result = query(citus_settings(self.db_name), sql)\n        return result\n\n    def get_plproxy_db_count(self):\n        sql = \"\"\"select count(uniqlo_id) from plproxy.execute_select_nestloop($proxy$ \n                    select max(app_id) as uniqlo_id\n                from {table_name}\n                where \n                date='{date}'\n                group by\n                app_id,\n                store_id,\n                device_id\n            $proxy$) t (uniqlo_id BIGINT);\"\"\".format(table_name=self.plproxy_table_name.format(\n            granularity=self.granularity[1]), date=self.check_date)\n        result = query(plproxy_dsn, sql)\n        return result\n\n\nclass TestUsageRoutinePlproxyCompletenessDaily(TestUsageRoutinePlproxyCompleteness):\n\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = ['DAY', 'daily']  # granularity type is list, like ['DAY', 'daily']\n\n    def test_routine_plproxy_completeness_daily(self):\n        self.test_routine_plproxy_completeness()\n\n\nclass TestUsageRoutinePlproxyCompletenessWeekly(TestUsageRoutinePlproxyCompleteness):\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = ['WEEK', 'weekly']  # granularity type is list, like ['DAY', 'daily']\n\n    def test_routine_plproxy_completeness_weekly(self):\n        self.test_routine_plproxy_completeness()\n\n\nclass TestUsageRoutinePlproxyCompletenessMonthly(TestUsageRoutinePlproxyCompleteness):\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = ['MONTH', 'monthly']  # granularity type is list, like ['DAY', 'daily']\n\n    def test_routine_plproxy_completeness_monthly(self):\n        self.test_routine_plproxy_completeness()\n        \n\nsuite = unittest.TestSuite()\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutinePlproxyCompletenessDaily))\nrunner = unittest.TextTestRunner()\nrunner.run(suite)"]},{"cell_type":"code","execution_count":0,"id":"20200702-024134_1172329843","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/"]},{"cell_type":"code","execution_count":0,"id":"20200706-093100_1659842288","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/granularity=weekly/"]},{"cell_type":"code","execution_count":0,"id":"20200706-075315_1002516212","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ndef get_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n    \n    \ndate_range = ['2020-06-18', '2020-06-19']\nspark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date={%s}/\" % \",\".join(date_range)).show()"]},{"cell_type":"code","execution_count":0,"id":"20200706-075528_1042105349","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n    \n    \nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndate_range = ['2020-06-14', '2020-06-15', '2020-06-16', '2020-06-17', '2020-06-18', '2020-06-19', '2020-06-20']\n\nspark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date={%s}/\" % \",\".join(date_range)).cache().createOrReplaceTempView(\"daily_df\")\n\nsql_text =\"\"\"\nWITH daily_agg_df AS (\n    SELECT app_id, device_code, country_code, publisher_id, company_id, parent_company_id,\n    sum(est_free_app_download) as est_free_app_download,\n    sum(est_paid_app_download) as est_paid_app_download,\n    sum(est_revenue) as est_revenue,\n    sum(est_organic_download) as est_organic_download,\n    sum(est_paid_download) as est_paid_download\n    from daily_df\n    group by\n    app_id, device_code, country_code, publisher_id, company_id, parent_company_id\n);\n\"\"\"\n\n\ntest_date='2020-06-20'\nnamespace = \"aa.store.market-size.v1\"\ningest_msg = {\n    \"namespace\": \"aa.store.market-size.v1\",\n    \"job_type\": \"routine\",\n    \"options\":{},\n    \"source\": [\n        {\n            \"name\":\"unified_df\",\n            \"data_encoding\": \"parquet\",\n            \"compression\": \"gzip\",\n            \"path\": [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/granularity=weekly/date={}\".format(test_date)],\n        }    \n    ]\n}\nrun(spark, ingest_msg, sql_text)\ndiff_df1 = spark.sql(\"\"\"select app_id, country_code, device_code, publisher_id, company_id, parent_company_id, est_free_app_download, est_paid_app_download,                 est_revenue, est_paid_download, est_organic_download\n                                from unified_df \n                            except all \n                            select app_id, country_code, device_code, publisher_id, company_id, parent_company_id, est_free_app_download, est_paid_app_download, est_revenue, est_paid_download, est_organic_download\n                                from daily_agg_df\"\"\")\ndiff_df2 = spark.sql(\"\"\"select app_id, country_code, device_code, publisher_id, company_id, parent_company_id, est_free_app_download, est_paid_app_download,                 est_revenue, est_paid_download, est_organic_download\n                                from daily_agg_df \n                            except all \n                            select app_id, country_code, device_code, publisher_id, company_id, parent_company_id, est_free_app_download, est_paid_app_download, est_revenue, est_paid_download, est_organic_download\n                                from unified_df\"\"\")\ndiff_df1.show()\ndiff_df2.show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200713-070840_638857026","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n    \n    \nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndate_range = ['2020-06-14', '2020-06-15', '2020-06-16', '2020-06-17', '2020-06-18', '2020-06-19', '2020-06-20']\ngranularity = 'weekly'\ndaily_category_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n                     \"store.app-est-category-load.v3/fact/\"\nagg_category_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v1/fact/\"\nspark.read.format(\"delta\").load(daily_category_path).where(\n        \"granularity='daily' and date between '{}' and '{}'\".format(\n            '2020-06-14', '2020-06-20')).createOrReplaceTempView(\"daily_df\")\nspark.read.format(\"delta\").load(agg_category_path).where(\n    \"granularity='{}' and date between '{}' and '{}'\".format(\n        granularity, '2020-06-20', '2020-06-20')).createOrReplaceTempView(\"unified_df\")\nsql_text = \"\"\"\n    SELECT app_id, category_id, device_code, country_code,\n    sum(est_free_app_download) as est_free_app_download,\n    sum(est_paid_app_download) as est_paid_app_download,\n    sum(est_revenue) as est_revenue,\n    sum(est_organic_download) as est_organic_download,\n    sum(est_paid_download) as est_paid_download\n    from daily_df\n    group by\n    app_id, device_code, country_code, category_id\n\"\"\"\n\nspark.sql(sql_text).createOrReplaceTempView(\"daily_agg_df\")\ndiff_df1 = spark.sql(\"\"\"select app_id, country_code, device_code, category_id,\n                            est_free_app_download, est_paid_app_download,\n                            est_revenue\n                            from daily_agg_df\n                            except all\n                        select app_id, country_code, device_code, category_id,\n                            est_free_app_download, est_paid_app_download,\n                            est_revenue\n                            from unified_df\"\"\")\ndiff_df2 = spark.sql(\"\"\"select app_id, country_code, device_code, category_id,\n                            est_free_app_download, est_paid_app_download,\n                            est_revenue\n                            from unified_df\n                            except all\n                        select app_id, country_code, device_code, category_id,\n                            est_free_app_download, est_paid_app_download,\n                            est_revenue\n                            from daily_agg_df\"\"\")\ndiff_df1.show()\ndiff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200706-095351_459038845","metadata":{},"outputs":[],"source":["\ndate_range = ['2020-06-14', '2020-06-15', '2020-06-16', '2020-06-17', '2020-06-18', '2020-06-19', '2020-06-20']\nspark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date={%s}/\" % \",\".join(date_range)).cache().createOrReplaceTempView(\"daily_df\")\nspark.sql(\"select * from daily_df where app_id=443948765 and country_code='HR' and device_code='ios-phone' and publisher_id=390200178\").show()\nspark.sql(\"\"\"SELECT app_id, device_code, country_code, publisher_id, company_id, parent_company_id,\n    sum(est_free_app_download) as est_free_app_download,\n    sum(est_paid_app_download) as est_paid_app_download,\n    sum(est_revenue) as est_revenue,\n    sum(est_organic_download) as est_organic_download,\n    sum(est_paid_download) as est_paid_download\n    from daily_df\n    group by\n    app_id, device_code, country_code, publisher_id, company_id, parent_company_id\"\"\").filter(\"app_id=443948765 and country_code='HR' and device_code='ios-phone' and publisher_id=390200178\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200707-082842_2002496057","metadata":{},"outputs":[],"source":["\ndaily_category_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n                     \"store.app-est-category-load.v3/fact/\"\ntest_date = ['2020-07-06', '2020-07-06']\ndaily_df = spark.read.format(\"delta\").load(daily_category_path).where(\"granularity='daily' and date between '{}' and '{}'\".format(test_date[0], test_date[1]))\ndaily_df.show()"]},{"cell_type":"code","execution_count":0,"id":"20200710-082938_1672135461","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v1/fact/granularity=weekly/date=2020-06-20/"]},{"cell_type":"code","execution_count":0,"id":"20200714-014049_475684678","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"qa-auto-pipeline-fiona-1\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom applications.auto_pipeline.temp_script.utils.base_test import PipelineTest"]},{"cell_type":"code","execution_count":0,"id":"20200710-082950_1630699939","metadata":{},"outputs":[],"source":["\nimport aaplproxy\nimport datetime\nimport unittest\nfrom dateutil.relativedelta import relativedelta\nimport croniter\n\nfrom aadatapipelinecore.core.utils.commandline import env\nfrom aadatapipelinecore.core.utils.encode import activate_system_utf8\nfrom aadatapipelinecore.core.utils.spark import create_spark\n\n\nclass PipelineTest(unittest.TestCase):\n    trigger_date_config = None\n    trigger_datetime = None\n    prev_etl_datetime = None\n    only_check_in_24hr = False\n\n    def __init__(self, methodName='runTest', trigger_datetime=None):\n        super(PipelineTest, self).__init__(methodName)\n        self.trigger_datetime = trigger_datetime or datetime.datetime.utcnow()\n        self.check_date_str = self._get_check_date_from_routing_config(self.trigger_datetime).strftime(\"%Y-%m-%d\")\n        self.check_date = self.check_date_str  # for compatibility with send email\n        self.prev_etl_datetime = self._get_pre_etl_completed_date()\n\n    def setUp(self):\n        super(PipelineTest, self).setUp()\n        # print \"Triggered Datetime : {}\".format(self.trigger_datetime)\n        # print \"Check date str : {}\".format(self.check_date_str)\n        self._verify_config()\n\n    @classmethod\n    def setUpClass(cls):\n        super(PipelineTest, cls).setUpClass()\n        activate_system_utf8()\n        env(PYTHONIOENCODING='utf8')\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext\n\n    def _verify_config(self):\n        self.assertIsNotNone(self.trigger_date_config)\n        self.assertIsNotNone(self.trigger_datetime)\n        self.assertIsNotNone(self.prev_etl_datetime)\n        self.assertIsNotNone(self.check_date_str)\n        self.assertIsNotNone(self.check_date)\n\n    def _get_check_date_from_routing_config(self, trigger_datetime):\n        \"\"\"\n        return the date of : <days_delta> ago from previous scheduled date&time according to <cron_time>.\n        e.g.\n        config = (\"0 9 * * *\", 1), today is 2019-10-27 8:00\n        so previous scheduled date&time is 2019-10-26 9:00\n        will return \"2019-10-25\"\n\n        e.g.\n        config = (\"0 7 * * *\", 1), today is 2019-10-27 8:00\n        so previous scheduled date&time is 2019-10-27 7:00\n        will return \"2019-10-26\"\n\n        Cron Time Format\n        Character\tDescriptor\t        Acceptable values\n        1\t        Minute\t            0 to 59, or * (no specific value)\n        2\t        Hour\t            0 to 23, or * for any value. All times UTC.\n        3\t        Day of the month\t1 to 31, or * (no specific value)\n        4\t        Month\t            1 to 12, or * (no specific value)\n        5\t        Day of the week\t    0 to 7 (0 and 7 both represent Sunday), or * (no specific value)\n\n        :param trigger_datetime: the test trigger date\n        :type trigger_datetime: object\n        :return: date obj of \"%Y-%m-%d\"\n        :type return: object\n        \"\"\"\n        schedule, days_delta = self.trigger_date_config\n        # here use UTC now\n        cron = croniter.croniter(schedule, trigger_datetime)\n        date = cron.get_prev(datetime.datetime) - datetime.timedelta(days=days_delta)\n        return date\n\n    def _get_pre_etl_completed_date(self):\n        schedule, _ = self.trigger_date_config\n        cron = croniter.croniter(schedule, self.trigger_datetime)\n        date = cron.get_prev(datetime.datetime)\n        return date\n\ndef string_to_datetime(date_str, convert_format=None):\n    return datetime.datetime.strptime(date_str, convert_format if convert_format else \"%Y-%m-%d\")\n\n\ndef datetime_to_string(date_time, convert_format=\"%Y-%m-%d\"):\n    return datetime.datetime.strftime(date_time, convert_format)\n\n\ndef get_date_list(granularity, check_date):\n    print granularity, check_date\n    if granularity == 'weekly':\n        end = string_to_datetime(check_date)\n        start = end - relativedelta(days=6)\n        result = [datetime_to_string(start), datetime_to_string(end)]\n    if granularity == 'monthly':\n        end = string_to_datetime(check_date)\n        start = string_to_datetime(check_date[0:7] + '-01')\n        result = [datetime_to_string(start), datetime_to_string(end)]\n    if granularity == 'quarterly':\n        end = string_to_datetime(check_date)\n        temp_date = end - relativedelta(months=2)\n        temp_date = datetime_to_string(temp_date)\n        start = string_to_datetime(temp_date[0:7] + '-01')\n        result = [datetime_to_string(start), datetime_to_string(end)]\n    if granularity == 'yearly':\n        end = string_to_datetime(check_date)\n        temp_date = end - relativedelta(months=11)\n        temp_date = datetime_to_string(temp_date)\n        start = string_to_datetime(temp_date[0:7] + '-01')\n        result = [datetime_to_string(start), datetime_to_string(end)]\n    return result\n\n\ndef test_store_category_pre_aggr(spark, test_date, granularity):\n    daily_category_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n                     \"store.app-est-category-load.v3/fact/\"\n    agg_category_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n                        \"store.app-est-category-pre-aggr.v1/fact/\"\n    spark.read.format(\"delta\").load(daily_category_path).where(\n        \"granularity='daily' and date between '{}' and '{}'\".format(\n            test_date[0], test_date[1])).cache().createOrReplaceTempView(\"daily_df\")\n    spark.read.format(\"delta\").load(agg_category_path).where(\n        \"granularity='{}' and date between '{}' and '{}'\".format(\n            granularity, test_date[0], test_date[1])).cache().createOrReplaceTempView(\"unified_df\")\n\n    sql_text = \"\"\"\n        SELECT app_id, category_id, device_code, country_code,\n        sum(est_free_app_download) as est_free_app_download,\n        sum(est_paid_app_download) as est_paid_app_download,\n        sum(est_revenue) as est_revenue,\n        sum(est_organic_download) as est_organic_download,\n        sum(est_paid_download) as est_paid_download\n        from daily_df\n        group by\n        app_id, device_code, country_code, category_id\n    \"\"\"\n\n    spark.sql(sql_text).createOrReplaceTempView(\"daily_agg_df\")\n    diff_df1 = spark.sql(\"\"\"select app_id, country_code, device_code, category_id,\n                                est_free_app_download, est_paid_app_download,\n                                est_revenue\n                                from daily_agg_df\n                                except all\n                            select app_id, country_code, device_code, category_id,\n                                est_free_app_download, est_paid_app_download,\n                                est_revenue\n                                from unified_df\"\"\")\n    diff_df2 = spark.sql(\"\"\"select app_id, country_code, device_code, category_id,\n                                est_free_app_download, est_paid_app_download,\n                                est_revenue\n                                from unified_df\n                                except all\n                            select app_id, country_code, device_code, category_id,\n                                est_free_app_download, est_paid_app_download,\n                                est_revenue\n                                from daily_agg_df\"\"\")\n    return diff_df1, diff_df2\n\n\nclass TestStoreCategoryPreAggr(PipelineTest):\n\n    def setUp(self):\n        super(TestStoreCategoryPreAggr, self).setUp()\n        self.check_date = None\n        self.granularity = None\n\n    def check_store_category_pre_aggr_accuracy(self):\n        print self.check_date\n        date_list = get_date_list(self.granularity, self.check_date)\n        print date_list\n        diff_df1, diff_df2 = test_store_category_pre_aggr(self.spark, date_list, self.granularity)\n        self.assertTrue(diff_df1.count() == 0, \"daily agg except pre agg is not empty. date: {}\".format(\n            self.check_date))\n        self.assertTrue(diff_df2.count() == 0, \"pre agg except daily agg is not empty. date: {}\".format(\n            self.check_date))\n\n\nclass TestStoreCategoryPreAggrWeekly(TestStoreCategoryPreAggr):\n    trigger_date_config = (\"0 12 * * *\", 8)\n    check_date = '2020-06-20'\n\n    def setUp(self):\n        self.granularity = \"weekly\"\n        self.check_date = '2020-06-20'\n\n    def test_store_category_pre_agg_accuracy_weekly(self):\n        self.check_store_category_pre_aggr_accuracy()\n\n\nclass TestStoreCategoryPreAggrMonthly(TestStoreCategoryPreAggr):\n    trigger_date_config = (\"0 12 * * *\", 8)\n    check_date = '2020-05-31'\n\n    def setUp(self):\n        self.granularity = \"monthly\"\n        self.check_date = '2020-05-31'\n\n    def test_store_category_pre_agg_accuracy_monthly(self):\n        self.check_store_category_pre_aggr_accuracy()\n\n\nclass TestStoreCategoryPreAggrQuarterly(TestStoreCategoryPreAggr):\n    trigger_date_config = (\"0 12 * * *\", 8)\n    check_date = '2020-03-31'\n\n    def setUp(self):\n        self.granularity = \"quarterly\"\n        self.check_date = '2020-05-31'\n\n    def test_store_category_pre_agg_accuracy_quarterly(self):\n        self.check_store_category_pre_aggr_accuracy()\n\n\nclass TestStoreCategoryPreAggrYearly(TestStoreCategoryPreAggr):\n    trigger_date_config = (\"0 12 * * *\", 8)\n    check_date = '2019-12-31'\n\n    def setUp(self):\n        self.granularity = \"yearly\"\n        self.check_date = '2019-12-31'\n    \n    def test_store_category_pre_agg_accuracy_yearly(self):\n        self.check_store_category_pre_aggr_accuracy()\n\n\nsuite = unittest.TestSuite()\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreCategoryPreAggrWeekly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreCategoryPreAggrMonthly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreCategoryPreAggrQuarterly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreCategoryPreAggrYearly))\nrunner = unittest.TextTestRunner()\nrunner.run(suite)\n"]},{"cell_type":"code","execution_count":0,"id":"20200714-022107_2060249312","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom applications.auto_pipeline.temp_script.utils.base_test import PipelineTest"]},{"cell_type":"code","execution_count":0,"id":"20200714-023945_2006761447","metadata":{},"outputs":[],"source":["\nfrom applications.auto_pipeline.temp_script.utils.base_test import PipelineTest"]},{"cell_type":"code","execution_count":0,"id":"20200713-075719_777794258","metadata":{},"outputs":[],"source":["\nimport datetime\nimport unittest\nimport aaplproxy\nfrom dateutil.relativedelta import relativedelta\nfrom applications.auto_pipeline.temp_script.utils.base_test import PipelineTest\n\n\ndef string_to_datetime(date_str, convert_format=None):\n    return datetime.datetime.strptime(date_str, convert_format if convert_format else \"%Y-%m-%d\")\n\n\ndef datetime_to_string(date_time, convert_format=\"%Y-%m-%d\"):\n    return datetime.datetime.strftime(date_time, convert_format)\n\n\ndef get_date_list(granularity, check_date):\n    if granularity == 'weekly':\n        end = string_to_datetime(check_date)\n        start = end - relativedelta(days=6)\n        result = [datetime_to_string(start), datetime_to_string(end)]\n    if granularity == 'monthly':\n        end = string_to_datetime(check_date)\n        start = string_to_datetime(check_date[0:7] + '-01')\n        result = [datetime_to_string(start), datetime_to_string(end)]\n    if granularity == 'quarterly':\n        end = string_to_datetime(check_date)\n        temp_date = end - relativedelta(months=2)\n        temp_date = datetime_to_string(temp_date)\n        start = string_to_datetime(temp_date[0:7] + '-01')\n        result = [datetime_to_string(start), datetime_to_string(end)]\n    if granularity == 'yearly':\n        end = string_to_datetime(check_date)\n        temp_date = end - relativedelta(months=11)\n        temp_date = datetime_to_string(temp_date)\n        start = string_to_datetime(temp_date[0:7] + '-01')\n        result = [datetime_to_string(start), datetime_to_string(end)]\n    return result\n\n\ndef test_store_est_pre_aggr(spark, test_date, granularity):\n    print granularity, test_date\n    daily_est_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n                     \"store.app-est-dna-log.v1/fact/\"\n    agg_est_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/\"\n    \n    spark.read.format(\"delta\").load(daily_est_path).where(\n        \"granularity='daily' and date between '{}' and '{}'\".format(\n            test_date[0], test_date[1])).cache().createOrReplaceTempView(\"daily_df\")\n    spark.read.format(\"delta\").load(agg_est_path).where(\n        \"granularity='{}' and date between '{}' and '{}'\".format(\n            granularity, test_date[0], test_date[1])).cache().createOrReplaceTempView(\"unified_df\")\n    \n    sql_text = \"\"\"\n        SELECT app_id, device_code, country_code, publisher_id, company_id, parent_company_id,\n        sum(est_free_app_download) as est_free_app_download,\n        sum(est_paid_app_download) as est_paid_app_download,\n        sum(est_revenue) as est_revenue,\n        sum(est_organic_download) as est_organic_download,\n        sum(est_paid_download) as est_paid_download\n        from daily_df\n        group by\n        app_id, device_code, country_code, publisher_id, company_id, parent_company_id\n    \"\"\"\n    \n    spark.sql(sql_text).createOrReplaceTempView(\"daily_agg_df\")\n    \n    diff_df1 = spark.sql(\"\"\"select app_id, country_code, device_code, publisher_id, company_id,\n                                parent_company_id, est_free_app_download, est_paid_app_download,\n                                est_revenue, est_paid_download, est_organic_download\n                                from daily_agg_df\n                                except all\n                            select app_id, country_code, device_code, publisher_id, company_id, \n                                parent_company_id, est_free_app_download, est_paid_app_download,\n                                est_revenue, est_paid_download, est_organic_download\n                                from unified_df\"\"\")\n    diff_df2 = spark.sql(\"\"\"select app_id, country_code, device_code, publisher_id, company_id,\n                                parent_company_id, est_free_app_download, est_paid_app_download,\n                                est_revenue, est_paid_download, est_organic_download\n                                from unified_df\n                                except all\n                            select app_id, country_code, device_code, publisher_id, company_id,\n                                parent_company_id, est_free_app_download, est_paid_app_download,\n                                est_revenue, est_paid_download, est_organic_download\n                                from daily_agg_df\"\"\")\n    diff_df1.show()\n    diff_df2.show()\n    return diff_df1, diff_df2\n\n\nclass TestStoreEstPreAggr(PipelineTest):\n    agg_est_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/\" \\\n                   \"granularity={granularity}/date={date}\"\n\n    def setUp(self):\n        super(TestStoreEstPreAggr, self).setUp()\n        self.check_date = None\n        self.granularity = None\n\n    def check_store_est_pre_aggr_accuracy(self):\n        date_list = get_date_list(self.granularity, self.check_date)\n        diff_df1, diff_df2 = test_store_est_pre_aggr(self.spark, date_list, self.granularity)\n        self.assertTrue(diff_df1.count() == 0, \"daily agg except pre agg is not empty. date: {}\".format(\n            self.check_date))\n        self.assertTrue(diff_df2.count() == 0, \"pre agg except daily agg is not empty. date: {}\".format(\n            self.check_date))\n\n\nclass TestStoreEstPreAggrWeekly(TestStoreEstPreAggr):\n    trigger_date_config = (\"0 12 * * *\", 8)\n    check_date = '2020-06-20'\n\n    def setUp(self):\n        self.granularity = \"weekly\"\n        self.check_date = '2020-06-20'\n\n    def test_store_est_pre_agg_accuracy_weekly(self):\n        self.check_store_est_pre_aggr_accuracy()\n\n\nclass TestStoreEstPreAggrMonthly(TestStoreEstPreAggr):\n    trigger_date_config = (\"0 12 * * *\", 8)\n    check_date = '2020-05-31'\n\n    def setUp(self):\n        self.granularity = \"monthly\"\n        self.check_date = '2020-05-31'\n\n    def test_store_est_pre_agg_accuracy_monthly(self):\n        self.check_store_est_pre_aggr_accuracy()\n\n\nclass TestStoreEstPreAggrQuarterly(TestStoreEstPreAggr):\n    trigger_date_config = (\"0 12 * * *\", 8)\n    check_date = '2020-03-31'\n\n    def setUp(self):\n        self.granularity = \"quarterly\"\n        self.check_date = '2020-03-31'\n\n    def test_store_est_pre_agg_accuracy_quarterly(self):\n        self.check_store_est_pre_aggr_accuracy()\n\n\nclass TestStoreEstPreAggrYearly(TestStoreEstPreAggr):\n    trigger_date_config = (\"0 12 * * *\", 8)\n    check_date = '2019-12-31'\n\n    def setUp(self):\n        self.granularity = \"yearly\"\n        self.check_date = '2019-12-31'\n\n    def test_store_est_pre_agg_accuracy_yearly(self):\n        self.check_store_est_pre_aggr_accuracy()\n        \n\nsuite = unittest.TestSuite()\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreEstPreAggrWeekly))\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreEstPreAggrMonthly))\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreEstPreAggrQuarterly))\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreEstPreAggrYearly))\nrunner = unittest.TextTestRunner()\nrunner.run(suite)"]},{"cell_type":"code","execution_count":0,"id":"20200714-022025_1874724153","metadata":{},"outputs":[],"source":["\nagg_est_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/\"\nprint spark.read.format(\"delta\").load(agg_est_path).where(\n        \"granularity='weekly' and date between '{}' and '{}'\".format(\n            '2020-06-20', '2020-06-20')).count()spark.read.format(\"delta\").load(agg_est_path).where(\n        \"granularity='weekly' and date between '{}' and '{}'\".format(\n            '2020-06-20', '2020-06-20')).count()"]},{"cell_type":"code","execution_count":0,"id":"20200714-063106_1657716287","metadata":{},"outputs":[],"source":["\nspark.read.format(\"delta\").load(daily_category_path).where(\n        \"granularity='daily' and date between '{}' and '{}'\".format(\n            '2020-06-14', '2020-06-20')).createOrReplaceTempView(\"daily_df\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}