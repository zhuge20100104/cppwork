{"cells":[{"cell_type":"code","execution_count":0,"id":"20200625-074541_177310356","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200625-074132_1949760244","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select distinct date from store.store_est_t_{}_fact_v1 where granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndict_test = {'weekly': ['2010-07-10', '2020-06-20'],\n             'monthly': ['2010-07-31', '2020-04-30'],\n             'quarterly': ['2010-09-30', '2020-03-31'],\n             'yearly': ['2010-12-31', '2019-12-31']\n             }\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n    \n    \ndef get_date_range(date_list, granularity):\n    result = []\n    start = datetime.datetime.strptime(date_list[0], '%Y-%m-%d')\n    end = datetime.datetime.strptime(date_list[1], '%Y-%m-%d')\n    if granularity == 'weekly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(weeks=1)\n    elif granularity == 'monthly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=1)\n    elif granularity == 'quarterly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=3)\n    elif granularity == 'yearly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=12)\n    # print result\n    return result\n\n\ndef check_store_db_illegal_date(date_list, graularity):\n    date_list = set(date_list)\n    db_date_tuple_list = query(aa_dsn, sql.format(graularity[0], graularity))\n    db_date_list = [datetime.datetime.strftime(x[0], '%Y-%m-%d') for x in db_date_tuple_list]\n    db_date_list = set(db_date_list)\n    print db_date_list.difference(date_list)\n\n\nfor key, value in dict_test.items():\n    print key, value\n    check_store_db_illegal_date(get_date_range(value, key), key)\n    "]},{"cell_type":"code","execution_count":0,"id":"20200625-084553_689507760","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select distinct date from store.store_est_category_t_{}_fact_v1 where granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndict_test = {'weekly': ['2010-07-10', '2020-06-20'],\n             'monthly': ['2010-07-31', '2020-04-30'],\n             'quarterly': ['2010-09-30', '2020-03-31'],\n             'yearly': ['2010-12-31', '2019-12-31']\n             }\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n    \n    \ndef get_date_range(date_list, granularity):\n    result = []\n    start = datetime.datetime.strptime(date_list[0], '%Y-%m-%d')\n    end = datetime.datetime.strptime(date_list[1], '%Y-%m-%d')\n    if granularity == 'weekly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(weeks=1)\n    elif granularity == 'monthly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=1)\n    elif granularity == 'quarterly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=3)\n    elif granularity == 'yearly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=12)\n    # print result\n    return result\n\n\ndef check_store_db_illegal_date(date_list, graularity):\n    date_list = set(date_list)\n    db_date_tuple_list = query(aa_dsn, sql.format(graularity[0], graularity))\n    db_date_list = [datetime.datetime.strftime(x[0], '%Y-%m-%d') for x in db_date_tuple_list]\n    db_date_list = set(db_date_list)\n    print db_date_list.difference(date_list)\n\n\nfor key, value in dict_test.items():\n    print key, value\n    check_store_db_illegal_date(get_date_range(value, key), key)\n    "]},{"cell_type":"code","execution_count":0,"id":"20200625-074406_1794718733","metadata":{},"outputs":[],"source":["\nend = '2012-09-30'\ngranularity = 'monthly'\npre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n            \"store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\npre_agg_df.show()"]},{"cell_type":"code","execution_count":0,"id":"20200625-081946_381353688","metadata":{},"outputs":[],"source":["\nend = '2015-04-25'\ngranularity = 'weekly'\npre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n            \"store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\nprint pre_agg_df.count()"]},{"cell_type":"code","execution_count":0,"id":"20200625-082735_702084906","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\nstart = '2012-01-01'\nend = '2012-12-31'\ngranularity = 'yearly'\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\n\n\nagg_df = category_daily_df.groupBy('app_id', 'country_code', 'device_code', 'category_id').agg(\n    sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), \n        sum('est_revenue').alias('revenue'))\nagg_df.createOrReplaceTempView(\"agg_df\")\n\nif granularity == 'weekly' or granularity == 'monthly':\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\nelse:\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n\npre_agg_df.createOrReplaceTempView(\"pre_agg_df\")\n\ndiff_df1 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df\"\"\")\ndiff_df2 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df\"\"\")\nprint agg_df.count(), pre_agg_df.count()\nprint diff_df1.count(), diff_df2.count()\ndiff_df1.show()\ndiff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200625-090840_88367553","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}