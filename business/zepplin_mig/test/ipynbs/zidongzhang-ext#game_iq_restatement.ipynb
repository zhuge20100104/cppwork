{"cells":[{"cell_type":"code","execution_count":0,"id":"20200407-064752_248383288","metadata":{},"outputs":[],"source":["%md\ns3://b2c-prod-data-pipeline-unified-dna/unified/dna.genre_id_product_mapping.v1/dimension/\ns3://b2c-prod-data-pipeline-unified-store-paid/unified/store.genre-est.v1/fact/granularity=daily/date={}/"]},{"cell_type":"code","execution_count":0,"id":"20200407-080620_706562057","metadata":{},"outputs":[],"source":["spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport pandas as pd\n\npd.set_option('expand_frame_repr', False)\nimport bo\ns3 = boto3.resource('s3')\ns3object = s3.Object('b2c-prod-data-pipeline-qa', 'tom/gameiq/2020-03-13/regression5.txt')\nlog = [] "]},{"cell_type":"code","execution_count":0,"id":"20200408-083607_830744705","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-dna/unified/dna.genre_id_product_mapping.v1/dimension/_partition_col=0/"]},{"cell_type":"code","execution_count":0,"id":"20200409-084128_420109798","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/ | tail -5"]},{"cell_type":"code","execution_count":0,"id":"20200410-031311_1839015691","metadata":{},"outputs":[],"source":["%%sh\n# date=2010-07-04/--date=2020-04-04/\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.genre-est.v1/fact/granularity=daily/ | tail -4\n"]},{"cell_type":"code","execution_count":0,"id":"20200413-055617_1940013970","metadata":{},"outputs":[],"source":["%%sh\n"]},{"cell_type":"code","execution_count":0,"id":"20200408-083523_532815107","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\ndf_dna = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.genre_id_product_mapping.v1/dimension/\").withColumn(\"genre_id\", F.explode(\"genre_id\"))\ndf_dna.show(3)\n\ndf_all_genre = df_dna.groupby(\"genre_id\").agg({'genre_id': 'count'})\nprint len(df_all_genre.collect())"]},{"cell_type":"code","execution_count":0,"id":"20200408-111403_1593386314","metadata":{},"outputs":[],"source":["\ndf_store_app = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-02-29/\")\ndf_store_app.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200408-083754_228548020","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.genre-est.v1/fact/granularity=daily/date=2016-07-24/"]},{"cell_type":"code","execution_count":0,"id":"20200408-083720_1743060280","metadata":{},"outputs":[],"source":["\n# All modifier_id is 100000\ndf_store_genre = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.genre-est.v1/fact/granularity=daily/date=2020-01-29/\")\n# df_store_genre = df_store_genre.groupBy(\"modifier_id\").agg({\"modifier_id\": \"count\"})\n# android-all|          AE|  303006|     171|    877|\ndf_store_genre = df_store_genre.where(\"device_code='android-all' and country_code='AE' and genre_id='303006'\")\ndf_store_genre.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200408-020607_504882708","metadata":{},"outputs":[],"source":["%%sh\naws s3 cp s3://b2c-prod-data-pipeline-qa/tom/gameiq/regression5.txt - "]},{"cell_type":"code","execution_count":0,"id":"20200407-071903_1767664686","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom applications.db_check_v1.common.constants import COUNTRY_CODE_MAPPING_BY_MARKET_CODE as COUNTRY_CODE_MAPPING\nfrom conf.settings import *\nfrom applications.db_check_v1.common.db_check_utils import query_df\n\n\n\nDEVICE_CODE_MAPPING_BY_DEVICE_ID = {\n    'google-play':{\n        1000: \"android-all\",\n        1001: \"android-phone\",\n        1002: \"android-tablet\",\n    },\n    'apple-store': {\n        2000: \"ios-all\",\n        2001: \"ios-phone\",\n        2002: \"ios-tablet\"\n    }\n}\nDEVICE_CODE_MAPPING = DEVICE_CODE_MAPPING_BY_DEVICE_ID\n\ndaily_est_dsn =(\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_DAILY_EST_NAME,\n        user=PG_DAILY_EST_ACCESS_ID,\n        host=PG_DAILY_EST_HOSTS[0][0],\n        password=PG_DAILY_EST_SECRET_KEY,\n        port=PG_DAILY_EST_HOSTS[0][1]\n    )\n)\n\nmapping_df_unified = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.genre_id_product_mapping.v1/dimension/\")\ndf_pid2gid = mapping_df_unified.select([\"product_id\", \"genre_id\"]).withColumn('genre_id', F.explode('genre_id'))\n\ndef compare(date):\n    #collect\n    df_store_app_est = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date))\n    est_unified_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.genre-est.v1/fact/granularity=daily/date={}/\".format(date))\n    \n    # #transform\n    df_store_app_est_trans = df_store_app_est.select([\"app_id\", \"device_code\", \"country_code\", \"free_app_download\", \"paid_app_download\", \"revenue\"]).withColumnRenamed(\"app_id\", \"product_id\")\n    # df_store_app_est_trans.show(1)\n    # df_pid2gid.show(3)\n    # print 'after join:'\n    giq_df = df_store_app_est_trans.join(df_pid2gid, df_pid2gid.product_id == df_store_app_est_trans.product_id, how='inner').groupBy([\"device_code\", \"country_code\", \"genre_id\"]).agg({\n            \"free_app_download\": \"sum\",\n            \"paid_app_download\": \"sum\",\n            \"revenue\": \"sum\",\n        }).withColumnRenamed(\"sum(free_app_download)\", \"free_app_download\").withColumnRenamed(\"sum(paid_app_download)\", \"paid_app_download\").withColumnRenamed(\"sum(revenue)\", \"revenue\")\n    # giq_df.show(1)\n    giq_df = giq_df.fillna({\"free_app_download\":0, \"paid_app_download\":0})\n    giq_df = giq_df.withColumn(\"download\", F.when(giq_df.free_app_download + giq_df.paid_app_download>0, giq_df.free_app_download + giq_df.paid_app_download).otherwise(F.lit(None)))\n\n    #compare\n    s1=giq_df.select([\"device_code\", \"country_code\", \"genre_id\", \"download\", \"revenue\"])\n    # s1.show(1)\n\n    s2=est_unified_df.select([\"device_code\", \"country_code\", \"genre_id\", \"download\", \"revenue\"])\n    # s2.show(1)\n\n    diff = s1.union(s2).subtract(s1.intersect(s2))\n    \n    ##########################\n    # DB & UNIFIED LAYER TEST\n\n    sql = \"\"\"\n    select device_id, store_id, date, genre_id, modifier_id, download, revenue \n    from plproxy.execute_select_nestloop($proxy$ \n    select device_id, store_id, date, genre_id, modifier_id, download, revenue\n    from aa.genre_store_daily_estimate\n    where date = '{}'\n    $proxy$) tbl (device_id SMALLINT, store_id INT, date DATE , genre_id BIGINT, modifier_id BIGINT, download BIGINT, revenue BIGINT);\"\"\".format(date)\n\n    # db_df = query_df(daily_est_dsn, sql)\n\n    # db_df.loc[db_df['device_id'].isin(DEVICE_CODE_MAPPING['google-play'])] = db_df.loc[db_df['device_id'].isin(DEVICE_CODE_MAPPING['google-play'])].replace({\"device_id\": DEVICE_CODE_MAPPING['google-play']})\n    # db_df.loc[db_df['device_id'].isin(DEVICE_CODE_MAPPING['apple-store'])] = db_df.loc[db_df['device_id'].isin(DEVICE_CODE_MAPPING['apple-store'])].replace({\"device_id\": DEVICE_CODE_MAPPING['apple-store']})\n\n    # db_df = db_df.replace({\"store_id\": COUNTRY_CODE_MAPPING['google-play']})\n    # db_df = db_df.replace({\"store_id\": COUNTRY_CODE_MAPPING['apple-store']})\n    \n    # db_df = db_df.rename(columns={'store_id': 'country_code'}).rename(columns={'device_id': 'device_code'}).rename(columns={'device_id': 'device_code'}).rename(columns={'device_id': 'device_code'})\n    \n    # est_unified_df = est_unified_df.toPandas()\n    # est_unified_df[\"modifier_id\"] = 100000\n\n    # diff_db = _compare_df(est_unified_df, db_df)\n    \n    if diff.count()>0:\n        print \"diff:\"\n        diff.show(2)\n        print \"diff groupby country:\"\n        country_group = diff.groupby(\"country_code\").agg({\"country_code\": \"count\"})\n        return [\"{}: FAIL UNIFIED, Diff length: {}\".format(date, diff.distinct().count()), country_group]\n    if len(diff_db)>0:\n        print diff_db\n        return \"{}: FAIL DB\".format(date)\n    return  \"{}: PASS\".format(date)\n    \n    \ndef get_date_list(start_date, end_date, freq=\"D\"):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\ndef _compare_df(df1, df2):\n    for diff_type in [\"left\", \"right\"]:\n        diff_df = df1.merge(df2, indicator=True, how=diff_type)  # .loc[lambda x : x['_merge']!='both']\n        diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n        if len(diff_df) != 0:\n            print diff_type\n            return diff_df\n    return []\n\ndef write_log(strobj, s3obj):\n    s3obj.put(Body=str(strobj))\n\ndate_list = get_date_list(\"2020-04-04\", \"2020-04-04\")\n# date_list = get_date_list(\"2018-01-01/\", \"2020-02-29/\")\ncountry_group_list = []\nfor date in date_list:\n    temp_log = []\n    try:\n        temp_log = compare(date)\n    except Exception, e:\n        temp_log = \"{}: ERROR\".format(date) \n    log.append(temp_log)\n    print temp_log[0]\n    country_group_list.append(temp_log[1])\n    # write_log(log, s3object)\n\n# for country_group in country_group_list:\n#     print \"country group count:\", country_group.count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200410-033830_1034986436","metadata":{},"outputs":[],"source":["\ncountry_group_list"]},{"cell_type":"code","execution_count":0,"id":"20200410-025720_1089874460","metadata":{},"outputs":[],"source":["\ncountry_group_list[0].subtract(country_group_list[1]).show()\n    "]},{"cell_type":"code","execution_count":0,"id":"20200407-080306_1095621419","metadata":{},"outputs":[],"source":["\nmapping_df_unified = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.genre_id_product_mapping.v1/dimension/\")\ndf_pid2gid = mapping_df_unified.select([\"product_id\", \"genre_id\"]).withColumn('genre_id', F.explode('genre_id'))\n\ndate = '2020-01-29'\n#collect\ndf_store_app_est = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date))\nest_unified_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.genre-est.v1/fact/granularity=daily/date={}/\".format(date))\n\n# #transform\ndf_store_app_est_trans = df_store_app_est.select([\"app_id\", \"device_code\", \"country_code\", \"free_app_download\", \"paid_app_download\", \"revenue\"]).withColumnRenamed(\"app_id\", \"product_id\")\ndf_store_app_est_trans.show(1)\ndf_pid2gid.show(3)\nprint 'after join:'\ngiq_df = df_store_app_est_trans.join(df_pid2gid, df_pid2gid.product_id == df_store_app_est_trans.product_id, how='inner').groupBy([\"device_code\", \"country_code\", \"genre_id\"]).agg({\n        \"free_app_download\": \"sum\",\n        \"paid_app_download\": \"sum\",\n        \"revenue\": \"sum\",\n    }).withColumnRenamed(\"sum(free_app_download)\", \"free_app_download\").withColumnRenamed(\"sum(paid_app_download)\", \"paid_app_download\").withColumnRenamed(\"sum(revenue)\", \"revenue\")\ngiq_df.show(3)\ngiq_df = giq_df.fillna({\"free_app_download\":0, \"paid_app_download\":0})\ngiq_df = giq_df.withColumn(\"download\", F.when(giq_df.free_app_download + giq_df.paid_app_download>0, giq_df.free_app_download + giq_df.paid_app_download).otherwise(F.lit(None)))\n\n#compare\ns1=giq_df.select([\"device_code\", \"country_code\", \"genre_id\", \"download\", \"revenue\"])\nprint \"s1:\"\ns1.show(2)\ns2=est_unified_df.select([\"device_code\", \"country_code\", \"genre_id\", \"download\", \"revenue\"])\nprint \"s2:\"\ns2.show(2)\ndiff = s1.union(s2).subtract(s1.intersect(s2))\ndiff.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200410-024902_1380730918","metadata":{},"outputs":[],"source":["\n# android-all|          AE|  303006|     171|    877|\ns1 = s1.where(\"device_code='android-all' and country_code='AE' and genre_id='303006'\")\ns1.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200410-023044_619918549","metadata":{},"outputs":[],"source":["\ndiff.groupby(\"country_code\").agg({\"country_code\": \"count\"}).count()"]},{"cell_type":"code","execution_count":0,"id":"20200409-024635_791266355","metadata":{},"outputs":[],"source":["\ndiff.count()"]},{"cell_type":"code","execution_count":0,"id":"20200409-031039_1772719202","metadata":{},"outputs":[],"source":["\ns1.count()"]},{"cell_type":"code","execution_count":0,"id":"20200409-032523_247746916","metadata":{},"outputs":[],"source":["\ns2.count()"]},{"cell_type":"code","execution_count":0,"id":"20200409-032532_1090890095","metadata":{},"outputs":[],"source":["\nmy_diff = s1.subtract(s2)\nmy_diff.count()"]},{"cell_type":"code","execution_count":0,"id":"20200409-032758_1781561754","metadata":{},"outputs":[],"source":["\nmy_diff2 = s2.subtract(s1)\nmy_diff2.count()"]},{"cell_type":"code","execution_count":0,"id":"20200409-033138_369321441","metadata":{},"outputs":[],"source":["\nmy_diff.show(10)"]},{"cell_type":"code","execution_count":0,"id":"20200409-033614_700461970","metadata":{},"outputs":[],"source":["\nmy_diff2.show(10)"]},{"cell_type":"code","execution_count":0,"id":"20200409-033622_1050852748","metadata":{},"outputs":[],"source":["\naaa = my_diff.intersect(my_diff2)\naaa.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200409-033754_336964685","metadata":{},"outputs":[],"source":["\nimport pandas as pd\ndf = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n     index=['cobra', 'viper', 'sidewinder'],\n     columns=['max_speed', 'shield'])\ndf"]},{"cell_type":"code","execution_count":0,"id":"20200409-033805_961289536","metadata":{},"outputs":[],"source":["\nprint df.loc[[True, True, False]]"]},{"cell_type":"code","execution_count":0,"id":"20200409-090443_1240897430","metadata":{},"outputs":[],"source":["\ndf_dna_mapping = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.genre_id_product_mapping.v1/dimension/\").withColumn(\"genre_id\", F.explode(\"genre_id\"))\ndf_dna_mapping.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200410-061924_819017788","metadata":{},"outputs":[],"source":["\ngranularity, date = 'daily', '2019-02-10'\ndf_store_genre = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.genre-est.v1/fact/granularity={}/date={}/\".format(granularity, date)).where(\"genre_id='302006'\")\ndf_store_genre.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200410-062044_1685199551","metadata":{},"outputs":[],"source":["\ndf_all_genre = df_dna_mapping.groupby(\"genre_id\").agg({'genre_id': 'count'})\nprint df_all_genre.count()\ndf_all_genre.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200410-062847_1855467482","metadata":{},"outputs":[],"source":["\ndf_all_genre = df_all_genre.select('genre_id')\ndf_store_genre = df_store_genre.select('genre_id')\ndiff_genre = df_store_genre.union(df_all_genre).subtract(df_store_genre.intersect(df_all_genre))\ndiff_genre.show()"]},{"cell_type":"code","execution_count":0,"id":"20200410-063236_181894395","metadata":{},"outputs":[],"source":["\ndf_all_genre.subtract(df_store_genre).show()"]},{"cell_type":"code","execution_count":0,"id":"20200410-063429_1445265893","metadata":{},"outputs":[],"source":["\nimport pandas as pd\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\ndef compare_genre(granularity, date):\n    df_store_genre = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.genre-est.v1/fact/granularity={}/date={}/\".format(granularity, date))\n    df_dna_mapping = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.genre_id_product_mapping.v1/dimension/\").withColumn(\"genre_id\", F.explode(\"genre_id\"))\n    \n    df_all_genre = df_dna_mapping.groupby(\"genre_id\").agg({'genre_id': 'count'})\n    \n    df_all_genre = df_all_genre.select('genre_id')\n    df_store_genre = df_store_genre.select('genre_id')\n    \n    # diff_genre = df_store_genre.union(df_all_genre).subtract(df_store_genre.intersect(df_all_genre))\n    # diff_genre.show()\n    \n    # diff_genre = df_all_genre.subtract(df_store_genre)\n    diff_genre = df_store_genre.subtract(df_all_genre)\n    # diff_genre.show()\n    return diff_genre\n\n# date=2010-07-04/--date=2020-04-04/\ndate_list = get_date_list(\"2018-01-01\", \"2020-02-29\")\n# date_list = get_date_list(\"2020-02-10\", \"2020-02-11\")\nprint \"Data length: \", len(date_list)\nlog = []\n\nschema = StructType([\n    StructField(\"genre_id\", IntegerType(), False)\n])\ndf = spark.createDataFrame([], schema)\n\nfor date in date_list:\n    res = compare_genre('daily', date) # :type res: list\n    if res.take(1):\n        # log.append((date, res))\n        log.append(date)\n        # df = log[0][1]\n        # for date, temp_df in log:\n        df = df.union(res).distinct()\n\nprint \"df:\"\ndf.show()\n\n# log  = [date for date, _ in log]\nprint log"]},{"cell_type":"code","execution_count":0,"id":"20200413-032122_720297863","metadata":{},"outputs":[],"source":["\n\nfrom conf.settings import PG_DNA_NAME, PG_DNA_ACCESS_ID, PG_DNA_HOSTS, PG_DNA_SECRET_KEY\nfrom applications.db_check_v1.common.db_check_utils import query_df\nfrom pyspark.sql.functions import explode\n\ndna_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_DNA_NAME,\n        user=PG_DNA_ACCESS_ID,\n        host=PG_DNA_HOSTS[0][0],\n        password=PG_DNA_SECRET_KEY,\n        port=PG_DNA_HOSTS[0][1]\n    )\n)\n# mapping_df_unified = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.genre_id_product_mapping.v1/dimension/\").toPandas()\nsql = 'SELECT genre_id FROM dna_genre_id_product_mapping'\nmapping_df_raw = query_df(dna_dsn, sql)\nspark_df = spark.createDataFrame(mapping_df_raw)\n# mapping_df_raw = mapping_df_raw.explode('genre_id')\nexploded_spark_df = spark_df.select(explode('genre_id').alias('genre_id'))\ngenre_count = exploded_spark_df.distinct().count()\nprint mapping_df_raw\nmapping_df_raw['created_time'] = mapping_df_raw['created_time'].dt.strftime('%Y-%m-%d')\nmapping_df_raw['last_updated_time'] = mapping_df_raw['last_updated_time'].dt.strftime('%Y-%m-%d')\nmapping_df_raw['genre_id'] = [str(map(int, l))  if l else 'None' for l in mapping_df_raw['genre_id']]\nmapping_df_raw['modifier_id'] = [str(map(int, l)) if l else 'None' for l in mapping_df_raw['modifier_id']]\n\n# mapping_df_unified['genre_id'] = mapping_df_unified['genre_id'].astype(\"str\")\n# mapping_df_unified['modifier_id'] = mapping_df_unified['modifier_id'].astype(\"str\")\n# print mapping_df_raw\n# # print mapping_df_unified\n\n# def _compare_df(df1, df2, on=None):\n#     for diff_type in [\"left\", \"right\"]:\n#         diff_df = df1.merge(df2, indicator=True, how=diff_type, on=on)  # .loc[lambda x : x['_merge']!='both']\n#         diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n#         print diff_df.empty\n#         if len(diff_df) != 0:\n#             print diff_type\n#             print diff_df\n\n\n# # # print mapping_df_raw.genre_id.dtypes\n# # # print '*'*100\n# # # print mapping_df_unified.genre_id.dtypes\n# # # print mapping_df_raw\n# # # print mapping_df_unified\n\n# _compare_df(mapping_df_raw, mapping_df_unified, on=[\"product_id\", \"genre_id\"]) # \"modifier_id\",\"last_updated_time\",  \"created_time\", \"created_by\", \"last_updated_by\", \"comments\", \n# print \"pass\""]},{"cell_type":"code","execution_count":0,"id":"20200420-033920_1700953680","metadata":{},"outputs":[],"source":["\nprint genre_count"]},{"cell_type":"code","execution_count":0,"id":"20200420-032627_49539741","metadata":{},"outputs":[],"source":["\nimport pandas as pd\npd.show_versions()"]},{"cell_type":"code","execution_count":0,"id":"20200413-031108_1089415557","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\nschema = StructType([\n    StructField(\"k\", StringType(), True), StructField(\"v\", IntegerType(), False)\n])\n\n# or df = sc.parallelize([]).toDF(schema)\n\ndf = spark.createDataFrame([], schema)"]},{"cell_type":"code","execution_count":0,"id":"20200413-031100_1497230905","metadata":{},"outputs":[],"source":["\ndf.show()"]},{"cell_type":"code","execution_count":0,"id":"20200410-091213_729112195","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\ntemp_df = spark.createDataFrame([Row(genre_id=0)])\ntemp_df.show()\ntemp_df = temp_df.union(spark.createDataFrame([Row(genre_id=302006), Row(genre_id=301007), Row(genre_id=207003), Row(genre_id=207006)]))\ntemp_df.union(spark.createDataFrame([Row(genre_id=302006), Row(genre_id=301007), Row(genre_id=207003), Row(genre_id=207006)])).distinct().show()"]},{"cell_type":"code","execution_count":0,"id":"20200410-095656_1114144372","metadata":{},"outputs":[],"source":["\nlen(log)"]},{"cell_type":"code","execution_count":0,"id":"20200410-064507_1010774574","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\ndf = spark.createDataFrame([Row(genre_id=0)])\nfor date, row_list in log[:100]:\n    temp_df = spark.createDataFrame(row_list)\n    df = df.union(temp_df).distinct()\n\ndf.show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200410-101430_484273205","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\ndf = spark.createDataFrame([Row(genre_id=0)])\nfor date, row_list in log[101:200]:\n    temp_df = spark.createDataFrame(row_list)\n    df = df.union(temp_df).distinct()\ndf.show()"]},{"cell_type":"code","execution_count":0,"id":"20200410-080748_1287842222","metadata":{},"outputs":[],"source":["\nprint log"]},{"cell_type":"code","execution_count":0,"id":"20200410-080757_1398078016","metadata":{},"outputs":[],"source":["\nimport unittest\nimport re\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df\nfrom aadatapipelinecore.core.utils.spark import create_spark\n# from conf.settings import PG_DNA_NAME, PG_DNA_ACCESS_ID, PG_DNA_HOSTS, PG_DNA_SECRET_KEY\n\n\nfrom conf import settings\ndef pg_settings(schema):\n    template = \"PG_\" + schema.upper() + \"_{property}\"\n    host, port = getattr(settings, template.format(property='HOSTS'))[0]\n    return (\n        \"dbname='{db}' user='{user}' password='{password}' \"\n        \"host='{host}' port='{port}'\".format(\n            db=getattr(settings, template.format(property='NAME')),\n            user=getattr(settings, template.format(property='ACCESS_ID')),\n            host=host,\n            password=getattr(settings, template.format(property='SECRET_KEY')),\n            port=port\n        )\n    )\n\nclass GenreEstUnifiedData(object):\n    \"\"\"\n    Get unified data\n    \"\"\"\n    _s3_bucket_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n                      \"store.genre-est.v1/fact/granularity=daily/date={}/\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date):\n        return self.spark.read.parquet(self._s3_bucket_path.format(date))\n\n\nclass DNAGenreIdProductMappingData(object):\n    \"\"\"\n    Get mapping data (DNA) from unified level\n    \"\"\"\n    _s3_bucket_path = \"s3://b2c-prod-data-pipeline-unified-dna/unified/\" \\\n                      \"dna.genre_id_product_mapping.v1/dimension/\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self):\n        return self.spark.read.parquet(self._s3_bucket_path)\n\n\nclass DNAMappingDBData(object):\n    \"\"\"\n    Get mapping data (DNA) from DB\n    \"\"\"\n    _pg_dsn = pg_settings('DNA')\n\n    def get(self, sql):\n        \"\"\"\n        :param sql: sql str\n        :return: mapping data\n        :rtype: pandas dataframe\n        \"\"\"\n        return query_df(self._pg_dsn, sql)\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    return [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n\n\ndef compare_df(df1, df2, on=None):\n    for diff_type in [\"left\", \"right\"]:\n        diff_df = df1.merge(df2, indicator=True, how=diff_type, on=on)  # .loc[lambda x : x['_merge']!='both']\n        diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n        if len(diff_df) != 0:\n            return diff_type, diff_df\n        return None, None\n\nclass TestGameIQMarketSizeFullRestatement(PipelineTest):\n    trigger_date_config = ('* * * * *', 0)\n\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext\n\n    def _compare_genre(self, date):\n        \"\"\"\n        :param date: date need to compare\n        :return: genre found in store_genre, but not in dna_mapping\n        :rtype diff_genre: spark.Dataframe\n        \"\"\"\n        df_store_genre = GenreEstUnifiedData(self.spark).get(date)\n        df_dna_mapping = DNAGenreIdProductMappingData(self.spark).get()\n        df_dna_mapping = df_dna_mapping.withColumn(\"genre_id\", F.explode(\"genre_id\"))\n\n        df_all_genre = df_dna_mapping.groupby(\"genre_id\").agg({'genre_id': 'count'}).select('genre_id')\n        df_store_genre = df_store_genre.select('genre_id')\n        # diff_genre = df_all_genre.subtract(df_store_genre)\n        diff_genre = df_store_genre.subtract(df_all_genre)\n\n        return diff_genre\n\n    # def test_game_iq_market_size_full_restatement(self):\n    #     # date_list = get_date_list(\"2019-07-04\", self.check_date_str)\n    #     date_list = get_date_list(\"2020-02-10\", \"2020-02-11\")\n    #     log = []\n    #     schema = StructType([\n    #         StructField(\"genre_id\", IntegerType(), False)\n    #     ])\n    #     unexpected_df = self.spark.createDataFrame([], schema)  # Create a empty spark.Dataframe\n    #     for date in date_list:\n    #         try:\n    #             res = self._compare_genre(date)\n    #             if res.take(1):\n    #                 log.append(date)\n    #                 # unexpected_df = unexpected_df.union(res).distinct()\n    #         except Exception as e:\n    #             if not re.search(r'Path does not exist:', str(e)):  # skip date that have no data.\n    #                 print e\n\n    #     self.assertEqual(log, [], msg=\"Unexpected genre {} in {}\".format(\n    #         [row['genre_id'] for row in unexpected_df.collect()], log\n    #     ))\n\nunittest.main(argv=[''], verbosity=2)\n%tb"]},{"cell_type":"code","execution_count":0,"id":"20200413-081441_1079311630","metadata":{},"outputs":[],"source":["\nfrom conf.settings import *\nfrom applications.db_check_v1.common.db_check_utils import query_df, query\nimport pandas as pd\npd.set_option('expand_frame_repr', False)\n\ndate = \"2020-01-01\"\nsql = \"\"\"\nselect device_id, store_id, date, genre_id, modifier_id, download, revenue from plproxy.execute_select_nestloop($proxy$ \n    select device_id, store_id, date, genre_id, modifier_id, download, revenue\n    from aa.genre_store_daily_estimate\n    where \n        date = '{}'\n$proxy$) tbl (device_id SMALLINT, store_id INT, date DATE , genre_id BIGINT, modifier_id BIGINT, download BIGINT, revenue BIGINT);\n\n\"\"\".format(date)\ndaily_est_dsn =(\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_DAILY_EST_NAME,\n        user=PG_DAILY_EST_ACCESS_ID,\n        host=PG_DAILY_EST_HOSTS[0][0],\n        password=PG_DAILY_EST_SECRET_KEY,\n        port=PG_DAILY_EST_HOSTS[0][1]\n    )\n)\n\ndb_df = query_df(daily_est_dsn, sql)\ndb_data = query(daily_est_dsn, sql)\n\ndb_genre = set([])\nfor row in db_data:\n    db_genre.add(row[3])\nprint len(db_genre), db_genre\n\n# db_df = db_df.loc[:, 'genre_id']\n# db_set = set(db_df.values.tolist())\n# print db_set"]},{"cell_type":"code","execution_count":0,"id":"20200414-105353_631031527","metadata":{},"outputs":[],"source":["\nmapping_data = DNAGenreIdProductMappingData(spark).get()"]},{"cell_type":"code","execution_count":0,"id":"20200414-123330_1390449493","metadata":{},"outputs":[],"source":["\nmapping_data = DNAGenreIdProductMappingData(spark).get()\nmapping_data = mapping_data.withColumn('genre_id', F.explode('genre_id')).select('genre_id').groupby('genre_id').count().select('genre_id').toPandas()\n# mapping_data = mapping_data.loc[:, 'genre_id']\n# all_mapping_data = set(mapping_data.values.tolist())\n# print all_mapping_data\nprint mapping_data"]},{"cell_type":"code","execution_count":0,"id":"20200414-121231_2090038467","metadata":{},"outputs":[],"source":["\nfloat(len(all_mapping_data - db_set)) / float(len(all_mapping_data))"]},{"cell_type":"code","execution_count":0,"id":"20200414-124535_1051223458","metadata":{},"outputs":[],"source":["\nlen(all_mapping_data)"]},{"cell_type":"code","execution_count":0,"id":"20200415-120811_705253027","metadata":{},"outputs":[],"source":["\n# ALL MAPPING DATA\nmapping_data = DNAGenreIdProductMappingData(spark).get()\nmapping_data = mapping_data.withColumn('genre_id', F.explode('genre_id')).select('genre_id').groupby('genre_id').count().select('genre_id').toPandas().loc[:, 'genre_id']\nall_mapping_data = set(mapping_data.values.tolist())\nprint all_mapping_data\n"]},{"cell_type":"code","execution_count":0,"id":"20200414-124636_2105788890","metadata":{},"outputs":[],"source":["\nfrom conf.settings import *\nfrom applications.db_check_v1.common.db_check_utils import query_df, query\nimport pandas as pd\n\nsql = \"\"\"\nselect device_id, store_id, date, genre_id, modifier_id, download, revenue from plproxy.execute_select_nestloop($proxy$ \n    select device_id, store_id, date, genre_id, modifier_id, download, revenue\n    from aa.genre_store_daily_estimate\n    where date = '{}'\n$proxy$) tbl (device_id SMALLINT, store_id INT, date DATE , genre_id BIGINT, modifier_id BIGINT, download BIGINT, revenue BIGINT);\n\"\"\"\ndaily_est_dsn =(\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_DAILY_EST_NAME,\n        user=PG_DAILY_EST_ACCESS_ID,\n        host=PG_DAILY_EST_HOSTS[0][0],\n        password=PG_DAILY_EST_SECRET_KEY,\n        port=PG_DAILY_EST_HOSTS[0][1]\n    )\n)\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    return [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n\ndef get_date_range(s_str, e_str, freq=\"Y\"):\n    from datetime import datetime\n    from dateutil.relativedelta import relativedelta\n    s = s_str.split('-')\n    e = e_str.split('-')\n    date_list = []\n    start_date = datetime(int(s[0]), int(s[1]), int(s[2]))\n    end_date = datetime(int(e[0]), int(e[1]), int(e[2]))\n    if freq == \"Y\":\n        while start_date <= end_date:\n            date_list.append(start_date.strftime(\"%Y-%m-%d\"))\n            start_date += relativedelta(years=+1)\n    elif freq == \"M\":\n        while start_date <= end_date:\n            date_list.append(start_date.strftime(\"%Y-%m-%d\"))\n            start_date += relativedelta(months=+1)\n    elif freq == \"D\":\n        while start_date <= end_date:\n            date_list.append(start_date.strftime(\"%Y-%m-%d\"))\n            start_date += relativedelta(days=+1)\n    else:\n        raise TypeError(\"freq must in ['Y', 'M', 'D']\")\n\n    return date_list\n\n\ndef temp_log_to_s3(log, name):\n    import boto3\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'tom/gameiq/{}.txt'.format(name))\n    log_str = \"\"\n    for k, v in log.items():\n        log_str += k + ',' + str(v) + ';'\n    log_str = log_str[:-1]\n    s3object.put(Body=str(log_str))\n\n    return log_str\n\ndef s3_to_temp_log(name):\n    import boto3\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'tom/gameiq/{}.txt'.format(name))\n    body = s3object.get()['Body'].read()\n    res = body.split(';')\n    res = [i.split(',') for i in res]\n    res = {i: int(j) for i, j in res}\n\n    return res\n\n\ndef get_date_and_count(date_list):\n    # all_mapping_data_length = float(len(all_mapping_data))\n    res = {}\n    for date in date_list:\n        db_data = query(daily_est_dsn, sql.format(date))\n        if not db_data:\n            continue\n        db_set = set([])\n        for row in db_data:\n            db_set.add(row[3])\n    \n        # diff_len = float(len(all_mapping_data - db_set))\n        # ratio = diff_len / all_mapping_data_length\n        # print date, ratio\n        # if ratio > 0.15:\n        db_set_len = len(db_set)\n        # if db_set_len <= 46 - 20 or db_set_len > 46:\n        res[date[:4]] =  db_set_len\n    return res\n    \n\n\nSPECIAL_DATE = ['2010-07-04', 37]\ndate_list = get_date_range('2010-01-01', '2020-04-16', freq='Y')\nmin_genre_count = get_date_and_count(date_list)\nmin_genre_count[SPECIAL_DATE[0][:4]] = SPECIAL_DATE[1]\nprint \"min_genre_count:\", min_genre_count\ntemp_log_to_s3(min_genre_count, 'genre_min_count')\n\ndate_list = get_date_range('2010-12-31', '2020-04-16', freq='Y')\nmax_genre_count = get_date_and_count(date_list)\nprint \"max_genre_count:\", max_genre_count\ntemp_log_to_s3(max_genre_count, 'genre_max_count')\n# min_genre_count = {'2020': 122, '2019': 123, '2018': 119, '2015': 108, '2014': 98, '2017': 115, '2016': 111, '2011': 46, '2010': 37, '2013': 90, '2012': 76}\n# Above code gets all year's min genre count\n\n# Next, I need to verify each day from 2010-07-04 to 2020-04-04\ndef verify_each_day(date_list, min_genre_count, max_genre_count, threshold=30):\n    for date in date_list:\n        db_data = query(daily_est_dsn, sql.format(date))\n        if not db_data:\n            continue\n        db_set = set([])\n        for row in db_data:\n            db_set.add(row[3])  # genre_id is the 3rd column\n        db_set_len = len(db_set)\n        lower_limit = min_genre_count[date[:4]]\n        if date[:4] in max_genre_count:\n            upper_limit = max_genre_count[date[:4]]\n            if db_set_len > upper_limit:\n                print \"out of UPPER limit...\"\n                print date, \"genre id count:\", db_set_len, \"expected count: less than\", upper_limit\n        else:\n            if db_set_len > lower_limit + threshold:\n                print \"out of UPPER limit...( > lower + {} )\".format(threshold)\n                print date, \"genre id count:\", db_set_len, \"expected count: less than\", lower_limit + threshold\n        if db_set_len < lower_limit - 5:\n            print \"out of LOWER limit...\"\n            print date, \"genre id count:\", db_set_len, \"expected count: greater than\", lower_limit - 5\n\ndate_list = get_date_range('2020-04-01', '2020-04-17', freq='D')\nverify_each_day(date_list, s3_to_temp_log('genre_min_count'), s3_to_temp_log('genre_max_count'))\nprint \"END\"\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-034834_1080634252","metadata":{},"outputs":[],"source":["\n\ndef get_date_range(s_str, e_str, freq=\"Y\"):\n    from datetime import datetime\n    from dateutil.relativedelta import relativedelta\n    s = s_str.split('-')\n    e = e_str.split('-')\n    date_list = []\n    start_date = datetime(int(s[0]), int(s[1]), int(s[2]))\n    end_date = datetime(int(e[0]), int(e[1]), int(e[2]))\n    if freq == \"Y\":\n        while start_date <= end_date:\n            date_list.append(start_date.strftime(\"%Y-%m-%d\"))\n            start_date += relativedelta(years=+1)\n    elif freq == \"M\":\n        while start_date <= end_date:\n            date_list.append(start_date.strftime(\"%Y-%m-%d\"))\n            start_date += relativedelta(months=+1)\n    elif freq == \"D\":\n        while start_date <= end_date:\n            date_list.append(start_date.strftime(\"%Y-%m-%d\"))\n            start_date += relativedelta(days=+1)\n    else:\n        raise TypeError(\"freq must in ['Y', 'M', 'D']\")\n    return date_list\n\n# print get_date_range('2010-01-10', '2010-01-12', freq='M')\n\n\nlog = {'2020': 122, '2019': 123, '2018': 119, '2015': 108, '2014': 98, '2017': 115, '2016': 111, '2011': 46, '2010': 37, '2013': 90, '2012': 76}\ndef temp_log_to_s3(log, name):\n    import boto3\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'tom/gameiq/{}.txt'.format(name))\n    log_str = \"\"\n    for k, v in log.items():\n        log_str += k + ',' + str(v) + ';'\n    log_str = log_str[:-1]\n    s3object.put(Body=str(log_str))\n    return log_str\n\ndef s3_to_temp_log(name):\n    import boto3\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'tom/gameiq/{}.txt'.format(name))\n    body = s3object.get()['Body'].read()\n    res = body.split(';')\n    res = [i.split(',') for i in res]\n    res = {i: int(j) for i, j in res}\n\n    return res\n\n# print temp_log_to_s3(log, 'genre_max_count')\nprint s3_to_temp_log('genre_max_count')"]},{"cell_type":"code","execution_count":0,"id":"20200416-073550_301919682","metadata":{},"outputs":[],"source":["%%sh\naws s3 cp s3://b2c-prod-data-pipeline-qa/aa.store.game_iq/genre_min_count.txt -\n"]},{"cell_type":"code","execution_count":0,"id":"20200711-062132_522562737","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h 10.2.25.198  -U app_bdp_usage_qa -d dna -p 6432 << EOF \nset search_path=public;\nselect * from dna_genre_id_product_mapping limit 3;\n-- select count(1) from ( SELECT distinct unnest(genre_id) FROM dna_genre_id_product_mapping ) as prod;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200415-055815_854851678","metadata":{},"outputs":[],"source":["%%sh\n\n#\\d+ aa.genre_store_daily_estimate_1000_2020\n\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \n-- \\d+ aa.genre_store_daily_estimate;\n\nselect distinct date, genre_id from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select distinct date, genre_id \n    from aa.genre_store_daily_estimate\n    where date = '2020-07-03'\n    LIMIT 5;\n\\$proxy\\$) tbl (date DATE, genre_id BIGINT)\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200711-113339_1645596641","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \nselect date,count(distinct(genre_id)) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select date,genre_id\n    from aa.genre_store_daily_estimate\n    where \n        date between '2019-01-01' and '2019-05-31'\n    group by date, genre_id\n\\$proxy\\$) tbl ( date DATE, genre_id BIGINT) group by date;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200414-125347_1326090058","metadata":{},"outputs":[],"source":["\n# ALL MAPPING DATA\nmapping_data = DNAGenreIdProductMappingData(spark).get()\nmapping_data = mapping_data.withColumn('genre_id', F.explode('genre_id')).select('genre_id').toPandas().loc[:, 'genre_id']\nall_mapping_data = set(mapping_data.values.tolist())\nprint all_mapping_data"]},{"cell_type":"code","execution_count":0,"id":"20200415-054427_1381413757","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")"]},{"cell_type":"code","execution_count":0,"id":"20200415-052800_76917792","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2020 App Annie Inc. All rights reserved.\n\"\"\"\nGame IQ - Market Size - Pre Aggregation full restatement Cases\n\"\"\"\nimport unittest\nimport re\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df, query\nfrom aadatapipelinecore.core.utils.spark import create_spark\n\n\n\nclass GenreEstUnifiedData(object):\n    \"\"\"\n    Get unified data\n    \"\"\"\n    _s3_bucket_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n                      \"store.genre-est.v1/fact/granularity=daily/date={}/\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date):\n        return self.spark.read.parquet(self._s3_bucket_path.format(date))\n\n\nclass GenreEstDBData(object):\n    \"\"\"\n    Get genre data from DB\n    \"\"\"\n    _pg_dsn =(\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_DAILY_EST_NAME,\n        user=PG_DAILY_EST_ACCESS_ID,\n        host=PG_DAILY_EST_HOSTS[0][0],\n        password=PG_DAILY_EST_SECRET_KEY,\n        port=PG_DAILY_EST_HOSTS[0][1]\n        )\n    )\n\n    def get(self, sql):\n        \"\"\"\n        :param sql: sql string\n        :return: sql result\n        :type sql: str\n        :rtype: list\n        \"\"\"\n        return query(self._pg_dsn, sql)\n\n\nclass DNAGenreIdProductMappingData(object):\n    \"\"\"\n    Get mapping data (DNA) from unified level\n    \"\"\"\n    _s3_bucket_path = \"s3://b2c-prod-data-pipeline-unified-dna/unified/\" \\\n                      \"dna.genre_id_product_mapping.v1/dimension/\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self):\n        return self.spark.read.parquet(self._s3_bucket_path)\n\n\nclass DNAMappingDBData(object):\n    \"\"\"\n    Get mapping data (DNA) from DB\n    \"\"\"\n    _pg_dsn = pg_settings('DNA')\n\n    def get(self, sql):\n        \"\"\"\n        :param sql: sql str\n        :type sql: str\n        :return: mapping data\n        :rtype: pandas dataframe\n        \"\"\"\n        return query_df(self._pg_dsn, sql)\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    return [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n\n\ndef compare_df(df1, df2, on=None):\n    for diff_type in [\"left\", \"right\"]:\n        diff_df = df1.merge(df2, indicator=True, how=diff_type, on=on)  # .loc[lambda x : x['_merge']!='both']\n        diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n        if not diff_df.empty:\n            return diff_type, diff_df\n        return None, None\n\n\nclass TestGameIQMarketSizeFullRestatement(PipelineTest):\n    trigger_date_config = ('* * * * 6', 0)\n    \n    @classmethod\n    def setUpClass(cls):\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext    \n\n    def _compare_genre(self, date):\n        \"\"\"\n        :param date: date need to compare\n        :type date: str\n        :return: genre found in store_genre, but not in dna_mapping\n        :rtype: spark.Dataframe\n        \"\"\"\n        df_store_genre = GenreEstUnifiedData(self.spark).get(date)\n        df_dna_mapping = DNAGenreIdProductMappingData(self.spark).get()\n        df_dna_mapping = df_dna_mapping.withColumn(\"genre_id\", F.explode(\"genre_id\"))\n\n        df_all_genre = df_dna_mapping.groupby(\"genre_id\").agg({'genre_id': 'count'}).select('genre_id')\n        df_store_genre = df_store_genre.select('genre_id')\n        # diff_genre = df_all_genre.subtract(df_store_genre)\n        diff_genre = df_store_genre.subtract(df_all_genre)\n\n        return diff_genre\n\n    def test_full_restatement_new(self):\n        # ALL MAPPING DATA\n        mapping_data = DNAGenreIdProductMappingData(self.spark).get()\n        mapping_data = mapping_data.withColumn('genre_id', F.explode('genre_id')).select('genre_id')\n        mapping_data = mapping_data.toPandas().loc[:, 'genre_id']\n        all_mapping_data = set(mapping_data.values.tolist())\n        sql = \"\"\"\n        select device_id, store_id, date, genre_id, modifier_id, download, revenue \n        from plproxy.execute_select_nestloop($proxy$ \n            select device_id, store_id, date, genre_id, modifier_id, download, revenue\n            from aa.genre_store_daily_estimate\n            where date='{}'\n        $proxy$) tbl (device_id SMALLINT, store_id INT, date DATE , \n        genre_id BIGINT, modifier_id BIGINT, download BIGINT, revenue BIGINT);\n        \"\"\"\n        all_mapping_data_length = float(len(all_mapping_data))\n        date_list = get_date_list('2020-03-04', self.check_date_str)\n        for date in date_list:\n            db_data = GenreEstDBData().get(sql.format(date))\n            if not db_data:\n                continue\n            db_set = set([])\n            for row in db_data:\n                db_set.add(row[3])\n\n            diff_len = float(len(all_mapping_data - db_set))\n            ratio = diff_len / all_mapping_data_length\n            if ratio > 0.15:\n                print date, diff_len\n\nunittest.main(argv=[''], verbosity=2)\n%tb"]},{"cell_type":"code","execution_count":0,"id":"20200415-054432_1650887088","metadata":{},"outputs":[],"source":["\n\nlog = {\n    '2010-12-31': 46,\n    '2011-12-31': 71,\n    '2012-12-31': 90,\n    '2013-12-31': 98,\n    '2014-12-31': 108,\n    '2015-12-31': 111,\n    '2016-12-31': 115,\n    '2017-12-31': 119,\n    '2018-12-31': 123,\n    '2019-12-31': 122\n}\ndef temp_log_to_s3(log, name):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'aa.store.game_iq/{}.json'.format(name))\n    # log_str = \"\"\n    # for k, v in log.items():\n    #     log_str += k + ',' + str(v) + ';'\n    # log_str = log_str[:-1]\n    s3object.put(Body=json.dumps(log))\n\n    return json.dumps(log)\n\n\ndef s3_to_temp_log(name):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'aa.store.game_iq/{}.json'.format(name))\n    body = s3object.get()['Body'].read()\n    print body\n    body = json.loads(body)\n    # res = body.split(';')\n    # res = [i.split(',') for i in res]\n    # res = {i: int(j) for i, j in res}\n\n    return body\n\n# def s3_to_temp_log():\n#     import boto3\n#     s3 = boto3.resource('s3')\n#     s3object = s3.Object('b2c-prod-data-pipeline-qa', 'tom/gameiq/test1.txt')\n#     body = s3object.get()['Body'].read()\n#     res = body.split(';')\n#     res = [i.split(',') for i in res]\n#     res = [(i, int(j)) for i, j in res]\n#     return res\n\n# print temp_log_to_s3(log, \"genre_min_count\")\nprint s3_to_temp_log('genre_min_count')"]},{"cell_type":"code","execution_count":0,"id":"20200416-031303_1173557945","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2020 App Annie Inc. All rights reserved.\n\"\"\"\nGame IQ - Market Size - Pre Aggregation full restatement Cases\n\"\"\"\n\nimport unittest\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df, query\nfrom conf.settings import *\nfrom aadatapipelinecore.core.utils.spark import create_spark\nfrom applications.db_check_v1.common.utils import get_date_list\n\n\nclass GenreEstDBData(object):\n    \"\"\"\n    Get genre data from DB\n    \"\"\"\n    _pg_dsn =(\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_DAILY_EST_NAME,\n        user=PG_DAILY_EST_ACCESS_ID,\n        host=PG_DAILY_EST_HOSTS[0][0],\n        password=PG_DAILY_EST_SECRET_KEY,\n        port=PG_DAILY_EST_HOSTS[0][1]\n        )\n    )\n\n    def get(self, sql):\n        \"\"\"\n        :param sql: sql string\n        :return: sql result\n        :type sql: str\n        :rtype: list\n        \"\"\"\n        return query(self._pg_dsn, sql)\n\n\nclass DNAMappingDBData(object):\n    \"\"\"\n    Get mapping data (DNA) from DB\n    \"\"\"\n    _pg_dsn =(\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_DNA_NAME,\n        user=PG_DNA_ACCESS_ID,\n        host=PG_DNA_HOSTS[0][0],\n        password=PG_DNA_SECRET_KEY,\n        port=PG_DNA_HOSTS[0][1]\n        )\n    )\n\n    def get(self, sql):\n        \"\"\"\n        :param sql: sql str\n        :type sql: str\n        :return: mapping data\n        :rtype: pandas dataframe\n        \"\"\"\n        return query_df(self._pg_dsn, sql)\n\ndef temp_log_to_s3(log, name):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'aa.store.game_iq/{}.json'.format(name))\n    s3object.put(Body=json.dumps(log))\n\n    return json.dumps(log)\n\n\ndef s3_to_temp_log(name):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'aa.store.game_iq/{}.json'.format(name))\n    body = s3object.get()['Body'].read()\n    body = json.loads(body)\n\n    return body\n\ndef get_date_and_count(sql, date_list):\n    res = {}\n    for date in date_list:\n        db_data = GenreEstDBData().get(sql.format(date))\n        if not db_data:\n            continue\n        db_set = set([])\n        for row in db_data:\n            db_set.add(row[3])\n        db_set_len = len(db_set)\n        res[date[:4]] = db_set_len\n    return res\n\nclass TestGameIQMarketSizeFullRestatement(PipelineTest):\n    trigger_date_config = ('30 8 * * 6', 0)\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext\n\n    def verify_each_day(self, spark, sql, date_list, min_genre_count, threshold=5):\n        \"\"\"\n        Regression test:\n        To make sure genre_id count in one year is less than this year's last day (12/31),\n        and greater than this year's first day (01/01).\n\n        :param threshold:\n        To ignore some minor error, there is a threshold (default value is 5):\n            LOWER limit - 5 < genre_id count (daily) < UPPER limit + 5\n\n        LOWER limit: Derive from last routine test. (Store in \"s3://b2c-prod-data-pipeline-qa/aa.store.game_iq.json\n        UPPER limit: All distinct genre_id count from DB.\n        \"\"\"\n        mapping_sql = 'SELECT genre_id FROM dna_genre_id_product_mapping'\n        mapping_df_raw = DNAMappingDBData().get(mapping_sql)\n        spark_df = spark.createDataFrame(mapping_df_raw)\n        exploded_spark_df = spark_df.select(explode('genre_id').alias('genre_id'))\n        upper_limit = exploded_spark_df.distinct().count()\n        for date in date_list:\n            db_data = GenreEstDBData().get(sql.format(date))\n            if not db_data:\n                continue\n            db_set = set([])\n            for row in db_data:\n                db_set.add(row[1])  # genre_id is the 2nd column\n            db_set_len = len(db_set)\n\n            # if check date is January 1st, there is no historical data to indicate lower limit,\n            # so I use last year lower limit as this year's lower limit.\n            if date[:4] in min_genre_count:\n                lower_limit = min_genre_count[date[:4]]\n            else:\n                temp_date = str(int(date[:4]) - 1)  # last year\n                print temp_date\n                lower_limit = min_genre_count[temp_date]\n\n            self.assertTrue(db_set_len <= upper_limit + threshold,\n                            msg=\"out of UPPER limit... {}, genre id count:, {}, \"\n                                \"expected count: less than {}\".format(date, db_set_len, upper_limit))\n\n            self.assertTrue(db_set_len >= lower_limit - threshold,\n                            msg=\"out of LOWER limit... {}, genre id count:, {}, \"\n                                \"expected count: less than {}\".format(date, db_set_len, lower_limit))\n\n    def test_full_restatement(self):\n        \"\"\"\n        scope :\n                dimension completeness : date, genre id\n        method:\n                check docstring in method \"verify_each_day\"\n        date: regression all date\n        \"\"\"\n        sql = '''\n            select distinct date, genre_id\n            from plproxy.execute_select_nestloop($proxy$\n            select distinct date, genre_id\n            from aa.genre_store_daily_estimate\n            where date = '{}'\n            $proxy$) tbl (date DATE, genre_id BIGINT);\n            '''\n        # Verify each day from 2010-07-04 to check_date\n        date_list = get_date_list('2020-03-01', self.check_date_str, freq='D')\n        print self.check_date_str\n        self.verify_each_day(self.spark, sql, date_list, s3_to_temp_log('genre_min_count'))\n\n        # Add the latest genre_id count to s3 for next db check.\n        # SPECIAL_DATE: I use the first day of each year as lower limit,\n        # but this data set is from 2010-07-04, so I manually add this special relation\n        # to log after test.\n        SPECIAL_DATE = ['2010-07-04', 37]\n        date_list = get_date_list('2010-01-01', self.check_date_str, freq='YS')\n        min_genre_count = get_date_and_count(sql, date_list)\n        min_genre_count[SPECIAL_DATE[0][:4]] = SPECIAL_DATE[1]\n        temp_log_to_s3(min_genre_count, 'genre_min_count')\n\n        # date_list = get_date_list('2010-12-31', self.check_date_str, freq='Y')\n        # # max_genre_count = {'2020': 122, '2019': 123, '2018': 119 ... ...}\n        # max_genre_count = get_date_and_count(sql, date_list)\n        # temp_log_to_s3(max_genre_count, 'genre_max_count')\n\n\nunittest.main(argv=[''], verbosity=2)\n%tb"]},{"cell_type":"code","execution_count":0,"id":"20200420-081645_1141239961","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/aa.store.game_iq/\naws s3 cp s3://b2c-prod-data-pipeline-qa/aa.store.game_iq/genre_min_count.json -"]},{"cell_type":"code","execution_count":0,"id":"20200420-023934_1192423894","metadata":{},"outputs":[],"source":["\ndef temp_log_to_s3(log, name):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'aa.store.game_iq/{}.json'.format(name))\n    # log_str = \"\"\n    # for k, v in log.items():\n    #     log_str += k + ',' + str(v) + ';'\n    # log_str = log_str[:-1]\n    s3object.put(Body=json.dumps(log))\n\n    return json.dumps(log)\n\n\ndef s3_to_temp_log(name):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'aa.store.game_iq/{}.json'.format(name))\n    body = s3object.get()['Body'].read()\n    print body\n    body = json.loads(body)\n    # res = body.split(';')\n    # res = [i.split(',') for i in res]\n    # res = {i: int(j) for i, j in res}\n\n    return body\nSPECIAL_DATE = ['2010-07-04', 37]\nsql = '''\n    select distinct date, genre_id\n    from plproxy.execute_select_nestloop($proxy$\n    select distinct date, genre_id\n    from aa.genre_store_daily_estimate\n    where date = '{}'\n    $proxy$) tbl (date DATE, genre_id BIGINT);\n    '''\ndate_list = get_date_list('2010-01-01', '2020-04-18', freq='YS')\nprint date_list\nmin_genre_count = get_date_and_count(sql, date_list)\nmin_genre_count[SPECIAL_DATE[0][:4]] = SPECIAL_DATE[1]\nprint temp_log_to_s3(min_genre_count, 'genre_min_count')"]},{"cell_type":"code","execution_count":0,"id":"20200416-031258_134752034","metadata":{},"outputs":[],"source":["\n{\"2020\": 122, \"2019\": 123, \"2018\": 119, \"2015\": 108, \"2014\": 98, \"2017\": 115, \"2016\": 111, \"2011\": 48, \"2010\": 37, \"2013\": 90, \"2012\": 77}"]},{"cell_type":"code","execution_count":0,"id":"20200416-081648_694629331","metadata":{},"outputs":[],"source":["\n\nfrom conf.settings import PG_DNA_NAME, PG_DNA_ACCESS_ID, PG_DNA_HOSTS, PG_DNA_SECRET_KEY\nfrom applications.db_check_v1.common.db_check_utils import query_df\nfrom pyspark.sql.functions import explode\n\ndna_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_DNA_NAME,\n        user=PG_DNA_ACCESS_ID,\n        host=PG_DNA_HOSTS[0][0],\n        password=PG_DNA_SECRET_KEY,\n        port=PG_DNA_HOSTS[0][1]\n    )\n)\n# mapping_df_unified = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.genre_id_product_mapping.v1/dimension/\").toPandas()\nsql = 'SELECT genre_id FROM dna_genre_id_product_mapping'\nmapping_df_raw = query_df(dna_dsn, sql)\nspark_df = spark.createDataFrame(mapping_df_raw)\nspark_df.show(3)\n# mapping_df_raw = mapping_df_raw.explode('genre_id')\nexploded_spark_df = spark_df.select(explode('genre_id').alias('genre_id'))\ngenre_count = exploded_spark_df.distinct().count()\n\n# mapping_df_raw['created_time'] = mapping_df_raw['created_time'].dt.strftime('%Y-%m-%d')\n# mapping_df_raw['last_updated_time'] = mapping_df_raw['last_updated_time'].dt.strftime('%Y-%m-%d')\n# mapping_df_raw['genre_id'] = [str(map(int, l))  if l else 'None' for l in mapping_df_raw['genre_id']]\n# mapping_df_raw['modifier_id'] = [str(map(int, l)) if l else 'None' for l in mapping_df_raw['modifier_id']]\n\n# mapping_df_unified['genre_id'] = mapping_df_unified['genre_id'].astype(\"str\")\n# mapping_df_unified['modifier_id'] = mapping_df_unified['modifier_id'].astype(\"str\")\n# print mapping_df_raw\n# # print mapping_df_unified\n\n# def _compare_df(df1, df2, on=None):\n#     for diff_type in [\"left\", \"right\"]:\n#         diff_df = df1.merge(df2, indicator=True, how=diff_type, on=on)  # .loc[lambda x : x['_merge']!='both']\n#         diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n#         print diff_df.empty\n#         if len(diff_df) != 0:\n#             print diff_type\n#             print diff_df\n\n\n# # # print mapping_df_raw.genre_id.dtypes\n# # # print '*'*100\n# # # print mapping_df_unified.genre_id.dtypes\n# # # print mapping_df_raw\n# # # print mapping_df_unified\n\n# _compare_df(mapping_df_raw, mapping_df_unified, on=[\"product_id\", \"genre_id\"]) # \"modifier_id\",\"last_updated_time\",  \"created_time\", \"created_by\", \"last_updated_by\", \"comments\", \n# print \"pass\""]},{"cell_type":"code","execution_count":0,"id":"20200420-055800_1267336555","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h 10.2.25.198  -U app_bdp_usage_qa -d dna -p 6432 << EOF \nset search_path=public;\nselect * from public.in_app_dna_info_mapping limit 3;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200420-055814_554317081","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}