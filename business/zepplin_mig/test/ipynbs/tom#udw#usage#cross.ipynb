{"cells":[{"cell_type":"code","execution_count":0,"id":"20201001-043113_108878602","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n-- adhoc\n\n-- total\n\nselect kpi, sum(count_a) as app_count, sum(sum_e) as sum_e from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select kpi, count(estimate) as count_a, sum(estimate) as sum_e\n    from ca.app_monthly\n    where \n        date ='2020-06-30'\n    group by kpi order by kpi desc\n\\$proxy\\$) tpl (kpi smallint, count_a bigint, sum_e double precision ) group by kpi order by kpi desc ;\n\n\n\nEOF\n\n    # SELECT count, up_sum, affinity_sum FROM (select kpi, count(estimate) as count, sum(estimate) as estimate from cross_a_a_legacy group by kpi)\n    # PIVOT ( sum(estimate) as sum FOR kpi IN (9 up_sum, 28 affinity_sum))\n    \n# select kpi, sum(count_a) as app_count, sum(sum_e) as sum_e from plproxy.execute_select_nestloop(\\$proxy\\$ \n#     select kpi, count(estimate) as count_a, sum(estimate) as sum_e\n#     from ca.app_monthly\n#     where \n#         date ='2020-06-30'\n#     group by kpi order by kpi desc\n# \\$proxy\\$) tpl (kpi smallint, count_a bigint, sum_e double precision ) group by kpi order by kpi desc ;\n\n#estimate DOUBLE PRECISION\n"]},{"cell_type":"code","execution_count":0,"id":"20201002-094024_1304830166","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n-- adhoc\n\n-- total\n\nselect sum(count) as count, sum(affinity_sum) as affinity_sum,  sum(up_sum) as up_sum from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select count(*) as count, sum(est_cross_product_affinity) as affinity_sum,  sum(est_cross_product_usage_penetration) as up_sum\n    from mw.domain_x_app_m\n    where \n        date ='2020-06-30'\n\\$proxy\\$) tpl (count bigint, affinity_sum real,up_sum real ) ;\n\n\n\nEOF\n\n    # SELECT count, up_sum, affinity_sum FROM (select kpi, count(estimate) as count, sum(estimate) as estimate from cross_a_a_legacy group by kpi)\n    # PIVOT ( sum(estimate) as sum FOR kpi IN (9 up_sum, 28 affinity_sum))\n    \n# select kpi, sum(count_a) as app_count, sum(sum_e) as sum_e from plproxy.execute_select_nestloop(\\$proxy\\$ \n#     select kpi, count(estimate) as count_a, sum(estimate) as sum_e\n#     from ca.app_monthly\n#     where \n#         date ='2020-06-30'\n#     group by kpi order by kpi desc\n# \\$proxy\\$) tpl (kpi smallint, count_a bigint, sum_e double precision ) group by kpi order by kpi desc ;\n\n#estimate DOUBLE PRECISION\n"]},{"cell_type":"code","execution_count":0,"id":"20201002-094112_1048238285","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n-- adhoc\n\n-- total\n\nselect sum(count) as count, sum(affinity_sum) as affinity_sum,  sum(up_sum) as up_sum from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select count(*) as count, sum(est_cross_product_affinity) as affinity_sum,  sum(est_cross_product_usage_penetration) as up_sum\n    from mw.app_x_domain_m\n    where \n        date ='2020-06-30'\n\\$proxy\\$) tpl (count bigint, affinity_sum real,up_sum real ) ;\n\n\n\nEOF\n\n    # SELECT count, up_sum, affinity_sum FROM (select kpi, count(estimate) as count, sum(estimate) as estimate from cross_a_a_legacy group by kpi)\n    # PIVOT ( sum(estimate) as sum FOR kpi IN (9 up_sum, 28 affinity_sum))\n    \n# select kpi, sum(count_a) as app_count, sum(sum_e) as sum_e from plproxy.execute_select_nestloop(\\$proxy\\$ \n#     select kpi, count(estimate) as count_a, sum(estimate) as sum_e\n#     from ca.app_monthly\n#     where \n#         date ='2020-06-30'\n#     group by kpi order by kpi desc\n# \\$proxy\\$) tpl (kpi smallint, count_a bigint, sum_e double precision ) group by kpi order by kpi desc ;\n\n#estimate DOUBLE PRECISION\n"]},{"cell_type":"code","execution_count":0,"id":"20201001-053354_800413936","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n-- adhoc\n\n-- total\nselect date, sum(count_a) as app_count, sum(sum_d) as sum_d, sum(sum_e) as sum_e from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select date, count(*) as count_a, sum(kpi) as sum_d, sum(estimate) as sum_e\n    from au.app_weekly\n    where \n        date between '2020-08-01' and '2020-08-31'\n    group by date order by date desc\n\\$proxy\\$) tpl (date date, count_a bigint, sum_d bigint, sum_e double precision ) group by date order by date desc ;\n\nselect date, sum(count_a) as app_count, sum(sum_d) as sum_d, sum(sum_e) as sum_e from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select date, count(*) as count_a, sum(kpi) as sum_d, sum(estimate) as sum_e\n    from au.app_monthly\n    where \n        date between '2020-08-01' and '2020-08-31'\n    group by date order by date desc\n\\$proxy\\$) tpl (date date, count_a bigint, sum_d bigint, sum_e double precision ) group by date order by date desc ;\n\n\n\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20201001-051928_2119671122","metadata":{},"outputs":[],"source":["\n\n# spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.cross-app.v4/fact/granularity=m/month=202006/\").createOrReplaceTempView(\"cross_m_a_legacy\")\n# spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.app-cross-domain.v4/fact/granularity=m/month=202006/\").createOrReplaceTempView(\"cross_a_m_legacy\")\n# spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.cross-domain.v4/fact/granularity=m/month=202006/\").createOrReplaceTempView(\"cross_m_m_legacy\")\n# spark.read.format(\"parquet\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity=monthly/month=2020-06/\").createOrReplaceTempView(\"cross_a_a_legacy\")\n\n\n# spark.sql(\"select * from cross_a_a_legacy where app_id=20600000016214 and seg_app_id=20600000013028 and store_id=10 and device_id=1001 \").show(2)\n# spark.sql(\"select * from cross_a_a_legacy where app_id=20600000013028 and seg_app_id=20600000016214 and store_id=10 and device_id=1001 \").show(2)\n\n# spark.sql(\"select kpi, count(kpi), sum(estimate) from cross_a_a_legacy where estimate <> 0 and estimate is not null group by kpi\").show(2)\n\n# spark.sql(\"select count(*), sum(est_cross_product_affinity), sum(est_cross_product_usage_penetration) from cross_m_a_legacy\").show(2)\n# spark.sql(\"select count(*), sum(est_cross_product_affinity), sum(est_cross_product_usage_penetration) from cross_a_m_legacy\").show(2)\n# spark.sql(\"select count(*), sum(est_cross_product_affinity), sum(est_cross_product_usage_penetration) from cross_m_m_legacy\").show(2)\n\n\n\n\n# spark.sql(\"\"\"\n# SELECT sum(count) as count, sum(up_sum) as up_sum, sum(affinity_sum) as affinity_sum FROM\n# (\n#     (SELECT  count(*) as count, sum(est_cross_product_usage_penetration) as up_sum, sum(est_cross_product_affinity) as affinity_sum from cross_m_m_legacy)\n#     UNION ALL\n#     (SELECT  count(*) as count, sum(est_cross_product_usage_penetration) as up_sum, sum(est_cross_product_affinity) as affinity_sum from cross_m_a_legacy)\n#     UNION ALL\n#     (SELECT  count(*) as count, sum(est_cross_product_usage_penetration) as up_sum, sum(est_cross_product_affinity) as affinity_sum from cross_a_m_legacy)\n#     UNION ALL\n#     (SELECT count, up_sum, affinity_sum FROM (select kpi, count(estimate) as count, sum(estimate) as estimate from cross_a_a_legacy group by kpi) PIVOT ( sum(estimate) as sum FOR kpi IN (9 up_sum, 28 affinity_sum)))\n# ) \n# \"\"\").show()\n\nspark.sql(\"\"\"\nSELECT * from (\n    (SELECT  count(*) as count, sum(est_cross_product_usage_penetration) as up_sum, sum(est_cross_product_affinity) as affinity_sum from cross_m_m_legacy)\n    UNION ALL\n    (SELECT  count(*) as count, sum(est_cross_product_usage_penetration) as up_sum, sum(est_cross_product_affinity) as affinity_sum from cross_m_a_legacy)\n    UNION ALL\n    (SELECT  count(*) as count, sum(est_cross_product_usage_penetration) as up_sum, sum(est_cross_product_affinity) as affinity_sum from cross_a_m_legacy)\n    UNION ALL\n    (SELECT count, up_sum, affinity_sum FROM (select kpi, count(estimate) as count, sum(estimate) as estimate from cross_a_a_legacy group by kpi) PIVOT ( sum(estimate) as sum FOR kpi IN (9 up_sum, 28 affinity_sum)))\n) order by count asc\n\"\"\").show()\n\n\n\nspark.sql(\"select * from cross_a_a_legacy\").show(2)\nspark.sql(\"select * from cross_m_a_legacy\").show(2)\nspark.sql(\"select * from cross_a_m_legacy\").show(2)\nspark.sql(\"select * from cross_m_m_legacy\").show(2)\n\n# spark.sql(\"select date, count(*) as app_count, sum(estimate) as sum_e from segments_au_legacy_monthly where date between '2010-01-01' and '2020-08-31' and kpi=1 group by date order by date desc \").show(9999, False)\n\n# spark.read.format(\"parquet\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=weekly/month=2020-06/\").createOrReplaceTempView(\"segments_au_legacy_weekly\")\n# spark.sql(\"select * from segments_au_legacy_weekly\").show(5)\n# spark.sql(\"select date, count(*) as app_count, sum(estimate) as sum_e from segments_au_legacy_weekly where date between '2010-01-01' and '2020-08-31' and kpi=1 group by date order by date desc \").show(9999, False)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20201001-063523_1043641938","metadata":{},"outputs":[],"source":["\n\n\n# spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.cross-product.v6/fact/cross_type=app_cross_app/granularity=monthly/date=2020-06-30/\").createOrReplaceTempView(\"cross\")\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.cross-product.v6/fact/\").createOrReplaceTempView(\"cross_unified\")\n\n\n\nspark.sql(\"select cross_type, count(est_cross_affinity) as count, sum(est_usage_penetration) as up_sum,sum(est_cross_affinity) as affinity_sum from cross_unified where date='2020-06-30' group by cross_type order by count asc\").show(20)\n# spark.sql(\"select cross_type, count(est_cross_affinity) from cross_unified where est_cross_affinity<>0 and est_cross_affinity is not null group by cross_type\").show(20)\n# spark.sql(\"select count(est_cross_affinity) as count, sum(est_usage_penetration) as up_sum, sum(est_cross_affinity) as affinity_sum from cross_unified where date='2020-06-30'\").show(20)\n# spark.sql(\"select cross_type,related_product_key,selected_product_key,est_cross_affinity, est_usage_penetration from cross_unified where date='2020-06-30' and country_code='US' and device_code='ios-phone' and selected_product_key in (454607051, 70010000046942) and related_product_key in (284882215,70010000000003)\").show(20)\n\n# spark.sql(\"select * from cross_unified where date='2020-06-30'\").show(20)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20201001-050849_804459280","metadata":{},"outputs":[],"source":["%%sh\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=weekly/month=2019-05/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.seg-by-product.v6/fact/granularity=monthly/date=2020-06-30/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.seg-by-product.v6/fact/granularity=weekly/date=2020-06-27/country_code=AU/device_code=android-phone/\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=monthly/month=2020-06/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=weekly/month=2020-06/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity=monthly/\n\n# \n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity=monthly/month=2020-06/\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_name=app/granularity=monthly/date=2020-08-31/\n\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.cross-app.v4/fact/granularity=m/month=202006/\naws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.app-cross-domain.v4/fact/granularity=m/month=202006/\naws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.cross-domain.v4/fact/granularity=m/month=202006/\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.cross-product.v6/fact/cross_type=app_cross_app/granularity=monthly/date=2020-06-30/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20201002-051141_13029099","metadata":{},"outputs":[],"source":["\n\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2010, 1, 1)\nend_date = datetime(2020, 8, 31)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\nDATE_GRANULARITY_MAPPINGLIST[\"monthly\"].reverse()\n\n#print(DATE_GRANULARITY_MAPPINGLIST[\"weekly\"])\n\nsql_template_a_x_a = \"\"\"\nselect kpi, sum(count_a) as app_count, sum(sum_e) as sum_e from plproxy.execute_select_nestloop($proxy$ \n    select kpi, count(estimate) as count_a, sum(estimate) as sum_e\n    from ca.app_monthly\n    where \n        date ='{date}'\n    group by kpi order by kpi desc\n$proxy$) tpl (kpi smallint, count_a bigint, sum_e double precision ) group by kpi order by kpi desc ;\n\"\"\"\n\nsql_template = \"\"\"\nselect sum(count) as count, sum(affinity_sum) as affinity_sum,  sum(up_sum) as up_sum from plproxy.execute_select_nestloop($proxy$ \n    select count(*) as count, sum(est_cross_product_affinity) as affinity_sum,  sum(est_cross_product_usage_penetration) as up_sum\n    from mw.{db_name}_m\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint, affinity_sum real,up_sum real ) ;\n\n\"\"\"\n# DATE_GRANULARITY_MAPPINGLIST[\"weekly\"] = [\"2020-06-30\"]\n\nfor date_str in DATE_GRANULARITY_MAPPINGLIST[\"monthly\"]:\n    try:\n        # app x app\n        result = query(PLPROXY_DSN, sql_template_a_x_a.format(date=date_str))\n        count =result[0][1]\n        affinity_sum = result[0][2]\n        up_sum = result[1][2]\n        # app x domain, domain x domain, domain x app\n        for db_name in [\"app_x_domain\", \"domain_x_domain\", \"domain_x_app\"]:\n            result = query(PLPROXY_DSN, sql_template.format(date=date_str, db_name=db_name))\n            count += result[0][0]\n            affinity_sum += result[0][1]\n            up_sum += result[0][2]\n        print \"{},{},{:.20g},{:.20g}\".format(date_str,count, up_sum, affinity_sum )\n    except Exception, e:\n        print \"{},{}\".format(date_str, \"ERROR\")\n\n\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}