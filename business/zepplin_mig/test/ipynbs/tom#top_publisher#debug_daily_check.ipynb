{"cells":[{"cell_type":"code","execution_count":0,"id":"20200806-105120_1521697693","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"tom-debug\")\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200806-105146_185176836","metadata":{},"outputs":[],"source":["\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200806-235227_1337011337","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20200806-105203_641517652","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2018 App Annie Inc. All rights reserved.\n# pylint: disable=E1101,C0412,C1801\n\n\"\"\"\nDB Check modules\n\"\"\"\n\nimport datetime\n\nfrom applications.db_check_v1.cases.store.app_est_publisher_v1.constants import PUB_EST_GRANULARITY_DB_MAPPING, \\\n    PUB_EST_GRANULARITY_CHECK_RULES, PUB_EST_GRANULARITY_FREQ_MAPPING, CITUS_META_DSN, PUB_EST_DSN\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df, etl_skip, query\nfrom applications.db_check_v1.common.utils import get_date_list\n\n\nDB_PUBLISHER_EST_METRICS = [\"est_free_app_download\", \"est_paid_app_download\", \"est_revenue\"]\nPUB_EST_START_DATE = datetime.datetime(2010, 7, 4)\n\n\nclass PublisherEstPreAggrEst(PipelineTest):\n    trigger_date_config = ('* 15 * * 2', 3)\n    granularity = \"\"\n\n    def check_aggr_by_sum_period(self, check_date, granularity):\n        # start date of publisher data\n        date_list = get_date_list(PUB_EST_START_DATE, check_date, freq=PUB_EST_GRANULARITY_FREQ_MAPPING[granularity])\n        start_date = (datetime.datetime.strptime(date_list[-2], '%Y-%m-%d') + datetime.timedelta(days=1)) \\\n            .strftime('%Y-%m-%d')\n        end_date = date_list[-1]\n        sql_template = \"SELECT sum(est_revenue) FROM store.{table}  where date between '{start}' and '{end}';\"\n        compare_table = PUB_EST_GRANULARITY_DB_MAPPING[PUB_EST_GRANULARITY_CHECK_RULES[granularity]]\n        compare_sql = sql_template.format(table=compare_table, start=start_date, end=end_date)\n        actual_table = PUB_EST_GRANULARITY_DB_MAPPING[granularity]\n        actual_sql = sql_template.format(table=actual_table, start=start_date, end=end_date)\n\n        compare_sum = query(PUB_EST_DSN, compare_sql)\n        actual_sum = query(PUB_EST_DSN, actual_sql)\n        msg = \"actual_sum {} , compare_sum {}, actual_sql: {}, compare_sql: {}\".format(\n            actual_sum, compare_sum, actual_sql, compare_sql)\n        self.assertEqual(compare_sum, actual_sum, msg)\n\n    def check_meta_data_of_tables(self, granularity):\n        sql_template = \"select app_stats -> 'latest_available_date' as latest_available_date \" \\\n                       \"from meta where table_name = '{table_name}' and is_active = True ;\"\n\n        sql = sql_template.format(table_name=PUB_EST_GRANULARITY_DB_MAPPING[granularity].split(\"_v\")[0])\n        latest_available_date = query(CITUS_META_DSN, sql)[0][0]\n        check_date = self._get_check_date_from_routing_config(datetime.datetime.now())\n        date_list = get_date_list(PUB_EST_START_DATE, check_date, freq=PUB_EST_GRANULARITY_FREQ_MAPPING[granularity])\n        self.assertEqual(latest_available_date, date_list[-1])\n\n    @etl_skip()\n    def test_publisher_est_pre_aggr_etl_completeness(self):\n        self._testMethodName = 'test_publisher_est_{}_pre_aggr_etl_completeness'.format(self.granularity)\n        self.check_aggr_by_sum_period(self.check_date, self.granularity)\n        self.check_meta_data_of_tables(self.granularity)\n\n\nclass TestIOSPublisherWeeklyPreAggrEst(PublisherEstPreAggrEst):\n    granularity = 'weekly'\n\n\nclass TestIOSPublisherMonthlyPreAggrEst(PublisherEstPreAggrEst):\n    granularity = 'monthly'\n\n\nclass TestIOSPublisherQuarterlyPreAggrEst(PublisherEstPreAggrEst):\n    granularity = 'quarterly'\n\n\nclass TestIOSPublisherYearlyPreAggrEst(PublisherEstPreAggrEst):\n    granularity = 'yearly'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200806-105224_1734861394","metadata":{},"outputs":[],"source":["\n\nfrom applications.db_check_v1.common.base_test import debug\nt_date = datetime.datetime(2017,01,04)\nmycaseList = [\n    TestIOSPublisherWeeklyPreAggrEst(trigger_datetime=t_date, methodName='test_publisher_est_pre_aggr_etl_completeness'),\n    TestIOSPublisherMonthlyPreAggrEst(trigger_datetime=t_date, methodName='test_publisher_est_pre_aggr_etl_completeness'),\n    TestIOSPublisherQuarterlyPreAggrEst(trigger_datetime=t_date, methodName='test_publisher_est_pre_aggr_etl_completeness'),\n    TestIOSPublisherYearlyPreAggrEst(trigger_datetime=t_date, methodName='test_publisher_est_pre_aggr_etl_completeness')\n]\n#debug(caseList)\n"]},{"cell_type":"code","execution_count":0,"id":"20200807-011506_823845261","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2018 App Annie Inc. All rights reserved.\n# pylint: disable=E1101,C0412,C1801\n\n\"\"\"\nDB Check modules\n\"\"\"\n\nimport datetime\n\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df, etl_skip\nfrom applications.db_check_v1.common.constants import COUNTRY_CODE_MAPPING_BY_MARKET_CODE as COUNTRY_CODE_MAPPING, \\\n    CATEGORY_ID_MAPPING_BY_MARLKET_AND_DEVICE_CODE as CATEGORY_ID_MAPPING\nfrom applications.db_check_v1.common.utils import get_week_start_end_date, get_date_list\nfrom applications.db_check_v1.cases.usage.basic_kpi_v3.test_basic_kpi_v3_routine_plproxy import CITUS_DSN as PUB_EST_DSN\nfrom applications.db_check_v1.cases.store.app_est_publisher_v1.constants import PUB_EST_DSN, PUB_EST_DB_METRICS\n\n\nclass PublisherEstRawData(object):\n    raw_s3_path = \"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY\" \\\n                  \"/date={}/\"\n    device_code_mapping = {\n        \"00\": \"android-all\",\n        \"01\": \"android-all\",\n        \"02\": \"android-all\",\n        \"10\": \"ios-phone\",\n        \"11\": \"ios-phone\",\n        \"12\": \"ios-phone\",\n        \"1100\": \"ios-tablet\",\n        \"1101\": \"ios-tablet\",\n        \"1102\": \"ios-tablet\",\n        # \"21000\": \"ios-all\",\n        # \"21001\": \"ios-all\",\n        # \"21002\": \"ios-all\",\n    }\n\n    metric_mapping = {\n        0: \"free_app_download\",\n        1: \"paid_app_download\",\n        2: \"revenue\",\n        101: \"free_app_download\",\n        100: \"paid_app_download\",\n        102: \"revenue\",\n        # 1000: \"free_app_download\",\n        # 1001: \"paid_app_download\",\n        # 1002: \"revenue\"\n    }\n\n    dimension_mapping = {\n        \"id\": \"publisher_id\",\n    }\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code):\n        df = self._get_raw_data_by_date_country(date, country_code)\n        df = self._parse_mapping(df)\n        df = self._parse_unified_format(df)\n        df = self._data_clean_up(df)\n        return df\n\n    def _data_clean_up(self, df):\n        # clean unknown mapping\n        category_id_list = list(set(CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].values() +\n                                    CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].values()))\n\n        country_code_list = list(set(COUNTRY_CODE_MAPPING[\"apple-store\"].values() +\n                                     COUNTRY_CODE_MAPPING[\"google-play\"].values()))\n\n        df = df[(df['category_id'].isin(category_id_list)) & (df['country_code'].isin(country_code_list))]\n        return df\n\n    def _parse_mapping(self, df):\n        # country_code mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"google-play\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"apple-store\"]})\n        df = df.rename(columns={'store_id': 'country_code'})\n\n        # category_id mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"]})\n\n        # device_code mapping\n        df[\"device_code\"] = df[\"platform_id\"].astype(str) + df[\"feed\"].astype(str)\n        df = df.replace({\"device_code\": self.device_code_mapping})\n\n        # granularity\n        df[\"granularity\"] = \"daily\"\n\n        # metrics mapping (from feed)\n        df = df.replace({\"feed\": self.metric_mapping})\n        return df\n\n    def _parse_unified_format(self, df):\n        df = df.rename(columns=self.dimension_mapping)\n        df = df.pivot_table(index=[\"publisher_id\", \"category_id\", \"device_code\", \"country_code\", \"granularity\"],\n                            columns='feed', values='est')\n        df.reset_index(inplace=True)\n        df.columns.name = None\n        return df\n\n    def _get_raw_data_by_date_country(self, date, country_code):\n        \"\"\"\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        |        id|store_id|category_id|platform_id|vertical|rank|feed|  est|platform|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        | 284417353|       0|       6006|          1|       1|   1|1002|45235|     ios|\n        | 349554266|       0|       6006|          1|       1|   2|1002|20732|     ios|\n        |1316153435|       0|       6006|          1|       1|   3|1002|15136|     ios|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        \"\"\"\n        ios_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"apple-store\"].items() if v == country_code]\n        gp_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"google-play\"].items() if v == country_code]\n        raw_df = self.spark.read.parquet(self.raw_s3_path.format(date)).\\\n            filter('store_id in ({})'.format(\",\".join(ios_store_ids + gp_store_ids))).toPandas()\n        return raw_df\n\n    def get_metrics_count(self, date):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n        # ios_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].keys()]\n        # gp_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].keys()]\n\n        fillter_sql = \"platform_id = {} and store_id in ({}) and feed in ({})\"\n        df = self.spark.read.parquet(self.raw_s3_path.format(date))\n        feed_ids_sql = \",\".join([str(x) for x in self.metric_mapping.keys()])\n\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id), feed_ids_sql)).count() + \\\n                    df.filter(fillter_sql.format(0, \",\".join(gp_store_id), feed_ids_sql)).count()\n        return count_all\n\n    def get_v1_raw_metrics_count(self, date):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n\n        df = self.spark.read.option(\"delimiter\", \"\\t\").csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/*/sbe_est_publisher/*/*.csv.gz\".format(date))\n        fillter_sql = \"_c2 = {} and _c0 in ({})\"  # _c2 > platform_id, _c0 > store_id\n\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id))).count() + \\\n                    df.filter(fillter_sql.format(0, \",\".join(gp_store_id))).count()\n        return count_all\n\n\nclass PublisherEstUnifiedData(object):\n    unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher-dna-log.v1/\" \\\n                      \"fact/granularity=daily/date={}/\"\n    available_device_codes = ['ios-phone', 'ios-tablet', 'android-all']\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code):\n        unified_df = self.spark.read.format(\"delta\").\\\n            load(self.unified_s3_path.format(date)).filter(\"country_code = '{}'\".format(country_code)).toPandas()\n        unified_df = unified_df.drop([\"_identifier\", \"revenue_iap\", \"revenue_non_iap\", \"date\"], axis=1)\n        return unified_df\n\n    def get_metrics_count(self, date):\n        df = self.spark.read.format(\"delta\").load(self.unified_s3_path.format(date))\n        metrics_count = 0\n        device_code_list_sql = \"','\".join(self.available_device_codes)\n        for metric in PUB_EST_DB_METRICS:\n            metrics_count += df.filter(\"device_code in ('{}') and {} is not null\".format(device_code_list_sql,\n                                                                                       metric)).count()\n        return metrics_count\n\n\nclass PublisherEstDBData(object):\n    def get(self, date):\n        sql = \"SELECT * FROM store.store_est_publisher_fact_v2 WHERE date='{}'\".format(date)\n        return query_df(PUB_EST_DSN, sql)\n\n    def get_metrics_count(self, date):\n        metrics_count = 0\n        for metric in PUB_EST_DB_METRICS:\n            sql = \"SELECT count(*) AS metrics_count FROM store.store_est_publisher_fact_v2 WHERE date='{}' AND {} IS NOT NULL\".format(date, metric)\n            data = query_df(PUB_EST_DSN, sql)\n            metrics_count += data.loc[0].metrics_count\n        return metrics_count\n\n\nclass TestPublisherEstWeekly(PipelineTest):\n    # Every Tuesday 15:00 UTC(23:00 BJ) time will refresh the data of last Full Week.\n    trigger_date_config = ('* 15 * * 2', 3)\n\n    def _compare_df(self, df1, df2, log=''):\n        for diff_type in [\"left\", \"right\"]:\n            diff_df = df1.merge(df2, indicator=True, how=diff_type)  # .loc[lambda x : x['_merge']!='both']\n            diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n            if len(diff_df) != 0:\n                print diff_type\n                print \"dataframe overview of df1 and df2\"\n                print df1\n                print df2\n                print \"dimension overview of diff df\"\n                print diff_df.country_code.unique()\n                print diff_df.category_id.unique()\n                print diff_df.device_code.unique()\n            self.assertEqual(len(diff_df), 0,\n                             msg=\"found mismatch when compare the raw, unified, db.\"\n                                 \" diff count is \\n {}, logs:{}\".format(len(diff_df), log))\n\n    # @etl_skip()\n    # def test_publisher_est_etl_accuracy(self):\n    #     # Every Tuesday 16:00 UTC time will refresh the data of last Full Week.\n    #     country_code = 'US'\n    #     start_date, end_date = get_week_start_end_date(self.check_date_str)\n    #     date_list = get_date_list(start_date, end_date)\n    #     for date in date_list:\n    #         raw_df = PublisherEstRawData(self.spark).get(date, country_code)\n    #         unified_df = PublisherEstUnifiedData(self.spark).get(date, country_code)\n    #         db_df = PublisherEstDBData().get(date)\n    #\n    #         self._compare_df(raw_df, unified_df, log=\"raw / unified - {}\".format(date))\n    #         self._compare_df(unified_df, db_df, log=\"unified / db - {}\".format(date))\n\n    @etl_skip()\n    def test_publisher_est_etl_completeness(self):\n        start_date, end_date = get_week_start_end_date(self.check_date_str)\n        date_list = get_date_list(start_date, end_date)\n        for date in date_list:\n            raw_count = PublisherEstRawData(self.spark).get_metrics_count(date)\n            unified_count = PublisherEstUnifiedData(self.spark).get_metrics_count(date)\n            db_count = PublisherEstDBData().get_metrics_count(date)\n            self.assertEqual(raw_count, unified_count)\n            self.assertEqual(raw_count, db_count)\n            self.assertTrue(db_count>0)\n\n    def test_publisher_est_etl_timelines(self):\n        # Every Tuesday 15:00 UTC(23:00 BJ) time will refresh the data of last Full Week.\n        # E.g. 2020-02-12 10:00 the data of 2020-02-02 ~ 2020-02-08 will be ready\n        trigger_datetime = datetime.datetime.strptime(\"2020-02-12 9:00:00\", '%Y-%m-%d %H:%M:%S')\n        check_date_str_actual = self._get_check_date_from_routing_config(trigger_datetime).strftime(\"%Y-%m-%d\")\n        self.assertEqual(\"2020-02-08\", check_date_str_actual)\n\nt_date = datetime.datetime(2020,8,5)\n\n\nmycaselist2 = [\n    TestPublisherEstWeekly(trigger_datetime=t_date, methodName='test_publisher_est_etl_timelines')\n]\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200807-011518_972499183","metadata":{},"outputs":[],"source":["\nprint t_date\nprint 123\ndebug(mycaselist2)\n"]},{"cell_type":"code","execution_count":0,"id":"20200806-105332_863477769","metadata":{},"outputs":[],"source":["\n\n\n\nfrom aadatapipelinecore.core.utils import email\n\ndef send_db_check_email(title, text_context, key=None):\n    default_recipients = [\"tom@appannie.com\"]\n    if 'Failed' in title or (key is not None and 'Failed' in key):\n        default_recipients.append(\"tom@appannie.com\")\n    email.send(title, text_context, default_recipients, sender='dev-qa-data-quality@appannie.com')\n\n\n\nimport datetime\nimport unittest\n\nfrom aadatapipelinecore.core.monitoring.pipeline_monitor import running, fail, task_success\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.utils.identifier import package_id\nfrom aadatapipelinecore.core.utils.spark import canned_spark, stop\n\nfrom applications.db_check_v1.cases.store.app_est_publisher_v1.test_app_est_publisher_aggr_v1 import \\\n    TestPublisherWeeklyPreAggrEst, TestPublisherMonthlyPreAggrEst, TestPublisherQuarterlyPreAggrEst, \\\n    TestPublisherYearlyPreAggrEst\nfrom applications.db_check_v1.cases.store.app_est_publisher_v1.test_app_est_publisher_v1 import \\\n    TestPublisherEstPreviewDaily, TestPublisherEstFinalWeekly\nfrom applications.db_check_v1.cases.store.download_attribution.test_download_attribution_pre_agg import \\\n    TestStoreDownloadAttrPreAgg\n\nfrom applications.db_check_v1.cases.store.genre_est_v1.test_game_iq_market_size_full_restatement import \\\n    TestGameIQMarketSizeFullRestatement\nfrom applications.db_check_v1.cases.advanced_review.test_advainced_review import TestAdvancedReview\nfrom applications.db_check_v1.cases.store.store_download_revenue.store_daily import TestStoreDownloadRevenue\nfrom applications.db_check_v1.cases.store.store_download_revenue.store_meta_db import TestStoreMetaDB\nfrom applications.db_check_v1.cases.store.store_download_revenue.store_pre_agg import \\\n    TestStoreDownloadRevenuePreAgg\nfrom applications.db_check_v1.cases.usage.genre_cross_v1.test_game_iq_cross_genre import TestGameIQCrossGenre\nfrom applications.db_check_v1.cases.advanced_review.test_topic_join_review import TestTopicJoinReview\nfrom applications.db_check_v1.cases.aso.test_aso import TestASO, TestASOSOVDataDaily, TestASOMetrics\nfrom applications.db_check_v1.cases.market.test_market import TestMarketDimensionDaily, \\\n    TestMarketLogsFactAndSeenDaily, TestMarketMonthlyCheck, TestMarketWeeklyCheck\nfrom applications.db_check_v1.cases.mobile_web.test_mobile_web import TestMobileWebDaily, \\\n    TestMobileWebWeekly, \\\n    TestMobileWebMonthly, TestMobileWebRetention, TestMobilWebDomainMeta\nfrom applications.db_check_v1.cases.sdk.test_sdk import TestSDKDaily, TestSDKWeekly, TestSDKMonthly\nfrom applications.db_check_v1.cases.store.app_rank_v1.test_app_rank_v1 import TestAppStoreRankDaily\nfrom applications.db_check_v1.cases.store.app_rank_v1.test_app_store_rank import TestAppStoreRank\nfrom applications.db_check_v1.cases.store.download_attribution.test_download_attribution import \\\n    TestStoreDownloadAttributionDB, TestStoreDownloadAttributionUnified, TestDownloadAttribution\nfrom applications.db_check_v1.cases.store.market_size_v1.test_market_size_v1 import TestMarketSizeWeekly\nfrom applications.db_check_v1.cases.usage.test_city_level import TestCityLevelWeekly, TestCityLevelMonthly\nfrom applications.db_check_v1.cases.store.store_app_v1.test_app_detail import TestAppDetailCompleteness\nfrom applications.db_check_v1.cases.usage.basic_kpi_v3.test_basic_kpi_v3_routine_raw import \\\n    TestUsageRoutineRawCompletenessPreviewWeekly, TestUsageRoutineRawCompletenessPreviewMonthly, \\\n    TestUsageRoutineRawCompletenessPreviewDaily, TestUsageRoutineRawAccuracyMonthly, \\\n    TestUsageRoutineRawAccuracyWeekly, TestUsageRoutineRawAccuracyDaily, TestUsageRoutineRawCompletenessFinalDaily, \\\n    TestUsageRoutineRawCompletenessFinalWeekly, TestUsageRoutineRawCompletenessFinalMonthly\nfrom applications.db_check_v1.cases.usage.basic_kpi_v3.test_basic_kpi_v3_routine_plproxy import \\\n    TestUsageRoutinePlproxyCompletenessPreviewDaily, TestUsageRoutinePlproxyCompletenessPreviewWeekly, \\\n    TestUsageRoutinePlproxyCompletenessPreviewMonthly, TestUsageRoutinePlproxyCompletenessFinalDaily, \\\n    TestUsageRoutinePlproxyCompletenessFinalMonthly, TestUsageRoutinePlproxyCompletenessFinalWeekly\nfrom applications.db_check_v1.common.html_report_test_runner import HTMLTestRunner\n\nsuite = unittest.TestSuite()\n\ndef send_message():\n    log_file = \"/tmp/db_check.log\"\n    with open(log_file, \"w\") as html_file:\n        \n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestDownloadAttribution))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestASO))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestASOSOVDataDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestASOMetrics))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMobileWebDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMobileWebWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMobileWebMonthly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMobileWebRetention))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMobilWebDomainMeta))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestCityLevelWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestCityLevelMonthly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestSDKDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestSDKWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestSDKMonthly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestAdvancedReview))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMarketDimensionDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMarketLogsFactAndSeenDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMarketWeeklyCheck))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMarketMonthlyCheck))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestAppStoreRank))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestAppStoreRankDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMarketSizeWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestTopicJoinReview))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestGameIQCrossGenre))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestGameIQMarketSizeFullRestatement))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestAppDetailCompleteness))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessPreviewDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessPreviewWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessPreviewMonthly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyMonthly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutinePlproxyCompletenessPreviewDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutinePlproxyCompletenessPreviewWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutinePlproxyCompletenessPreviewMonthly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessFinalDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessFinalWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessFinalMonthly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutinePlproxyCompletenessFinalDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutinePlproxyCompletenessFinalWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutinePlproxyCompletenessFinalMonthly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreDownloadRevenue))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreDownloadRevenuePreAgg))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreDownloadAttrPreAgg))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreMetaDB))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreDownloadAttributionDB))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreDownloadAttributionUnified))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestPublisherEstPreviewDaily))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestPublisherEstFinalWeekly))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestPublisherWeeklyPreAggrEst))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestPublisherMonthlyPreAggrEst))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestPublisherQuarterlyPreAggrEst))\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestPublisherYearlyPreAggrEst))\n        # runner = unittest.TextTestRunner(file, verbosity=2, buffer=True)\n\n        runner = HTMLTestRunner(\n            stream=html_file,\n            title='DB Test Report',\n            description='This db_check the report output by Tech Team.'\n        )\n\n        failed_count = 0\n        result_list = runner.run(suite).result\n        for result in result_list:\n            if result[0] == 1 or result[0] == 2:\n                failed_count += 1\n\n    with open(log_file, 'r') as html_file:\n        today = datetime.date.today()\n        str_today = today.strftime(\"%Y-%m-%d\")\n\n        title = \"Data Refresh Check Report - \" + str_today + \" - \"\n        if failed_count == 0:\n            title += \"Passed\"\n        else:\n            title += \"Failed\"\n        send_db_check_email(title, html_file.read())\n\n\nsend_message()"]},{"cell_type":"code","execution_count":0,"id":"20200807-010111_368612509","metadata":{},"outputs":[],"source":["\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200807-010158_1006335341","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}