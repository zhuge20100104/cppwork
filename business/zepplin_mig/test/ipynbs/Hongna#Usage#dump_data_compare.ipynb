{"cells":[{"cell_type":"code","execution_count":0,"id":"20201119-075545_444789250","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY, PG_AA_HOSTS, PG_AA_NAME,PG_AA_ACCESS_ID,PG_AA_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\nPG_AA_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2013, 1, 1)\nend_date = datetime(2014, 12, 31)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n    }\n\n\n# DATE_GRANULARITY_MAPPINGLIST[\"monthly\"].reverse()\n# DATE_GRANULARITY_MAPPINGLIST[\"weekly\"].reverse()\n# DATE_GRANULARITY_MAPPINGLIST[\"daily\"].reverse()"]},{"cell_type":"code","execution_count":0,"id":"20201120-084650_1298967908","metadata":{},"outputs":[],"source":["\nbasic_dump=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact/granularity=monthly/\"\nbasic=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/\"\nsegment_by_age_gender_sql=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity={granularity}/month={date}/\"\nsegment_by_audience=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ad_app.v2/fact/granularity={granularity}/month={date}/\"\nsegment_by_product_sql=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity={granularity}/month={date}\"\napp_cross_app=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity={granularity}/month={date}/\"\napp_retention=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-rt_app.v2/fact/granularity=monthly/\"\nstart_str=\"2013-01\"\nend_str=\"2014-12\"\n\ngranularity_list = [\"weekly\"]\n\ndef get_dump_count(path_str):\n    for granularity in granularity_list:\n        for date_str in DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            spark.read.parquet(path_str.format(granularity=granularity, date=date_str[:-3])).filter(\"date = '{date}'\".format(date=date_str)).createOrReplaceTempView(\"test\")\n            df = spark.sql(\"select count(1) from test where kpi=1 and estimate is not null and estimate <> 0\").collect()\n            \n            print str(granularity)+str(\" \")+str(date_str)+str(\" \")+str(df[0][0])\n\ndef tmp_check():\n    tmp_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity=monthly/month=2014-12/\"\n    spark.read.parquet(tmp_path).createOrReplaceTempView(\"test\")\n    spark.sql(\"select distinct store_id from test order by store_id desc limit 100\").show(100, False)\nget_dump_count(segment_by_product_sql)"]},{"cell_type":"code","execution_count":0,"id":"20201119-075621_546240017","metadata":{},"outputs":[],"source":[" \n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom pyspark.sql import Row\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\nfrom pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType, ShortType\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\nseg_by_product_country = \"\"\"\nselect distinct store_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct store_id\n    from au.app_{granularity}\n    WHERE date='{date}'\n    and kpi=1\n    and estimate is not null\n    and estimate <> 0\n$proxy$) tpl (store_id int);\n\"\"\"\n\napp_x_app_country = \"\"\"\nselect distinct store_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct store_id\n    from ca.app_{granularity}\n    where \n        date ='{date}'\n        and kpi = 9\n        and estimate is not null\n        and estimate <> 0\n$proxy$) tpl (store_id int);\n\"\"\"\nsegment_by_product=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity={granularity}/month={date}\"\napp_x_app = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity={granularity}/month={date}/\"\n\ndef get_plproxy_result(sql_str):\n    plproxy_result = []\n    result = query(PLPROXY_DSN, sql_str)\n    df_data = [Row(store_id=r[0]) for r in result]\n    _schema =StructType([StructField(\"store_id\", IntegerType(), False)])\n    df_plproxy = spark.createDataFrame(data=df_data, schema=_schema)\n    return df_plproxy\n    \ndef get_unified_result(unified_path, granularity, date):\n    spark.read.parquet(unified_path.format(granularity=granularity, date=date[:-3])).filter(\"date = '{date}'\".format(date=date)).createOrReplaceTempView(\"test\")\n    spark.sql(\"select distinct store_id from test where kpi=9 and estimate <> 0 and estimate is not null\").createOrReplaceTempView(\"df_unified\")\n    \ndef compare():\n    granularity_list = [\"monthly\"]\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            df_plproxy = get_plproxy_result(app_x_app_country.format(granularity=granularity, date=date))\n            get_unified_result(app_x_app, granularity, date)\n            df_plproxy.createOrReplaceTempView(\"df_plproxy\")\n            df = spark.sql(\"select store_id from df_unified except select store_id from df_plproxy\").collect()\n            print granularity, date, [_d[0] for _d in df]\n            # print [_d[0] for _d in df]\n\ncompare()            \n"]},{"cell_type":"code","execution_count":0,"id":"20201119-094721_160778110","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}