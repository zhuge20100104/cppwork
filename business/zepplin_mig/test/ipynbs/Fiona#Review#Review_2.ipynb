{"cells":[{"cell_type":"code","execution_count":0,"id":"20200313-054858_726510321","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import date_format\n\ndf =spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(\"(process_date between '2020-02-24' and '2020-02-26') and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\").cache()\n\ndf = df.withColumn(\"time\",df[\"time\"].cast('date'))\nnew_df = df.withColumn(\"event_month\",date_format('time', 'yyyy-MM'))\nresult =  new_df.groupBy(\"event_month\").agg({\"*\":\"count\"}).cache().collect()\n\n\nprint result\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-070450_1268420732","metadata":{},"outputs":[],"source":["\nprint result[0]\nspark.createDataFrame(result).withColumnRenamed(\"count(1)\",\"agg_count\").coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/event_time_repartition_streaming_20200220_20200310_count/\")\n\n# result.coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/streaming_20200220_20200310_count/\")\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-071300_1148155621","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport datetime\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom pyspark.sql.functions import date_format\n\n\n# df = spark.read.option('basePath','s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/').parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={hourly,daily,weekly}/process_date={2020-01-16,2020-01-17,2020-01-18,2020-01-19,2020-01-20,2020-01-21,2020-01-22,2020-01-23,2020-01-24,2020-01-25,2020-01-26,2020-01-27,2020-01-28,2020-01-29,2020-01-30,2020-01-31,2020-02-01,2020-02-02,2020-02-03,2020-02-04,2020-02-05,2020-02-06,2020-02-07,2020-02-08,2020-02-09,2020-02-10,2020-02-11,2020-02-12,2020-02-13,2020-02-14,2020-02-15,2020-02-16,2020-02-17,2020-02-18,2020-02-19,2020-02-20,2020-02-21,2020-02-22,2020-02-23,2020-02-24}\").where(\"market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\").cache()\ndf =spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(\"(process_date between '2020-02-20' and '2020-03-09') and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\").cache()\ndf = df.withColumn(\"time\",df[\"time\"].cast('date'))\nnew_df = df.withColumn(\"event_month\",date_format('time', 'yyyy-MM'))\nresult =  new_df.groupBy(\"event_month\").agg({\"*\":\"count\"}).cache().collect()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-093312_1286683754","metadata":{},"outputs":[],"source":["\nreview_v4 = df.withColumn(\"event_month\",date_format('time', 'yyyy-MM')).filter(\"event_month='2013-05'\").select(\"review_id\",\"_identifier\").cache()\n# df.withColumn(\"event_month\",date_format('time', 'yyyy-MM')).filter(\"event_month='2013-05'\").groupBy(\"_identifier\").agg({\"*\":\"count\"}).orderBy(df._identifier.desc()).show()"]},{"cell_type":"code","execution_count":0,"id":"20200313-094711_2018147883","metadata":{},"outputs":[],"source":["\n\n\nload_temp= spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_temp/review_load_temp_streaming_20200220_20200310/event_month=2013-05\").filter(\"process_date != '2020-03-10'\").select(\"review_id\",\"_identifier\").cache()"]},{"cell_type":"code","execution_count":0,"id":"20200313-102247_1251441927","metadata":{},"outputs":[],"source":["\nprint load_temp.select(\"review_id\").distinct().count()\nprint review_v4.select(\"review_id\").distinct().count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-095100_1235283496","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\n\nreview_v4.createOrReplaceTempView(\"raw\")\nload_temp.createOrReplaceTempView(\"unified\")\nresult_test = spark.sql(\"select count(_identifier), _identifier from (select review_id,_identifier from unified EXCEPT select review_id,_identifier from raw) as prod group by _identifier order by _identifier desc \")\n# df.name.rlike\n# print result_test.filter(result_test._identifier.rlike(\"220200310*\") ).show(30,False)\nresult_test.show()"]},{"cell_type":"code","execution_count":0,"id":"20200312-135708_1067501059","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/aa.review/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-073240_727066569","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.utils import AnalysisException\n\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_load_temp/review_agg_v3/\", depth_is_1=True)\nprint len(path_list)\n\ndef get_month_count(path):\n    df1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).select(\"review_id\")\n    month=path.split(\"=\")[1].replace(\"/\",\"\")\n    try:\n        df2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date={}*/\".format(month)).filter(\"market_code in ('apple-store','google-play')\").select(\"review_id\")\n        return month, df1.union(df2).select(\"review_id\").distinct().count()\n    except AnalysisException:\n        print 'no monthly data'.format(month)\n        return month, df1.select(\"review_id\").distinct().count()\n        \n\n# df.coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/historical_data\")\n\n\ndef es_doc(month):\n    month = month.replace(\"-\",\"\")\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp\", \"C38vEJEuraCw\")\n    ios_url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1-*-{}/_count\".format(\n        month)\n    ios_resp = plain_get(ios_url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    ios_doc = json.loads(ios_resp.text)\n    return ios_doc[\"count\"]\n\n\nfor path in path_list[::-1]:\n    month, unified_count = get_month_count(path)\n    spark.createDataFrame(result).withColumnRenamed(\"count(1)\",\"agg_count\").coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/streaming_20200220_20200310_count/\")\n\n    \n    # db_count = es_doc(month)\n    # if unified_count!= db_count:\n    #     print 'not equal!!!! month {}, unified count is {}, db count is {}, db-unified diff is {} '.format(month, unified_count, db_count, db_count-unified_count)\n    # else:\n    #     print 'month {}, unified count is {}, db count is {}'.format(month, unified_count, db_count)"]},{"cell_type":"code","execution_count":0,"id":"20200222-041027_387152868","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\n\ndf = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_by_process_date.v3/date=2019-01-14\").withColumn(\"id_1\", F.monotonically_increasing_id()).cache().filter(\"_identifier = '220200218115410210'\")\nprint df.take(20)"]},{"cell_type":"code","execution_count":0,"id":"20200222-044210_553368299","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\n\n# new_df = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_by_process_date.v3/date=2020-02*\").withColumn(\"id_1\", F.monotonically_increasing_id()).cache().filter(\"id_1 = '3796751090178'\")\nnew_df.filter(\"id_1='3796751090127'\")\nprint new_df.take(20)"]},{"cell_type":"code","execution_count":0,"id":"20200222-040618_758884675","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\n\ndf = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_by_process_date.v3/date=2020-02-*\").withColumn(\"id_1\", F.monotonically_increasing_id()).cache().filter(\"id_1 = '3994319585282'\").show(20,False)"]},{"cell_type":"code","execution_count":0,"id":"20200222-052113_400411866","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/\naws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_month.v3/\n \n"]},{"cell_type":"code","execution_count":0,"id":"20200214-101838_1731186692","metadata":{},"outputs":[],"source":["\n\ndf = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json.v3/date=2019-12-01/0-11\").filter(\"index is null\")\nprint df.count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200214-102009_1437428201","metadata":{},"outputs":[],"source":["\n\nfrom aadatapipelinecore.core.log import logger\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\ndest_path='s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_by_process_date.v3/date={}/'\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = ['process_granularity={hourly, daily, weekly}/process_date=%s/' % item for item in [\"2020-02-11\"]]\nprint path_list[0]\nfor path in path_list:\n    print path.split(\"/\")[1].split(\"=\")[1]\n    count_1 = spark.read.option('basePath','s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/').parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/{}\".format(path)).where(\"market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\").select(\"review_id\").distinct().count()\n    df = spark.read.json(dest_path.format(path.split(\"/\")[1].split(\"=\")[1])).cache()\n    count_2 = df.filter(\"index is not null\").count()\n    count_3 = df.filter(\"_identifier is not null\").count()\n    print count_1, count_2, count_3\n    if count_1 == count_2 == count_3 :\n        print 'pass', path\n    else:\n        print \"not equal!!!!!!\", count_1 , count_2, count_3,  path\n"]},{"cell_type":"code","execution_count":0,"id":"20200214-101335_1143772724","metadata":{},"outputs":[],"source":["\n#int-ss-review_v1-google-play-android-all-201901\n\n\ndef es_doc():\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp\",\"C38vEJEuraCw\")\n    url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1-*-200808*/_count\"\n    resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    doc = json.loads(resp.text)\n    print doc\n    \nes_doc()\n"]},{"cell_type":"code","execution_count":0,"id":"20200213-105920_1788499814","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\n# from aadatapipelinecore.core.fs.device import meta_bucket\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_d = s3_bucket_list.all(prefix=\"review-uniform/DAILY/\", depth_is_1=True)\ndate_list_raw = [x.split(\"/\")[2] for x in path_d]\n\ns3_bucket_list_unified = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_d_unified = s3_bucket_list_unified.all(prefix=\"unified/review.v1/fact/process_granularity=daily/\", depth_is_1=True)\ndate_list_unified = [x.split(\"/\")[4].split(\"=\")[1] for x in path_d_unified]\n\nnot_exist_data_set = sorted(set(date_list_raw) - set(date_list_unified))\nlogger.info('Need to check !!! date is not existed!!!!! {}'.format(not_exist_data_set))\n\nfor x in not_exist_data_set:\n    el = \"review-uniform/DAILY/{}/\".format(x)\n    path_d.remove(el)\nprint path_d[899]\n# logger.info('test date list {}'.format(path_d))\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n# compared the data from uniform(raw data) to unified data\nraw_path_d = \"s3://prod_appannie_uniform_public_app_store_data/\"\n\nunified_path_d = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date={unified_process_date}/process_hour={unified_process_hour}/device_code={unified_device_code}/market_code={unified_market_code}\"\n\n\ndef count_raw_unified_data_daily():\n    for p in path_d[899:900]:\n        process_hour_raw_path = s3_bucket_list.all(p, depth_is_1=True)\n        for x in process_hour_raw_path:\n            # logger.info('raw path with process hour {}'.format(x))\n            platforms_process_hour_raw_path_list = s3_bucket_list.all(x, depth_is_1=True)\n            for raw_path in platforms_process_hour_raw_path_list:\n                raw_count_sum, root_raw_data_path = calculate_sum_data(raw_path)\n                # logger.info('raw_count_sum : {}'.format(raw_count_sum))\n                global unified_count\n\n                unified_count = 0\n                if raw_count_sum == 0:\n                    continue\n                upath = unified_path_d.format(unified_process_date=root_raw_data_path.split(\"/\")[2],\n                                              unified_device_code=platforms[root_raw_data_path.split(\"/\")[4]][\n                                                  \"device_code\"],\n                                              unified_market_code=platforms[root_raw_data_path.split(\"/\")[4]][\n                                                  \"market_code\"],\n                                              unified_process_hour=str(int(root_raw_data_path.split(\"/\")[3])))\n\n                try:\n                    unified_count = spark.read.parquet(upath).count()\n                    print 'raw_count_sum : {} , unified_count : {} ,unified path is : {} '.format(\n                            raw_count_sum,\n                            unified_count, root_raw_data_path)\n\n                except AnalysisException as e:\n                    print 'the path is not exist for unified level!!! the raw path is {}, unified path is {}'.format(\n                        raw_path_d + root_raw_data_path, upath)\n\n            # logger.info( 'unified_count', unified_count\n\n                if raw_count_sum != unified_count:\n                    print 'count is not equal!!! the raw path is {}, unified path is {} , raw count is : {}, unified count is : {}'.format(\n                            raw_path_d + root_raw_data_path, upath,\n                            raw_count_sum, unified_count)\n\n\ndef calculate_sum_data(deep_raw_path):\n    logger.info(deep_raw_path)\n    raw_count = spark.read.csv(raw_path_d + deep_raw_path, sep=\"\\t\").count()\n    return raw_count, deep_raw_path\n\n\ncount_raw_unified_data_daily()\n"]},{"cell_type":"code","execution_count":0,"id":"20200121-071844_934916659","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport types\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n\n# get current path list\n# path=[]\n# for object_summary in my_bucket.objects.filter(Prefix=\"review-uniform/MONTHLY/\"):\n#    path.append(object_summary.key[:object_summary.key.rfind(\"/\")])\n\n# path = list(dict.fromkeys(path))\n# print path\n\n# compared the data from uniform(raw data) to unified data\nraw_path = \"s3://prod_appannie_uniform_public_app_store_data/\"\nunified_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date={unified_process_date}/process_hour=23/device_code={unified_device_code}/market_code={unified_market_code}\"\n\n\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_h = s3_bucket_list.all(prefix=\"review-uniform/MONTHLY/2016-08-31/\", depth_is_1=True)\n\nprint path_h\n\ndef check_raw_unified_data_equals():\n    t = unittest.TestCase('run')\n\n    for p in path_h:\n        raw_result = spark.read.csv(raw_path + p, sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\",\n                                                                       \"_c2 as product_version\", \"_c3 as user_id\",\n                                                                       \"_c4 as user_name\", \"_c5 as user_language\",\n                                                                       \"_c6 as user_device\", \"_c7 as user_purchased\",\n                                                                       \"_c8 as rating\", \"_c9 as country\",\n                                                                       \"_c10 as date\", \"_c11 as language\",\n                                                                       \"_c12 as title\", \"_c13 as title_t\",\n                                                                       \"_c14 as content\", \"_c15 as content_t\",\n                                                                       \"_c16 as reply\", \"_c17 as reply_t\",\n                                                                       \"_c18 as reply_date\").cache().take(1)\n        print raw_result\n        check_id = raw_result[0][\"id\"]\n\n        upath = unified_path.format(unified_process_date=p.split(\"/\")[2],\n                                    unified_device_code=platforms[p.split(\"/\")[3]][\"device_code\"],\n                                    unified_market_code=platforms[p.split(\"/\")[3]][\"market_code\"])\n        unified_result = spark.read.parquet(upath).filter(\"review_id='{test_id}'\".format(test_id=check_id)).take(2)\n         \n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\"\n                      }\n\n        for key, value in column_dic.items():\n            check_value(key, raw_result[0][key], unified_result[0][value], p)\n\n\ndef check_value(index, exp, act, p):\n    if exp:\n        if index == 'rating' or index == \"product_id\" :\n            exp = int(exp)\n            act= int(act)\n            if exp == act:\n                pass\n                # print 'equals'\n            if exp != act:\n                print 'number NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act)\n        elif index == 'user_purchased':\n            exp = str(exp).lower()\n            act= str(act).lower()\n            if exp == act:\n                pass\n                # print 'equals'\n            if exp != act:\n                print 'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act)\n        else:\n            if exp.lower() == act.lower():\n                pass\n                # print 'equals'\n            if exp != act:\n                print 'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act)\n\n\ncheck_raw_unified_data_equals()\n"]},{"cell_type":"code","execution_count":0,"id":"20200121-094522_1244816150","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport types\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n\nraw_path = \"s3://prod_appannie_uniform_public_app_store_data/\"\nunified_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=weekly/process_date={unified_process_date}/process_hour=23/device_code={unified_device_code}/market_code={unified_market_code}\"\n\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_h = s3_bucket_list.all(prefix=\"review-uniform/WEEKLY/2018-12-11/23/\", depth_is_1=True)\n\nprint path_h\n\ndef check_raw_unified_data_equals_weekly():\n    t = unittest.TestCase('run')\n    \n    for p in path_h:\n        df = spark.read.csv(raw_path + p, sep=\"\\t\").take(1)\n        print '#################################### lenght of columns', len(df.columns)\n        raw_result = df.selectExpr(\"_c0 as id\", \"_c1 as product_id\",\n                                                                       \"_c2 as product_version\", \"_c3 as user_id\",\n                                                                       \"_c4 as user_name\", \"_c5 as user_language\",\n                                                                       \"_c6 as user_device\", \"_c7 as user_purchased\",\n                                                                       \"_c8 as rating\", \"_c9 as country\",\n                                                                       \"_c10 as date\", \"_c11 as language\",\n                                                                       \"_c12 as title\", \"_c13 as title_t\",\n                                                                       \"_c14 as content\", \"_c15 as content_t\",\n                                                                       \"_c16 as reply\", \"_c17 as reply_t\",\n                                                                       \"_c18 as reply_date\").cache().take(1)\n        print raw_result\n        check_id = raw_result[0][\"id\"]\n\n        upath = unified_path.format(unified_process_date=p.split(\"/\")[2],\n                                    unified_device_code=platforms[p.split(\"/\")[4]][\"device_code\"],\n                                    unified_market_code=platforms[p.split(\"/\")[4]][\"market_code\"])\n        unified_result = spark.read.parquet(upath).filter(\"review_id='{test_id}'\".format(test_id=check_id)).take(2)\n         \n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\"}#,\"userreview_url\":\"userreview_url\",\"reply_id\":\"reply_id\"\n                   #   }\n\n        for key, value in column_dic.items():\n            check_value(key, raw_result[0][key], unified_result[0][value], p)\n\n\ndef check_value(index, exp, act, p):\n    print index, exp, act, p\n    if exp:\n        if index == 'rating' or index == \"product_id\" :\n            exp = int(exp)\n            act= int(act)\n            if exp == act:\n                pass\n                # print 'equals'\n            if exp != act:\n                print 'number NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act)\n        elif index == 'user_purchased':\n            exp = str(exp).lower()\n            act= str(act).lower()\n            if exp == act:\n                pass\n                # print 'equals'\n            if exp != act:\n                print 'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act)\n        else:\n            if exp.lower() == act.lower():\n                pass\n                # print 'equals'\n            if exp != act:\n                print 'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act)\n\n\ncheck_raw_unified_data_equals_weekly()\n"]},{"cell_type":"code","execution_count":0,"id":"20200119-071503_708406647","metadata":{},"outputs":[],"source":["\n\ndf1= spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/MONTHLY/2009-08-31/gp\",sep=\"\\t\").cache()\ndf2 =spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date=2009-08-31/process_hour=23/device_code=android-all/\").cache()\n\nprint df2.join(df1, df1._c0==df2.review_id, 'outer' ).filter(\"_c0 is null\").count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200120-034311_1542182341","metadata":{},"outputs":[],"source":["\nimport unittest\nimport boto3\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3 \nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_d= s3_bucket_list.all(prefix=\"review-uniform/DAILY/\", depth_is_1=True )\npath_d = path_d[8:]\n# print path_d\n\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n# s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2017-06-30/00/wp/40700000020383.gz/\n# compared the data from uniform(raw data) to unified data\nraw_path_d=\"s3://prod_appannie_uniform_public_app_store_data/\"\nunified_path_d=\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date={unified_process_date}/process_hour={unified_process_hour}/device_code={unified_device_code}/market_code={unified_market_code}\"\n\ndef count_raw_unified_data_daily():\n    t = unittest.TestCase('run')\n    unified_count=0\n    for p in path_d:\n\n        # print data_path\n        hour_raw_path=s3_bucket_list.all(p, depth_is_1=True )\n        print hour_raw_path\n        hour_raw_path = ['review-uniform/DAILY/2019-12-02/23/ios/']\n        print hour_raw_path\n        for x in hour_raw_path:\n            print x\n            deep_raw_path=s3_bucket_list.all(x, depth_is_1=True )\n            # deep_raw_path = \"review-uniform/DAILY/2019-12-02/23/ios/\"\n            raw_count_sum , dp_path = calculate_sum_data(deep_raw_path)\n\n            print  'dp_path', dp_path   \n            upath=unified_path_d.format(unified_process_date=dp_path.split(\"/\")[2],unified_device_code=platforms[dp_path.split(\"/\")[4]][\"device_code\"],unified_market_code=platforms[dp_path.split(\"/\")[4]][\"market_code\"],unified_process_hour=dp_path.split(\"/\")[3])\n            print 'unified path is : ', upath\n            try:\n                unified_count = spark.read.parquet(upath).count()\n            except AnalysisException as e:\n                print 'the path is not exist for unified level!!! the path is', raw_path_d+p, upath\n        \n        # print 'unified_count', unified_count\n\n            if raw_count_sum != unified_count:\n                print 'count is not equal!!! the path is : ', raw_path_d+p, upath, \"raw is: {} , unified is : {} \".format(raw_count_sum, unified_count)\n\n\ndef calculate_sum_data(deep_raw_path):\n    print deep_raw_path\n    count_one_day=0\n    for dp in deep_raw_path:\n        unified_count=0\n        print 'raw_path is: ', raw_path_d+dp\n        if 'wp' in raw_path_d+dp:\n            raw_count = spark.read.csv(raw_path_d+dp,sep=\"\\t\").count()\n            count_one_day = count_one_day+raw_count\n        # print 'raw_count', raw_count\n            if raw_count == 0:\n            # print 'raw is empty, path is : ' , raw_path+p\n                continue\n        \n    print \"calculate sum data\", deep_raw_path[0] \n    print deep_raw_path[0].split(\"/\")[3]\n    print 'count is : ', count_one_day\n    return count_one_day, deep_raw_path[1]\n\ncount_raw_unified_data_daily()\n\n# calculate_sum_data([\"review-uniform/DAILY/2019-12-02/23/ios/\"])\n"]},{"cell_type":"code","execution_count":0,"id":"20200119-071531_1245888086","metadata":{},"outputs":[],"source":["\nimport unittest\nimport boto3\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\ns3 = boto3.resource('s3')\nmy_bucket = s3.Bucket('prod_appannie_uniform_public_app_store_data')\n\n\n# get current path list\n#path=[]\n#for object_summary in my_bucket.objects.filter(Prefix=\"review-uniform/MONTHLY/\"):\n#    path.append(object_summary.key[:object_summary.key.rfind(\"/\")])\n\n#path = list(dict.fromkeys(path))\n# print path\n\n# compared the data from uniform(raw data) to unified data\nraw_path=\"s3://prod_appannie_uniform_public_app_store_data/\"\nunified_path=\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date={unified_process_date}/process_hour=23/device_code={unified_device_code}/market_code={unified_market_code}\"\n\ndef check_raw_unified_data_equals():\n    t = unittest.TestCase('run')\n    # get current path list\n    path=[]\n    for object_summary in my_bucket.objects.filter(Prefix=\"review-uniform/MONTHLY/\"):\n        path.append(object_summary.key[:object_summary.key.rfind(\"/\")])\n\n    path = list(dict.fromkeys(path))\n\n    for p in path:\n        raw_result = spark.read.csv(raw_path+p,sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\", \"_c2 as product_version\", \"_c3 as user_id\", \"_c4 as user_name\" , \"_c5 as user_language\", \"_c6 as user_device\", \"_c7 as user_purchased\", \"_c8 as rating\", \"_c9 as country\", \"_c10 as date\",\"_c11 as language\", \"_c12 as title\", \"_c13 as title_t\", \"_c14 as content\", \"_c15 as content_t\", \"_c16 as reply\", \"_c17 as reply_t\", \"_c18 as reply_date\").cache().take(1)\n    \n        check_id = raw_result[0][\"id\"]\n    \n    \n        upath=unified_path.format(unified_process_date=p.split(\"/\")[2],unified_device_code=platforms[p.split(\"/\")[3]][\"device_code\"],unified_market_code=platforms[p.split(\"/\")[3]][\"market_code\"] )\n        result= spark.read.parquet(upath).filter(\"review_id='{test_id}'\".format(test_id=check_id)).take(2)\n        check_value('1', raw_result[0][\"product_id\"] , str(result[0][\"app_id\"]), p)\n        check_value('2', raw_result[0][\"product_version\"], result[0][\"product_version\"], p )\n        check_value('3',raw_result[0][\"user_id\"], result[0][\"user_id\"], p)\n        check_value( '4',raw_result[0][\"user_name\"],  result[0][\"user_name\"], p )\n        check_value('5', raw_result[0][\"user_language\"] , result[0][\"user_language\"], p)\n        check_value('6', raw_result[0][\"user_device\"],result[0][\"user_device\"], p)\n        check_value('7', raw_result[0][\"user_purchased\"].lower() ,str(result[0][\"user_purchased\"]).lower(), p)\n        check_value('8', raw_result[0][\"rating\"] ,str(result[0][\"rating\"]), p)\n        check_value('9', raw_result[0][\"country\"] , str(result[0][\"country_code\"]), p)\n        check_value('10', raw_result[0][\"date\"], result[0][\"time\"], p)\n        check_value('11', raw_result[0][\"title\"] ,str(result[0][\"title\"]), p)\n        check_value('12', raw_result[0][\"content\"] ,result[0][\"content\"],p)\n        check_value('13', raw_result[0][\"reply\"] ,result[0][\"reply\"], p)\n        check_value('14', str(raw_result[0][\"reply_date\"]), str(result[0][\"reply_date\"]), p)\n\ndef check_value(index, exp,act,p):\n   if exp != act:\n       print 'index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act)\n\n\n# def check_raw_unified_data_are_equals():\n#     t = unittest.TestCase('run')\n#     for p in path:\n#         raw_count = spark.read.csv(raw_path+p,sep=\"\\t\").count()\n#         # print raw_path+p\n#         # print 'raw_count', raw_count\n#         if raw_count == 0:\n#             # print 'raw is empty, path is : ' , raw_path+p\n#             continue\n        \n#         upath=unified_path.format(unified_process_date=p.split(\"/\")[2],unified_device_code=platforms[p.split(\"/\")[3]][\"device_code\"],unified_market_code=platforms[p.split(\"/\")[3]][\"market_code\"] )\n#         try:\n#             unified_count = spark.read.parquet(upath).count()\n#         except AnalysisException as e:\n#             print 'the path is not exist for unified level!!! the path is', raw_path+p, upath\n        \n#         # print 'unified_count', unified_count\n\n#         if raw_count != unified_count:\n#             print 'count is not equal!!! the path is : ', raw_path+p, upath, \"raw is: {} , unified is : {} \".format(raw_count, unified_count)\n\n\n#     print raw_result[0]\n#     check_id = raw_result[0][\"id\"]\n\n#     result= spark.read.parquet(upath).filter(\"review_id='{test_id}'\".format(test_id=check_id)).take(2)\n#     print result\n    \n#     print raw_result[0][\"product_id\"] == str(result[0][\"app_id\"])\n#     print raw_result[0][\"product_version\"] == result[0][\"product_version\"]\n#     print raw_result[0][\"user_id\"] == result[0][\"user_id\"]\n#     print raw_result[0][\"user_name\"] == result[0][\"user_name\"]\n#     print raw_result[0][\"user_language\"] == result[0][\"user_language\"]\n#     print raw_result[0][\"user_device\"] == result[0][\"user_device\"]\n#     print raw_result[0][\"user_purchased\"].lower() == str(result[0][\"user_purchased\"]).lower()\n#     print raw_result[0][\"rating\"] == str(result[0][\"rating\"])\n#     print raw_result[0][\"country\"] == str(result[0][\"country_code\"])\n#     print raw_result[0][\"date\"] == result[0][\"time\"]\n#     print raw_result[0][\"title\"] == str(result[0][\"title\"])\n#     print raw_result[0][\"content\"] == result[0][\"content\"]\n#     print raw_result[0][\"reply\"] == result[0][\"reply\"]\n#     print str(raw_result[0][\"reply_date\"]) == str(result[0][\"reply_date\"])\n\n#     # print raw_count[0][\"product_id\"] = result\n    \n    # print raw_path+p\n    # print 'raw_count', raw_count\n    #if raw_count == 0:\n    #    print 'raw is empty, path is : ' , raw_path+p\n        \n    #upath=unified_path.format(unified_process_date=p.split(\"/\")[2],unified_device_code=platforms[p.split(\"/\")[3]][\"device_code\"],unified_market_code=platforms[p.split(\"/\")[3]][\"market_code\"] )\n    #unified_count = spark.read.parquet(upath).count()\n    # print 'unified_count', unified_count\n\n    #if raw_count != unified_count:\n    #    print 'path is : ', raw_path+p, upath, \"raw is: {} , unified is : {} \".format(raw_count, unified_count)\n\n\n\ncheck_raw_unified_data_equals()\n"]},{"cell_type":"code","execution_count":0,"id":"20200119-103318_540870504","metadata":{},"outputs":[],"source":["%python\nimport boto3\nfrom aadatapipelinecore.core.fs.device import s3 \nfrom aadatapipelinecore.core.fs import Conf\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\nl =  s3_bucket_list.all(prefix=\"review-uniform/DAILY/\", depth_is_1=True )\nprint l[12:]\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200120-034954_1328898423","metadata":{},"outputs":[],"source":["\n\nprint spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/MONTHLY/2014-03-31/amazon\",sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\", \"_c2 as product_version\", \"_c3 as user_id\", \"_c4 as user_name\" , \"_c5 as user_language\", \"_c6 as user_device\", \"_c7 as user_purchased\", \"_c8 as rating\", \"_c9 as country\", \"_c10 as date\",\"_c11 as language\", \"_c12 as title\", \"_c13 as title_t\", \"_c14 as content\", \"_c15 as content_t\", \"_c16 as reply\", \"_c17 as reply_t\", \"_c18 as reply_date\").filter(\"id='067dc71871af99bcc6050253761eab0e'\").show()\nprint '####################'\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date=2014-03-31/process_hour=23/\").filter(\"review_id='067dc71871af99bcc6050253761eab0e'\").show(20,False,vertical=True)\n\n\n# raw_path is:  s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2017-06-08/00/wp/\n# unified path is :  s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-06-08/process_hour=00/device_code=windows-phone/market_code=windows-store\n# the path is not exist for unified level!!! the path is s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2017-06-08/ s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-06-08/process_hour=00/device_code=windows-phone/market_code=windows-store\n"]},{"cell_type":"code","execution_count":0,"id":"20200119-103349_443321105","metadata":{},"outputs":[],"source":["\n# print spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/MONTHLY/2017-06-30/amazon/part-00000\",sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\", \"_c2 as product_version\", \"_c3 as user_id\", \"_c4 as user_name\" , \"_c5 as user_language\", \"_c6 as user_device\", \"_c7 as user_purchased\", \"_c8 as userreview_url\", \"_c9 as rating\", \"_c10 as country\",\"_c11 as date\", \"_c12 as language\", \"_c13 as title\", \"_c14 as title_t\", \"_c15 as content\", \"_c16 as content_t\", \"_c17 as reply_id\", \"_c18 as reply\", \"_c19 as reply_t\", \"_c20 as reply_date\").filter(\"id='gp:AOqpTOFEMK10tMaetYLgLXvtiQaWlmepbOWKdNtQ9QRZz5acpAeEcYl9mVnxuN9rdJA0wxLUOJa5NviecYCPmg'\").show(10,False, vertical=False)\nprint spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/MONTHLY/2017-06-30/amazon/part-00000\",sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\", \"_c2 as product_version\", \"_c3 as user_id\", \"_c4 as user_name\" , \"_c5 as user_language\", \"_c6 as user_device\", \"_c7 as user_purchased\", \"_c8 as rating\", \"_c9 as country\", \"_c10 as date\",\"_c11 as language\", \"_c12 as title\", \"_c13 as title_t\", \"_c14 as content\", \"_c15 as content_t\", \"_c16 as reply\", \"_c17 as reply_t\", \"_c18 as reply_date\").cache().filter(\"id='R17N52YEZW6F65'\").show(20,False, vertical=True)\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date=2017-06-30\").cache().filter(\"review_id='R17N52YEZW6F65'\").show(20,False,vertical=True)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200120-035020_1089144872","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://prod_appannie_uniform_public_app_store_data/review-uniform/WEEKLY/2018-12-10/23/ios/ |head -3\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-06-14/process_hour=14/device_code=windows-phone/market_code=windows-store/\n\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/ # --recursive --human --summarize | tail -30\n# aws s3 ls s3://prod_appannie_uniform_public_app_store_data/review-uniform/HOURLY/2017-07-01/00/gp/20600007065190.gz  --recursive --human --summarize | tail -30\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=hourly/process_date=2017-07-01/process_hour=23/device_code=android-all/market_code=google-play/ --recursive --human --summarize | tail -30\n\n# 3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=hourly/process_date=2017-07-01/process_hour=23/device_code=android-all/market_code=google-play/\n# #aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-06-10/process_hour=23/device_code=android-all/market_code=google-play/\n# review-uniform/DAILY/2017-05-30/\n"]},{"cell_type":"code","execution_count":0,"id":"20200119-103358_151809080","metadata":{},"outputs":[],"source":["\n\ndf1= spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2018-09-09/00/amazon/\",sep=\"\\t\").cache()\nprint df1.count()\ndf2 =spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2018-09-09/process_hour=0/device_code=android-all/market_code=amazon-store/\").cache()\nprint df2.count()\nprint df2.join(df1, df1._c0==df2.review_id, 'outer' ).filter(\"review_id is null\").count()\n# 2020-02-01 02:03:23,278 - INFO - MainThread - aadatapipelinecore - count_raw_unified_data_daily - 79 - count is not equal!!! the raw path is s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2018-09-09/00/amazon/, unified path is s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2018-09-09/process_hour=0/device_code=android-all/market_code=amazon-store , raw count is : 5540362, unified count is : 5538749\n"]},{"cell_type":"code","execution_count":0,"id":"20200120-100732_36937150","metadata":{},"outputs":[],"source":["\nprint spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2017-07-30/14/win/40600000049538.gz/\",sep=\"\\t\").show()\n\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-07-30/process_hour=0/device_code=android-all/market_code=amazon-store\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200120-100742_2064901192","metadata":{},"outputs":[],"source":["%%sh\necho 'raw:'\naws s3 ls s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2017-06-22/14/win/40600000049538.gz\n \necho 'unified:'\naws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-06-22/process_hour=1/\n"]},{"cell_type":"code","execution_count":0,"id":"20200126-092412_1411655384","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_d = s3_bucket_list.all(prefix=\"review-uniform/DAILY/\", depth_is_1=True)\ndate_list_raw = [x.split(\"/\")[2] for x in path_d]\n\ns3_bucket_list_unified = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_d_unified = s3_bucket_list_unified.all(prefix=\"unified/review.v1/fact/process_granularity=daily/\", depth_is_1=True)\ndate_list_unified = [x.split(\"/\")[4].split(\"=\")[1] for x in path_d_unified]\n\nnot_exist_data_set = sorted(set(date_list_raw) - set(date_list_unified))\n# logger.info('Need to check !!! date is not existed!!!!! {}'.format(not_exist_data_set))\n\nfor x in not_exist_data_set:\n    el = \"review-uniform/DAILY/{}/\".format(x)\n    path_d.remove(el)\n# logger.info('test date list {}'.format(path_d))\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n# compared the data from uniform(raw data) to unified data\nraw_path_d = \"s3://prod_appannie_uniform_public_app_store_data/\"\n\nunified_path_d = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date={unified_process_date}/process_hour={unified_process_hour}/device_code={unified_device_code}/market_code={unified_market_code}\"\n\n\ndef calculate_sum_data(deep_raw_path):\n    logger.info(\"deep_raw_path {}\".format(deep_raw_path))\n    count_one_day = 0\n    # for dp in deep_raw_path:\n        # raw_count = spark.read.csv(raw_path_d + dp, sep=\"\\t\").count()\n    count_one_day = count_one_day + 1\n        # logger.info('count for one day {}'.format(count_one_day))\n        # logger.info('deep_raw_path[0] is {}'.format(deep_raw_path[0]))\n        # logger.info('dp is : {} '.format(dp))\n\n    return count_one_day, deep_raw_path\n\n\ndef count_raw_unified_data_daily():\n    for p in path_d:\n        process_hour_raw_path = s3_bucket_list.all(p, depth_is_1=True)\n        for x in process_hour_raw_path:\n            logger.info('raw path with process hour {}'.format(x))\n            platforms_process_hour_raw_path_list = s3_bucket_list.all(x, depth_is_1=True)\n            for raw_path in platforms_process_hour_raw_path_list: \n                print \"items in platforms_process_hour_raw_path_list\" , raw_path\n                # raw_count_sum, root_raw_data_path = calculate_sum_data(raw_path)\n                # logger.info('root_raw_data_path : {}'.format(raw_path))\n                # logger.info('raw_count_sum : {}'.format(raw_count_sum))\n            \ncount_raw_unified_data_daily()\n"]},{"cell_type":"code","execution_count":0,"id":"20200126-141658_1768683039","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_d = s3_bucket_list.all(prefix=\"review-uniform/DAILY/\", depth_is_1=True)\ndate_list_raw = [x.split(\"/\")[2] for x in path_d]\n\ns3_bucket_list_unified = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_d_unified = s3_bucket_list_unified.all(prefix=\"unified/review.v1/fact/process_granularity=daily/\", depth_is_1=True)\ndate_list_unified = [x.split(\"/\")[4].split(\"=\")[1] for x in path_d_unified]\n\nnot_exist_data_set = sorted(set(date_list_raw) - set(date_list_unified))\n# logger.info('Need to check !!! date is not existed!!!!! {}'.format(not_exist_data_set))\n\nfor x in not_exist_data_set:\n    el = \"review-uniform/DAILY/{}/\".format(x)\n    path_d.remove(el)\nprint path_d[699]"]},{"cell_type":"code","execution_count":0,"id":"20200128-061637_1590930020","metadata":{},"outputs":[],"source":["\nfrom elasticsearch import Elasticsearch\nimport time\nimport datetime\nfrom pyspark.sql import Row\n\n\ndef get_connection():\n    es_connection = Elasticsearch([\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/\"],http_auth=(\"bdp_qa\",\"6trmJcZBJrKWdtN2\"),port=19200)\n    return es_connection\ncon_es=get_connection()\nquery='''{\"query\":{\"bool\":{\"must\":[{\"match\":{\"market_code\":\"apple-store\"}}]}},\"aggs\":{\"review_id\":{\"cardinality\":{\"field\":\"_id\"}}},\"size\":1,\"sort\":[{\"date\":{\"order\":\"desc\"}}]}'''\ndef get_result(query):\n    result = con_es.search(index=\"int-ss-review_v2-apple-store-ios-all-201703\", body=query)\n    print result[\"hits\"][\"total\"][\"value\"]\n\nget_result(query)"]},{"cell_type":"code","execution_count":0,"id":"20200206-062537_2024296195","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}