{"cells":[{"cell_type":"code","execution_count":0,"id":"20200306-053918_1894176984","metadata":{},"outputs":[],"source":["%md\nreview migration:\noriginal data: ```s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={hourly, daily, weekly, monthly}/```\nDelta lake data: ```s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact```\nprocess date : from beginning to 2020-02-24\n\ntest point:\n1. maybe some duplicate data generated during migration, so please help to do the review distinct count check\n2. take like 100 samples from different platform do the detail check"]},{"cell_type":"code","execution_count":0,"id":"20200306-054032_73625518","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=weekly/process_date=2020-01-27/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/_delta_log/\naws s3 cp s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/_delta_log/00000000000000000000.json -\n\n# aws s3 ls \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2020-01-25/process_hour=23/device_code=ios-all/market_code=apple-store/\" --recursive --human-readable --summarize"]},{"cell_type":"code","execution_count":0,"id":"20200306-062659_771510336","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/_delta_log/\n\n# aws s3 ls \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=daily/process_date=2017-06-30/process_hour=23/device_code=ios-all/market_code=apple-store/\" --recursive --human-readable --summarize | tail -5\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200331-072535_749308359","metadata":{},"outputs":[],"source":["\ndf_ori = spark.read.format('delta').option(\"versionAsOf\", '106').load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\")\ndf_ori.show(1, False, True)\n\ndf_del = spark.read.format('delta').option(\"versionAsOf\", '2920').load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\")\ndf_del.show(1, False, True)\n\nprint df_ori.filter(\"process_granularity='weekly' and process_date='2020-01-27'\").count()\nprint df_del.filter(\"process_granularity='weekly' and process_date='2020-01-27'\").count()\n\ndf_a = df_ori.filter(\"process_granularity='weekly' and process_date='2020-01-27'\")\ndf_b = df_del.filter(\"process_granularity='weekly' and process_date='2020-01-27'\")\ndf_c = df_a.subtract(df_b)\ndf_c.show(1)"]},{"cell_type":"code","execution_count":0,"id":"20200331-113847_604254820","metadata":{},"outputs":[],"source":["\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", '2020-02-19').load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\")\ndf.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200331-111050_1614413632","metadata":{},"outputs":[],"source":["\ndf_ori.createOrReplaceTempView(\"table\")\ndf_ori2 = spark.sql(\"DESCRIBE HISTORY delta.`table`\")"]},{"cell_type":"code","execution_count":0,"id":"20200331-111423_367226555","metadata":{},"outputs":[],"source":["\nlatest_version = spark.sql(\"SELECT max(version) FROM (DESCRIBE HISTORY delta.`/mnt/delta/events`)\").collect()\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version[0][0]).load(\"/mnt/delta/events\")\n\n# Every query that stems off df will use the same snapshot"]},{"cell_type":"code","execution_count":0,"id":"20200331-104441_600056945","metadata":{},"outputs":[],"source":["\nprint df_ori.filter(\"process_granularity='weekly' and process_date='2020-01-27'\").count()\nprint df_del.filter(\"process_granularity='weekly' and process_date='2020-01-27'\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200331-103742_443444345","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={}/process_date={}/\".format('weekly', '2020-01-27'))\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=weekly/process_date=2020-01-27/\")\n\ndf1.show(1, False, True)\ndf2.show(1, False, True)\n\nprint df1.count()\nprint df2.count()"]},{"cell_type":"code","execution_count":0,"id":"20200331-085008_1193531844","metadata":{},"outputs":[],"source":["\nprint df_ori.where(\"_identifier='220200218032358710'\").count()\nprint df_del.where(\"_identifier LIKE '220200218%' and process_granularity='weekly' and device_code='android-all' and market_code='google-play'\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200331-095040_2057329635","metadata":{},"outputs":[],"source":["\ndf_ori.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200313-012724_1063856203","metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":0,"id":"20200306-070853_1974759587","metadata":{},"outputs":[],"source":["\ns3_path1 = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-06-10/\"\ndf1 = spark.read.parquet(s3_path1)\nprint df1.distinct().count()\ndf1.show(5, truncate=False, vertical=True)\ndf1.unpersist()"]},{"cell_type":"code","execution_count":0,"id":"20200310-080901_448668015","metadata":{},"outputs":[],"source":["\ns3_path2 = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=daily/process_date=2017-06-10/\"\ndf2 = spark.read.parquet(s3_path2)\nprint df2.distinct().count()\ndf2.show(5, truncate=False, vertical=True)"]},{"cell_type":"code","execution_count":0,"id":"20200310-021408_1686863915","metadata":{},"outputs":[],"source":["\nimport datetime, re\nfrom collections import defaultdict\n\n# start_date = datetime.date(1998, 3, 31)\n# end_date = datetime.date(2020, 2, 24)\nstart_date = datetime.date(2019, 1, 1)\nend_date = datetime.date(2020, 2, 23)\ndate_range = end_date - start_date\ndate_range = date_range.days + 1\ngranularity_list = ['hourly', 'daily', 'weekly', 'monthly']\n# granularity_list = ['hourly']\ndevice_code_list = ['ios-all', 'android-all', 'windows-phone', 'all']\nfails_list = []\nfails_detail_list = []\n\n# test code\n# granularity = \"hourly\"\n# process_date = \"2019-02-02\"\ndef detail_check(num, df_orig, df_delta, granularity, process_date):\n    fails_dict = defaultdict(list)\n    # df_diff = df_delta.head(num).subtract(df_orig.head(num))\n    # return df_diff\n    orig_review_ids = [row[\"review_id\"] for row in df_orig.head(num)]\n    delta_review_ids = [row[\"review_id\"] for row in df_delta.head(num)]\n    print(\"orig:\", orig_review_ids)\n    print(\"delta:\", delta_review_ids)\n    return orig_review_ids - delta_review_ids\n    # for review_id in review_ids:\n    #     if not df_delta.where(\"review_id='{}'\".format(review_id)):\n    #     fails_dict[\"granularity={}, process_date={}\".format(granularity, process_date)].append(review_id)\n    \n    # return fails_dict\n\nfor granularity in granularity_list:\n    for _ in range(date_range):\n        process_date = start_date.strftime(\"%Y-%m-%d\")\n        try:\n            orig_s3_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={}/process_date={}/\"\n            delta_s3_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={}/process_date={}/\"\n            \n            df_orig = spark.read.parquet(orig_s3_path.format(granularity, process_date)).distinct().sort(\"review_id\")\n            df_delta = spark.read.parquet(delta_s3_path.format(granularity, process_date)).distinct().sort(\"review_id\")\n            \n            dis_count_orig = df_orig.count()\n            dis_count_delta = df_delta.count()\n            \n            # print \"No distinct: \", count_orig, count_delta\n            # print \"Distinct: \", dis_count_orig, dis_count_delta\n            if not dis_count_orig == dis_count_delta:\n                fails_list.append((granularity, process_date))\n                print dis_count_orig - dis_count_delta\n            fails_detail_list.append(detail_check(10, df_orig, df_delta, granularity, process_date))\n            # detail_check(10, df_orig, df_delta, granularity, process_date)\n            print \"DONE\"\n        except Exception as e:\n            if re.search(r\"Path does not exist:\", str(e)):\n                print e\n        df_orig.unpersist()\n        df_delta.unpersist()\n        start_date += datetime.timedelta(days=1)\n"]},{"cell_type":"code","execution_count":0,"id":"20200310-024610_1802170540","metadata":{},"outputs":[],"source":["\nprint(fails_list)\nprint(fails_dict_list)\nprint df_orig.head(10)\nprint df_delta.head(10)"]},{"cell_type":"code","execution_count":0,"id":"20200310-065251_8141971","metadata":{},"outputs":[],"source":["\nstart_date = datetime.date(1998, 3, 31)\nend_date = datetime.date(2020, 2, 24)\ndate_range = end_date - start_date\ndate_range = date_range.days + 1\nprint(date_range)"]},{"cell_type":"code","execution_count":0,"id":"20200313-020449_1185442852","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.loader import es\nfrom applications.db_check_v1.common.table_common_info import urn\n\nclass AdvancedReviewDBData(object):\n    \"\"\"\n    Get data from database (ElasticSearch)\n    \"\"\"\n    _common_config = {\"database\": \"adv_review\"}\n    _index_config = \"int-ss-advancedreview_v1*\"\n\n    def get(self):\n        \"\"\"\n        :return: elasticsearch result\n        :rtype: dict\n        \"\"\"\n        query_body = {\n            \"track_total_hits\": \"true\",\n            \"query\": {\n                        \"match_all\": {}\n                }\n            }\n        es_conn = es.connection(urn, self._common_config)\n        es_data = es_conn.search(\n            index=self._index_config,\n            body=query_body)\n        return es_data\n        \nes_data = AdvancedReviewDBData().get()"]},{"cell_type":"code","execution_count":0,"id":"20200313-020842_2028345137","metadata":{},"outputs":[],"source":["\nes_data"]},{"cell_type":"code","execution_count":0,"id":"20200313-021023_1317806553","metadata":{},"outputs":[],"source":["\nes_total = es_data[\"hits\"][\"total\"][\"value\"]\nprint es_total"]},{"cell_type":"code","execution_count":0,"id":"20200313-022058_1551228877","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132 -p 5432 -U citus_bdp_usage_qa -d aa_citus_db << EOF\n\nSET search_path=advancedreview;\n\nBEGIN;\nselect count(*) from advancedreview_topic_fact_v3;\n\nSELECT * FROM \n\nCOMMIT;\nEOF "]},{"cell_type":"code","execution_count":0,"id":"20200313-031053_1155940974","metadata":{},"outputs":[],"source":["%python,\nimport unittest\n\ndef add(a, b):\n    return a+b\n\nclass TestES(unittest.TestCase):\n    def test_add(self):\n        self.assertTrue(add(2, 1)==1+2, \"Failed\")\n\nif __name__=='__main__':\n    unittest.main(argv=[''], exit=False, verbosity=2)"]},{"cell_type":"code","execution_count":0,"id":"20200313-022912_1876064330","metadata":{},"outputs":[],"source":["%python\nfrom \n# Copyright (c) 2019 App Annie Inc. All rights reserved.\n\n\"\"\"\nAdvanced Review Database Check:\n    1. The number of data in \"unified layer\" is equal to elasticsearch\n\"\"\"\n\nimport unittest\nfrom collections import defaultdict\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.table_common_info import urn\nfrom aadatapipelinecore.core.loader import es\n\n\nclass AdvancedReviewUnifiedData(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/\" \\\n                       \"unified/advancedreview.topic.v1/fact/process_date={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date):\n        \"\"\"\n        :param date: process_date in s3 bucket path\n        :type date: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(date))\n        return unified_df\n\n\nclass AdvancedReviewDBData(object):\n    \"\"\"\n    Get data from database (ElasticSearch)\n    \"\"\"\n    _common_config = {\"database\": \"adv_review\"}\n    _index_config = \"int-ss-advancedreview_v1*\"\n\n    def get(self, country_code, device_code, identifier):\n        \"\"\"\n        :return: elasticsearch result\n        :rtype: dict\n        \"\"\"\n        query_body = {\n            \"track_total_hits\": \"true\",\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\"match\": {\"country_code\": country_code}},\n                        {\"match\": {\"device_code\": device_code}},\n                        {\"match\": {\"_identifier\": identifier}}\n                    ]\n                }\n            }\n        }\n        es_conn = es.connection(urn, self._common_config)\n        es_data = es_conn.search(\n            index=self._index_config,\n            body=query_body)\n        return es_data\n\n\nclass TestTopicJoinReview(PipelineTest):\n    \"\"\"\n    Compare ES data amount with unified review data\n    \"\"\"\n    trigger_date_config = (\"10 10 * * *\", 1)\n\n    def setUp(self):\n        self.country_code_list = [\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n        self.device_code_list = [\"ios-all\", \"android-all\"]\n        self.granularity = 'daily'\n        self.failed_ids = defaultdict(list)\n\n    def _compare_diff(self, device_code, country_code, unified_df, identifier):\n        unified_count = unified_df.where(\n            \"country_code='{}' AND device_code='{}'\".format(country_code, device_code)).count()\n        es_data = AdvancedReviewDBData().get(country_code, device_code, identifier)\n        es_total = es_data[\"hits\"][\"total\"][\"value\"]\n        if not es_total == unified_count:\n            self.failed_ids[device_code].append(country_code)\n\n    def test_advanced_review_topic_join_review(self):\n        unified_df = AdvancedReviewUnifiedData(self.spark).get(self.check_date_str)\n        identifier = unified_df.collect()[0][\"_identifier\"]\n        for device_code in self.device_code_list:\n            for country_code in self.country_code_list:\n                self._compare_diff(device_code, country_code, unified_df, identifier)\n        \n        print failed_ids\n        self.assertTrue(len(self.failed_ids) == 0, self.failed_ids)\n\nif __name__=='__main__':\n    unittest.main(argv=[''], exit=False, verbosity=2)"]},{"cell_type":"code","execution_count":0,"id":"20200313-031143_888512949","metadata":{},"outputs":[],"source":["%python\nfrom aadatapipelinecore.core.utils.spark import create_spark\nspark = create_spark()"]},{"cell_type":"code","execution_count":0,"id":"20200313-082924_1072640986","metadata":{},"outputs":[],"source":["\nspark"]},{"cell_type":"code","execution_count":0,"id":"20200313-101821_1583237671","metadata":{},"outputs":[],"source":["\nfrom applications.db_check_v1.common.base_test import PipelineTest\n\na = PipelineTest.setUpClass()\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-101910_727270939","metadata":{},"outputs":[],"source":["\nspark.sparkContext"]},{"cell_type":"code","execution_count":0,"id":"20200313-102117_895417911","metadata":{},"outputs":[],"source":["\nimport pyspark"]},{"cell_type":"code","execution_count":0,"id":"20200313-102135_1985609707","metadata":{},"outputs":[],"source":["\nfrom pyspark import SparkContext, SparkConf\n\nconf_cust = SparkConf().setAppName('test').setMaster('local')\nsc = SparkContext(conf=conf)"]},{"cell_type":"code","execution_count":0,"id":"20200313-103006_1108499958","metadata":{},"outputs":[],"source":["%\nfrom pyspark import SparkContext, SparkConf\n\nconf_cust = SparkConf().setAppName('test').setMaster('local')\nsc = SparkContext(conf=conf)"]},{"cell_type":"code","execution_count":0,"id":"20200330-041404_795897899","metadata":{},"outputs":[],"source":["\ns3_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\"\ndf1 = spark.read.format('delta').load(s3_path).where(\"process_granularity='weekly'\")\nprint df1.distinct().count()\n\ndf2 = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(\"process_granularity='weekly'\")\nprint df2.distinct().count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-103105_1870279735","metadata":{},"outputs":[],"source":["\nimport datetime\nimport re\n\n# def review_migration(spark):\n\n# granularity_list = ['hourly']\n# granularity_list = ['daily']\n# granularity_list = ['weekly']\ngranularity_list = ['monthly']\n# granularity_list = ['hourly', 'daily', 'weekly', 'monthly']\n# granularity_list = ['weekly']\nfails_list = []\nfails_detail_list = []\norig_s3_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={}/process_date={}/\"\ndelta_s3_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\"\n# df_orig = spark.read.parquet(orig_s3_path)\n# df_delta = spark.read.format('delta').load(delta_s3_path)\n\n# df_ori = spark.read.format('delta').option(\"versionAsOf\", '106').load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\")\n# df_ori.show(1, False, True)\n\n# df_del = spark.read.format('delta').option(\"versionAsOf\", '2920').load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\")\n# df_del.show(1, False, True)\n\nfor granularity in granularity_list:\n    # start_date = datetime.date(1998, 3, 31)\n    # end_date = datetime.date(2020, 2, 24)\n    start_date = datetime.date(2006, 3, 31)\n    end_date = start_date\n    # start_date = datetime.date(2020, 1, 11)\n    # end_date = datetime.date(2020, 2, 11)\n    date_range = end_date - start_date\n    date_range = date_range.days + 1\n    # print date_range\n    for _ in range(date_range):\n        process_date = start_date.strftime(\"%Y-%m-%d\")\n        try:\n            df_orig_temp = spark.read.parquet(orig_s3_path.format(granularity, process_date))\n            df_delta_temp = spark.read.format('delta').load(delta_s3_path).where(\"process_granularity='{}' and process_date='{}'\".format(granularity, process_date))\n            # dis_count_orig = df_orig_temp.distinct().count()\n            # dis_count_delta = df_delta_temp.distinct().count()\n            # print \"org = {}, delta = {}\".format(dis_count_orig, dis_count_delta)\n            # if not dis_count_orig == dis_count_delta:\n            #     print \"Failed\"\n            #     fails_list.append((granularity, process_date))\n            #     print \"Diff = \", dis_count_orig - dis_count_delta\n            #     fails_list.append((granularity, process_date))\n            df_sub = df_orig_temp.select('review_id').subtract(df_delta_temp.select('review_id'))\n            if df_sub.take(1):\n                orig_count, delta_count = df_orig_temp.distinct().count(), df_delta_temp.distinct().count()\n                print \"-----------------------------------------------\"\n                print 'process_date:', process_date, 'granularity:', granularity\n                print 'orignal data:', orig_count, 'delta data:', delta_count\n                # print \"Diff length: \", len(df_sub.collect())\n                fails_list.append((granularity, process_date))\n            df_orig_temp.unpersist()\n            df_delta_temp.unpersist()\n        except Exception as e:\n            if not re.search(r'Path does not exist:', str(e)):\n                print e\n\n        start_date += datetime.timedelta(days=1)\n\nprint \"fails_list\", fails_list\n# def main(spark, params):\n# review_migration(spark)\n\n# main(spark, None)\n"]},{"cell_type":"code","execution_count":0,"id":"20200408-093531_55193959","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date=2006-03-31/process_hour=23/device_code=android-all/market_code=amazon-store/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=weekly/process_date=2017-09-11/process_hour=23/device_code=ios-all/market_code=apple-store/\naws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-12-21/"]},{"cell_type":"code","execution_count":0,"id":"20200408-093529_63767765","metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":0,"id":"20200407-100058_567046572","metadata":{},"outputs":[],"source":["\naa = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-12-21/\").count()\nprint aa"]},{"cell_type":"code","execution_count":0,"id":"20200408-031305_202403866","metadata":{},"outputs":[],"source":["\nbb = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=hourly/process_date=2018-09-02/\").count()\nprint bb"]},{"cell_type":"code","execution_count":0,"id":"20200408-091050_2081881427","metadata":{},"outputs":[],"source":["\nfails_list = [('hourly', '2018-09-02'), ('hourly', '2019-01-18'), ('daily', '2017-12-21'), ('daily', '2019-04-03'), ('daily', '2019-04-19'), ('weekly', '2017-09-11'), ('monthly', '2006-03-31')]\nlog = []\nfor g, p in fails_list:\n    aa = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={}/process_date={}/\".format(g, p)).take(1)\n    bb = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity={}/process_date={}/\".format(g, p)).take(1)\n    print '---------------------------------'\n    print g, p\n    print aa[0]['_identifier'], bb[0]['_identifier']\n    log.append((g, p, aa[0]['_identifier']))\nprint log"]},{"cell_type":"code","execution_count":0,"id":"20200407-100144_1045339881","metadata":{},"outputs":[],"source":["\ndf_orig_temp.distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200407-100404_871063722","metadata":{},"outputs":[],"source":["\n9763666 - 4405157"]},{"cell_type":"code","execution_count":0,"id":"20200408-093520_144220234","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20200407-095856_1883752923","metadata":{},"outputs":[],"source":["\ndf_orig_temp.show(3, True, True)"]},{"cell_type":"code","execution_count":0,"id":"20200407-095945_76328818","metadata":{},"outputs":[],"source":["\ndf_delta_temp.show(3, True, True)"]},{"cell_type":"code","execution_count":0,"id":"20200407-100007_770310220","metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":0,"id":"20200316-072001_616594237","metadata":{},"outputs":[],"source":["\na = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={}/process_date={}/\".format('hourly', '2020-02-11'))\na.show(1, False, True)"]},{"cell_type":"code","execution_count":0,"id":"20200402-114729_222901589","metadata":{},"outputs":[],"source":["\nb = spark.read.format('delta').load(delta_s3_path).where(\"process_granularity='{}' and process_date='{}'\".format('hourly', '2020-02-11'))\nb.show(1, False, True)"]},{"cell_type":"code","execution_count":0,"id":"20200402-115052_1410638520","metadata":{},"outputs":[],"source":["\na.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200402-115110_2005822700","metadata":{},"outputs":[],"source":["\nb.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200402-120346_996653718","metadata":{},"outputs":[],"source":["\na.select('review_id').subtract(b.select('review_id')).show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200402-120719_338337934","metadata":{},"outputs":[],"source":["\nb.select('review_id').subtract(a.select('review_id')).show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200402-114548_1098549662","metadata":{},"outputs":[],"source":["\nprint u\"Except can only be performed on tables with the compatible column types. string <> boolean at the 20th column of the second table;;\\n'Except false\\n:- Relation[_identifier#1798L,app_id#1799L,content#1800,content_language#1801,country_code#1802,language#1803,product_version#1804,rating#1805,reply#1806,reply_date#1807,reply_id#1808,reply_language#1809,review_id#1810,time#1811,title#1812,title_language#1813,user_device#1814,user_id#1815,user_language#1816,user_name#1817,user_purchased#1818,user_review_url#1819,process_hour#1820,device_code#1821,market_code#1822] parquet\\n+- Project [_identifier#1848L, app_id#1849L, content#1850, content_language#1851, country_code#1852, device_code#1853, language#1854, market_code#1855, process_hour#1858, product_version#1859, rating#1860, reply#1861, reply_date#1862, reply_id#1863, reply_language#1864, review_id#1865, time#1866, title#1867, title_language#1868, user_device#1869, user_id#1870, user_language#1871, user_name#1872, user_purchased#1873, user_review_url#1874]\\n   +- Filter ((process_granularity#1857 = hourly) && (process_date#1856 = 2020-01-18))\\n      +- Relation[_identifier#1848L,app_id#1849L,content#1850,content_language#1851,country_code#1852,device_code#1853,language#1854,market_code#1855,process_date#1856,process_granularity#1857,process_hour#1858,product_version#1859,rating#1860,reply#1861,reply_date#1862,reply_id#1863,reply_language#1864,review_id#1865,time#1866,title#1867,title_language#1868,user_device#1869,user_id#1870,user_language#1871,... 3 more fields] parquet\\n\"\n"]},{"cell_type":"code","execution_count":0,"id":"20200402-115410_2015347842","metadata":{},"outputs":[],"source":["\na_set = set(['_identifier'\n,'app_id'\n,'content'\n,'content_language'\n,'country_code'\n,'language'\n,'product_version'\n,'rating'\n,'reply'\n,'reply_date'\n,'reply_id'\n,'reply_language'\n,'review_id'\n,'time'\n,'title'\n,'title_language'\n,'user_device'\n,'user_id'\n,'user_language'\n,'user_name'\n,'user_purchased'\n,'user_review_url'\n,'process_hour'\n,'device_code'\n,'market_code'])\n\nb_set = set([\n'_identifier'\n,'app_id'\n,'content'\n,'content_language'\n,'country_code'\n,'device_code'\n,'language'\n,'market_code'\n,'process_date'\n,'process_granularity'\n,'process_hour'\n,'product_version'\n,'rating'\n,'reply'\n,'reply_date'\n,'reply_id'\n,'reply_language'\n,'review_id'\n,'time'\n,'title'\n,'title_language'\n,'user_device'\n,'user_id'\n,'user_language'\n,'user_name'\n,'user_purchased'\n,'user_review_url'])\na_set - b_set"]},{"cell_type":"code","execution_count":0,"id":"20200402-120138_1413731870","metadata":{},"outputs":[],"source":["\nimport datetime\nimport re\n\n# def review_migration(spark):\n\n# granularity_list = ['hourly']\ngranularity_list = ['daily']\n# granularity_list = ['weekly']\n# granularity_list = ['monthly']\n# granularity_list = ['hourly', 'daily', 'weekly', 'monthly']\n# granularity_list = ['weekly']\nfails_list = []\nfails_detail_list = []\norig_s3_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={}/process_date={}/\"\ndelta_s3_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\"\n# df_orig = spark.read.parquet(orig_s3_path)\n# df_delta = spark.read.format('delta').load(delta_s3_path)\n\n# df_ori = spark.read.format('delta').option(\"versionAsOf\", '106').load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\")\n# df_ori.show(1, False, True)\n\n# df_del = spark.read.format('delta').option(\"versionAsOf\", '2920').load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\")\n# df_del.show(1, False, True)\n\nfor granularity in granularity_list:\n    # start_date = datetime.date(1998, 3, 31)\n    # end_date = datetime.date(2020, 2, 24)\n    start_date = datetime.date(2017, 12, 21)\n    end_date = start_date\n    # start_date = datetime.date(2020, 1, 11)\n    # end_date = datetime.date(2020, 2, 11)\n    date_range = end_date - start_date\n    date_range = date_range.days + 1\n    # print date_range\n    for _ in range(date_range):\n        process_date = start_date.strftime(\"%Y-%m-%d\")\n        try:\n            df_orig_temp = spark.read.parquet(orig_s3_path.format(granularity, process_date))\n            df_delta_temp = spark.read.format('delta').load(delta_s3_path).where(\"process_granularity='{}' and process_date='{}'\".format(granularity, process_date))\n            \n            df_sub = df_orig_temp.select('review_id').subtract(df_delta_temp.select('review_id'))\n            if df_sub.take(1):\n                orig_count, delta_count = df_orig_temp.count(), df_delta_temp.count()\n                print \"-----------------------------------------------\"\n                print 'process_date:', process_date, 'granularity:', granularity\n                print 'orignal data:', orig_count, 'delta data:', delta_count\n                # print \"Diff length: \", len(df_sub.collect())\n                fails_list.append((granularity, process_date))\n            df_orig_temp.unpersist()\n            df_delta_temp.unpersist()\n        except Exception as e:\n            if not re.search(r'Path does not exist:', str(e)):\n                print e\n\n        start_date += datetime.timedelta(days=1)\n\nprint \"fails_list\", fails_list\n# def main(spark, params):\n# review_migration(spark)\n\n# main(spark, None)\n"]},{"cell_type":"code","execution_count":0,"id":"20200316-072253_1946876613","metadata":{},"outputs":[],"source":["\nimport datetime\nimport re\n\n\ndef detail_check(num, df_orig, df_delta):\n    # df_diff = df_delta.head(num).subtract(df_orig.head(num))\n    # return df_diff\n    orig_review_ids = [row[\"review_id\"] for row in df_orig.head(num)]\n    delta_review_ids = [row[\"review_id\"] for row in df_delta.head(num)]\n    if not orig_review_ids == delta_review_ids:\n        print(\"orig:\", orig_review_ids)\n        print(\"delta:\", delta_review_ids)\n    return orig_review_ids == delta_review_ids, orig_review_ids, delta_review_ids\n    # for review_id in review_ids:\n    #     if not df_delta.where(\"review_id='{}'\".format(review_id)):\n    #     fails_dict[\"granularity={}, process_date={}\".format(granularity, process_date)].append(review_id)\n\n    # return fails_dict\n\n\ndef review_migration(spark):\n\n    granularity_list = ['hourly', 'daily', 'weekly', 'monthly']\n    # granularity_list = ['hourly']\n    # device_code_list = ['ios-all', 'android-all', 'windows-phone', 'all']\n    fails_list = []\n    fails_detail_list = []\n\n    for granularity in granularity_list:\n        start_date = datetime.date(1998, 3, 31)\n        end_date = datetime.date(2020, 2, 24)\n        # start_date = datetime.date(2020, 1, 1)\n        # end_date = datetime.date(2020, 1, 2)\n        date_range = end_date - start_date\n        date_range = date_range.days + 1\n        for _ in range(date_range):\n            process_date = start_date.strftime(\"%Y-%m-%d\")\n            try:\n                orig_s3_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={}/process_date={}/\"\n                delta_s3_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity={}/process_date={}/\"\n\n                df_orig = spark.read.parquet(orig_s3_path.format(granularity, process_date)).distinct().sort(\n                    \"review_id\")\n                df_delta = spark.read.parquet(delta_s3_path.format(granularity, process_date)).distinct().sort(\n                    \"review_id\")\n\n                dis_count_orig = df_orig.count()\n                dis_count_delta = df_delta.count()\n\n                # print \"No distinct: \", count_orig, count_delta\n                # print \"Distinct: \", dis_count_orig, dis_count_delta\n                if not dis_count_orig == dis_count_delta:\n                    fails_list.append((granularity, process_date))\n                    print dis_count_orig - dis_count_delta\n                fails_detail_list.append(detail_check(10, df_orig, df_delta))\n                # detail_check(10, df_orig, df_delta, granularity, process_date)\n                df_orig.unpersist()\n                df_delta.unpersist()\n                print \"DONE\"\n            except Exception as e:\n                if re.search(r'Path does not exist:', str(e)):\n                    # print e\n                    pass\n\n            start_date += datetime.timedelta(days=1)\n    for i in fails_detail_list:\n        if not i[0]:\n            print \"fails_detail_list = \", i\n\n\ndef main(spark, params):\n    review_migration(spark)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}