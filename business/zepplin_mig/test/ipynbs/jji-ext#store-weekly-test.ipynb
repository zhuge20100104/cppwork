{"cells":[{"cell_type":"code","execution_count":0,"id":"20200625-014534_288143136","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nstart_date = datetime.datetime.strptime('2020-01-01', '%Y-%m-%d')\nend_date = datetime.datetime.strptime('2020-01-31', '%Y-%m-%d')\ngranularity = 'quarterly'\n\n\ndef get_date_range(granularity, start_date):\n    if granularity == 'weekly':\n        end_date = start_date + relativedelta(weeks=1)\n    elif granularity == 'monthly':\n        end_date = start_date + relativedelta(months=1)\n    elif granularity == 'quarterly':\n        end_date = start_date + relativedelta(months=3)\n    elif granularity == 'yearly':\n        end_date = start_date + relativedelta(months=12)\n    return start_date\n\n\nstart = start_date\nwhile start < end_date:\n    end = get_date_range(granularity, start)\n\n    category_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n        \"store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\n\n    agg_df = category_daily_df.groupBy('app_id', 'country_code', 'device_code', 'category_id').agg(\n        sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), \n            sum('est_revenue').alias('revenue'))\n    agg_df.createOrReplaceTempView(\"agg_df\")\n\n    if granularity == 'weekly' or granularity == 'monthly':\n        pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n            \"store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n    else:\n        pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n            \"store.app-est-category-pre-aggr.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n\n    pre_agg_df.createOrReplaceTempView(\"pre_agg_df\")\n\n    diff_df1 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from agg_df \n                            except all \n                            select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from pre_agg_df\"\"\")\n    diff_df2 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from pre_agg_df \n                            except all \n                            select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from agg_df\"\"\")\n    # print agg_df.count(), pre_agg_df.count()\n    diff_count1 = diff_df1.count()\n    diff_count2 = diff_df2.count()\n    if diff_count1 != 0 or diff_count2 != 0:\n        if diff_count1 != 0:\n            df_write_result = diff_df1.take(20)\n        else:\n            df_write_result = diff_df2.take(20)\n\n        from aadatapipelinecore.core.utils.retry import retry\n\n        def write_test_result(result_df):\n            result_df.write.format(\"delta\").save(\n                \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_store_dump_v1_count_0608/daily/\",\n                mode=\"append\",\n                partitionBy=[\"type\"])\n        retry(write_test_result, (df_write_result,), {}, interval=10)\n\n        print \"Store Category Test FAIL!!!!! date: {}, diff_count1: {}, diff_count2: {}\".format(end, diff_count1, diff_count2)\n    elif diff_count1 == 0 and diff_count2 == 0:\n        print \"Store Category Test PASS! date: {}, diff_count1: {}, diff_count2: {}\".format(end, diff_count1, diff_count2)\n    start = end"]},{"cell_type":"code","execution_count":0,"id":"20200625-015008_177319205","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/granularity=weekly/"]},{"cell_type":"code","execution_count":0,"id":"20200625-015155_534750061","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/granularity=weekly/"]},{"cell_type":"code","execution_count":0,"id":"20200625-015750_1161212454","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}