{"cells":[{"cell_type":"code","execution_count":0,"id":"20200610-122249_450032386","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nimport psycopg2\nimport datetime\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\n\nimport aaplproxy\n\n\nstart='2020-04-17'\nend='2020-04-18'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\ndates.reverse()\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n\n\ndef test_download_attribution_db(test_date):\n    print 'test_data:' , test_date\n\n    spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\"granularity='daily' and  date in ('{}') \".format(test_date)).createOrReplaceTempView(\"unified_download_attribution\")\n\n    sql_text = \"\"\"\n    \n    WITH download_attribution AS (\n      SELECT app_id, coalesce(free_app_download, 0 ) as free_app_download, coalesce(paid_app_download, 0 ) as paid_app_download, coalesce(revenue, 0 ) as revenue, device_code, country_code, organic_download_share from unified_download_attribution\n    );\n    \n    WITH caculate_data AS (\n        SELECT CAST(ROUND (organic_download_share * (free_app_download+paid_app_download)) AS int) AS est_organic_download, \n        CAST(free_app_download+paid_app_download - ROUND(organic_download_share * (free_app_download+paid_app_download)) AS int) as est_paid_download,\n        app_id, \n        free_app_download AS est_free_app_download, \n        paid_app_download AS est_paid_app_download, \n        revenue AS est_revenue, \n        device_code, \n        country_code\n        FROM download_attribution\n    );\n    \n    \n        -- rank_unified,store_unified\n    WITH unified_data_test AS \n    ( \n                    SELECT          store_unified.country_code, \n                                    store_unified.device_code, \n                                    store_unified.free_app_download AS est_free_app_download , \n                                    store_unified.paid_app_download AS est_paid_app_download, \n                                    store_unified.revenue           AS est_revenue, \n                                    store_unified.revenue_iap       AS est_revenue_iap, \n                                    store_unified.revenue_non_iap   AS est_revenue_non_iap, \n                                    rank_unified.category_id, \n                                    rank_unified.app_id, \n                                    rank_unified.free_app_download, \n                                    rank_unified.paid_app_download, \n                                    rank_unified.revenue, \n                                    rank_unified.revenue_iap, \n                                    rank_unified.revenue_non_iap, \n                                    rank_unified.granularity, \n                                    rank_unified.date \n                    FROM            rank_unified \n                    FULL OUTER JOIN store_unified \n                    ON              rank_unified.app_id = store_unified.app_id \n                    AND             rank_unified.country_code = store_unified.country_code \n                    AND             rank_unified.device_code = store_unified.device_code \n                    AND             rank_unified.date = store_unified.date );\n    \n    \n    \n    WITH unified_rank_filter_data_free_app_download AS \n    ( \n           SELECT * \n           FROM   unified_data_test \n           WHERE ( ( ( \n                                free_app_download<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                free_app_download<=4000 \n                         AND    country_code==\"WW\" ) ) \n           OR     ( ( \n                                paid_app_download<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                paid_app_download<=4000 \n                         AND    country_code==\"WW\" ) ) \n           OR     ( ( \n                                revenue<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                revenue<=4000 \n                         AND    country_code==\"WW\" ) ) )\n           AND    device_code!='ios-all'\n    );\n    \n    \n    \n           WITH unified_category_filter_data_free_app_download AS \n    ( \n           SELECT * ,\n           CASE WHEN (free_app_download > 1000 and country_code !='WW') or (free_app_download > 4000 and country_code =='WW' ) or (free_app_download is null or free_app_download <= 0) Then null else est_free_app_download END as est_free_app_download_category,\n           CASE WHEN (paid_app_download > 1000 and country_code !='WW') or (paid_app_download > 4000 and country_code =='WW' ) or (paid_app_download is null or paid_app_download <= 0) Then null else est_paid_app_download END as est_paid_app_download_category,\n           CASE WHEN (revenue > 1000 and country_code !='WW') or (revenue > 4000 and country_code =='WW') or (revenue is null or revenue <= 0) Then null else est_revenue  END as est_revenue_category\n           FROM   unified_rank_filter_data_free_app_download \n\n\n    );    \n\n            WITH download_attribution_join_rank_value AS (\n            \n            SELECT caculate_data.* FROM caculate_data \n            JOIN unified_category_filter_data_free_app_download\n            ON caculate_data.app_id = unified_category_filter_data_free_app_download.app_id\n            AND caculate_data.country_code = unified_category_filter_data_free_app_download.country_code\n            AND caculate_data.device_code = unified_category_filter_data_free_app_download.device_code\n\n            )\n    \n    \"\"\"\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n             {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n                # \"path\": est_list,\n    \n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n                # \"path\": rank_list,\n    \n            }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n\n\n    # device_code=[\"ios-phone\",\"ios-tablet\",\"android-all\"]\n\n    def citus_row(test_data):\n        def get_data_in_citus(date):\n            citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n    \n            sql = '''SELECT \n                        coalesce(est_organic_download, 0 ) AS est_organic_download,\n                        coalesce(est_paid_download, 0 ) AS est_paid_download,\n                        app_id,\n                        coalesce(est_free_app_download, 0 ) AS est_free_app_download,\n                        coalesce(est_paid_app_download, 0 ) AS est_paid_app_download,\n                        coalesce(est_revenue, 0 ) AS est_revenue,\n                        device_code,\n                        country_code\n                        FROM store.store_est_fact_v1 \n                        WHERE date='{}' \n                     '''.format(date)\n        \n            db_data = query(citus_dsn_, sql)\n            return db_data\n\n        def query(dsn, sql):\n            with psycopg2.connect(dsn) as conn:\n                conn.autocommit = True\n                with conn.cursor() as cur:\n                    cur.execute(sql)\n                    result = cur.fetchall()\n                    conn.commit()\n            return result\n    \n        result = get_data_in_citus(test_data)\n    # print result\n\n        return [Row(est_organic_download=r[0], est_paid_download=r[1], app_id=r[2], est_free_app_download=r[3], est_paid_app_download=r[4], est_revenue=r[5], device_code=r[6], country_code=r[7]) for r in result]\n\n\n    test_rdd = sc.parallelize(map(citus_row, [test_date]), 1).cache()\n\n    import pyspark\n    test_rdd.unpersist()\n    print test_rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n    test_rdd.getStorageLevel()\n    print(test_rdd.getStorageLevel())\n    # print test_rdd.collect()\n    new_rdd = test_rdd.flatMap(lambda x: x)\n    # print new_rdd.collect()\n\n    schema=StructType([StructField(\"app_id\", LongType(), True),  \n                   StructField(\"country_code\", StringType(), True), \n                   StructField(\"device_code\", StringType(), True), \n                   StructField(\"est_free_app_download\", LongType(), True), \n                   StructField(\"est_organic_download\", LongType(), True), \n                   StructField(\"est_paid_app_download\", LongType(), True),\n                   StructField(\"est_paid_download\", LongType(), True),\n                   StructField(\"est_revenue\", LongType(), True)\n                   ])\n\n    new_rdd.toDF(schema).createOrReplaceTempView(\"data_from_db\")\n\n    spark.sql(\"select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue  from data_from_db except all select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from download_attribution_join_rank_value \").show()\n    spark.sql(\"select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from download_attribution_join_rank_value except all select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from data_from_db \").show()\n\n    \n# PGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \n\n    \nsc.parallelize(map(test_download_attribution_db, dates), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200609-074539_1063856203","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-category-load.v3/fact/granularity=daily/date=2020-05-02/device_code=android-all/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/granularity=daily/date=2020-05-02/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/granularity=daily/date=2020-05-02/device_code=android-all/\n# aws s3 ls s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2018-05-12/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/granularity=daily/date=2018-05-10/\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v4/fact/granularity=daily/date=2020-07-11/ --recursive\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200609-082430_630401813","metadata":{},"outputs":[],"source":["\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v4/fact/granularity=daily/date=2020-07-03/\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200609-074602_1936759211","metadata":{},"outputs":[],"source":["\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/granularity=daily/date=2018-05-10/\").where(\"app_id='284882215' and country_code='US'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200510-063133_131535984","metadata":{},"outputs":[],"source":["%%sh\n# 2018-02-17, 2018-06-02, 2018-06-30, 2019-04-27, 2020-03-28, 2020-04-04\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v2/fact/granularity=daily/date=2020-04-04/device_code=android-all/\naws s3 ls s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2018-02-17/AU/ --recursive\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2018-02-17 --recursive | head -3\naws s3 ls s3://b2b-prod-int-data-pipeline-unified/unified/app-int.download-attribution.v1/metric/month=2018-02/date=2018-02-17/device_id=1001/store_id=1/ --recursive\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/download-attribution.v1/fact/month=2020-03/date=2020-03-28/ --recursive\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/download-attribution.v1/fact/month=2018-02/date=2018-02-17/device_id=1001/store_id=1/  --recursive\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/legacy_unified_backup/app-int.download-attribution.v1/fact/month=2018-02/date=2018-02-17/device_id=1001/store_id=1/  --recursive"]},{"cell_type":"code","execution_count":0,"id":"20200418-080906_1911405444","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2016-09-03\"\nend = \"2020-04-19\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append((\n            [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n            [i.strftime(\"%Y-%m-%d\") for i in item], \n            # [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v2/fact/granularity=daily/date={}\".format(i) for i in item], \n            [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]))\n        \nfor x in test_path:\n    print x[1]\n# [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(test_date)]"]},{"cell_type":"code","execution_count":0,"id":"20200418-055047_2014735472","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\nstart = \"2016-08-28\"\nend = \"2020-04-11\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\n# for x in sar_list:\n#     for key,item in x.items():\n#         test_path.append(\n#             (\n#                 [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n#                 [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v2/fact/granularity=daily/date={}\".format(i) for i in item], \n#                 [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n#             )\n#         )\n        \nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append(\n            (\n                [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n                [i.strftime(\"%Y-%m-%d\") for i in item], \n                [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n            )\n        )\n# print test_path\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n\n\ndef test_download_attribution(test_data):\n    # print test_data\n    spark.read.option(\"basePath\",\n                      \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\").parquet(\n        test_data[0][0]).createOrReplaceTempView(\n        \"download_attribution\")\n\n    # spark.read.option(\"basePath\",\n    #                   \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily\").parquet(\n    #     \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % \",\".join(test_path[0][1])).createOrReplaceTempView(\n    #     \"store_unified\")\n\n    # spark.read.option(\"basePath\",\n    #                   \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2016-09-03/\").parquet(\n    #     test_data[0][0]).createOrReplaceTempView(\n    #     \"download_attribution\")\n    \n    print test_data[1]\n    spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v2/fact/\").where(\"granularity='daily' and  date in ('{}') \".format(\"','\".join(\n        test_data[1]))).createOrReplaceTempView(\"test_unified_download_attribution\")\n\n    sql_text = \"\"\"\n\n    WITH download_attribution_1 AS (\n    \n     select device_code, country_code, device_code, granularity, date, product_id, CAST(est_non_organic_download_share as decimal) as est_non_organic_download_share, \n        case when device_code='android-phone' THEN 'android-all' \n             when device_code='ios-phone' THEN 'ios-tablet' END AS new_device_code from download_attribution\n    );\n\n    WITH download_attribution_2\n    AS (\n        select distinct * from ( select new_device_code as device_code, country_code, granularity, date, product_id, CAST(est_non_organic_download_share as decimal) as est_non_organic_download_share from download_attribution_1 \n        union all\n        select * from download_attribution ) as t1 where device_code!='android-phone'\n\n    );\n    \n    WITH union_data AS (\n    select *, store_unified.device_code as unified_device_code , store_unified.country_code as unified_country_code\n    from store_unified full outer join download_attribution_2\n    on store_unified.device_code=download_attribution_2.device_code and\n    store_unified.country_code=UPPER(download_attribution_2.country_code) and\n    store_unified.app_id=download_attribution_2.product_id\n    where est_non_organic_download_share is not null\n    );\n    \n    \n    WITH calculate_data_prepare AS (\n    select app_id,  coalesce(free_app_download, 0 ) as free_app_download,  coalesce(paid_app_download, 0 )  as paid_app_download,revenue, unified_device_code, unified_country_code, est_non_organic_download_share from union_data\n    );\n    \n    \n\n    \n    WITH caculate_data AS (\n    select app_id,free_app_download,paid_app_download,revenue, unified_device_code, unified_country_code, est_non_organic_download_share, \n    round (( 1 - est_non_organic_download_share ) *(free_app_download+paid_app_download)) as organic_download, ( free_app_download+paid_app_download - ROUND(( 1- est_non_organic_download_share) * (free_app_download+paid_app_download))) as paid_download    from calculate_data_prepare\n    );\n    \n    \n    \n    WITH compare_data_raw AS (\n    select app_id,free_app_download, paid_app_download, paid_download, organic_download, unified_device_code as device_code, unified_country_code as country_code, (1 - est_non_organic_download_share) AS organic_download_share from caculate_data\n    );\n    \n    \n    WITH compare_data_unified AS (\n    select app_id,coalesce(free_app_download, 0 ) as free_app_download,  coalesce(paid_app_download, 0 ) as paid_app_download, paid_download, organic_download,  device_code,  country_code, organic_download_share from test_unified_download_attribution\n    );\n    \n    \n    WITH compare_data_unified_add_to_est AS (\n    select app_id, ( coalesce(free_app_download, 0 )  + coalesce(paid_app_download, 0 ) - (paid_download + organic_download)  ) as diff ,  device_code,  country_code from test_unified_download_attribution\n    );\n    \n    \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": test_data[2]\n            }\n            # ,\n            # {\n            #     \"data_encoding\": \"parquet\",\n            #     \"compression\": \"gzip\",\n            #     \"name\": \"test_unified_download_attribution\",\n            #     \"path\":test_data[1]\n            # }\n    \n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    \n    # spark.sql(\"select * from download_attribution where product_id=284035177 and country_code='ww'\").show()\n    # spark.sql(\"select * from compare_data_unified where device_code='ios-tablet' \").show()\n    # spark.sql(\"select * from compare_data_raw\").show()\n\n    # spark.sql(\"select * from caculate_data where app_id=20600000009072 and unified_country_code='WW' and unified_device_code='android-all'\").show()\n    spark.sql(\"select * from compare_data_raw where app_id is not null except all select * from compare_data_unified where app_id is not null\").show()\n    spark.sql(\"select * from compare_data_unified except all select * from compare_data_raw\").show()\n    # spark.sql(\"sselect count(*) from compare_data_raw where app_id is not null\").show()\n    # spark.sql(\"select count(*) from compare_data_unified \").show()\n\n    # spark.sql(\"select * from compare_data_unified_add_to_est where diff !=0  \").show()\n    eject_all_caches(spark)\n\n    \n# for x in test_path:\n    # print x\n    # test_download_attribution(x)\n\n    # WITH caculate_data AS (\n    # select app_id,free_app_download,paid_app_download,revenue ,unified_granularity, unified_device_code, unified_country_code, est_non_organic_download_share, \n\n   \n    \nsc.parallelize(map(test_download_attribution, test_path), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200428-011026_391802847","metadata":{},"outputs":[],"source":["%%sh\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/download-attribution.v1/fact/month=\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/download-attribution.v1/fact/\n# aws s3 ls s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2018-06-02/  --recursive\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2018-06-02/ --recursive\n# aws s3 ls s3://b2b-prod-int-data-pipeline-unified/unified/app-int.download-attribution.v1/metric/month=2018-06/date=2018-06-02/  --recursive\n\n# aws s3 ls s3://b2b-prod-int-data-pipeline-unified/unified/app-int.download-attribution.v1/metric/month=\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/granularity=daily/date=2020-04-01/device_code=ios-phone/\n \n \n# aws s3 ls s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\n\n# echo 'hahaha'\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/download-attribution.v1/fact/month=2018-06/date=2018-06-02/\n# # echo 'hahaha'\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/legacy_unified_backup/app-int.download-attribution.v1/fact/\n# # echo 'hahaha'\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/download-attribution.v1/fact/month=2018-06/date=2018-06-02/\n# aws s3 ls s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2018-06-02/  --recursive\n\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily\n \n\n"]},{"cell_type":"code","execution_count":0,"id":"20200426-085336_777676177","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\nstart = \"2016-08-28\"\nend = \"2020-04-25\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\n# for x in sar_list:\n#     for key,item in x.items():\n#         test_path.append(\n#             (\n#                 [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n#                 [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v2/fact/granularity=daily/date={}\".format(i) for i in item], \n#                 [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n#             )\n#         )\n        \nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append(\n            (\n                [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n                [i.strftime(\"%Y-%m-%d\") for i in item], \n                [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n            )\n        )\n# print test_path\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n\n\ndef test_download_attribution(test_data):\n    # print test_data\n    spark.read.option(\"basePath\",\n                      \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\").parquet(\n        test_data[0][0]).createOrReplaceTempView(\n        \"download_attribution\")\n\n    # spark.read.option(\"basePath\",\n    #                   \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily\").parquet(\n    #     \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % \",\".join(test_path[0][1])).createOrReplaceTempView(\n    #     \"store_unified\")\n\n    # spark.read.option(\"basePath\",\n    #                   \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2016-09-03/\").parquet(\n    #     test_data[0][0]).createOrReplaceTempView(\n    #     \"download_attribution\")\n    \n    print test_data[1]\n    spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\"granularity='daily' and  date in ('{}') \".format(\"','\".join(\n        test_data[1]))).createOrReplaceTempView(\"test_unified_download_attribution\")\n\n    sql_text = \"\"\"\n    \n    WITH download_attribution AS (\n        select *, CAST(est_non_organic_download_share as decimal(36,20)) as new_est_non_organic_download_share from download_attribution\n    );\n\n    WITH download_attribution_1 AS (\n    \n     select device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share, \n        case when device_code='android-phone' THEN 'android-all' \n             when device_code='ios-phone' THEN 'ios-tablet' END AS new_device_code from download_attribution\n    );\n\n    WITH download_attribution_2\n    AS (\n        select distinct * from ( select new_device_code as device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share as est_non_organic_download_share from download_attribution_1 \n        union all\n        select device_code, country_code, granularity, date, product_id, est_non_organic_download_share from download_attribution ) as t1 where device_code!='android-phone'\n\n    );\n    \n    WITH union_data AS (\n    select *, store_unified.device_code as unified_device_code , store_unified.country_code as unified_country_code\n    from store_unified full outer join download_attribution_2\n    on store_unified.device_code=download_attribution_2.device_code and\n    store_unified.country_code=UPPER(download_attribution_2.country_code) and\n    store_unified.app_id=download_attribution_2.product_id\n    where est_non_organic_download_share is not null\n    );\n    \n    \n    WITH calculate_data_prepare AS (\n    select app_id, coalesce(free_app_download, 0) as free_app_download,  coalesce(paid_app_download, 0 )  as paid_app_download, revenue, unified_device_code, unified_country_code, est_non_organic_download_share from union_data where not (free_app_download is null and paid_app_download is null) \n    );\n    \n    \n\n    \n    WITH caculate_data AS (\n    select app_id,free_app_download,paid_app_download,revenue, unified_device_code, unified_country_code, est_non_organic_download_share from calculate_data_prepare\n    );\n    \n    \n    \n    WITH compare_data_raw AS (\n    select app_id,free_app_download, paid_app_download, unified_device_code as device_code, unified_country_code as country_code, (1 - est_non_organic_download_share) AS organic_download_share from caculate_data\n    );\n    \n    \n    WITH compare_data_unified AS (\n    select app_id,coalesce(free_app_download, 0 ) as free_app_download,  coalesce(paid_app_download, 0 ) as paid_app_download,  device_code,  country_code, organic_download_share from test_unified_download_attribution\n    );\n    \n    \n\n    \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": test_data[2]\n            }\n            # ,\n            # {\n            #     \"data_encoding\": \"parquet\",\n            #     \"compression\": \"gzip\",\n            #     \"name\": \"test_unified_download_attribution\",\n            #     \"path\":test_data[1]\n            # }\n    \n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    \n    # spark.sql(\"select * from download_attribution where product_id=284035177 and country_code='ww'\").show()\n    # spark.sql(\"select * from compare_data_unified where device_code='ios-tablet' \").show()\n    # spark.sql(\"select * from compare_data_raw\").show()\n\n    # spark.sql(\"select * from caculate_data where app_id=20600000009072 and unified_country_code='WW' and unified_device_code='android-all'\").show()\n    spark.sql(\"select * from compare_data_raw where app_id is not null except all select * from compare_data_unified where app_id is not null\").show()\n    spark.sql(\"select * from compare_data_unified except all select * from compare_data_raw\").show()\n    count_1 = spark.sql(\"select count(*) from compare_data_raw where app_id is not null\").take(1)\n    count_2 = spark.sql(\"select count(*) from compare_data_unified \").take(1)\n    if count_1[0][0] != count_2[0][0]:\n        print 'failed!!!!!!!!!!!!!'\n\n    # spark.sql(\"select * from compare_data_unified_add_to_est where diff !=0  \").show()\n    eject_all_caches(spark)\n\n    \n\n    \nsc.parallelize(map(test_download_attribution, test_path), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200429-125722_797112200","metadata":{},"outputs":[],"source":["\n# spark.read.option(\"basePath\",\n#                       \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\").parquet(\n#         \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2020-04-18/*/\").createOrReplaceTempView(\n#         \"download_attribution\")\n\n# spark.sql(\"select *, CAST(est_non_organic_download_share as decimal(36,20)) as new_est_non_organic_download_share from download_attribution \").show(20,False)\n\n\nspark.sql(\"select * from compare_data_raw where app_id=581983831 and country_code='AU' order by free_app_download desc \").show()\nspark.sql(\"select * from compare_data_unified where app_id=581983831 and country_code='AU' order by free_app_download desc \").show()\nspark.sql(\"select * from union_data where  app_id=581983831 and unified_country_code='AU' order by free_app_download desc \").show()\n\n\n# spark.sql(\"select * from compare_data_raw where  country_code='AU' order by free_app_download desc \").show()\n# spark.sql(\"select * from compare_data_unified where country_code='AU' order by free_app_download desc \").show()\nspark.sql(\"select * from union_data where unified_country_code='AU' and free_app_download is null and paid_app_download is not null order by free_app_download desc \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200430-040021_229583018","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\nstart = \"2016-08-28\"\nend = \"2016-08-05\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\n# for x in sar_list:\n#     for key,item in x.items():\n#         test_path.append(\n#             (\n#                 [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n#                 [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v2/fact/granularity=daily/date={}\".format(i) for i in item], \n#                 [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n#             )\n#         )\n        \nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append(\n            (\n                [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n                [i.strftime(\"%Y-%m-%d\") for i in item], \n                [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n            )\n        )\n# print test_path\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n\n\ndef test_download_attribution(test_data):\n    # print test_data\n    spark.read.option(\"basePath\",\n                      \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\").parquet(\n        test_data[0][0]).createOrReplaceTempView(\n        \"download_attribution\")\n\n    # spark.read.option(\"basePath\",\n    #                   \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily\").parquet(\n    #     \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % \",\".join(test_path[0][1])).createOrReplaceTempView(\n    #     \"store_unified\")\n\n    # spark.read.option(\"basePath\",\n    #                   \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2016-09-03/\").parquet(\n    #     test_data[0][0]).createOrReplaceTempView(\n    #     \"download_attribution\")\n    \n    print test_data[1]\n    spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\"granularity='daily' and  date in ('{}') \".format(\"','\".join(\n        test_data[1]))).createOrReplaceTempView(\"test_unified_download_attribution\")\n\n    sql_text = \"\"\"\n    \n    WITH download_attribution AS (\n        select *, CAST(est_non_organic_download_share as decimal(36,20)) as new_est_non_organic_download_share from download_attribution\n    );\n\n    WITH download_attribution_1 AS (\n    \n     select device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share, \n        case when device_code='android-phone' THEN 'android-all' \n             when device_code='ios-phone' THEN 'ios-tablet' END AS new_device_code from download_attribution\n    );\n\n    WITH download_attribution_2\n    AS (\n        select distinct * from ( select new_device_code as device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share as est_non_organic_download_share from download_attribution_1 \n        union all\n        select device_code, country_code, granularity, date, product_id, est_non_organic_download_share from download_attribution ) as t1 where device_code!='android-phone'\n\n    );\n    \n    WITH union_data AS (\n    select *, store_unified.device_code as unified_device_code , store_unified.country_code as unified_country_code\n    from store_unified full outer join download_attribution_2\n    on store_unified.device_code=download_attribution_2.device_code and\n    store_unified.country_code=UPPER(download_attribution_2.country_code) and\n    store_unified.app_id=download_attribution_2.product_id\n    where est_non_organic_download_share is not null\n    );\n    \n    \n    WITH calculate_data_prepare AS (\n    select app_id, coalesce(free_app_download, 0) as free_app_download,  coalesce(paid_app_download, 0 )  as paid_app_download, revenue, unified_device_code, unified_country_code, est_non_organic_download_share from union_data where not (free_app_download is null and paid_app_download is null) \n    );\n    \n    \n\n    \n    WITH caculate_data AS (\n    select app_id,free_app_download,paid_app_download,revenue, unified_device_code, unified_country_code, est_non_organic_download_share from calculate_data_prepare\n    );\n    \n    \n    \n    WITH compare_data_raw AS (\n    select app_id,free_app_download, paid_app_download, unified_device_code as device_code, unified_country_code as country_code, (1 - est_non_organic_download_share) AS organic_download_share from caculate_data\n    );\n    \n    \n    WITH compare_data_unified AS (\n    select app_id,coalesce(free_app_download, 0 ) as free_app_download,  coalesce(paid_app_download, 0 ) as paid_app_download,  device_code,  country_code, organic_download_share from test_unified_download_attribution\n    );\n    \n    \n\n    \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": test_data[2]\n            }\n            # ,\n            # {\n            #     \"data_encoding\": \"parquet\",\n            #     \"compression\": \"gzip\",\n            #     \"name\": \"test_unified_download_attribution\",\n            #     \"path\":test_data[1]\n            # }\n    \n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    \n    # spark.sql(\"select * from download_attribution where product_id=284035177 and country_code='ww'\").show()\n    # spark.sql(\"select * from compare_data_unified where device_code='ios-tablet' \").show()\n    # spark.sql(\"select * from compare_data_raw\").show()\n\n    # spark.sql(\"select * from caculate_data where app_id=20600000009072 and unified_country_code='WW' and unified_device_code='android-all'\").show()\n    spark.sql(\"select * from compare_data_raw where app_id is not null except all select * from compare_data_unified where app_id is not null\").show()\n    spark.sql(\"select * from compare_data_unified except all select * from compare_data_raw\").show()\n    count_1 = spark.sql(\"select count(*) from compare_data_raw where app_id is not null\").take(1)\n    count_2 = spark.sql(\"select count(*) from compare_data_unified \").take(1)\n    if count_1[0][0] != count_2[0][0]:\n        print 'failed!!!!!!!!!!!!!'\n\n    # spark.sql(\"select * from compare_data_unified_add_to_est where diff !=0  \").show()\n    eject_all_caches(spark)\n\n    \n\n    \nsc.parallelize(map(test_download_attribution, test_path), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200425-051205_185421729","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import broadcast\n\nspark.sql(\"select broadcast(*) from compare_data_raw where app_id=1450580739 and country_code='TH' order by free_app_download desc \").explain()\n# spark.sql(\"select * from compare_data_unified where app_id=1450580739 and country_code='TH' order by free_app_download desc \").show()\n# spark.sql(\"select * from union_data where  app_id=1450580739 and unified_country_code='TH' order by free_app_download desc \").show()\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/legacy_unified_backup/app-int.download-attribution.v1/fact/month=2018-02/date=2018-02-17/\").where(\"app_id=20600004857029 and store_id=2\").show(2)"]},{"cell_type":"code","execution_count":0,"id":"20200426-035448_113692544","metadata":{},"outputs":[],"source":["\n# ['2017-09-02', '2017-09-01', '2017-08-31', '2017-08-30', '2017-08-29', '2017-08-28', '2017-08-27']\n# ['2018-02-17', '2018-02-16', '2018-02-15', '2018-02-14', '2018-02-13', '2018-02-12', '2018-02-11']\n# ['2019-04-27', '2019-04-26', '2019-04-25', '2019-04-24', '2019-04-23', '2019-04-22', '2019-04-21']\n# ['2020-02-22', '2020-02-21', '2020-02-20', '2020-02-19', '2020-02-18', '2020-02-17', '2020-02-16']\n# d1 = ['2020-02-29', '2020-02-28', '2020-02-27', '2020-02-26', '2020-02-25', '2020-02-24', '2020-02-23']\n# ['2020-03-07', '2020-03-06', '2020-03-05', '2020-03-04', '2020-03-03', '2020-03-02', '2020-03-01']\n# ['2020-03-14', '2020-03-13', '2020-03-12', '2020-03-11', '2020-03-10', '2020-03-09', '2020-03-08']\n# ['2020-03-21', '2020-03-20', '2020-03-19', '2020-03-18', '2020-03-17', '2020-03-16', '2020-03-15']\n# ['2020-03-28', '2020-03-27', '2020-03-26', '2020-03-25', '2020-03-24', '2020-03-23', '2020-03-22']\nd1 = ['2020-04-04', '2020-04-03', '2020-04-02', '2020-04-01', '2020-03-31', '2020-03-30', '2020-03-29']\n\n# d1= ['2018-06-02', '2018-06-01', '2018-05-31', '2018-05-30', '2018-05-29', '2018-05-28', '2018-05-27']\nday=\"2020-04-04\"\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v2/fact/\").where(\"granularity='daily' and  date in ('{}') \".format(\"','\".join(d1))).where(\"app_id=1450580739 and country_code='TH' and date='2020-04-01'  \").orderBy(\"date\").show(200)\n\n# spark.read.option(\"basePath\",\n#                       \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily\").parquet(\n#         \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\"% ','.join(d1)).where(\"app_id=1450580739 and country_code='TH' and device_code in ('ios-phone', 'ios-tablet') \").orderBy(\"date\").show()\n        \n# spark.read.option(\"basePath\",\n#                       \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\").parquet(\n#         \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*\".format(day)).where(\"product_id=1450580739 and country_code='th'\").show()\n        \nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v2/fact/granularity=daily/date=2020-04-01/\").where(\"app_id=1450580739 and country_code='TH' and device_code in ('ios-phone', 'ios-tablet') \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200428-065319_757831943","metadata":{},"outputs":[],"source":["\n# print download attribution\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\"granularity='daily' and  date in ('2020-02-22', '2020-02-21', '2020-02-20', '2020-02-19', '2020-02-18', '2020-02-17', '2020-02-16') \").where(\"app_id=1029094059 and country_code='US' and date='2020-02-16'\").show(200)\n# est \n# spark.read.option(\"basePath\",\n#                       \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily\").parquet(\n#         \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-02-{16,17,18,19,20,21,22}\").where(\"app_id=1029094059 and country_code='US' and date='2020-02-16' and device_code='ios-phone'\").show()\n# # share value\n# spark.read.option(\"basePath\",\n#                       \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\").parquet(\n#         \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2020-02-22/*\").where(\"product_id=1029094059 and country_code='us'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200426-042253_1083854313","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nstart = \"2018-02-11\"\nend = \"2018-02-18\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\n\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append(\n            (\n                [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n                [i.strftime(\"%Y-%m-%d\") for i in item], \n                [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n            )\n        )\nprint test_path[0][2]\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\nsql_text =\"\"\"\n\n\"\"\"\nnamespace = \"aa.store.market-size.v1\"\ningest_msg = {\n    \"namespace\": \"aa.store.market-size.v1\",\n    \"job_type\": \"routine\",\n    \"options\":{},\n    \"source\": [\n        {\n            \"data_encoding\": \"parquet\",\n            \"compression\": \"gzip\",\n            \"name\": \"store_unified\",\n            \"path\": ['s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2018-02-13', 's3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2018-02-14']\n        }\n    ]\n}\nrun(spark, ingest_msg, sql_text)\nspark.sql(\"select * from store_unified where app_id=20600004857029 and country_code='CA'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200416-130735_2134628438","metadata":{},"outputs":[],"source":["   \nANDROID_CATEGORIES = [\n    (1, 400000), (2, 400001), (3, 400022), (4, 400023), (5, 400024),\n    (6, 400008), (7, 400011), (8, 400014), (9, 400017), (10, 400020),\n    (11, 400025), (12, 400030), (13, 400031), (14, 400032), (15, 400033),\n    (16, 400035), (17, 400036), (18, 400038), (19, 400040), (20, 400042),\n    (21, 400043), (22, 400044), (23, 400058), (24, 400046), (25, 400047),\n    (26, 400048), (27, 400050), (28, 400051), (29, 400052), (30, 400053),\n    (31, 400054), (32, 400055), (33, 400056), (34, 400045), (35, 400057),\n    (36, 400059), (37, 400060), (38, 400002), (39, 400003), (40, 400021),\n    (41, 400004), (42, 400005), (43, 400006), (44, 400007), (46, 400009),\n    (47, 400010), (48, 400012), (49, 400013), (51, 400015), (52, 400016),\n    (54, 400018), (55, 400019), (56, 400061), (57, 400063), (58, 400064),\n    (59, 400065), (60, 400062), (61, 400066), (62, 400067), (63, 400068),\n    (64, 400069), (65, 400070), (66, 400026), (67, 400027), (68, 400041),\n    (69, 400028), (70, 400029), (71, 400034), (72, 400037), (73, 400039),\n    (75, 400049)\n]\n\n\nIOS_CATEGORIES = [\n    (36, 100000), (100, 100021), (360, 100030), (361, 100031), (362, 100032),\n    (363, 100033), (6000, 100023), (6001, 100077), (6002, 100076), (6003, 100075),\n    (6004, 100073), (6005, 100072), (6006, 100070), (6007, 100069), (6008, 100068),\n    (6009, 100067), (6010, 100066), (6011, 100065), (6012, 100034), (6013, 100029),\n    (6014, 100001), (6015, 100027), (6016, 100026), (6017, 100025), (6018, 100022),\n    (6020, 100064), (6021, 100035), (6022, 100024), (6023, 100028), (6024, 100071),\n    (6025, 100074), (7001, 100002), (7002, 100003), (7003, 100004), (7004, 100005),\n    (7005, 100006), (7006, 100007), (7007, 100008), (7008, 100009), (7009, 100010),\n    (7010, 100011), (7011, 100012), (7012, 100013), (7013, 100014), (7014, 100015),\n    (7015, 100016), (7016, 100017), (7017, 100018), (7018, 100019), (7019, 100020),\n    (13001, 100053), (13002, 100046), (13003, 100049), (13004, 100054), (13005, 100060),\n    (13006, 100037), (13007, 100036), (13008, 100038), (13009, 100039), (13010, 100040),\n    (13011, 100041), (13012, 100042), (13013, 100043), (13014, 100044), (13015, 100045),\n    (13017, 100047), (13018, 100048), (13019, 100050), (13020, 100051), (13021, 100052),\n    (13023, 100055), (13024, 100056), (13025, 100057), (13026, 100058), (13027, 100059),\n    (13028, 100061), (13029, 100062), (13030, 100063)\n]\n\n\nIOS_STORE_COUNTRY_MAPPING = [\n    (0, 'WW'), (143575, 'AL'), (143563, 'DZ'), (143564, 'AO'), (143538, 'AI'),\n    (143540, 'AG'), (143505, 'AR'), (143524, 'AM'), (143460, 'AU'), (143445, 'AT'),\n    (143568, 'AZ'), (143539, 'BS'), (143559, 'BH'), (143541, 'BB'), (143565, 'BY'),\n    (143446, 'BE'), (143555, 'BZ'), (143576, 'BJ'), (143542, 'BM'), (143577, 'BT'),\n    (143556, 'BO'), (143525, 'BW'), (143503, 'BR'), (143543, 'VG'), (143560, 'BN'),\n    (143526, 'BG'), (143578, 'BF'), (143579, 'KH'), (143455, 'CA'), (143580, 'CV'),\n    (143544, 'KY'), (143581, 'TD'), (143483, 'CL'), (143465, 'CN'), (143501, 'CO'),\n    (143582, 'CG'), (143495, 'CR'), (143494, 'HR'), (143557, 'CY'), (143489, 'CZ'),\n    (143458, 'DK'), (143545, 'DM'), (143508, 'DO'), (143509, 'EC'), (143516, 'EG'),\n    (143506, 'SV'), (143518, 'EE'), (143583, 'FJ'), (143447, 'FI'), (143442, 'FR'),\n    (143584, 'GM'), (143443, 'DE'), (143573, 'GH'), (143448, 'GR'), (143546, 'GD'),\n    (143504, 'GT'), (143585, 'GW'), (143553, 'GY'), (143510, 'HN'), (143463, 'HK'),\n    (143482, 'HU'), (143558, 'IS'), (143467, 'IN'), (143476, 'ID'), (143449, 'IE'),\n    (143491, 'IL'), (143450, 'IT'), (143511, 'JM'), (143462, 'JP'), (143528, 'JO'),\n    (143517, 'KZ'), (143529, 'KE'), (143493, 'KW'), (143586, 'KG'), (143587, 'LA'),\n    (143519, 'LV'), (143497, 'LB'), (143588, 'LR'), (143520, 'LT'), (143451, 'LU'),\n    (143515, 'MO'), (143530, 'MK'), (143531, 'MG'), (143589, 'MW'), (143473, 'MY'),\n    (143532, 'ML'), (143521, 'MT'), (143590, 'MR'), (143533, 'MU'), (143468, 'MX'),\n    (143591, 'FM'), (143523, 'MD'), (143592, 'MN'), (143547, 'MS'), (143593, 'MZ'),\n    (143594, 'NA'), (143484, 'NP'), (143452, 'NL'), (143461, 'NZ'), (143512, 'NI'),\n    (143534, 'NE'), (143561, 'NG'), (143457, 'NO'), (143562, 'OM'), (143477, 'PK'),\n    (143595, 'PW'), (143485, 'PA'), (143597, 'PG'), (143513, 'PY'), (143507, 'PE'),\n    (143474, 'PH'), (143478, 'PL'), (143453, 'PT'), (143498, 'QA'), (143487, 'RO'),\n    (143469, 'RU'), (143598, 'ST'), (143479, 'SA'), (143535, 'SN'), (143599, 'SC'),\n    (143600, 'SL'), (143464, 'SG'), (143496, 'SK'), (143499, 'SI'), (143601, 'SB'),\n    (143472, 'ZA'), (143466, 'KR'), (143454, 'ES'), (143486, 'LK'), (143548, 'KN'),\n    (143549, 'LC'), (143550, 'VC'), (143554, 'SR'), (143602, 'SZ'), (143456, 'SE'),\n    (143459, 'CH'), (143470, 'TW'), (143603, 'TJ'), (143572, 'TZ'), (143475, 'TH'),\n    (143551, 'TT'), (143536, 'TN'), (143480, 'TR'), (143604, 'TM'), (143552, 'TC'),\n    (143537, 'UG'), (143492, 'UA'), (143481, 'AE'), (143444, 'GB'), (143441, 'US'),\n    (143514, 'UY'), (143566, 'UZ'), (143502, 'VE'), (143471, 'VN'), (143571, 'YE'),\n    (143605, 'ZW')]\n    \n\n\nANDROID_STORE_COUNTRY_MAPPING = [\n    (17, 'AR'), (1, 'AU'), (35, 'AT'), (61, 'AZ'), (11, 'BE'), (18, 'BR'), (47, 'BG'),\n    (2, 'CA'), (13, 'CL'), (3, 'CN'), (52, 'CO'), (64, 'CR'), (80, 'HR'), (36, 'CZ'),\n    (38, 'DK'), (62, 'EC'), (33, 'EG'), (20, 'FI'), (6, 'FR'), (4, 'DE'), (46, 'GR'),\n    (16, 'HK'), (37, 'HU'), (19, 'IN'), (21, 'ID'), (39, 'IE'), (40, 'IL'), (8, 'IT'),\n    (9, 'JP'), (53, 'KZ'), (95, 'KE'), (50, 'KW'), (86, 'LV'), (65, 'LB'), (78, 'LT'),\n    (24, 'MY'), (26, 'MX'), (23, 'NL'), (41, 'NZ'), (74, 'NG'), (42, 'NO'), (54, 'PK'),\n    (56, 'PE'), (31, 'PH'), (28, 'PL'), (43, 'PT'), (84, 'PR'), (73, 'QA'), (44, 'RO'),\n    (22, 'RU'), (51, 'SA'), (32, 'SG'), (45, 'SK'), (14, 'ZA'), (27, 'KR'), (5, 'ES'),\n    (34, 'SE'), (12, 'CH'), (30, 'TW'), (29, 'TH'), (25, 'TR'), (48, 'UA'), (49, 'AE'),\n    (7, 'GB'), (10, 'US'), (15, 'VN'), (1000, 'WW')\n]\n\n\ndef device_code_to_feed(market_code, device_code, metric_name):\n    mapping = [\n        ['apple-store',0,'ios-phone','est_free_app_download'],\n        ['apple-store',1,'ios-phone','est_paid_app_download'],\n        ['apple-store',2,'ios-phone','est_revenue'],\n        ['apple-store',101,'ios-tablet','est_free_app_download'],\n        ['apple-store',100,'ios-tablet','est_paid_app_download'],\n        ['apple-store',102,'ios-tablet','est_revenue'],\n        ['google-play',0,'android-all','est_free_app_download'],\n        ['google-play',1,'android-all','est_paid_app_download'],\n        ['google-play',2,'android-all','est_revenue'],\n    ]\n    return [x for x in mapping if (x[0], x[2], x[3]) == (market_code, device_code, metric_name)][0][1]\n\n\n\ndef country_code_to_id(market_code, code):\n    if market_code == 'apple-store':\n        ios_mapping = {_code:_id for (_id, _code) in IOS_STORE_COUNTRY_MAPPING}\n        return ios_mapping[code]\n    else:\n        gp_mapping = {_code:_id for (_id, _code) in ANDROID_STORE_COUNTRY_MAPPING}\n        return gp_mapping[code]\n\n\ndef category_to_legacy_category(market_code, legacy):\n    if market_code == 'apple-store':\n        ios_category = {_category:_legacy_category for (_legacy_category,_category) in IOS_CATEGORIES }\n        return ios_category[legacy]\n    else:\n        gp_category =  {_category:_legacy_category for (_legacy_category,_category) in ANDROID_CATEGORIES }\n        return gp_category[legacy]\n\n\ncountry_code_to_id('apple-store',14380)\n# 13028, 100061\n# 71, 400034\n# category_to_legacy_category(\"apple-store\",100000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200506-033538_1625196575","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nimport psycopg2\nimport datetime\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\n\nimport aaplproxy\n\n\nstart='2020-04-17'\nend='2020-04-18'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\ndates.reverse()\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n\n\ndef test_download_attribution_db(test_date):\n    print 'test_data:' , test_date\n\n    spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\"granularity='daily' and  date in ('{}') \".format(test_date)).createOrReplaceTempView(\"unified_download_attribution\")\n\n    sql_text = \"\"\"\n    \n    WITH download_attribution AS (\n      SELECT app_id, coalesce(free_app_download, 0 ) as free_app_download, coalesce(paid_app_download, 0 ) as paid_app_download, coalesce(revenue, 0 ) as revenue, device_code, country_code, organic_download_share from unified_download_attribution\n    );\n    \n    WITH caculate_data AS (\n        SELECT CAST(ROUND (organic_download_share * (free_app_download+paid_app_download)) AS int) AS est_organic_download, \n        CAST(free_app_download+paid_app_download - ROUND(organic_download_share * (free_app_download+paid_app_download)) AS int) as est_paid_download,\n        app_id, \n        free_app_download AS est_free_app_download, \n        paid_app_download AS est_paid_app_download, \n        revenue AS est_revenue, \n        device_code, \n        country_code\n        FROM download_attribution\n    );\n    \n    \n        -- rank_unified,store_unified\n    WITH unified_data_test AS \n    ( \n                    SELECT          store_unified.country_code, \n                                    store_unified.device_code, \n                                    store_unified.free_app_download AS est_free_app_download , \n                                    store_unified.paid_app_download AS est_paid_app_download, \n                                    store_unified.revenue           AS est_revenue, \n                                    store_unified.revenue_iap       AS est_revenue_iap, \n                                    store_unified.revenue_non_iap   AS est_revenue_non_iap, \n                                    rank_unified.category_id, \n                                    rank_unified.app_id, \n                                    rank_unified.free_app_download, \n                                    rank_unified.paid_app_download, \n                                    rank_unified.revenue, \n                                    rank_unified.revenue_iap, \n                                    rank_unified.revenue_non_iap, \n                                    rank_unified.granularity, \n                                    rank_unified.date \n                    FROM            rank_unified \n                    FULL OUTER JOIN store_unified \n                    ON              rank_unified.app_id = store_unified.app_id \n                    AND             rank_unified.country_code = store_unified.country_code \n                    AND             rank_unified.device_code = store_unified.device_code \n                    AND             rank_unified.date = store_unified.date );\n    \n    \n    \n    WITH unified_rank_filter_data_free_app_download AS \n    ( \n           SELECT * \n           FROM   unified_data_test \n           WHERE ( ( ( \n                                free_app_download<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                free_app_download<=4000 \n                         AND    country_code==\"WW\" ) ) \n           OR     ( ( \n                                paid_app_download<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                paid_app_download<=4000 \n                         AND    country_code==\"WW\" ) ) \n           OR     ( ( \n                                revenue<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                revenue<=4000 \n                         AND    country_code==\"WW\" ) ) )\n           AND    device_code!='ios-all'\n    );\n    \n    \n    \n           WITH unified_category_filter_data_free_app_download AS \n    ( \n           SELECT * ,\n           CASE WHEN (free_app_download > 1000 and country_code !='WW') or (free_app_download > 4000 and country_code =='WW' ) or (free_app_download is null or free_app_download <= 0) Then null else est_free_app_download END as est_free_app_download_category,\n           CASE WHEN (paid_app_download > 1000 and country_code !='WW') or (paid_app_download > 4000 and country_code =='WW' ) or (paid_app_download is null or paid_app_download <= 0) Then null else est_paid_app_download END as est_paid_app_download_category,\n           CASE WHEN (revenue > 1000 and country_code !='WW') or (revenue > 4000 and country_code =='WW') or (revenue is null or revenue <= 0) Then null else est_revenue  END as est_revenue_category\n           FROM   unified_rank_filter_data_free_app_download \n\n\n    );    \n\n            WITH download_attribution_join_rank_value AS (\n            \n            SELECT caculate_data.* FROM caculate_data \n            JOIN unified_category_filter_data_free_app_download\n            ON caculate_data.app_id = unified_category_filter_data_free_app_download.app_id\n            AND caculate_data.country_code = unified_category_filter_data_free_app_download.country_code\n            AND caculate_data.device_code = unified_category_filter_data_free_app_download.device_code\n\n            )\n    \n    \"\"\"\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n             {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n                # \"path\": est_list,\n    \n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n                # \"path\": rank_list,\n    \n            }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n\n\n    # device_code=[\"ios-phone\",\"ios-tablet\",\"android-all\"]\n\n    def citus_row(test_data):\n        def get_data_in_citus(date):\n            citus_dsn_ = (\n                \"dbname='{db}' user='{user}' password='{password}' \"\n                \"host='{host}' port='{port}'\".format(\n                    db=\"aa_citus_db\",\n                    user=\"citus_bdp_prod_app_int_qa\",\n                    host=\"10.2.6.141\",\n                    password=\"wZw8cfBuuklIskVG\",\n                    port=5432\n                )\n            )\n    \n            sql = '''SELECT \n                        coalesce(est_organic_download, 0 ) AS est_organic_download,\n                        coalesce(est_paid_download, 0 ) AS est_paid_download,\n                        app_id,\n                        coalesce(est_free_app_download, 0 ) AS est_free_app_download,\n                        coalesce(est_paid_app_download, 0 ) AS est_paid_app_download,\n                        coalesce(est_revenue, 0 ) AS est_revenue,\n                        device_code,\n                        country_code\n                        FROM store.store_est_fact_v1 \n                        WHERE date='{}' \n                     '''.format(date)\n        \n            db_data = query(citus_dsn_, sql)\n            return db_data\n\n        def query(dsn, sql):\n            with psycopg2.connect(dsn) as conn:\n                conn.autocommit = True\n                with conn.cursor() as cur:\n                    cur.execute(sql)\n                    result = cur.fetchall()\n                    conn.commit()\n            return result\n    \n        result = get_data_in_citus(test_data)\n    # print result\n\n        return [Row(est_organic_download=r[0], est_paid_download=r[1], app_id=r[2], est_free_app_download=r[3], est_paid_app_download=r[4], est_revenue=r[5], device_code=r[6], country_code=r[7]) for r in result]\n\n\n    test_rdd = sc.parallelize(map(citus_row, [test_date]), 1).cache()\n\n    import pyspark\n    test_rdd.unpersist()\n    print test_rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n    test_rdd.getStorageLevel()\n    print(test_rdd.getStorageLevel())\n    # print test_rdd.collect()\n    new_rdd = test_rdd.flatMap(lambda x: x)\n    # print new_rdd.collect()\n\n    schema=StructType([StructField(\"app_id\", LongType(), True),  \n                   StructField(\"country_code\", StringType(), True), \n                   StructField(\"device_code\", StringType(), True), \n                   StructField(\"est_free_app_download\", LongType(), True), \n                   StructField(\"est_organic_download\", LongType(), True), \n                   StructField(\"est_paid_app_download\", LongType(), True),\n                   StructField(\"est_paid_download\", LongType(), True),\n                   StructField(\"est_revenue\", LongType(), True)\n                   ])\n\n    new_rdd.toDF(schema).createOrReplaceTempView(\"data_from_db\")\n\n    spark.sql(\"select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue  from data_from_db except all select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from download_attribution_join_rank_value \").show()\n    spark.sql(\"select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from download_attribution_join_rank_value except all select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from data_from_db \").show()\n\n    \n# PGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \n\n    \nsc.parallelize(map(test_download_attribution_db, dates), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200506-122124_183182947","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nimport psycopg2\nimport datetime\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nstart = '2020-04-17'\nend = '2020-04-18'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\ndates.reverse()\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\ndef test_download_attribution_db(test_date):\n    print 'test_data:', test_date\n\n    spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\n        \"granularity='daily' and  date in ('{}') \".format(test_date)).createOrReplaceTempView(\n        \"unified_download_attribution\")\n\n    sql_text = \"\"\"\n\n\n\n        -- rank_unified,store_unified\n    WITH unified_data_test AS \n    ( \n                    SELECT          store_unified.country_code, \n                                    store_unified.device_code, \n                                    store_unified.free_app_download AS est_free_app_download , \n                                    store_unified.paid_app_download AS est_paid_app_download, \n                                    store_unified.revenue           AS est_revenue, \n                                    store_unified.revenue_iap       AS est_revenue_iap, \n                                    store_unified.revenue_non_iap   AS est_revenue_non_iap, \n                                    rank_unified.category_id, \n                                    rank_unified.app_id, \n                                    rank_unified.free_app_download, \n                                    rank_unified.paid_app_download, \n                                    rank_unified.revenue, \n                                    rank_unified.revenue_iap, \n                                    rank_unified.revenue_non_iap, \n                                    rank_unified.granularity, \n                                    rank_unified.date \n                    FROM            rank_unified \n                    FULL OUTER JOIN store_unified \n                    ON              rank_unified.app_id = store_unified.app_id \n                    AND             rank_unified.country_code = store_unified.country_code \n                    AND             rank_unified.device_code = store_unified.device_code \n                    AND             rank_unified.date = store_unified.date );\n\n\n\n    WITH unified_rank_filter_data_free_app_download AS \n    ( \n           SELECT * \n           FROM   unified_data_test \n           WHERE ( ( ( \n                                free_app_download<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                free_app_download<=4000 \n                         AND    country_code==\"WW\" ) ) \n           OR     ( ( \n                                paid_app_download<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                paid_app_download<=4000 \n                         AND    country_code==\"WW\" ) ) \n           OR     ( ( \n                                revenue<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                revenue<=4000 \n                         AND    country_code==\"WW\" ) ) )\n           AND    device_code!='ios-all'\n    );\n\n\n\n           WITH unified_category_filter_data_free_app_download AS \n    ( \n           SELECT * ,\n           CASE WHEN (free_app_download > 1000 and country_code !='WW') or (free_app_download > 4000 and country_code =='WW' ) or (free_app_download is null or free_app_download <= 0) Then null else est_free_app_download END as est_free_app_download_category,\n           CASE WHEN (paid_app_download > 1000 and country_code !='WW') or (paid_app_download > 4000 and country_code =='WW' ) or (paid_app_download is null or paid_app_download <= 0) Then null else est_paid_app_download END as est_paid_app_download_category,\n           CASE WHEN (revenue > 1000 and country_code !='WW') or (revenue > 4000 and country_code =='WW') or (revenue is null or revenue <= 0) Then null else est_revenue  END as est_revenue_category\n           FROM   unified_rank_filter_data_free_app_download \n\n\n    );    \n\n     WITH download_attribution AS (\n      SELECT app_id, coalesce(free_app_download, 0 ) as free_app_download, coalesce(paid_app_download, 0 ) as paid_app_download, coalesce(revenue, 0 ) as revenue, device_code, country_code, organic_download_share from unified_download_attribution\n    );\n    \n    WITH download_attribution_join_rank_value AS (\n\n            SELECT unified_category_filter_data_free_app_download.app_id, \n            unified_category_filter_data_free_app_download.country_code, \n            unified_category_filter_data_free_app_download.device_code, \n            coalesce(unified_category_filter_data_free_app_download.est_free_app_download_category, 0) AS est_free_app_download,\n            coalesce(unified_category_filter_data_free_app_download.est_paid_app_download_category, 0) AS est_paid_app_download,\n            unified_category_filter_data_free_app_download.est_revenue_category AS est_revenue,\n            download_attribution.organic_download_share\n            FROM download_attribution \n            JOIN unified_category_filter_data_free_app_download\n            ON download_attribution.app_id = unified_category_filter_data_free_app_download.app_id\n            AND download_attribution.country_code = unified_category_filter_data_free_app_download.country_code\n            AND download_attribution.device_code = unified_category_filter_data_free_app_download.device_code\n\n            );\n\n\n   \n\n    WITH caculate_data AS (\n        SELECT CAST(ROUND (organic_download_share * (est_free_app_download+est_paid_app_download)) AS int) AS est_organic_download, \n        CAST(est_free_app_download+est_paid_app_download - ROUND(organic_download_share * (est_free_app_download+est_paid_app_download)) AS int) as est_paid_download,\n        app_id, \n        est_free_app_download, \n        est_paid_app_download, \n        est_revenue, \n        device_code, \n        country_code\n        FROM download_attribution_join_rank_value\n    );\n\n\n        \n    \"\"\"\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n                # \"path\": est_list,\n\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n                # \"path\": rank_list,\n\n            }\n        ]\n    }\n\n    run(spark, ingest_msg, sql_text)\n\n    # device_code=[\"ios-phone\",\"ios-tablet\",\"android-all\"]\n\n    def citus_row(test_data):\n        def get_data_in_citus(date):\n            citus_dsn_ = (\n                \"dbname='{db}' user='{user}' password='{password}' \"\n                \"host='{host}' port='{port}'\".format(\n                    db=\"aa_citus_db\",\n                    user=\"citus_bdp_usage_qa\",\n                    host=\"10.2.10.132\",\n                    password=\"dNzWtSV3pKTx\",\n                    port=5432\n                )\n            )\n\n            sql = '''SELECT \n                        coalesce(est_organic_download, 0 ) AS est_organic_download,\n                        coalesce(est_paid_download, 0 ) AS est_paid_download,\n                        app_id,\n                        coalesce(est_free_app_download, 0 ) AS est_free_app_download,\n                        coalesce(est_paid_app_download, 0 ) AS est_paid_app_download,\n                        coalesce(est_revenue, 0 ) AS est_revenue,\n                        device_code,\n                        country_code\n                        FROM store.store_est_category_fact_v6 \n                        WHERE date='{}' \n                     '''.format(date)\n\n            db_data = query(citus_dsn_, sql)\n            return db_data\n\n        def query(dsn, sql):\n            with psycopg2.connect(dsn) as conn:\n                conn.autocommit = True\n                with conn.cursor() as cur:\n                    cur.execute(sql)\n                    result = cur.fetchall()\n                    conn.commit()\n            return result\n\n        result = get_data_in_citus(test_data)\n        # print result\n\n        return [\n            Row(est_organic_download=r[0], est_paid_download=r[1], app_id=r[2], est_free_app_download=r[3],\n                est_paid_app_download=r[4], est_revenue=r[5], device_code=r[6], country_code=r[7]) for r in\n            result]\n\n    test_rdd = sc.parallelize(map(citus_row, [test_date]), 1).cache()\n\n    import pyspark\n    test_rdd.unpersist()\n    print test_rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n    test_rdd.getStorageLevel()\n    print(test_rdd.getStorageLevel())\n    # print test_rdd.collect()\n    new_rdd = test_rdd.flatMap(lambda x: x)\n    # print new_rdd.collect()\n\n    schema = StructType([StructField(\"app_id\", LongType(), True),\n                         StructField(\"country_code\", StringType(), True),\n                         StructField(\"device_code\", StringType(), True),\n                         StructField(\"est_free_app_download\", LongType(), True),\n                         StructField(\"est_organic_download\", LongType(), True),\n                         StructField(\"est_paid_app_download\", LongType(), True),\n                         StructField(\"est_paid_download\", LongType(), True),\n                         StructField(\"est_revenue\", LongType(), True)\n                         ])\n\n    new_rdd.toDF(schema).createOrReplaceTempView(\"data_from_db\")\n\n    # spark.sql(\"select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from download_attribution_join_rank_value\").show()\n    spark.sql(\n        \"select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download from data_from_db except all select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download from caculate_data \").show()\n    spark.sql(\n        \"select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download from caculate_data except all select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download from data_from_db \").show()\n\n\nsc.parallelize(map(test_download_attribution_db, dates), 1)\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-134907_217159705","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n--select  app_id,store_id,est_organic_download_share  from plproxy.execute_select_nestloop(\\$proxy\\$ \n--select  app_id,store_id,est_organic_download_share\n--    from da.app_da_daily_estimate limit 5 \\$proxy\\$) tbl \n--      (app_id BIGINT,store_id INT, est_organic_download_share FLOAT ) order by est_organic_download_share  asc limit 5 ;\n\nselect  sum(cnt)   from plproxy.execute_select_nestloop(\\$proxy\\$ \nselect  count(1) as cnt from da.app_da_daily_estimate where date='2016-09-24' \\$proxy\\$) tbl \n   (cnt bigint);\n\nselect  app_id,store_id,est_organic_download_share  from plproxy.execute_select_nestloop(\\$proxy\\$ \nselect  app_id,store_id,est_organic_download_share\n    from da.app_da_daily_estimate where app_id=1133281508 and date='2016-09-24' \\$proxy\\$) tbl \n      (app_id BIGINT,store_id INT, est_organic_download_share FLOAT ) order by est_organic_download_share  asc limit 5 ;\n\n\n\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200506-122733_801296238","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from data_from_db where app_id=1064572106 and country_code='WW' and device_code='ios-phone' \").show()\n# spark.sql(\"select * from unified_rank_filter_data_free_app_download where app_id=1064572106 and country_code='WW' and device_code='ios-phone' \").show()\n# spark.sql(\"select * from download_attribution_join_rank_value where app_id=1064572106 and country_code='WW' and device_code='ios-phone' \").show()\n# spark.sql(\"select * from caculate_data where app_id=1064572106 and country_code='WW' and device_code='ios-phone' \").show()\nspark.sql(\"select count(*) from caculate_data\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200430-120117_1818812635","metadata":{},"outputs":[],"source":["\n\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\n\n# device_code=[\"ios-phone\",\"ios-tablet\",\"android-all\"]\n\ndef citus_row(test_data):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_citus_db\",\n                user=\"citus_bdp_usage_qa\",\n                host=\"10.2.10.132\",\n                password=\"dNzWtSV3pKTx\",\n                port=5432\n            )\n        )\n\n        sql = '''SELECT \n                    coalesce(est_organic_download, 0 ) AS est_organic_download,\n                    coalesce(est_paid_download, 0 ) AS est_paid_download,\n                    app_id,\n                    coalesce(est_free_app_download, 0 ) AS est_free_app_download,\n                    coalesce(est_paid_app_download, 0 ) AS est_paid_app_download,\n                    coalesce(est_revenue, 0 ) AS est_revenue,\n                    device_code,\n                    country_code\n                    FROM store.store_est_fact_v6 \n                    WHERE date='{}' \n                 '''.format(date)\n        \n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    \n    result = get_data_in_citus(test_data)\n    # print result\n\n    return [Row(est_organic_download=r[0], est_paid_download=r[1], app_id=r[2], est_free_app_download=r[3], est_paid_app_download=r[4], est_revenue=r[5], device_code=r[6], country_code=r[7]) for r in result]\n\n\ntest_rdd = sc.parallelize(map(citus_row, [\"2020-04-17\"]), 1).cache()\n\nimport pyspark\ntest_rdd.unpersist()\nprint test_rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\ntest_rdd.getStorageLevel()\nprint(test_rdd.getStorageLevel())\n# print test_rdd.collect()\nnew_rdd = test_rdd.flatMap(lambda x: x)\n# print new_rdd.collect()\n\nschema=StructType([StructField(\"app_id\", LongType(), True),  \n                   StructField(\"country_code\", StringType(), True), \n                   StructField(\"device_code\", StringType(), True), \n                   StructField(\"est_free_app_download\", LongType(), True), \n                   StructField(\"est_organic_download\", LongType(), True), \n                   StructField(\"est_paid_app_download\", LongType(), True),\n                   StructField(\"est_paid_download\", LongType(), True),\n                   StructField(\"est_revenue\", LongType(), True)\n                   ])\n\nnew_rdd.toDF(schema).createOrReplaceTempView(\"data_from_db\")\n\nspark.sql(\"select * from data_from_db where  app_id=1064572106 and  country_code='WW' and device_code='ios-phone'\").show()\n# spark.sql(\"select app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue  from data_from_db except all select app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from download_attribution_join_rank_value \").show()\n# spark.sql(\"select distinct app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from download_attribution_join_rank_value except all select app_id, country_code, device_code, est_free_app_download, est_organic_download, est_paid_app_download, est_paid_download, est_revenue from data_from_db \").show(2000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200506-061045_1761773792","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from data_from_db where app_id=1460772578 and country_code='FI' and device_code='ios-phone' \").show()\nspark.sql(\"select * from download_attribution_join_rank_value where app_id=1460772578 and country_code='FI' and device_code='ios-phone' \").show()\n\n# spark.sql(\"select * from caculate_data where app_id=1391307262 and country_code='GB' and device_code='ios-phone' except all select * from data_from_db where app_id=1391307262 and country_code='GB' and device_code='ios-phone'\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200506-064959_445082143","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132  -U citus_bdp_usage_qa -d aa_citus_db -p 5432 << EOF \nset search_path=store;\n\nselect count(*) from (select app_id, category_id, country_code, device_code, est_free_app_download, est_paid_app_download, est_revenue, est_organic_download , est_paid_download, date from store.store_est_category_fact_v6 where granularity='daily' and date in ('2020-04-18') ) as t ;\n-- select * from store.store_est_fact_v6 where est_free_app_download is null or est_paid_app_download is null  and granularity='daily' and date='2020-04-17' ;\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200506-070720_336403470","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}