{"cells":[{"cell_type":"code","execution_count":0,"id":"20200621-023452_360128365","metadata":{},"outputs":[],"source":["D\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nios_feed = {1: \"0,1,2,100,101,102\"}\nandroid_feed = {0: \"0,1,2\"}\n\n\ndef plproxy_row(date, device_feed_dict):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n\n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n    sql = '''select feed_id, store_id, sum(cnt) from plproxy.execute_select_nestloop($$\n    select feed_id, store_id, count(app_id) as cnt\n    from (select distinct feed_id, store_id, app_id from aa.app_store_daily_estimate_{}\n        where date = '{}'  and  feed_id in ({}) and store_id not in (1) and isunique='t' ) as prod group by feed_id, store_id\n        $$) tbl (feed_id smallint, store_id int, cnt bigint) group by feed_id, store_id'''.format(\n        device_feed_dict.keys()[0], date, ','.join(device_feed_dict.values())\n        )\n\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [Row(feed_id=r[0], store_id=r[1], app_id_count=r[2]) for r in rows]\n\nresult_prproxy = plproxy_row('2020-04-14', ios_feed)"]},{"cell_type":"code","execution_count":0,"id":"20200621-024828_2009022129","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-04-29\").cache()\ndf1.createOrReplaceTempView(\"final\")\ndf2 = spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_PREVIEW/version=2.0.0/range_type=DAY/date=2020-04-29\").cache()\ndf2.createOrReplaceTempView(\"preview\")\n\nspark.sql('''SELECT DISTINCT id, store_id, category_id, platform, feed \n                FROM final \n                WHERE \n                    ( \n                     rank<=1000 AND store_id!=0 AND platform='ios'   \n                     OR rank<=4000 AND store_id=0 AND platform='ios'  \n                     OR rank<=1000 AND store_id!=1000 AND platform='android'  \n                     OR rank<=4000 AND store_id=1000 AND platform='android' \n                    )\n                AND feed IN (0,1,2,101,100,102)\n                AND ( \n                        ( store_id Not In (1,2,3,4,5,6) AND platform='ios' )\n                        OR\n                        ( store_id Not In (1001,1002,1003,1004,1005,1006, 1007) AND platform='android' )\n                    )    \n                 ''').createOrReplaceTempView(\"final_filter\")\nspark.sql('''SELECT DISTINCT id, store_id, category_id, platform, feed \n                FROM preview\n                WHERE \n                    ( \n                     rank<=1000 AND store_id!=0 AND platform='ios'   \n                     OR rank<=4000 AND store_id=0 AND platform='ios'  \n                     OR rank<=1000 AND store_id!=1000 AND platform='android'  \n                     OR rank<=4000 AND store_id=1000 AND platform='android' \n                    )\n                AND feed IN (0,1,2,101,100,102)\n                AND ( \n                        ( store_id Not In (1,2,3,4,5,6) AND platform='ios' )\n                        OR\n                        ( store_id Not In (1001,1002,1003,1004,1005,1006, 1007) AND platform='android' )\n                    )    \n                ''').createOrReplaceTempView(\"preview_filter\")\n                \nspark.sql(\"SELECT COUNT(1) FROM ( SELECT id FROM preview_filter EXCEPT SELECT id FROM final_filter ) AS prod\").show()\nspark.sql(\"SELECT * FROM ( SELECT id FROM preview_filter EXCEPT SELECT id FROM final_filter ) AS prod\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200621-060718_2088430990","metadata":{},"outputs":[],"source":["\n\n# spark.sql(\"select * from final where id=553329913\").show()\n# spark.sql(\"select * from preview where id=553329913\").show()\n\nspark.sql(\"select * from final_filter where id=406372707\").show()\nspark.sql(\"select * from preview_filter where id=406372707\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200621-061954_1018271581","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nSELECT app_id, feed_id, store_id, estimate, category_id, device_id, rank, date \nFROM plproxy.execute_select_nestloop(\\$proxy\\$ \n    SELECT Distinct app_id, feed_id, store_id, estimate, category_id, device_id, rank, date\n    FROM aa.app_store_daily_estimate_0\n    WHERE store_id=143619 \n    and date BETWEEN '2020-11-29' AND '2020-11-29' \n    ORDER BY category_id ASC, date DESC Limit 1 \\$proxy\\$) \n        tbl (app_id BIGINT,feed_id SMALLINT,store_id INT, estimate INT, category_id INT, device_id SMALLINT, rank INT, date Date ) \nORDER BY category_id ASC, date DESC limit 1 ;\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200621-062141_2106705809","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\n-- est_free_app_download + est_paid_app_download = organic_download + paid_download\n-- select * from store_est_fact_v6 where date = '2016-08-26' limit 5 ;\nselect count(distinct app_id) from store_est_fact_v6 where date between '2020-04-29' and '2020-04-29'  limit 5 ;\n-- paid_download is Zero\n-- select sum(est_paid_download) from store_est_category_fact_v7 where date < '2016-08-26' ;\n-- select sum(est_paid_download) from store_est_fact_v6 where date < '2016-08-26' ;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200619-030623_751530630","metadata":{},"outputs":[],"source":["\n\nnot_equal_data = spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/proproxy_data_compare_category_06_15/\").cache()\nnot_equal_data.createOrReplaceTempView(\"temp\")\nfilter_df = spark.sql(\"select distinct * from temp where date > '2019-07-14' order by date desc, result_from\").cache()\nfilter_df.createOrReplaceTempView(\"temp_filter\")\ndiff_statisc = spark.sql('''\nSELECT (p.sum_est_free_app_download - c.sum_est_free_app_download) As diff_est_free_app_download,\n        (p.sum_est_paid_app_download - c.sum_est_paid_app_download) As diff_est_paid_app_download,\n        (p.sum_est_revenue - c.sum_est_revenue) As diff_est_revenue,\n        p.device_code, p.date\nFROM temp_filter p\nCROSS JOIN temp_filter c\nWHERE p.result_from = 'proproxy'\nAND c.result_from = 'citus'\nAND p.date = c.date\nAND p.device_code = c.device_code\nORDER BY date desc\n''').show(20000)\n\n\n\n# print \"%table {}\\t{}\\t{}\\t{}\\t{}\\t \".format( \"device_code\", \"diff_est_free_app_download\" , \"diff_est_paid_app_download\", \"diff_est_revenue\", \"date\")\n\n# for x in diff_statisc:\n#     print \"{}\\t{}\\t{}\\t{}\\t{}\\t\".format(x[\"device_code\"], x[\"diff_est_free_app_download\"], x[\"diff_est_paid_app_download\"], x[\"diff_est_revenue\"], x[\"date\"])\n"]},{"cell_type":"code","execution_count":0,"id":"20200618-075248_1485577941","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ntemp_date_range=[\"2014-11-12\"]\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\ndf_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\")\\\n        .schema(csv_schema)\\\n            .csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"% (temp_date_range[0]), sep=\"\\t\")\\\n                .withColumn(\"platform\", F.lit(\"ios\"))\\\n                    .select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\ndf_1.createOrReplaceTempView(\"daily_data\")\nspark.sql(\"select count(1) from daily_data where store_id = 1\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200618-082345_2031850583","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nios_feed = {1: \"0,1,2,100,101,102\"}\nandroid_feed = {0: \"0,1,2\"}\n\n\ndef plproxy_row(date, device_feed_dict):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n\n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n    sql = '''select feed_id, store_id, Cast(sum(cnt) as int) as metric_sum from plproxy.execute_select_nestloop($$\n    select feed_id, store_id, sum(estimate)  as cnt\n    from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_{}\n        where date = '{}'  and  feed_id in ({}) and store_id not in (1) and isunique='t' ) as prod group by feed_id, store_id\n        $$) tbl (feed_id smallint, store_id int, cnt bigint) group by feed_id, store_id'''.format(\n        device_feed_dict.keys()[0], date, ','.join(device_feed_dict.values())\n        )\n\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [Row(feed_id=r[0], store_id=r[1], metric_sum=r[2]) for r in rows]\n\nresult_prproxy = plproxy_row('2011-12-31', ios_feed)\ndb_id = spark.createDataFrame(result_prproxy).cache()\ndb_id.createOrReplaceTempView(\"db_result\")\n\ndf_3 = spark.sql(\"select feed as feed_id, sum(est) as metric_sum, store_id from ( select distinct id, store_id, feed, est from daily_data where  ( ((rank<=4000 and store_id=0 ) or (rank<=1000 and store_id not in (0,1,2,3,4,5,6))) and feed in (0,1,2,101,100,102)) ) as prod group by feed, store_id\").cache()\ndf_3.createOrReplaceTempView(\"unified_group\")\n\nspark.sql(\"select * from ( select * from db_result except all select * from unified_group ) as prod order by store_id, feed_id\").show()\nspark.sql(\"select * from (select * from unified_group except all select * from db_result) as prod order by store_id, feed_id\").show()\nspark.sql(\"select * from daily_data where store_id=143477 and feed=100 and id in (485585283, 456927907, 447065901) \").show()\nspark.sql(\"select id, est from ( select distinct id,est from daily_data where  ( ((rank<=4000 and store_id=0 ) or (rank<=1000 and store_id not in (0,1,2,3,4,5,6))) and feed in (100) and store_id=143477)) as prod order by id desc\").show(20000)"]},{"cell_type":"code","execution_count":0,"id":"20200618-085257_608047580","metadata":{},"outputs":[],"source":["%%sh\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# SELECT app_id, feed_id, store_id, estimate, category_id, device_id, rank, date \n# FROM plproxy.execute_select_nestloop(\\$proxy\\$ \n#     SELECT Distinct app_id, feed_id, store_id, estimate, category_id, device_id, rank, date\n#     FROM aa.app_store_daily_estimate_1\n#     WHERE app_id In (485585283, 456927907, 447065901) \n#     AND date BETWEEN '2011-12-31' AND '2011-12-31' \n#     AND store_id = 143477 \n#     AND feed_id = 100 \n#     ORDER BY category_id ASC, date DESC Limit 500 \\$proxy\\$) tbl (app_id BIGINT,feed_id SMALLINT,store_id INT, estimate INT, category_id INT, device_id SMALLINT, rank INT, date Date ) \n# ORDER BY category_id ASC, date DESC limit 500 ;\n\n# EOF\n\n\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# select distinct app_id, estimate from plproxy.execute_select_nestloop(\\$proxy\\$ \n# select distinct app_id, estimate\n#     from aa.app_store_daily_estimate_1\n#     where \n#       date between '2011-12-31' and '2011-12-31' and feed_id=100 and store_id=143477  \\$proxy\\$) tbl \n#       (app_id BIGINT, estimate int ) order by app_id desc limit 20000 ;\n\n# EOF\n\n\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# SELECT date, Sum(cnt) \n# FROM plproxy.execute_select_nestloop(\\$proxy\\$ \n#     SELECT date, Count(1) as cnt from (\n#         SELECT Distinct app_id, feed_id, store_id, estimate, category_id, device_id, rank, date\n#         FROM aa.app_store_daily_estimate_1\n#         WHERE date Between '2011-05-01' And '2011-06-01' \n#         AND rank > 1000 \n#         AND store_id NOT in (0,1) ) AS prod\n#         GROUP BY date \\$proxy\\$) tbl (date DATE, cnt BIGINT) \n#     GROUP BY date \n#     ORDER BY date;\n# EOF\n\n\n\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# SELECT app_id, feed_id, store_id, estimate, category_id, device_id, rank, date \n# FROM plproxy.execute_select_nestloop(\\$proxy\\$ \n#     SELECT distinct app_id, feed_id, store_id, estimate, category_id, device_id, rank, date\n#     FROM aa.app_store_daily_estimate_1\n#     WHERE date BETWEEN '2011-12-31' AND '2011-12-31' \n#     AND rank > 1000 \n#     AND store_id Not In (0,1) \n#     ORDER BY category_id Asc, date Desc limit 500 \\$proxy\\$) tbl (app_id BIGINT,feed_id SMALLINT,store_id INT, estimate INT, category_id INT, device_id SMALLINT, rank INT, date Date )\n# ORDER BY category_id Asc, date Desc limit 500 ;\n\n# EOF\n\n\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nSELECT date, Sum(cnt) \nFROM plproxy.execute_select_nestloop(\\$proxy\\$ \n    SELECT date, Count(1) as cnt from (\n        SELECT Distinct app_id, feed_id, store_id, estimate, category_id, device_id, rank, date\n        FROM aa.app_store_daily_estimate_1\n        WHERE date In ('2014-11-12')\n        AND store_id in (1) ) AS prod\n        GROUP BY date \\$proxy\\$) tbl (date DATE, cnt BIGINT) \n    GROUP BY date \n    ORDER BY date;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200615-060853_265269033","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\n\nios_feed = {1: \"0,1,2,100,101,102\"}\nandroid_feed = {0: \"0,1,2\"}\n\n\ndef citus_row(date):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select device_code, cast(sum(est_free_app_download) as int) as est_free_app_download, cast(sum(est_paid_app_download) as int) as est_paid_app_download, cast(sum(est_revenue) as int) as est_revenue from store.store_est_fact_v1 where date='{}' group by device_code\".format(\n            date)\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n\n    result = get_data_in_citus(date)\n    return [Row(device_code=r[0], sum_est_free_app_download=r[1], sum_est_paid_app_download=r[2],\n                sum_est_revenue=r[3]) for r in result]\n\n\ndef plproxy_row(date, device_feed_dict):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n\n    settings = build_db_settings(urn, \"DAILY_EST\")\n    \n    \n    if device_feed_dict.keys()[0] == 1:\n        store_id = ['0','1']\t\n    else:\t\n        store_id = ['1000','1001']\n    \n\n    sql = '''select feed_id, Cast(sum(cnt) as int) as metric_sum from plproxy.execute_select_nestloop($$\n    select feed_id, sum(estimate)  as cnt\n    from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_{}\n        where date = '{}'  and  feed_id in ({}) and isunique='t'  and (\n                ( rank<=1000 AND store_id not in ({})  )\n            OR ( rank<=4000 AND store_id={} )\n        ))  as prod group by feed_id \n        $$) tbl (feed_id smallint, cnt bigint) group by feed_id '''.format(\n        device_feed_dict.keys()[0], date, ','.join(device_feed_dict.values()), ','.join(store_id), str(store_id[0])\n    )\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [Row(feed_id=r[0], metric_sum=r[1]) for r in rows]\n\n\ndef generate_plploxy_result(spark, r1, r2):\n    schema = StructType([\n        StructField(\"feed_id\", StringType(), True),\n        StructField(\"metric_sum\", IntegerType(), True)])\n    df1 = spark.createDataFrame(r1, schema)\n    df1.createOrReplaceTempView(\"plploxy_ios\")\n\n    df2 = spark.createDataFrame(r2, schema)\n    df2.createOrReplaceTempView(\"plploxy_android\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='100' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='102' THEN \"sum_est_revenue\"\n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_ios\n    ''').createOrReplaceTempView(\"plploxy_metric\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_android\n    ''').createOrReplaceTempView(\"plploxy_metric_android\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"ios-tablet\"\n    WHEN feed_id='100' THEN \"ios-tablet\"\n    WHEN feed_id='102' THEN \"ios-tablet\"\n    WHEN feed_id='0' THEN \"ios-phone\"\n    WHEN feed_id='1' THEN \"ios-phone\"\n    WHEN feed_id='2' THEN \"ios-phone\"\n    END AS device_code from plploxy_metric\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"android-all\"\n    WHEN feed_id='1' THEN \"android-all\"\n    WHEN feed_id='2' THEN \"android-all\"\n    END AS device_code from plploxy_metric_android\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code_android\")\n\n    spark.sql('''\n    SELECT * FROM plploxy_metric_device_code_android\n    UNION ALL\n    SELECT * FROM plploxy_metric_device_code\n    ''').createOrReplaceTempView(\"all_device\")\n\n    spark.sql('''\n    SELECT \n        device_code, sum_est_free_app_download , sum_est_paid_app_download , sum_est_revenue\n    FROM\n          all_device\n     PIVOT (\n        max(metric_sum) \n        FOR metric IN ('sum_est_free_app_download','sum_est_paid_app_download', 'sum_est_revenue')\n      )\n    ''').createOrReplaceTempView(\"after_pivot\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_free_app_download FROM after_pivot\n    WHERE sum_est_free_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_1\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_paid_app_download FROM after_pivot\n    WHERE sum_est_paid_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_2\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_revenue FROM after_pivot\n    WHERE sum_est_revenue is not null\n    ''').createOrReplaceTempView(\"after_pivot_3\")\n\n    return spark.sql('''\n    SELECT c.device_code, c.sum_est_free_app_download, c.sum_est_paid_app_download, d.sum_est_revenue\n    FROM (\n    SELECT a.device_code, a.sum_est_free_app_download, b.sum_est_paid_app_download FROM after_pivot_1 a\n    JOIN after_pivot_2 b\n    ON a.device_code=b.device_code\n    ) AS c\n    JOIN after_pivot_3 d\n    on c.device_code=d.device_code\n    order by device_code desc\n    ''')\n\n\ndef generate_citus_result(spark, citus_data):\n    schema = StructType([\n    StructField(\"device_code\", StringType(), True),\n    StructField(\"sum_est_free_app_download\", IntegerType(), True),\n    StructField(\"sum_est_paid_app_download\", IntegerType(), True),\n    StructField(\"sum_est_revenue\", IntegerType(), True)])\n\n    df_3 = spark.createDataFrame(citus_data, schema)\n    df_3.createOrReplaceTempView(\"citus_data\")\n    return spark.sql(\"select * from citus_data order by device_code desc\")\n\n\ndef test_store_prproxy_data(spark, d):\n    r1 = plproxy_row(d, ios_feed)\n    r2 = plproxy_row(d, android_feed)\n    citus_reseult = citus_row(d)\n\n    d1 = generate_plploxy_result(spark,r1, r2)\n    d2 = generate_citus_result(spark,citus_reseult)\n\n    d1.createOrReplaceTempView(\"plploxy_r\")\n    d2.createOrReplaceTempView(\"citus_r\")\n    plproxy_except_result = spark.sql(\"select * from plploxy_r except all select * from citus_r\")\n    citus_except_result = spark.sql(\"select * from citus_r except all select * from plploxy_r\")\n\n    plproxy_except_result= plproxy_except_result.withColumn(\"result_from\", F.lit(\"proproxy\")).withColumn(\"date\", F.lit(str(d)))\n    citus_except_result = citus_except_result.withColumn(\"result_from\", F.lit(\"citus\")).withColumn(\"date\", F.lit(str(d)))\n\n    result = plproxy_except_result.union(citus_except_result)\n\n    df_write_result = result\n    # write_test_result(df_write_result)\n    plproxy_except_result.show()\n    citus_except_result.show()\n\n# def write_test_result(df_write_result):\n#         df_write_result.write.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/proproxy_data_compare_est_06_15/\",\n#                                           mode=\"append\",\n#                                           partitionBy=[\"date\"])\n\n\n\n\n\nstart = \"2014-06-27\"\nend = \"2014-06-28\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\ndates.sort(reverse=True)\n\nfor d in dates:\n    print d\n    test_store_prproxy_data(spark, d)\n"]},{"cell_type":"code","execution_count":0,"id":"20200623-123245_1612383337","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\nios_feed = {1: \"0,1,2,100,101,102\"}\nandroid_feed = {0: \"0,1,2\"}\ndef citus_row(date):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select device_code, cast(sum(est_free_app_download) as int) as est_free_app_download, cast(sum(est_paid_app_download) as int) as est_paid_app_download, cast(sum(est_revenue) as int) as est_revenue from store.store_est_category_fact_v1 where date='{}' group by device_code\".format(\n            date)\n        db_data = query(citus_dsn_, sql)\n        return db_data\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    result = get_data_in_citus(date)\n    return [Row(device_code=r[0], sum_est_free_app_download=r[1], sum_est_paid_app_download=r[2],\n                sum_est_revenue=r[3]) for r in result]\ndef plproxy_row(date, device_feed_dict):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n    settings = build_db_settings(urn, \"DAILY_EST\")\n    if device_feed_dict.keys()[0] == 1:\n        store_id = ['0','1']\t\n    else:\t\n        store_id = ['1000','1001']\n    sql = '''select feed_id, Cast(sum(cnt) as int) as metric_sum from plproxy.execute_select_nestloop($$\n    select feed_id, sum(estimate)  as cnt\n    from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_{}\n        where date = '{}'  and  feed_id in ({})  and (\n                ( rank<=1000 AND store_id not in ({})  )\n            OR ( rank<=4000 AND store_id={} )\n        )) as prod group by feed_id \n        $$) tbl (feed_id smallint, cnt bigint) group by feed_id '''.format(\n        device_feed_dict.keys()[0], date, ','.join(device_feed_dict.values()), ','.join(store_id), str(store_id[0])\n    )\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [Row(feed_id=r[0], metric_sum=r[1]) for r in rows]\ndef generate_plploxy_result(spark, r1, r2):\n    schema = StructType([\n        StructField(\"feed_id\", StringType(), True),\n        StructField(\"metric_sum\", IntegerType(), True)])\n    df1 = spark.createDataFrame(r1, schema)\n    df1.createOrReplaceTempView(\"plploxy_ios\")\n    df2 = spark.createDataFrame(r2, schema)\n    df2.createOrReplaceTempView(\"plploxy_android\")\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='100' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='102' THEN \"sum_est_revenue\"\n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_ios\n    ''').createOrReplaceTempView(\"plploxy_metric\")\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_android\n    ''').createOrReplaceTempView(\"plploxy_metric_android\")\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"ios-tablet\"\n    WHEN feed_id='100' THEN \"ios-tablet\"\n    WHEN feed_id='102' THEN \"ios-tablet\"\n    WHEN feed_id='0' THEN \"ios-phone\"\n    WHEN feed_id='1' THEN \"ios-phone\"\n    WHEN feed_id='2' THEN \"ios-phone\"\n    END AS device_code from plploxy_metric\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code\")\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"android-all\"\n    WHEN feed_id='1' THEN \"android-all\"\n    WHEN feed_id='2' THEN \"android-all\"\n    END AS device_code from plploxy_metric_android\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code_android\")\n    spark.sql('''\n    SELECT * FROM plploxy_metric_device_code_android\n    UNION ALL\n    SELECT * FROM plploxy_metric_device_code\n    ''').createOrReplaceTempView(\"all_device\")\n    spark.sql('''\n    SELECT \n        device_code, sum_est_free_app_download , sum_est_paid_app_download , sum_est_revenue\n    FROM\n          all_device\n     PIVOT (\n        max(metric_sum) \n        FOR metric IN ('sum_est_free_app_download','sum_est_paid_app_download', 'sum_est_revenue')\n      )\n    ''').createOrReplaceTempView(\"after_pivot\")\n    spark.sql('''\n    SELECT device_code, sum_est_free_app_download FROM after_pivot\n    WHERE sum_est_free_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_1\")\n    spark.sql('''\n    SELECT device_code, sum_est_paid_app_download FROM after_pivot\n    WHERE sum_est_paid_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_2\")\n    spark.sql('''\n    SELECT device_code, sum_est_revenue FROM after_pivot\n    WHERE sum_est_revenue is not null\n    ''').createOrReplaceTempView(\"after_pivot_3\")\n    return spark.sql('''\n    SELECT c.device_code, c.sum_est_free_app_download, c.sum_est_paid_app_download, d.sum_est_revenue\n    FROM (\n    SELECT a.device_code, a.sum_est_free_app_download, b.sum_est_paid_app_download FROM after_pivot_1 a\n    JOIN after_pivot_2 b\n    ON a.device_code=b.device_code\n    ) AS c\n    JOIN after_pivot_3 d\n    on c.device_code=d.device_code\n    order by device_code desc\n    ''')\n    \ndef generate_citus_result(spark, citus_data):\n    schema = StructType([\n    StructField(\"device_code\", StringType(), True),\n    StructField(\"sum_est_free_app_download\", IntegerType(), True),\n    StructField(\"sum_est_paid_app_download\", IntegerType(), True),\n    StructField(\"sum_est_revenue\", IntegerType(), True)])\n    df_3 = spark.createDataFrame(citus_data, schema)\n    df_3.createOrReplaceTempView(\"citus_data\")\n    return spark.sql(\"select * from citus_data order by device_code desc\")\n    \n    \ndef test_store_prproxy_data(spark, d):\n    r1 = plproxy_row(d, ios_feed)\n    r2 = plproxy_row(d, android_feed)\n    citus_reseult = citus_row(d)\n    d1 = generate_plploxy_result(spark,r1, r2)\n    d2 = generate_citus_result(spark,citus_reseult)\n    d1.createOrReplaceTempView(\"plploxy_r\")\n    d2.createOrReplaceTempView(\"citus_r\")\n    plproxy_except_result = spark.sql(\"select * from plploxy_r except all select * from citus_r\")\n    citus_except_result = spark.sql(\"select * from citus_r except all select * from plploxy_r\")\n    plproxy_except_result= plproxy_except_result.withColumn(\"result_from\", F.lit(\"proproxy\")).withColumn(\"date\", F.lit(str(d)))\n    citus_except_result = citus_except_result.withColumn(\"result_from\", F.lit(\"citus\")).withColumn(\"date\", F.lit(str(d)))\n    result = plproxy_except_result.union(citus_except_result)\n    df_write_result = result\n    # write_test_result(df_write_result)\n    plproxy_except_result.show()\n    citus_except_result.show()\n# def write_test_result(df_write_result):\n#         df_write_result.write.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/proproxy_data_compare_category_06_15/\",\n#                                           mode=\"append\",\n#                                           partitionBy=[\"date\"])\n        # from aadatapipelinecore.core.utils.retry import retry\n        # retry(write_test_result,(df_write_result,),{},interval=10)\nstart = \"2014-06-27\"\nend = \"2014-06-28\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\ndates.sort(reverse=True)\nfor d in dates:\n    print d\n    test_store_prproxy_data(spark, d)"]},{"cell_type":"code","execution_count":0,"id":"20200615-071336_271402277","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\n\nios_feed = {1: \"0,1,2,100,101,102\"}\nandroid_feed = {0: \"0,1,2\"}\n\n\ndef citus_row(date):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select device_code, cast(sum(est_free_app_download) as int) as est_free_app_download, cast(sum(est_paid_app_download) as int) as est_paid_app_download, cast(sum(est_revenue) as int) as est_revenue from store.store_est_category_fact_v1 where date='{}' group by device_code\".format(\n            date)\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n\n    result = get_data_in_citus(date)\n    return [Row(device_code=r[0], sum_est_free_app_download=r[1], sum_est_paid_app_download=r[2],\n                sum_est_revenue=r[3]) for r in result]\n\n\ndef plproxy_row(date, device_feed_dict):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n\n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n    sql = '''select feed_id, Cast(sum(cnt) as int) as metric_sum from plproxy.execute_select_nestloop($$\n    select feed_id, sum(estimate)  as cnt\n    from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_{}\n        where date = '{}'  and  feed_id in ({})  ) as prod group by feed_id \n        $$) tbl (feed_id smallint, cnt bigint) group by feed_id '''.format(\n        device_feed_dict.keys()[0], date, ','.join(device_feed_dict.values())\n    )\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [Row(feed_id=r[0], metric_sum=r[1]) for r in rows]\n\n\ndef generate_plploxy_result(spark, r1, r2):\n    schema = StructType([\n        StructField(\"feed_id\", StringType(), True),\n        StructField(\"metric_sum\", IntegerType(), True)])\n    df1 = spark.createDataFrame(r1, schema)\n    df1.createOrReplaceTempView(\"plploxy_ios\")\n\n    df2 = spark.createDataFrame(r2, schema)\n    df2.createOrReplaceTempView(\"plploxy_android\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='100' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='102' THEN \"sum_est_revenue\"\n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_ios\n    ''').createOrReplaceTempView(\"plploxy_metric\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_android\n    ''').createOrReplaceTempView(\"plploxy_metric_android\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"ios-tablet\"\n    WHEN feed_id='100' THEN \"ios-tablet\"\n    WHEN feed_id='102' THEN \"ios-tablet\"\n    WHEN feed_id='0' THEN \"ios-phone\"\n    WHEN feed_id='1' THEN \"ios-phone\"\n    WHEN feed_id='2' THEN \"ios-phone\"\n    END AS device_code from plploxy_metric\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"android-all\"\n    WHEN feed_id='1' THEN \"android-all\"\n    WHEN feed_id='2' THEN \"android-all\"\n    END AS device_code from plploxy_metric_android\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code_android\")\n\n    spark.sql('''\n    SELECT * FROM plploxy_metric_device_code_android\n    UNION ALL\n    SELECT * FROM plploxy_metric_device_code\n    ''').createOrReplaceTempView(\"all_device\")\n\n    spark.sql('''\n    SELECT \n        device_code, sum_est_free_app_download , sum_est_paid_app_download , sum_est_revenue\n    FROM\n          all_device\n     PIVOT (\n        max(metric_sum) \n        FOR metric IN ('sum_est_free_app_download','sum_est_paid_app_download', 'sum_est_revenue')\n      )\n    ''').createOrReplaceTempView(\"after_pivot\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_free_app_download FROM after_pivot\n    WHERE sum_est_free_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_1\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_paid_app_download FROM after_pivot\n    WHERE sum_est_paid_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_2\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_revenue FROM after_pivot\n    WHERE sum_est_revenue is not null\n    ''').createOrReplaceTempView(\"after_pivot_3\")\n\n    return spark.sql('''\n    SELECT c.device_code, c.sum_est_free_app_download, c.sum_est_paid_app_download, d.sum_est_revenue\n    FROM (\n    SELECT a.device_code, a.sum_est_free_app_download, b.sum_est_paid_app_download FROM after_pivot_1 a\n    JOIN after_pivot_2 b\n    ON a.device_code=b.device_code\n    ) AS c\n    JOIN after_pivot_3 d\n    on c.device_code=d.device_code\n    order by device_code desc\n    ''')\n\n\ndef generate_citus_result(spark, citus_data):\n    schema = StructType([\n    StructField(\"device_code\", StringType(), True),\n    StructField(\"sum_est_free_app_download\", IntegerType(), True),\n    StructField(\"sum_est_paid_app_download\", IntegerType(), True),\n    StructField(\"sum_est_revenue\", IntegerType(), True)])\n\n    df_3 = spark.createDataFrame(citus_data, schema)\n    df_3.createOrReplaceTempView(\"citus_data\")\n    return spark.sql(\"select * from citus_data order by device_code desc\")\n\n\ndef test_store_prproxy_data(spark, d):\n    r1 = plproxy_row(d, ios_feed)\n    r2 = plproxy_row(d, android_feed)\n    citus_reseult = citus_row(d)\n\n    d1 = generate_plploxy_result(spark,r1, r2)\n    d2 = generate_citus_result(spark,citus_reseult)\n\n    d1.createOrReplaceTempView(\"plploxy_r\")\n    d2.createOrReplaceTempView(\"citus_r\")\n    plproxy_except_result = spark.sql(\"select * from plploxy_r except all select * from citus_r\")\n    citus_except_result = spark.sql(\"select * from citus_r except all select * from plploxy_r\")\n\n    plproxy_except_result= plproxy_except_result.withColumn(\"result_from\", F.lit(\"proproxy\")).withColumn(\"date\", F.lit(str(d)))\n    citus_except_result = citus_except_result.withColumn(\"result_from\", F.lit(\"citus\")).withColumn(\"date\", F.lit(str(d)))\n\n    result = plproxy_except_result.union(citus_except_result)\n\n    df_write_result = result\n    write_test_result(df_write_result)\n    plproxy_except_result.show()\n    citus_except_result.show()\n\ndef write_test_result(df_write_result):\n        df_write_result.write.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/proproxy_data_compare_category_06_15/\",\n                                          mode=\"append\",\n                                          partitionBy=[\"date\"])\n\n        # from aadatapipelinecore.core.utils.retry import retry\n        # retry(write_test_result,(df_write_result,),{},interval=10)\n\n\n\n\nstart = \"2020-02-04\"\nend = \"2020-02-05\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\ndates.sort(reverse=True)\n\nfor d in dates:\n    print d\n    test_store_prproxy_data(spark, d)\n"]},{"cell_type":"code","execution_count":0,"id":"20200615-065050_1390424365","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-11-29/ios/sbe_est_app_preview/143619/\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-qa/aa.store/proproxy_data_compare_category_06_15/2018-05-10\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200615-065258_1090822848","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}