{"cells":[{"cell_type":"code","execution_count":0,"id":"20200615-073843_365571860","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-06-20/  --recursive\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-06-20 --recursive\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-category-pre-aggr.v3/fact/granularity=weekly/date=2019-01-12/device_code=ios-phone/\n \n# store.download-attribution-pre-aggr.v3/\n"]},{"cell_type":"code","execution_count":0,"id":"20200615-073726_1258042095","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/granularity=weekly/date=2019-01-12/device_code=ios-phone/\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200615-082233_1985694463","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2020-03-29\"\nend = \"2020-05-02\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append((str(real_date1 + datetime.timedelta(days)), temp))\nprint sar_list\nfor d in sar_list:\n    daily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(\"granularity='daily' and date in ('%s')\" % (\"','\".join(sar_list[0][1])))\n    daily_download_attr.createOrReplaceTempView(\"daily_pre_load_data\")\n    \n    daily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='weekly' and date in ('%s')\" % (sar_list[0][0]))\n    daily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    \n\n    spark.sql(\"select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified except all select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code\").show()\n\n    spark.sql(\"select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code except all select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200616-104433_1540302894","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2020-02-01\"\nend = \"2020-04-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\n# sar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(str(real_date1 + datetime.timedelta(days)))\n#     if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n#         temp=list()\n#         while dates:\n#             temp.append(str(dates.pop()))\n#         sar_list.append((str(real_date1 + datetime.timedelta(days)), temp))\n# print sar_list\n\nprint dates\nprint dates[-1]\n# sql_where = \"granularity='daily' and date in ('%s')\" % (\"','\".join(dates))\nsql_where = \"granularity='daily' and date between '2019-03-01' and '2019-03-31'\"\n\nprint sql_where\ndaily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(sql_where)\ndaily_download_attr.createOrReplaceTempView(\"daily_pre_load_data\")\n\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='monthly' and date='2019-03-31' \")\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    \nspark.sql(\"select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified except all select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code\").show()\n\nspark.sql(\"select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code except all select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified\").show()\n\n\nspark.sql(\"select * from download_attribution_weekly_unified\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200616-123252_1783801256","metadata":{},"outputs":[],"source":["\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='monthly'\" )\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n\ntest_date_list = spark.sql(\"select distinct date from download_attribution_weekly_unified\").collect()\n\nl = [str(d[0]) for d in test_date_list]\nl.sort(reverse=False)\nfor x in l:\n    print x"]},{"cell_type":"code","execution_count":0,"id":"20200616-105248_2003144946","metadata":{},"outputs":[],"source":["%%sh\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/granularity=monthly/\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/granularity=daily/date=2020-03-22/"]},{"cell_type":"code","execution_count":0,"id":"20200615-094622_1323478314","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2016-08-28\"\nend = \"2020-05-31\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append((str(real_date1 + datetime.timedelta(days))))\n\nprint sar_list"]},{"cell_type":"code","execution_count":0,"id":"20200615-094755_219888910","metadata":{},"outputs":[],"source":["\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='weekly'\" )\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n\ntest_date_list = spark.sql(\"select distinct date from download_attribution_weekly_unified\").collect()\n\nl = [str(d[0]) for d in test_date_list]\nl.sort(reverse=False)\nprint l"]},{"cell_type":"code","execution_count":0,"id":"20200615-102159_611753815","metadata":{},"outputs":[],"source":["\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='weekly'\" )\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n\nspark.sql(\"select * from download_attribution_weekly_unified where date = '2020-04-09' and granularity='weekly'  \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200616-071930_1435914486","metadata":{},"outputs":[],"source":["\nimport datetime\nimport calendar\n\nstart = \"2010-07-04\"\nend = \"2010-08-04\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\n\nmonth_day=list()\nfor days in xrange(date_range.days):\n    month_day.append(real_date1 + datetime.timedelta(days))\n\n\ntest_list= sorted(list(set([ d.strftime(\"%Y-%m-%d\")[:7] for d in month_day ])))\n\nprint test_list\n\n# for m in test_list:\n#     print calendar.monthrange(int(m[:4]),int(m[5:7]))\n#     print \"-\".join([str(m[:4]), str(calendar.monthrange(int(m[:4]),int(m[-2:]))[0]), str(calendar.monthrange(int(m[:4]),int(m[-2:]))[1])])\n\n\ndef last_day_of_month(date):\n    next_month = date.replace(day=28) + datetime.timedelta(days=4)  # this will never fail\n    return next_month - datetime.timedelta(days=next_month.day)\nprint last_day_of_month(start)"]},{"cell_type":"code","execution_count":0,"id":"20200625-020408_1044522697","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/granularity=daily/date=2017\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/granularity=daily/date=2017\n \n"]},{"cell_type":"code","execution_count":0,"id":"20200625-033738_180609704","metadata":{},"outputs":[],"source":["\ndownload_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(\"granularity='daily' and date between '2017-04-01' and '2017-06-30'\").cache()\ndownload_attr.createOrReplaceTempView(\"download_attr\")\n\n\nest = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '2017-04-01' and '2017-06-30'\").cache()\nest.createOrReplaceTempView(\"est\")\n\nspark.sql(\"select date, count(date) as date_count from download_attr where app_id= 20600002647354  and country_code='US' group by date \").show(10000)\nspark.sql(\"select date, count(date) as date_count from est where app_id= 20600002647354 and country_code='US' group by date \").show(10000)\n\n# spark.sql(\"select date,count(date) ( select date from download_attr ) as prod group by date \").show(1000)\n# spark.sql(\"select date,count(date) (select date from est ) as prod group by date\").show(1000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200625-030025_1540731302","metadata":{},"outputs":[],"source":["%%sh\ndate"]},{"cell_type":"code","execution_count":0,"id":"20200615-103008_482987018","metadata":{},"outputs":[],"source":["\nimport datetime\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nstart = '2020-02-23'\nend = '2020-03-01'\ngranularity = 'weekly'\n\n\ngranularity_table_dict = {'weekly': ['store.app-est.v3', 'store_est_t_w_fact_v1', ''],\n                         'monthly': ['store.app-est.v3', 'store_est_t_m_fact_v1'],\n                         'quarterly' : ['store.app-est-pre-aggr.v3', 'store_est_t_q_fact_v1'],\n                         'yearly' : ['store.app-est-pre-aggr.v3', 'store_est_t_y_fact_v1']\n                        }\n\ndownload_attribution_bucket='store.download-attribution-pre-aggr.v3'\n\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append((str(real_date1 + datetime.timedelta(days)), temp))\nprint sar_list\n\n\n\ndef citus_row(date, granularity):\n    def get_data_in_citus(date):\n        print 'get data in citus ', date\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = '''SELECT  \n                    cast(sum(est_free_app_download) as bigint) as est_free_app_download, \n                    cast(sum(est_paid_app_download) as bigint) as est_paid_app_download, \n                    cast(sum(est_revenue) as bigint) as est_revenue, \n                    cast(sum(est_organic_download) as bigint) as est_organic_download ,  \n                    cast(sum(est_paid_download) as bigint) as est_paid_download\n                FROM store.{}\n                WHERE date in ('{}') '''.format(granularity_table_dict[granularity][1],date)\n        db_data = query(citus_dsn_, sql)\n        return db_data\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    result = get_data_in_citus(date)\n    return [Row(sum_est_free_app_download=r[0], sum_est_paid_app_download=r[1], sum_est_revenue=r[2], sum_est_organic_download=r[3], sum_est_paid_download=r[4] ) for r in result]\n\n\nfor d in sar_list:\n    print d[0]\n\n    daily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/{}/fact/\".format(download_attribution_bucket)).where(\"granularity='%s' and date in ('%s')\" % (granularity, d[0]))\n    daily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    est_weekly_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n                    \"{}/fact/\".format(granularity_table_dict[granularity][0])).where(\"granularity='{}' and date in ('{}')\".format(granularity,d[0] ))\n    est_weekly_df.createOrReplaceTempView(\"est_weekly_df\")\n\n\n    citus_result = citus_row(d[0], 'weekly')\n\n    schema = StructType([\n    StructField(\"sum_est_free_app_download\", LongType(), True),\n    StructField(\"sum_est_paid_app_download\", LongType(), True),\n    StructField(\"sum_est_revenue\", LongType(), True),\n    StructField(\"sum_est_organic_download\", LongType(), True),\n    StructField(\"sum_est_paid_download\", LongType(), True)])\n\n    df_3 = spark.createDataFrame(citus_result, schema)\n    df_3.createOrReplaceTempView(\"citus_data\")\n\n    \n    # compare est paid download by DB and download attribution\n    spark.sql(\"select sum(est_paid_app_download) as sum_est_paid_download from download_attribution_weekly_unified except all select sum_est_paid_download from citus_data\").show()\n    spark.sql(\"select sum_est_paid_download from citus_data except all select sum(est_paid_app_download) as sum_est_paid_download from download_attribution_weekly_unified\").show()\n\n    # compare est weekly data with DB metric sum(free_app_download), sum(paid_app_download), sum(revenue) \n    spark.sql(\"select sum(free_app_download), sum(paid_app_download), sum(revenue) from est_weekly_df except all select sum_est_free_app_download, sum_est_paid_app_download, sum_est_revenue from citus_data \").show()\n    spark.sql(\"select sum_est_free_app_download, sum_est_paid_app_download, sum_est_revenue from citus_data except all select sum(free_app_download), sum(paid_app_download), sum(revenue) from est_weekly_df \").show()\n\n\n    # compare organic + paid = free_app + paid_app in DB\n    list_result =  spark.sql(\"select sum_est_free_app_download + sum_est_paid_app_download,  sum_est_organic_download + sum_est_paid_download from citus_data \").collect()\n    if list_result[0][0] != list_result[0][1]:\n        print 'compare compare organic + paid = free_app + paid_app failed!! , organic + paid= {}, free_app + paid_app = {}'.format(list_result[0][0], list_result[0][1])\n    else:\n        print 'pass!'"]},{"cell_type":"code","execution_count":0,"id":"20200625-042502_962998659","metadata":{},"outputs":[],"source":["\n\nspark.sql(\"select * from citus_data\").show()\n# list_result =  spark.sql(\"select sum_est_free_app_download + sum_est_paid_app_download,  sum_est_organic_download + sum_est_paid_download from citus_data \").collect()\n\n# print list_result[0][0]\n# print list_result[0][1]"]},{"cell_type":"code","execution_count":0,"id":"20200625-025115_732800491","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\nselect sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue),  sum(est_organic_download),  sum(est_paid_download) from store.store_est_fact_v1 where date between '2020-02-23' and '2020-02-29' and granularity='daily' limit 3 ;\nselect sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue),  sum(est_organic_download),  sum(est_paid_download) from store.store_est_t_w_fact_v1 where date between  '2020-02-23' and '2020-02-29' and granularity='weekly' limit 3 ;\nSELECT  \n                    cast(sum(est_free_app_download) as bigint\t) as est_free_app_download, \n                    cast(sum(est_paid_app_download) as bigint\t) as est_paid_app_download, \n                    cast(sum(est_revenue) as bigint\t) as est_revenue, \n                    cast(sum(est_organic_download) as bigint\t) as est_organic_download ,  \n                    cast(sum(est_paid_download) as bigint\t) as est_paid_download\n                FROM store.store_est_t_w_fact_v1\n                WHERE date in ('2020-02-29')\nEOF\n\n# select sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue),  sum(est_organic_download),  sum(est_paid_download) from store.store_est_t_w_fact_v1 where date = '2017-07-15' and granularity='weekly' limit 3 ;\n# select distinct date from store.store_est_t_w_fact_v1 where date between '2017-06-25' and '2017-07-15' and granularity='weekly' limit 10;\n"]},{"cell_type":"code","execution_count":0,"id":"20200625-035425_91370984","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\n\nselect * from store.store_est_category_t_w_fact_v1 limit 3 ;\nselect * from store.store_est_category_t_m_fact_v1 where date='2019-01-12' limit 3 ;\nselect * from store.store_est_category_t_q_fact_v1 limit 3 ;\nselect * from store.store_est_category_t_y_fact_v1 limit 3 ;\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200625-021412_364419153","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from est_weekly_df where app_id=20600004908242 and country_code='WW'\").show()\nspark.sql(\"select * from download_attribution_weekly_unified where app_id=20600004908242 and country_code='WW'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200625-021703_1805844339","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}