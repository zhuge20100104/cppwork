{"cells":[{"cell_type":"code","execution_count":0,"id":"20200623-064650_501741552","metadata":{},"outputs":[],"source":["\n\n\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\nstart = '2019-01-01'\nend = '2019-12-31'\ngranularity = 'quarterly'\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\nagg_df = category_daily_df.groupBy('app_id', 'country_code', 'device_code', 'category_id').agg(\n    sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), \n        sum('est_revenue').alias('revenue'))\nagg_df.createOrReplaceTempView(\"agg_df\")\nif granularity == 'weekly' or granularity == 'monthly':\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\nelse:\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\npre_agg_df.createOrReplaceTempView(\"pre_agg_df\")\ndiff_df1 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df\"\"\")\ndiff_df2 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df\"\"\")\nprint agg_df.count(), pre_agg_df.count()\nprint diff_df1.count(), diff_df2.count()\ndiff_df1.show()\ndiff_df2.show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200624-132045_469348984","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20200624-124623_1555584529","metadata":{},"outputs":[],"source":["\n\nstart = '2020-01-01'\nend = '2020-03-31'\ngranularity = 'quarterly'\nquarterly_category_bucket =\"store.app-est-category-pre-aggr.v3\"\nquarterly_est_bucket= \"store.app-est-pre-aggr.v3\"\n\n\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"{}/fact/\".format(quarterly_category_bucket)).where(\"granularity='{}' and date between '{}' and '{}'\".format(granularity, start, end))\ncategory_daily_df.createOrReplaceTempView(\"category_daily_df\")\nspark.sql(\"\"\" select date, app_id, country_code, device_code, category_id,\n                    free_app_download,paid_app_download, revenue\n                from  category_daily_df\"\"\").createOrReplaceTempView(\"unpivot_category_daily_df\")\n# spark.sql(\"\"\"select app_id, country_code, device_code, category_id, metric, sum(est) as est\n#                 from unpivot_category_daily_df\n#                 where est is not null\n#                 group by\n#                 app_id,\n#                 country_code,\n#                 device_code,\n#                 category_id,\n#                 metric\n# \"\"\").createOrReplaceTempView(\"unpivot_category_agg_df\")\nest_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"{}/fact/\".format(quarterly_est_bucket)).where(\"granularity='{}' and date between '{}' and '{}'\".format(granularity, start, end))\nest_daily_df.createOrReplaceTempView(\"est_daily_df\")\nspark.sql(\"\"\" select app_id, country_code, device_code, \n                 free_app_download, paid_app_download, revenue\n            FROM est_daily_df \"\"\").createOrReplaceTempView(\"unpivot_est_agg_df\")\nspark.sql(\"\"\"select d1.app_id, d1.country_code, d1.device_code, d1.category_id, d2.free_app_download, d2.paid_app_download, d2.revenue\n                from unpivot_category_daily_df as d1 \n                join unpivot_est_agg_df as d2 \n                on\n                d1.app_id=d2.app_id\n                and d1.country_code=d2.country_code\n                and d1.device_code=d2.device_code\n                and d1.free_app_download=d2.free_app_download\n                and d1.paid_app_download=d2.paid_app_download\n                and d1.revenue=d2.revenue \"\"\").createOrReplaceTempView(\"unpivot_agg_df\")\n# spark.sql(\"\"\" select * FROM unpivot_agg_df\n#                 pivot(\n#                     max(est) FOR metric IN ('free_app_download','revenue', 'paid_app_download')\n#                 )\n# \"\"\").createOrReplaceTempView(\"agg_df\")\nspark.sql(\"\"\"select * from unpivot_agg_df where category_id=100009 and country_code='WW' and device_code='ios-phone' order by paid_app_download desc, app_id desc limit 1000\"\"\").show(1000)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200624-105250_357068524","metadata":{},"outputs":[],"source":["\nstart = '2019-12-29'\nend = '2020-01-04'\n\ndaily_category_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}' and app_id=1225683141 and country_code='US' and device_code='ios-phone'\".format(start, end)).show(200)\n\nweekly_df_category = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='weekly' and date between '{}' and '{}'  and app_id=1225683141 and country_code='US' and device_code='ios-phone'\".format(start, end))\nweekly_df_category.createOrReplaceTempView(\"weekly_category\")\n\nweekly_df_est = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v3/fact/\").where(\"granularity='weekly' and date between '{}' and '{}'  and app_id=1225683141 and country_code='US' and device_code='ios-phone'\".format(start, end))\nweekly_df_est.createOrReplaceTempView(\"weekly_est\")\n\nspark.sql(\"\")\n\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200624-105157_276279442","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v3/fact/granularity=quarterly/ --recursive | sort -n\n \n "]},{"cell_type":"code","execution_count":0,"id":"20200624-021719_1965155874","metadata":{},"outputs":[],"source":["\nstart = '2019-12-29'\nend = '2020-01-04'\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df.createOrReplaceTempView(\"daily_df\")\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v3/fact/\").where(\"granularity='weekly' and date='{}' and data_stage='final'\".format(end))\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\nspark.sql(\"select date, app_id,device_code,country_code,category_id,free_app_download from quarterly_df where app_id=1009836739 and country_code='AM' and device_code='ios-phone'\").show()\nspark.sql(\"select date, app_id,device_code,country_code,category_id,est_free_app_download from daily_df where app_id=1009836739 and country_code='AM' and device_code='ios-phone' order by category_id\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200624-020939_369705461","metadata":{},"outputs":[],"source":["\nstart = '2019-09-29'\nend = '2019-10-05'\ndaily_df_1 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df_1.where(\"app_id=538212549 and country_code='US' and device_code='ios-phone'\").show(999)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200622-065455_2023738478","metadata":{},"outputs":[],"source":["\nstart = '2019-10-05'\nend = '2019-10-05'\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='weekly' and date between '{}' and '{}'\".format(start, end))\ndaily_df.filter(\"app_id=538212549 and country_code='US' and device_code='ios-phone'\").show(999)\n\n\n# spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact\").where(\"granularity='monthly' and date between '{}' and '{}' and app_id=538212549 and country_code='MU' and device_code='ios-phone' and category_id=100000\".format(start, end)).show()"]},{"cell_type":"code","execution_count":0,"id":"20200620-063310_2017099075","metadata":{},"outputs":[],"source":["\nimport datetime\nstart = \"2016-08-28\"\nend = \"2020-03-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append(str(real_date1 + datetime.timedelta(days)))\n\nfor x in sar_list:\n    print x\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200620-064432_1542932194","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\n\ndate='2020-02-23'\n\ndef plproxy_row(date):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n\n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n    sql = '''select app_id from plproxy.execute_select_nestloop($$ \n                select app_id from (\n                    select distinct app_id,store_id ,device_id, date, est_organic_download_share\n                        from da.app_da_daily_estimate_1001 where date='{}' and store_id=10 )\n                        as prod $$) tbl \n              (app_id bigint ) order by app_id  ;\n            '''.format(\n            date\n    )\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [Row(app_id=r[0]) for r in rows]\n\n\ndef citus_row(date):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select app_id from store.store_est_fact_v6 where date = '{}' and country_code='US' and est_paid_download !=0  and device_code='android-all' order by app_id\".format(\n            date)\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n\n    result = get_data_in_citus(date)\n    return [Row(app_id=r[0]) for r in result]\n\n\nplproxy_result = plproxy_row(date)\ncitus_result = citus_row(date)\n\ndf_plproxy = spark.createDataFrame(plproxy_result).cache()\ndf_plproxy.createOrReplaceTempView(\"temp_plproxy\")\n\ncitus_df = spark.createDataFrame(citus_result).cache()\ncitus_df.createOrReplaceTempView(\"temp_citus\")\nspark.sql(\"select * from temp_citus except select * from temp_plproxy \").show()\ndiff_df_list = spark.sql(\"select * from temp_plproxy except select * from temp_citus \").collect()\nprint 'not equal count', len(diff_df_list)\ndiff_list = [str(x[0]) for x in diff_df_list]\ndf = spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={}\".format(date)).cache()\ndf.createOrReplaceTempView(\"temp\")\nspark.sql(\"select * from temp where id in ({}) and store_id=10 and feed in (0,1)\".format(\",\".join(diff_list))).show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200620-042303_1610473079","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nselect distinct app_id,store_id,device_id, date from plproxy.execute_select_nestloop(\\$proxy\\$ \nselect  distinct app_id,store_id ,device_id, date\n    from da.app_da_daily_estimate_2001 where date='2019-12-22' and app_id = 1225683141 and store_id=143441\n     limit 5 \\$proxy\\$) tbl \n      (app_id BIGINT, store_id INT, device_id SMALLINT, date Date )  ;\n\nEOF\n\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# select sum(cnt) as sum from plproxy.execute_select_nestloop(\\$proxy\\$ \n# select count(1) as cnt from (\n# select distinct app_id,store_id ,device_id, date, est_organic_download_share\n#     from da.app_da_daily_estimate_1001 where ( date  between '2020-01-01' and '2020-01-01') and store_id=5 )\n#     as prod\n#  \\$proxy\\$) tbl \n#       (cnt bigint )  ;\n\n# EOF\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# select app_id from plproxy.execute_select_nestloop(\\$proxy\\$ \n# select app_id from (\n# select distinct app_id,store_id ,device_id, date, est_organic_download_share\n#     from da.app_da_daily_estimate_1001 where ( date  between '2020-01-01' and '2020-01-01') and store_id=5 )\n#     as prod\n#  \\$proxy\\$) tbl \n#       (app_id bigint ) order by app_id  ;\n\n# EOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200620-055521_180391107","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\n\n-- est_free_app_download + est_paid_app_download = organic_download + paid_download\n\n--select * from store_est_fact_v6 where date = '2020-03-05' and country_code='AU' and device_code='ios-phone' and est_paid_download is not null order by est_paid_download desc limit 5\n-- select * from store_est_category_fact_v7 where date = '2020-01-01' and country_code='US'  and device_code='android-all' and app_id = 20600006409047;\n\n-- select (sum(est_free_app_download) + sum(est_paid_app_download)) as sum_free_paid, (sum(est_paid_download) + sum(est_organic_download)) as sum_orgainc_paid from store_est_fact_v6 where date between '2019-08-29' and '2020-05-30' limit 5 ;\n\n\n-- select * from store_est_category_fact_v7 where app_id=446366839 and country_code='AU'  and device_code='ios-phone' and date between '2017-06-05' and '2017-06-05' limit 5 ;\n\n\nselect (sum(est_free_app_download) + sum(est_paid_app_download)) as sum_total, (sum(est_paid_download) + sum(est_organic_download)) as sum_orgainc_paid  from store_est_fact_v1 where  country_code='US' and est_paid_download !=0  and device_code='ios-phone' and date between '2020-05-10' and '2020-05-30' limit 3 ;\nselect  (sum(est_free_app_download) + sum(est_paid_app_download)) as sum_total, (sum(est_paid_download) + sum(est_organic_download)) as sum_orgainc_paid from store_est_category_fact_v1 where  country_code='US' and est_paid_download !=0  and device_code='ios-phone' and date between '2020-05-10' and '2020-05-30' limit 3 ;\n\nselect * from store_est_fact_v1 where date between '2020-05-15' and '2020-05-15' and country_code='US' and device_code='ios-phone' limit 5 \n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200615-073843_365571860","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\n\n-- select * from store_est_t_w_fact_v6 where device_code='ios-tablet' and country_code='GB' and date ='2019-06-15' and est_paid_download!=0 limit 3;\n-- select * from store_est_t_m_fact_v6 where device_code='android-all' and country_code='JP' and date ='2019-07-31'  and est_paid_download!=0  limit 3;\n--select * from store_est_t_y_fact_v6 where device_code='ios-phone' and country_code='WW' and date ='2019-12-31' and est_paid_download!=0 limit 3;\n--select * from store_est_t_y_fact_v6 where device_code='android-all' and country_code='WW' and date ='2019-12-31' and est_paid_download!=0 limit 3;\n\n-- select * from store_est_category_t_q_fact_v7 limit 3;\n-- select * from store_est_category_t_y_fact_v7 limit 3;\n\n--select count(distinct date) from store_est_category_t_w_fact_v7 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30'  ;\n--select distinct date from store_est_t_m_fact_v6 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30' order by date \n-- select distinct date from store_est_category_t_m_fact_v7 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30' order by date;\n\n\n-- select * from store_est_category_fact_v7 where date = '2020-01-01' and country_code='US'  and device_code='android-all' and app_id = 20600006409047;\n\n-- select (sum(est_free_app_download) + sum(est_paid_app_download)) as sum_free_paid, (sum(est_paid_download) + sum(est_organic_download)) as sum_orgainc_paid from store_est_fact_v6 where date between '2019-08-29' and '2020-05-30' limit 5 ;\n\n\n-- select * from store_est_category_fact_v7 where app_id=446366839 and country_code='AU'  and device_code='ios-phone' and date between '2017-06-05' and '2017-06-05' limit 5 ;\n\n\n\n-- select sum(est_organic_download), sum(est_paid_download) from store_est_fact_v6 where date between '2019-10-01' and '2019-10-31' and device_code='ios-phone' and country_code='US' and app_id=1225683141 limit 3;\n-- select * from store_est_t_m_fact_v6 where device_code='ios-phone' and country_code='US' and date ='2019-10-31' and app_id=1225683141 limit 3;\n--select sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_fact_v6 where date between '2019-10-01' and '2019-12-31' and --device_code='ios-phone' and country_code='US' and app_id=1225683141 limit 3;\n--select * from store_est_t_q_fact_v6 where device_code='ios-phone' and country_code='US' and date ='2019-12-31' and app_id=1225683141 limit 3;\n\n\n\n\n--select distinct date from store_est_t_q_fact_v6 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30' order by date;\n--select distinct date from store_est_t_y_fact_v6 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30' order by date\n\n\n\n--select sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_fact_v6 where date between '2019-10-01' and '2019-10-31' and device_code='ios-phone' and country_code='US' and app_id=1225683141 limit 3;\n-- select sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_fact_v6 where date between '2019-11-01' and '2019-11-30' and device_code='ios-phone' and country_code='US' and app_id=1225683141 limit 3;\n\n\n\nselect sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_fact_v6 where date between '2015-10-01' and '2015-12-31' and device_code='ios-phone' and country_code='US' limit 3;\n\n\nselect sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_t_q_fact_v6 where date between '2015-12-31' and '2015-12-31' and device_code='ios-phone' and country_code='US'  limit 3;\n\n\n# select date, sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_t_w_fact_v6 where date between '2019-10-01' and '2019-12-31' and device_code='ios-phone' and country_code='US' and app_id=1225683141 group by date limit 3;\n\n\n# select sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_fact_v6 where date between '2019-12-22' and '2019-12-28' and device_code='ios-phone' and country_code='US' and app_id=1225683141 limit 3;\n\n\n# select date, sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_t_w_fact_v6 where date between '2019-12-22' and '2019-12-28' and device_code='ios-phone' and country_code='US' and app_id=1225683141 group by date limit 3;\n\n\n\n# select date, sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_t_m_fact_v6 where date between '2019-10-01' and '2019-12-31' and device_code='ios-phone' and country_code='US' and app_id=1225683141 group by date limit 3;\n\n\n# select sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_fact_v6 where date between '2019-10-01' and '2019-12-31' and device_code='ios-phone' and country_code='US' and app_id=1225683141 limit 3;\n\n\n\n\n\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200615-073726_1258042095","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\n\n-- est_free_app_download + est_paid_app_download = organic_download + paid_download\n-- select * from store_est_fact_v6 where date = '2016-08-26' limit 5 ;\n-- select (sum(est_free_app_download) + sum(est_paid_app_download)) as sum_free_paid, (sum(est_paid_download) + sum(est_organic_download)) as sum_orgainc_paid from store_est_fact_v6 where date between '2019-08-29' and '2020-05-30' limit 5 ;\n\n\n-- paid_download is Zero\n-- select sum(est_paid_download) from store_est_category_fact_v7 where date < '2016-08-26' ;\n-- select sum(est_paid_download) from store_est_fact_v6 where date < '2016-08-26' ;\n\n-- three situations of download attribution with null value\nselect * from store_est_fact_v6 where date between '2016-08-29' and '2020-05-30' and est_paid_download is not null and est_paid_app_download is null limit 2 ;\nselect * from store_est_fact_v6 where date between '2016-08-29' and '2020-05-30' and est_paid_download is not null and est_paid_app_download is null and est_paid_download !=0  limit 2 ;\nselect * from store_est_fact_v6 where date between '2016-08-29' and '2020-05-30' and est_paid_download is null and est_paid_app_download is not null limit 2 ;\nselect * from store_est_fact_v6 where date between '2016-08-29' and '2020-05-30' and est_paid_download is null and est_paid_app_download is null limit 2 ;\n\n\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200615-082233_1985694463","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2020-03-29\"\nend = \"2020-05-02\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append((str(real_date1 + datetime.timedelta(days)), temp))\nprint sar_list\nfor d in sar_list:\n    daily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(\"granularity='daily' and date in ('%s')\" % (\"','\".join(sar_list[0][1])))\n    daily_download_attr.createOrReplaceTempView(\"daily_pre_load_data\")\n    \n    daily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='weekly' and date in ('%s')\" % (sar_list[0][0]))\n    daily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    \n\n    spark.sql(\"select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified except all select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code\").show()\n\n    spark.sql(\"select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code except all select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200616-104433_1540302894","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2020-02-01\"\nend = \"2020-04-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\n# sar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(str(real_date1 + datetime.timedelta(days)))\n#     if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n#         temp=list()\n#         while dates:\n#             temp.append(str(dates.pop()))\n#         sar_list.append((str(real_date1 + datetime.timedelta(days)), temp))\n# print sar_list\n\nprint dates\nprint dates[-1]\n# sql_where = \"granularity='daily' and date in ('%s')\" % (\"','\".join(dates))\nsql_where = \"granularity='daily' and date between '2019-03-01' and '2019-03-31'\"\n\nprint sql_where\ndaily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(sql_where)\ndaily_download_attr.createOrReplaceTempView(\"daily_pre_load_data\")\n\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='monthly' and date='2019-03-31' \")\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    \nspark.sql(\"select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified except all select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code\").show()\n\nspark.sql(\"select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code except all select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified\").show()\n\n\nspark.sql(\"select * from download_attribution_weekly_unified\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200616-123252_1783801256","metadata":{},"outputs":[],"source":["\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='monthly'\" )\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n\ntest_date_list = spark.sql(\"select distinct date from download_attribution_weekly_unified\").collect()\n\nl = [str(d[0]) for d in test_date_list]\nl.sort(reverse=False)\nfor x in l:\n    print x"]},{"cell_type":"code","execution_count":0,"id":"20200616-105248_2003144946","metadata":{},"outputs":[],"source":["%%sh\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/granularity=daily/date=2020-05"]},{"cell_type":"code","execution_count":0,"id":"20200620-080143_319088285","metadata":{},"outputs":[],"source":["\n\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(\"granularity='daily' and date='2017-06-05'\" )\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_daily_unified\")\n\nspark.sql(\"select * from download_attribution_daily_unified\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200615-094622_1323478314","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2016-08-28\"\nend = \"2020-05-31\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append((str(real_date1 + datetime.timedelta(days))))\n\nprint sar_list"]},{"cell_type":"code","execution_count":0,"id":"20200615-094755_219888910","metadata":{},"outputs":[],"source":["\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='weekly'\" )\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n\ntest_date_list = spark.sql(\"select distinct date from download_attribution_weekly_unified\").collect()\n\nl = [str(d[0]) for d in test_date_list]\nl.sort(reverse=False)\nprint l"]},{"cell_type":"code","execution_count":0,"id":"20200615-102159_611753815","metadata":{},"outputs":[],"source":["\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='weekly'\" )\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n\nspark.sql(\"select * from download_attribution_weekly_unified where date = '2020-04-09' and granularity='weekly'  \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200616-071930_1435914486","metadata":{},"outputs":[],"source":["\nimport datetime\nimport calendar\n\nstart = \"2010-07-04\"\nend = \"2010-08-04\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\n\nmonth_day=list()\nfor days in xrange(date_range.days):\n    month_day.append(real_date1 + datetime.timedelta(days))\n\n\ntest_list= sorted(list(set([ d.strftime(\"%Y-%m-%d\")[:7] for d in month_day ])))\n\nprint test_list\n\n# for m in test_list:\n#     print calendar.monthrange(int(m[:4]),int(m[5:7]))\n#     print \"-\".join([str(m[:4]), str(calendar.monthrange(int(m[:4]),int(m[-2:]))[0]), str(calendar.monthrange(int(m[:4]),int(m[-2:]))[1])])\n\n\ndef last_day_of_month(date):\n    next_month = date.replace(day=28) + datetime.timedelta(days=4)  # this will never fail\n    return next_month - datetime.timedelta(days=next_month.day)\nprint last_day_of_month(start)"]},{"cell_type":"code","execution_count":0,"id":"20200615-103008_482987018","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2020-01-05\"\nend = \"2020-02-02\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append((str(real_date1 + datetime.timedelta(days)), temp))\nprint sar_list\nfor d in sar_list:\n    daily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(\"granularity='daily' and date in ('%s')\" % (\"','\".join(sar_list[0][1])))\n    daily_download_attr.createOrReplaceTempView(\"daily_pre_load_data\")\n    \n    daily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='weekly' and date in ('%s')\" % (sar_list[0][0]))\n    daily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    \n\n    spark.sql(\"select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified except all select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code\").show()\n\n    spark.sql(\"select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code except all select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified\").show()\n\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}