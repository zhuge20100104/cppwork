{"cells":[{"cell_type":"code","execution_count":0,"id":"20201116-054929_1066817648","metadata":{},"outputs":[],"source":["\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2020, 1, 01)\nend_date = datetime(2020, 3, 31)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\nDATE_GRANULARITY_MAPPINGLIST[\"monthly\"].reverse()\nDATE_GRANULARITY_MAPPINGLIST[\"weekly\"].reverse()\nDATE_GRANULARITY_MAPPINGLIST[\"daily\"].reverse()\n\nprint DATE_GRANULARITY_MAPPINGLIST[\"monthly\"]\nprint DATE_GRANULARITY_MAPPINGLIST[\"weekly\"]\nprint DATE_GRANULARITY_MAPPINGLIST[\"daily\"]"]},{"cell_type":"code","execution_count":0,"id":"20201116-055006_2058301624","metadata":{},"outputs":[],"source":["\ngranularity_list = [\"monthly\", \"weekly\", \"daily\"]\ncategory_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.category.v6/dimension/product_type_code=app/\"\nusage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=app/\"\nfor granularity in granularity_list:\n    filter_str = \"date between '2020-01-01' and '2020-03-31' and granularity_code = '{granularity}'\".format(granularity=granularity)\n    print filter_str\n    spark.read.format(\"delta\").load(category_path).filter(filter_str).createOrReplaceTempView(\"category_view\")\n    spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str).createOrReplaceTempView(\"basic_view\")\n    spark.sql(\"\"\"\n    select basic.DATE_KEY,\n        basic.DEVICE_CODE,\n        basic.COUNTRY_CODE,basic.COUNTRY_KEY,\n        basic.date,\n        basic.DEVICE_KEY,basic.GRANULARITY_CODE,basic.GRANULARITY_KEY,basic.MARKET_CODE,\n        basic.MARKET_KEY,basic.PRODUCT_KEY,basic.PRODUCT_TYPE_CODE,basic.PRODUCT_TYPE_KEY,\n        basic.EST_TOTAL_TIME_MILLISECONDS_OF_MAIN_CATEGORY,basic.EST_WIFI_BYTES,basic.EST_ACTIVE_USERS,\n        basic.EST_TOTAL_BYTES,basic.EST_TOTAL_SESSION_COUNT_OF_MAIN_CATEGORY,\n        basic.EST_TOTAL_BYTES_OF_MAIN_CATEGORY,basic.EST_TOTAL_ACTIVE_DAYS,basic.EST_INSTALL_BASE,\n        basic.EST_POPULATION,basic.EST_TOTAL_TIME_MILLISECONDS,basic.EST_TOTAL_SESSION_COUNT,\n        basic.DEVICE_FORM_FACTOR_CODE,basic.DEVICE_FORM_FACTOR_KEY,basic.PARENT_DEVICE_CODE,basic.PARENT_DEVICE_KEY,basic.PLATFORM_CODE,basic.PLATFORM_KEY,\n        basic.est_usage_penetration,\n        map.category_key as category_key,\n        map.unified_category_key as unified_category_key\n    from basic_view basic\n    join category_view map\n    on basic.date = map.date\n    and basic.product_key = map.product_key\n    and basic.granularity_code = map.granularity_code\n    and basic.country_code = map.country_code\n    and basic.device_code = map.device_code\n    \"\"\").createOrReplaceTempView(\"basic_with_category_{granularity}\".format(granularity=granularity))\n    # spark.sql(\"select count(1) from category_view\").show(1, False)\n    # spark.sql(\"select count(1) from basic_with_category_{granularity}\".format(granularity=granularity)).show(1, False)\n    \n"]},{"cell_type":"code","execution_count":0,"id":"20201116-055022_88416033","metadata":{},"outputs":[],"source":["\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom pyspark.sql import Row\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\nfrom pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType, ShortType\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\n# legacy_category_list = [36, 6014, 7012, 6004, 6008]\nunified_category_list = [800000, 800001, 800005, 800035, 800031]\nlegacy_category_list = [36]\ncategory_mapping = {\n    36: 800000,\n    6014:800001,\n    7012:800005,\n    6004:800035,\n    6008:800031\n}\n\naggr_sql = \"\"\"select  app_id, rank, kpi, estimate from plproxy.execute_select($proxy$\nSELECT app_id, rank, kpi, estimate\nFROM mu.category_monthly_2001_143441 where \ndate in ('2020-01-31', '2020-02-29','2020-03-31') \nand rank <= 1000 \nand category_id = {legacy_category_id} $proxy$)\n t (app_id bigint, rank integer, kpi smallint, estimate double precision) order by rank asc;\n\"\"\"\n\naggr_sql_aa = \"\"\"\nSELECT app_id\n    FROM plproxy.execute_select_nestloop($proxy$\n        SELECT app_id, MAX(estimate) AS estimate\n        FROM mu.category_weekly_2001_143441\n        WHERE date BETWEEN '2020-08-21' AND '2020-08-30' AND category_id = 6014 AND rank <= 1000\n        GROUP BY app_id\n        ORDER BY MAX(estimate) DESC\n        LIMIT 1000\n    $proxy$) t (app_id BIGINT, estimate FLOAT8)\n    GROUP BY app_id\n    ORDER BY MAX(estimate) DESC, app_id ASC\n    LIMIT 1000\n\"\"\"\n\nsingle_sql =  \"\"\"select  app_id, rank, kpi, estimate from plproxy.execute_select($proxy$\nSELECT app_id, rank, kpi, estimate\nFROM mu.category_daily_2001_143441\nwhere date = '2020-07-01'\nand rank <= 1000 \nand category_id = {legacy_category_id} $proxy$)\n t (app_id bigint, rank integer, kpi smallint, estimate double precision) order by rank asc;\n\"\"\"\n\ndomain_single_dql = \"\"\"\nselect  domain_id, rank_est_usage_penetration, est_usage_penetration from plproxy.execute_select($proxy$\nSELECT domain_id, rank_est_usage_penetration, est_usage_penetration\nFROM mw.category_m_ip_us where \ndate = '2020-09-30' \nand rank_est_usage_penetration <= 1000 \nand category_id = {unified_category_id} $proxy$)\n t (domain_id bigint, rank_est_usage_penetration integer, est_usage_penetration double precision) order by rank_est_usage_penetration asc;\n\"\"\" \n\n\n\ndef get_plproxy_result(sql_str):\n    plproxy_result = []\n    result = query(PLPROXY_DSN, sql_str)\n    # print result\n    # distinct_domain_id = result[1][0]\n    # for _r in result:\n    #     plproxy_result.append(_r[0])\n        \n    df_data = [Row(app_id=r[0], rank=r[1], kpi=r[2], estimate=r[3]) for r in result]\n    # # print df_data[1]\n    _schema =StructType([StructField(\"app_id\", LongType(), False), \n    StructField(\"rank\", IntegerType(), False),\n    StructField(\"kpi\", ShortType(), False),\n    StructField(\"estimate\", DoubleType(), False)])\n    df_plproxy = spark.createDataFrame(data=df_data, schema=_schema)\n    # df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n    # spark.sql(\"select * from plproxy_df_new\").show(10000, False)\n    return df_plproxy\n    # return df_plproxy\n    \ndef get_unified_data():\n    domain_unified_source_path = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity=w/month=202005/date=2020-05-16\"\n    \n    spark.read.format(\"delta\").load(unified_source_path).createOrReplaceTempView(\"test_unified\")\n    spark.sql(\"select distinct domain_id from test_unified where  est_average_active_users <> 0 and est_average_active_users is not null order by domain_id asc\").createOrReplaceTempView(\"unified_df_new\")\n\ndef get_plproxy_data():\n     df_plproxy=get_plproxy_result()\n     df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n     spark.sql(\"select count(distinct app_id) from plproxy_df_new\").show(10, False)\n    #  spark.sql(\"select * from plproxy_df_new order by rank asc\").show(10000, False)\n     \n     spark.sql(\"select app_id, avg(estimate) as estimate from plproxy_df_new group by app_id order by estimate desc\").show(10000, False)\n\n\n\ndef compare_single():\n    \n    for category in legacy_category_list:\n        df_plproxy=get_plproxy_result(single_sql.format(legacy_category_id=category))\n        df_plproxy.createOrReplaceTempView(\"plproxy_df\")\n        spark.sql(\"\"\"\n            select product_key as app_id \n            from basic_with_category_daily \n            where date ='2020-07-01'\n            and device_code = 'ios-phone' \n            and country_code='US' \n            and unified_category_key={unified_category} \n            order by est_usage_penetration desc limit 1000\n            \"\"\"\n            .format(unified_category=category_mapping[category])).createOrReplaceTempView(\"apps_new\")\n        spark.sql(\"\"\"\n            select app_id \n            from plproxy_df \n            except \n            select app_id \n            from apps_new\"\"\").createOrReplaceTempView(\"plproxy_diff_new\")\n        spark.sql(\"\"\"\n            select app_id \n            from apps_new \n            except \n            select app_id \n            from plproxy_df\"\"\").createOrReplaceTempView(\"new_diff_plproxy\")\n        \n        print \"category is {category}\".format(category=category)\n        # spark.sql(\"select app_id from plproxy_df\").show(1000, False)\n        # spark.sql(\"select app_id from apps_new\").show(1000, False)\n        \n        spark.sql(\"\"\"\n            select count(1) as plproxy_diff_new \n            from plproxy_diff_new\"\"\").show(10, False)\n        spark.sql(\"\"\"\n            select count(1) as new_diff_plproxy \n            from new_diff_plproxy\"\"\").show(10, False)\n        # spark.sql(\"\"\"\n        #     select app_id as plproxy_diff_new \n        #     from plproxy_diff_new\"\"\").show(10, False)\n        # spark.sql(\"\"\"\n        #     select app_id as new_diff_plproxy \n        #     from new_diff_plproxy\"\"\").show(10, False)\n\n\ndef compare_aggr():\n    # basic_with_category_{granularity}; .format(date='2020-08-31', legacy_category_id=36)\n    \n    for category in legacy_category_list:\n        df_plproxy=get_plproxy_result(aggr_sql.format(legacy_category_id=category))\n        df_plproxy.createOrReplaceTempView(\"plproxy_df\")\n        spark.sql(\"\"\"\n            select app_id,\n                max(estimate) as estimate \n            from plproxy_df \n            group by app_id \n            order by estimate desc \n            limit 1000\"\"\").createOrReplaceTempView(\"plproxy_aggr\")\n        \n        spark.sql(\"\"\"\n        \n            select product_key as app_id, \n                sum(est_active_users)/sum(est_population) as aggr_up \n            from basic_with_category_daily\n            where date between '2020-01-31' and '2020-03-31'  \n            and device_code = 'ios-phone'\n            and granularity_code = 'monthly'\n            and country_code = 'US'\n            and product_type_code='app'\n            and unified_category_key={unified_category} \n            group by app_id \n            order by aggr_up desc \n            limit 1000\"\"\"\n            \n            .format(unified_category=category_mapping[category])).createOrReplaceTempView(\"apps_new\")\n        spark.sql(\"select app_id from plproxy_aggr except select app_id from apps_new\").createOrReplaceTempView(\"plproxy_diff_new\")\n        spark.sql(\"select app_id from apps_new except select app_id from plproxy_aggr\").createOrReplaceTempView(\"new_diff_plproxy\")\n        print \"category is {category}\".format(category=category)\n        spark.sql(\"select app_id,estimate from plproxy_aggr\").show(1000, False)\n        # spark.sql(\"select app_id from apps_new\").show(1000, False)\n        #spark.sql(\"select count(1) as plproxy_diff_new from plproxy_diff_new\").show(10, False)\n        #spark.sql(\"select count(1) as new_diff_plproxy from new_diff_plproxy\").show(10, False)\n        # # spark.sql(\"select app_id from plproxy_df order by app_id limit 10\").show(10, False)\n        # spark.sql(\"select app_id from apps_new order by app_id limit 10\").show(10, False)\n        \n        # spark.sql(\"select app_id as plproxy_diff_new from plproxy_diff_new\").show(10, False)\n        # spark.sql(\"select app_id as new_diff_plproxy from new_diff_plproxy\").show(10, False)\n        \n    \n# compare_single()   \ncompare_aggr()\n# unified_source_path = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity=w/date=2020-05-16\"\n# spark.read.format(\"delta\").load(unified_source_path).show(10)\n"]},{"cell_type":"code","execution_count":0,"id":"20201116-055058_1781920688","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}