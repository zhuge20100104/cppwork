{"cells":[{"cell_type":"code","execution_count":0,"id":"20191122-012310_224552904","metadata":{},"outputs":[],"source":["\nfrom aaintdatapipeline.core.conf import Conf\nfrom aaintdatapipeline.core.fs.device import S3Bucket, specified_bucket\nfrom aaintdatapipeline.core.fs.device import unified_bucket\nimport zlib\n\nbucket_name = \"prod_appannie_appletv\"\ns3 = S3Bucket(Conf(bucket_name = bucket_name))\nconf = Conf(\n        bucket_name=bucket_name,\n        bucket_class=S3Bucket\n)\npath = \"country-ranks/{_date}/23/{_country_code}\".format(_date='2019-09-16', _country_code=\"CN\")\nbucket = specified_bucket(conf)\nraw_data = zlib.decompress(bucket.get(\"country-ranks/2019-09-16/23/CN\"))\n\nfor _data in raw_data.splitlines():\n    print _data\n    break\n    if \"MUSIC_AND_AUDIO\" in _data:\n        _data_list = _data.split(\",\")\n        _rank_list = _data_list[4].split(\" \")\n        print _data_list[:3]\n        print len(_rank_list)\n        for _rank in _rank_list:\n            print _rank\n\n\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191122-012324_1882212220","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql import functions as F\nimport datetime\nimport pandas as pd\n\ndevice_code = \"tv-os-tv\"\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\nbegin_date = datetime.datetime.strptime(\"2012-12-26\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2019-11-25\", '%Y-%m-%d')\ndate_list = get_date_list(begin_date, end_date, \"D\")\nmetrics = [\"free_download\", \"new_paid_download\", \"revenue\", \"paid_download\", \"new_free_download\"]\n\n\nprint \"%table date\\tcount\"\n\n\nfor date_str in date_list:\n    count = 0\n    try:\n        tv_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-free/unified/app-tech.store.app-rank.v1/fact/date={}/device_code={}/\".format(date_str, device_code))\n    except Exception,e:\n        print \"\\t\".join([date_str, str(-1)])\n        continue\n\n    for metric in metrics:\n        metric_max_str = \"{}_max\".format(metric)\n        metric_count_str = \"{}_count\".format(metric)\n        df_unified = tv_df.groupBy([\"country_code\",\"category_id\"]).agg(F.max(metric).alias(metric_max_str),F.count(metric).alias(metric_count_str)).collect()\n\n\n\n\n    for rank in df_unified:\n        if rank[metric_count_str]!=0 and rank[metric_max_str]!=rank[metric_count_str]:\n            #print \",\".join([date_str, rank.country_code, str(rank.category_id), str(rank[metric_max_str]), str(rank[metric_count_str])])\n            count+=1\n    print \"\\t\".join([date_str, str(count)])\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-030813_1362923005","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql import functions as F\nimport datetime\nimport pandas as pd\n\ndevice_code = \"mac-os-mac\"\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\nbegin_date = datetime.datetime.strptime(\"2012-12-26\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2019-11-25\", '%Y-%m-%d')\ndate_list = get_date_list(begin_date, end_date, \"D\")\nmetrics = [\"free_download\", \"new_paid_download\", \"revenue\", \"paid_download\", \"new_free_download\"]\n\n\n# print \"%table date\\tcount\"\ndate_list = [\"2018-02-21\", \"2019-07-29\"]\n\n\nfor date_str in date_list:\n    count = 0\n    try:\n        tv_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-free/unified/app-tech.store.app-rank.v1/fact/date={}/device_code={}/\".format(date_str, device_code))\n    except Exception,e:\n        print \"\\t\".join([date_str, str(-1)])\n        continue\n\n    for metric in metrics:\n        metric_max_str = \"{}_max\".format(metric)\n        metric_count_str = \"{}_count\".format(metric)\n        df_unified = tv_df.groupBy([\"country_code\",\"category_id\"]).agg(F.max(metric).alias(metric_max_str),F.count(metric).alias(metric_count_str)).collect()\n\n        for rank in df_unified:\n            if rank[metric_count_str]!=0 and rank[metric_max_str]!=rank[metric_count_str]:\n                print \",\".join([date_str, rank.country_code, str(rank.category_id), str(rank[metric_max_str]), str(rank[metric_count_str])])\n                count+=1\n    print \"\\t\".join([date_str, str(count)])"]},{"cell_type":"code","execution_count":0,"id":"20191122-013523_533369276","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-free/unified/app-tech.store.app-rank.v1/fact/date=2019-01-24/\n"]},{"cell_type":"code","execution_count":0,"id":"20191122-033711_636594796","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql import functions as F\nimport datetime\nimport pandas as pd\n\ndevice_code = \"android-all\"\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\nbegin_date = datetime.datetime.strptime(\"2012-12-26\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2019-11-25\", '%Y-%m-%d')\ndate_list = get_date_list(begin_date, end_date, \"D\")\nmetrics = [\"free_download\", \"new_paid_download\", \"revenue\", \"paid_download\", \"new_free_download\"]\n\n\n# print \"%table date\\tcount\"\ndate_list = [\"2014-02-09\"]\n\n\nfor date_str in date_list:\n    count = 0\n    try:\n        tv_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-free/unified/app-tech.store.app-rank.v3/fact/date={}/device_code={}/\".format(date_str, device_code))\n    except Exception,e:\n        print e.msg\n        print \"\\t\".join([date_str, str(-1)])\n        continue\n\n    for metric in metrics:\n        metric_max_str = \"{}_max\".format(metric)\n        metric_count_str = \"{}_count\".format(metric)\n        df_unified = tv_df.groupBy([\"country_code\",\"category_id\"]).agg(F.max(metric).alias(metric_max_str),F.count(metric).alias(metric_count_str)).collect()\n\n        for rank in df_unified:\n            if rank[metric_count_str]!=0 and rank[metric_max_str]!=rank[metric_count_str]:\n                print \",\".join([date_str, rank.country_code, str(rank.category_id), metric,  str(rank[metric_max_str]), str(rank[metric_count_str])])\n                count+=1\n    print \"\\t\".join([date_str, str(count)])"]},{"cell_type":"code","execution_count":0,"id":"20191122-035957_1025915709","metadata":{},"outputs":[],"source":["\ndate_str = \"2014-02-09\"\ndevice_code = \"android-all\"\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-free/unified/app-tech.store.app-rank.v3/fact/date={}/device_code={}/\".format(date_str, device_code)).filter(\"country_code='US' and category_id=400046 and revenue is not null\").orderBy(\"revenue\").show(1000)"]},{"cell_type":"code","execution_count":0,"id":"20191128-092522_2030348715","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://prod_appannie_android/country-ranks/2014-02-09/23/"]},{"cell_type":"code","execution_count":0,"id":"20191128-093357_1371021725","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}