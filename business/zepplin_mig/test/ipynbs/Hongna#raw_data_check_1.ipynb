{"cells":[{"cell_type":"code","execution_count":0,"id":"20200928-084128_749963893","metadata":{},"outputs":[],"source":["\n# est_average_active_days * est_active_users as est_total_active_days,\n# est_average_bytes_per_session * est_active_users * est_average_session_count_per_user as est_total_bytes,\n# est_active_users * est_average_session_count_per_user / est_share_of_session_product_in_main_category AS est_total_session_count_of_main_category,\n# est_active_users * est_average_session_count_per_user * est_average_bytes_per_session / est_share_of_bytes_product_in_main_category AS est_total_bytes_of_main_category,\n# est_active_users * est_average_time_per_user_milliseconds / est_share_of_time_product_in_main_category AS est_total_time_milliseconds_of_main_category,\n# est_average_bytes_per_session * est_active_users * est_average_session_count_per_user * est_percentage_of_bytes_wifi_in_total * 1024 * 1024 * 1024 * 1024 AS est_wifi_bytes\n\nusage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=app/\"\nusage_basic_before_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\"\ngranularity_list = ['monthly']\ndef test_bytes():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_after = spark.sql(\"\"\"select count(1) from after_trans\n            where (est_total_bytes = 0 and est_average_bytes_per_session <> 0 and est_average_session_count_per_user <> 0)\n            or (est_total_session_count_of_main_category = 0 and est_share_of_session_product_in_main_category <> 0) \n            or (est_total_bytes_of_main_category = 0 and est_share_of_bytes_product_in_main_category <> 0)\n            or (est_total_time_milliseconds_of_main_category = 0 and est_share_of_time_product_in_main_category <> 0)\n            or (est_wifi_bytes = 0 and est_percentage_of_bytes_wifi_in_total <> 0 and est_average_session_count_per_user <> 0 and est_average_bytes_per_session <> 0)\"\"\").collect()\n            if df_after[0][0] != 0:\n                print granularity, date, \"count\", df_after[0][0]\n            else:\n                print granularity, date, \"PASS\"\n                \ndef test_bytes_tmp():\n    # monthly 2019-06-30 count 9646\n    # monthly 2019-05-31 count 9061\n    filter_str_after = \"date between '2019-01-31' and '2019-12-31' and granularity_code = 'monthly' and product_key = 20600000006618 and device_code = 'android-phone' and country_code = 'US'\"\n    spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n    # spark.sql(\"select count(1) from after_trans where est_total_bytes = 0 and est_average_bytes_per_session <> 0 and est_average_session_count_per_user <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_total_session_count_of_main_category = 0 and est_share_of_session_product_in_main_category <> 0\").show(10, False)\n    spark.sql(\"select date, est_total_bytes_of_main_category*1024*1024 as total_bytes_of_category, est_share_of_bytes_product_in_main_category/1024/1024 as share_of_bytes, est_average_bytes_per_session, est_average_session_count_per_user from after_trans order by date asc\").show(100, False)\n    # spark.sql(\"select count(1) from after_trans where est_total_time_milliseconds_of_main_category = 0 and est_share_of_time_product_in_main_category <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_wifi_bytes = 0 and est_percentage_of_bytes_wifi_in_total <> 0 and est_average_session_count_per_user <> 0 and est_average_bytes_per_session <> 0\").show(10, False)\n    \n    \ndef test_other_metrics():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            s = \"\"\"select count(1) from after_trans\n            where (est_install_penetration = 0  and est_open_rate <> 0)\n            or (est_install_base = 0 and est_open_rate <> 0)\n            or (est_population = 0 and est_usage_penetration <> 0)\n            or (est_total_time_milliseconds = 0  and est_average_session_count_per_user <> 0)\"\"\"\n            df_after = spark.sql(s).collect()\n            if df_after[0][0] != 0:\n                print granularity, date, \"count\", df_after[0][0]\n            else:\n                print granularity, date, \"PASS\"\n\ndef test_other_metrics_tmp():\n    filter_str_after = \"date = '2019-08-31' and granularity_code = 'monthly'\"\n    filter_str = \"date ='2019-08-31' and granularity = 'monthly'\"\n    spark.read.format(\"delta\").load(usage_basic_before_transform).filter(filter_str).createOrReplaceTempView(\"before_trans\")\n    spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n    # spark.sql(\"select count(1) from after_trans where est_install_penetration = 0  and est_open_rate <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_install_base = 0 and est_open_rate <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_population = 0 and est_usage_penetration <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_total_time_milliseconds = 0  and est_average_session_count_per_user <> 0\").show(10, False)\n    spark.sql(\"select count(1) from before_trans where est_total_time = 0 and est_average_session_per_user <> 0\").show(10, False)\n    spark.sql(\"select app_id, date, granularity, country_code, est_total_time, est_average_session_per_user, est_average_time_per_user, est_average_session_duration from before_trans where est_total_time = 0 and est_average_session_per_user <> 0 order by app_id desc limit 10\").show(10, False)\n    # spark.sql(\"select count(1) from before_trans\").show(1, False)\n\ntest_bytes_tmp()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20201119-083116_466210984","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=app/granularity_code=monthly/"]},{"cell_type":"code","execution_count":0,"id":"20201013-022720_967961530","metadata":{},"outputs":[],"source":["\ndomain_x_domain_source_path = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.cross-domain.v4/fact/granularity=m/\"\n# raw_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_CROSS_DOMAIN_METRICS/version=v1.0.0/granularity=MONTH/date={date}/platform=2/\"\n# raw_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_CROSS_DOMAIN_METRICS/version=v1.0.0/granularity=MONTH/\"\n\nfor date in DATE_GRANULARITY_MAPPINGLIST[\"monthly\"]:\n    raw_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_CROSS_DOMAIN_METRICS/version=v1.0.0/granularity=MONTH/date={date}/\".format(date=date)\n\n    spark.read.format(\"parquet\").load(raw_path).createOrReplaceTempView(\"test_raw\")\n    # spark.sql(\"select * from test_raw limit 1\").show()\n    df = spark.sql(\"select count (1) as count from test_raw\")\n    print df.collect(), df.collectAsList(), df.first()\n    \n# spark.sql(\"select distinct product_name from domain_view\").show(100, False)\n\n# spark.sql(\"select distinct domain_id from test where country_code='us' and device_code = 'ip' order by domain_id desc\").show(1000, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20201009-101528_803785013","metadata":{},"outputs":[],"source":["\nunified_retention_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.retention-days.v6/fact/granularity=monthly/\"\nraw_retention_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_DOMAIN_RETENTION_METRICS/version=v1.0.0/granularity=MONTH/\"\nspark.read.format(\"parquet\").load(raw_retention_path).createOrReplaceTempView(\"test\")\nspark.sql(\"select * from test limit 1\").show()\n# spark.sql(\"select date, count(*), sum(estimate) from test where product_type_name = 'domain_only' group by date order by date desc\").show(100, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20201013-022910_801566244","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY, PG_AA_HOSTS, PG_AA_NAME,PG_AA_ACCESS_ID,PG_AA_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\nPG_AA_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2013, 1, 1)\nend_date = datetime(2014, 12, 31)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\n# DATE_GRANULARITY_MAPPINGLIST[\"monthly\"].reverse()\n# DATE_GRANULARITY_MAPPINGLIST[\"weekly\"].reverse()\n# DATE_GRANULARITY_MAPPINGLIST[\"daily\"].reverse()"]},{"cell_type":"code","execution_count":0,"id":"20201016-025104_1979356083","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path='usage';\n--select date, count(*) from usage_basic_kpi_fact_v7 where date >= '2020-06-28' and granularity = 'daily' and country_code = 'MO' and device_code = 'android-phone' group by date order by date;\n--SELECT COUNT(country_code) FROM (SELECT DISTINCT country_code FROM usage_basic_kpi_fact_v7 WHERE date = '2020-07-19' AND granularity = 'daily' AND device_code IN ('android-phone', 'android-tablet')) country_tbl;\n--select date, granularity, device_code, est_installs, est_install_base, est_usage_penetration / est_install_penetration, est_open_rate from usage_basic_kpi_fact_v6 where date >= '2020-09-05' limit 10;\n--select date, granularity, device_code, count(*) from usage_basic_kpi_fact_v6 where date >='2020-08-29' and device_code like 'android%' group by date, granularity, device_code order by granularity, date, device_code;\n--select est_average_active_users, country_code, est_population from usage_basic_kpi_fact_v1 where date = '2020-09-01' and device_code like 'ios%' and granularity = 'daily' and country_code = 'CN' limit 10;\n--select * from usage_basic_kpi_fact_v1 where app_id = 711923939 and date in ('2019-06-01', '2019-06-08', '2019-06-15') and device_code like 'ios%' and granularity = 'daily' and country_code = 'CN' limit 10;\n\\x\nselect  sum(est_average_active_users) as sum_AU, sum(est_population) as sum_POP from usage_basic_kpi_fact_v1 where app_id = 20600004251992 and date between '2019-02-24' and '2019-06-29' and device_code = 'android-phone' and granularity = 'weekly' and country_code in ('CA', 'SG', 'ZA') group by app_id;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20201028-070345_1232588308","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom pyspark.sql import Row\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\nfrom pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType, ShortType, StringType, DateType\n\n\ndomain_basic = \"\"\"select distinct domain_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct domain_id\n    from mw.domain_basic_m\n    where \n        date ='2020-09-30'\n        and est_average_active_users <> 0\n        and est_average_active_users is not null\n$proxy$) tpl (domain_id bigint) order by domain_id desc;\n\"\"\"\n\ndomain_retention = \"\"\"\nselect distinct domain_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct domain_id\n    from mw.domain_retention_m\n    where \n       date ='2020-09-30'\n        and retention_days = 0\n        and estimate <> 0\n        and estimate is not null\n$proxy$) tpl (domain_id bigint) ;\n\"\"\"\n\napp_x_domain =\"\"\"\nselect distinct cross_domain_id as domain_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct cross_domain_id\n    from mw.app_x_domain_m\n    where \n        date ='2020-09-30'\n        and est_cross_product_usage_penetration <> 0\n        and est_cross_product_usage_penetration is not null\n$proxy$) tpl (cross_domain_id bigint) ;\n\"\"\"\n\ndomain_x_app =\"\"\"\nselect distinct domain_id as domain_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct domain_id\n    from mw.domain_x_app_m\n    where \n        date ='2020-09-30'\n        and est_cross_product_usage_penetration <> 0\n        and est_cross_product_usage_penetration is not null\n$proxy$) tpl (domain_id bigint) ;\n\"\"\"\n\ndomain_x_domain =\"\"\"\nselect distinct domain_id as domain_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct domain_id\n    from mw.domain_x_domain_m\n    where \n        date ='2020-09-30'\n        and est_cross_product_usage_penetration <> 0\n        and est_cross_product_usage_penetration is not null\n$proxy$) tpl (domain_id bigint) ;\n\"\"\"\n\ndomain_meta_sql = \"\"\"\nselect domain_id, name, is_disabled, status, sensitive_status, first_supported_date from aa_domain_metadata where first_supported_date < '2020-10-01' and is_disabled = 'f' and sensitive_status = 0;\n\"\"\"\n\ndef get_domain_meta():\n    result = query(PG_AA_DSN, domain_meta_sql)\n    df_data = [Row(domain_id=r[0][0], name=r[0][1], is_disabled=r[0][2], status=r[0][3], sensitive_status=r[0][4], first_supported_date=r[0][5]) for r in result]\n    _schema =StructType([StructField(\"domain_id\", LongType(), False),StructField(\"is_disabled\", StringType(), True), \n    StructField(\"status\", ShortType(), False),\n    StructField(\"sensitive_status\", ShortType(), False),\n    StructField(\"first_supported_date\", DateType(), False)])\n    domain_meta = spark.createDataFrame(data=df_data, schema=_schema)\n    return domain_meta\n    \ndef get_plproxy_result(sql_str):\n    result = query(PLPROXY_DSN, sql_str)\n    df_data = [Row(domain_id=r[0]) for r in result]\n    _schema =StructType([StructField(\"domain_id\", LongType(), False)])\n    df_plproxy = spark.createDataFrame(data=df_data, schema=_schema)\n    # df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n    # spark.sql(\"select * from plproxy_df_new\").show(10000, False)\n    return df_plproxy\n    \ndef get_dim():\n    dim_path = \"s3://b2c-prod-data-pipeline-unified-store-free/unified/store.product.v2/dimension/\"\n    spark.read.format(\"delta\").load(dim_path).createOrReplaceTempView(\"test_dim\")\n\ndef get_raw_data_domain_basic():\n    raw_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_DOMAIN_BASIC_METRICS/version=v1.0.0/granularity=MONTH/date=2020-09-30/\"\n    # spark.read.format(\"delta\").load(dim_path).printSchema()\n    \n    # spark.read.format(\"delta\").load(unified_source_path).createOrReplaceTempView(\"test_unified\")\n    spark.read.format(\"parquet\").load(raw_path).createOrReplaceTempView(\"test_raw\")\n    spark.sql(\"select distinct domain from test_raw  where metric_name = 'AU' and value is not null and value <> 0 \").createOrReplaceTempView(\"raw_domain\")\n    spark.sql(\"select distinct dim.product_key from raw_domain raw left join test_dim dim on raw.domain = dim.product_name order by dim.product_key desc\").createOrReplaceTempView(\"raw_domain_id\")\n    \ndef get_raw_date_domain_retention():\n    raw_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_DOMAIN_RETENTION_METRICS/version=v1.0.0/granularity=MONTH/date=2020-09-30/\"\n    spark.read.format(\"parquet\").load(raw_path).createOrReplaceTempView(\"test_raw\")\n    spark.sql(\"select distinct domain from test_raw  where metric_name = 'D0' and value is not null and value <> 0 \").createOrReplaceTempView(\"raw_domain\")\n    spark.sql(\"select distinct dim.product_key from raw_domain raw left join test_dim dim on raw.domain = dim.product_name order by dim.product_key desc\").createOrReplaceTempView(\"raw_domain_id\")\n    \ndef get_raw_domain_unified_attribution():\n    raw_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_DOMAIN_RETENTION_METRICS/version=v1.0.0/granularity=MONTH/date=2020-09-30/\"\n    spark.read.format(\"parquet\").load(raw_path).createOrReplaceTempView(\"test_raw\")\n    spark.sql(\"select distinct domain from test_raw  where metric_name = 'D0' and value is not null and value <> 0 \").createOrReplaceTempView(\"raw_domain\")\n    spark.sql(\"select distinct dim.product_key from raw_domain raw left join test_dim dim on raw.domain = dim.product_name order by dim.product_key desc\").createOrReplaceTempView(\"raw_domain_id\")\n    \ndef get_raw_app_x_domain():\n    raw_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_APP_CROSS_DOMAIN_METRICS/version=v1.0.0/granularity=MONTH/date=2020-09-30/\"\n    spark.read.format(\"parquet\").load(raw_path).createOrReplaceTempView(\"test_raw\")\n    spark.sql(\"select distinct cross_domain as domain from test_raw  where metric_name = 'UP' and value is not null and value <> 0 \").createOrReplaceTempView(\"raw_domain\")\n    spark.sql(\"select distinct dim.product_key from raw_domain raw left join test_dim dim on raw.domain = dim.product_name order by dim.product_key desc\").createOrReplaceTempView(\"raw_domain_id\")\n    \ndef get_raw_domain_x_app():\n    raw_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_CROSS_APP_METRICS/version=v1.0.0/granularity=MONTH/date=2020-09-30/\"\n    spark.read.format(\"parquet\").load(raw_path).createOrReplaceTempView(\"test_raw\")\n    spark.sql(\"select distinct domain as domain from test_raw  where metric_name = 'UP' and value is not null and value <> 0 \").createOrReplaceTempView(\"raw_domain\")\n    spark.sql(\"select distinct dim.product_key from raw_domain raw left join test_dim dim on raw.domain = dim.product_name order by dim.product_key desc\").createOrReplaceTempView(\"raw_domain_id\")\n    \ndef get_raw_domain_x_domain():\n    raw_path = \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_CROSS_DOMAIN_METRICS/version=v1.0.0/granularity=MONTH/date=2020-09-30/\"\n    spark.read.format(\"parquet\").load(raw_path).createOrReplaceTempView(\"test_raw\")\n    spark.sql(\"select distinct domain as domain from test_raw  where metric_name = 'UP' and value is not null and value <> 0 \").createOrReplaceTempView(\"raw_domain\")\n    spark.sql(\"select distinct dim.product_key from raw_domain raw left join test_dim dim on raw.domain = dim.product_name order by dim.product_key desc\").createOrReplaceTempView(\"raw_domain_id\")\n\ndef get_plproxy_data():\n     df_plproxy=get_plproxy_result()\n     df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n     spark.sql(\"select count(distinct app_id) from plproxy_df_new\").show(10, False)\n    #  spark.sql(\"select * from plproxy_df_new order by rank asc\").show(10000, False)\n     \n     spark.sql(\"select app_id, avg(estimate) as estimate from plproxy_df_new group by app_id order by estimate desc\").show(10000, False)\n\ndef compare():\n    get_dim()\n    print \"step 0\"\n    domain_meta = get_domain_meta()\n    print \"stepp 1\"\n    df_plproxy=get_plproxy_result(domain_x_domain)\n    print \"stepp 2\"\n    get_raw_domain_x_domain()\n    df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n    print \"stepp 3\"\n    domain_meta.createOrReplaceTempView(\"domain_meta_df\")\n    print \"step 4\"\n    spark.sql(\"select product_key from raw_domain_id raw join domain_meta_df meta on raw.product_key = meta.domain_id\").createOrReplaceTempView(\"domain_after_filter\")\n    print \"step 5\"\n    spark.sql(\"select distinct product_key as domain_id from domain_after_filter except select distinct domain_id from plproxy_df_new \").createOrReplaceTempView(\"domain_diff\")\n    print \"step 6\"\n    # spark.sql(\"select distinct domain_id from plproxy_df_new except select distinct product_key as domain_id from raw_domain_id   \").createOrReplaceTempView(\"domain_diff\")\n    spark.sql(\"select count(1) from domain_diff\").show(10, False)\n    print \"step 7\"\n    spark.sql(\"select * from domain_diff order by domain_id desc\").show(100, False)\n    \n    \ncompare()\n# unified_source_path = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity=w/date=2020-05-16\"\n# spark.read.format(\"delta\").load(unified_source_path).show(10)"]},{"cell_type":"code","execution_count":0,"id":"20201009-122210_2055854604","metadata":{},"outputs":[],"source":["\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\napp_basic_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from mu.app_{granularity}\n    where date='{date}'\n        and kpi=1\n        and estimate is not null\n        and estimate > 0\n$proxy$) tpl (count bigint);\"\"\"\n\n\nsegment_by_age_gender_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from ag.app_{granularity}\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint) ;\n\"\"\"\n\nsegment_by_audience_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from ad.app_{granularity}\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint) ;\n\"\"\"\n# check the country\n# segment_by_product_sql = \"\"\"\n# select distinct store_id AS store_id from plproxy.execute_select_nestloop($proxy$ \n#     select distinct store_id AS store_id\n#     from au.app_monthly\n#     WHERE date='2015-01-31'\n#         and kpi=1\n#         and estimate is not null\n#         and estimate <> 0\n\n# $proxy$) tpl (store_id int) order by store_id;\n# \"\"\"\n\nsegment_by_product_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from au.app_{granularity}\n    WHERE date='{date}'\n    and kpi=1\n    and estimate is not null\n    and estimate <> 0\n\n$proxy$) tpl (count bigint);\n\"\"\"\n\n\n\napp_cross_app = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from ca.app_{granularity}\n    where \n        date ='{date}'\n        and kpi = 9\n        and estimate is not null\n        and estimate <> 0\n$proxy$) tpl (count bigint);\n\"\"\"\n\n# app_retention = \"\"\"\n# select sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n#     select count(1) as count\n#     from rt.app_{granularity}\n#     where \n#         date ='{date}'\n# $proxy$) tpl (count bigint) ;\n# \"\"\"\n\napp_retention = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from rt.app_{granularity}\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint) ;\n\"\"\"\n\ngranularity_list = [\"monthly\"]\n\ndef get_plproxy_data(sql_str):\n    for granularity in granularity_list:\n        for date_str in DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            result = query(PLPROXY_DSN, sql_str.format(granularity=granularity,date=date_str))\n            count = result[0][0]\n            print str(granularity)+str(\" \")+str(date_str)+str(\" \")+str(count)\n\n# def get_plproxy_data(sql_str):\n#     result = query(PLPROXY_DSN, sql_str)\n#     count = result[0][0]\n#     for r in result:\n#         print(r[0])\n#     # print str(\"monthly\")+str(\" \")+str(\"2017-05-31\")+str(\" \")+str(result)\n            \ndef get_unified_data():\n    print 1\n\nget_plproxy_data(app_cross_app)  "]},{"cell_type":"code","execution_count":0,"id":"20201026-080221_1527919333","metadata":{},"outputs":[],"source":["\nbasic_dump=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact/granularity=monthly/\"\nbasic=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/\"\nsegment_by_age_gender_sql=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity={granularity}/month={date}/\"\nsegment_by_audience=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ad_app.v2/fact/granularity={granularity}/month={date}/\"\nsegment_by_product_sql=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity={granularity}/month={date}\"\napp_cross_app=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity={granularity}/month={date}/\"\napp_retention=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-rt_app.v2/fact/granularity=monthly/\"\nstart_str=\"2013-01\"\nend_str=\"2014-12\"\n\ngranularity_list = [\"weekly\"]\n\ndef get_dump_count(path_str):\n    for granularity in granularity_list:\n        for date_str in DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            spark.read.parquet(path_str.format(granularity=granularity, date=date_str[:-3])).filter(\"date = '{date}'\".format(date=date_str)).createOrReplaceTempView(\"test\")\n            df = spark.sql(\"select count(1) from test where kpi=1 and estimate is not null and estimate <> 0\").collect()\n            \n            print str(granularity)+str(\" \")+str(date_str)+str(\" \")+str(df[0][0])\n\ndef tmp_check():\n    tmp_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity=monthly/month=2014-12/\"\n    spark.read.parquet(tmp_path).createOrReplaceTempView(\"test\")\n    spark.sql(\"select distinct store_id from test order by store_id desc limit 100\").show(100, False)\nget_dump_count(segment_by_product_sql)"]},{"cell_type":"code","execution_count":0,"id":"20201119-074334_626663311","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity=monthly/month=2014-12/"]},{"cell_type":"code","execution_count":0,"id":"20201010-070951_146929377","metadata":{},"outputs":[],"source":["\nbasic_dump=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact/granularity=monthly/\"\nbasic=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/\"\nsegment_by_age_gender_sql=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity=/\"\nsegment_by_audience=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ad_app.v2/fact/granularity=monthly/\"\nsegment_by_product=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=weekly/\"\napp_cross_app=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity=monthly/\"\napp_retention=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-rt_app.v2/fact/granularity=monthly/\"\nstart_str=\"2015-01\"\nend_str=\"2017-05\"\n\nspark.read.parquet(app_retention).filter(\"date between '2020-08-01' and '2020-08-31' and app_id = 20600000009039 and store_id = 10 and gender=0 and age=0\").createOrReplaceTempView(\"test\")\nspark.sql(\"select * from test limit 1\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20201026-033833_1339204156","metadata":{},"outputs":[],"source":["\nseg_by_prdouct = \"s3://aardvark-prod-pdx-mdm-to-int/crossapps_appused/version=1.0.0/\"\nseg_by_age_gender = \"s3://aardvark-prod-pdx-mdm-to-int/audience_demographics/version=1.0.0/range_type=MONTH/\"\nspark.read.parquet(seg_by_prdouct).filter(\"date between '2020-09-01' and '2020-09-30' and range_type = 'MONTH'\").createOrReplaceTempView(\"test\")\nspark.sql(\"select * from test where CROSSAPP_UP is not null and CROSSAPP_UP <> 0 limit 1\").show()\nspark.sql(\"select date, count(1) from test where CROSSAPP_UP is not null and CROSSAPP_UP <> 0 group by date order by date desc\").show(10, False)"]},{"cell_type":"code","execution_count":0,"id":"20201010-022006_1562528397","metadata":{},"outputs":[],"source":["\nfor date_str in DATE_GRANULARITY_MAPPINGLIST[\"monthly\"]:\n    try:\n        # app x app\n        result = query(PLPROXY_DSN, sql_template_a_x_a.format(date=date_str))\n        count =result[0][1]\n        affinity_sum = result[0][2]\n        up_sum = result[1][2]\n        # app x domain, domain x domain, domain x app\n        for db_name in [\"app_x_domain\", \"domain_x_domain\", \"domain_x_app\"]:\n            result = query(PLPROXY_DSN, sql_template.format(date=date_str, db_name=db_name))\n            count += result[0][0]\n            affinity_sum += result[0][1]\n            up_sum += result[0][2]\n        print \"{},{},{:.20g},{:.20g}\".format(date_str,count, up_sum, affinity_sum )\n    except Exception, e:\n        print \"{},{}\".format(date_str, \"ERROR\")\n"]},{"cell_type":"code","execution_count":0,"id":"20201009-094813_1237959544","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n-- adhoc\n\nselect distinct domain_id from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select distinct domain_id\n    from mw.domain_basic_w\n    where \n        date = '2020-05-16' and  est_average_active_users <> 0 and est_average_active_users is not null\n\\$proxy\\$) tpl (domain_id bigint) order by domain_id asc;\n\n\nEOF\n#estimate DOUBLE PRECISION"]},{"cell_type":"code","execution_count":0,"id":"20201010-060449_122337901","metadata":{},"outputs":[],"source":["%%sh\nselect country_code, sum(count_value) as count from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select country_code, count(*) as count_value\n    from mw.domain_basic_m \n    where \n        date ='2020-01-31' and  est_average_active_users <> 0 and est_average_active_users is not null group by country_code\n\\$proxy\\$) tpl (country_code char(2), count_value bigint) group by country_code order by count desc;\n\n\nselect sum(count_value) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select count(*) as count_value \n    from mw.domain_basic_m \n    where \n        date ='2020-01-31' and  est_average_active_users <> 0 and est_average_active_users is not null and domain_id = 70010000386932\n\\$proxy\\$) tpl ( count_value bigint);"]},{"cell_type":"code","execution_count":0,"id":"20200928-084018_133415156","metadata":{},"outputs":[],"source":["%%sh\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n# internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\n           select  date, count(*) from plproxy.execute_select(\\$proxy\\$\nSELECT * FROM mw.domain_x_domain_m where date = '2020-09-30' \\$proxy\\$)\n t (granularity char(1), device_code char(2), country_code char(2), date date, domain_id bigint, cross_domain_id bigint,\n est_cross_product_affinity real, est_cross_product_usage_penetration real) group by date order by date desc;\n\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200929-023105_895095538","metadata":{},"outputs":[],"source":["%%sh\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n# internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\n          select  date, count(*), cast(sum(est_cross_product_affinity) as double precision) from plproxy.execute_select(\\$proxy\\$\nSELECT * FROM mw.domain_x_domain_m where date between '2020-01-01' and '2020-10-01' \\$proxy\\$)\n t (granularity char(1), device_code char(2), country_code char(2), date date, domain_id bigint, cross_domain_id bigint,\n est_cross_product_affinity real, est_cross_product_usage_penetration real) group by date order by date desc;\n\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200929-025723_201502330","metadata":{},"outputs":[],"source":["%%sh\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n# internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\n           select  count(*) from plproxy.execute_select(\\$proxy\\$\nSELECT * FROM mu.app_daily where date between '2020-07-01' and '2020-07-31' and kpi = 1\nand estimate is not null and estimate <> 0 \\$proxy\\$)\n t (device_id smallint, store_id int, date date, kpi smallint, app_id bigint, estimate double precision);\n\n\n\n\n\n\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200928-084232_1280551999","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity==monthly/"]},{"cell_type":"code","execution_count":0,"id":"20201119-030036_709692179","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}