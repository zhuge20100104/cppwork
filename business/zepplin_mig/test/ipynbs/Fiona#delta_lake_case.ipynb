{"cells":[{"cell_type":"code","execution_count":0,"id":"20200227-052436_553725946","metadata":{},"outputs":[],"source":["\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-aso/unified/app-tech.aso.sov-search-ads-keyword-fact.v1/fact/granularity=daily/date=2020-02-18/\").select(\"keyword_id\").distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200218-082627_1224818347","metadata":{},"outputs":[],"source":["\n\n\nspark.read.format(\"delta\").option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/par*\").where(\"process_date between '2020-02-11' and '2020-02-14'\").groupBy(\"_identifier\").count().show(100)"]},{"cell_type":"code","execution_count":0,"id":"20200207-153710_1043958843","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import desc\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_load_temp/review_load_temp_v4/\", depth_is_1=True)\nprint path_list[107]\nfor path in path_list[120:]:\n    dup_list = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/{}/'.format(path.split(\"/\")[3])).select('review_id','process_date','process_hour').distinct().groupBy(\"review_id\").agg({\"*\":\"count\"}).where(\"count(1)>1\").orderBy(\"count(1)\",ascending = False).take(3)\n\n    for x in dup_list:\n        result1 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/{}'.format(path.split(\"/\")[3])).filter(\"review_id='{}'\".format(x[\"review_id\"])).groupBy(\"process_date\",\"process_hour\",\"time\").agg({\"*\":\"count\"}).orderBy(desc(\"time\"),desc(\"process_date\"),desc(\"process_hour\")).head(1)\n        result2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/{}\".format(path.split(\"/\")[3])).filter(\"review_id='{}'\".format(x[\"review_id\"])).head(1)\n        # print result1,result2\n        if result1[0][\"process_date\"]==result2[0][\"process_date\"] and result1[0][\"process_hour\"]==result2[0][\"process_hour\"] and result1[0][\"time\"]==result2[0][\"time\"]  :\n            print 'pass', path\n        else:\n            print 'failed!!!',result1, result2,x"]},{"cell_type":"code","execution_count":0,"id":"20200208-024432_1806466074","metadata":{},"outputs":[],"source":["\n\n\nspark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2019-01/').filter(\"review_id='gp:AOqpTOHQ9qg91CJhYS7vTwxcJfBnLAr1dzOTDcruejCHxrsuI33EjSzLglUghYCivJ2lqj5H452DXKhrsNnEZw'\").orderBy(\"process_date\",\"process_hour\",ascending=False).show()"]},{"cell_type":"code","execution_count":0,"id":"20200208-024541_26606354","metadata":{},"outputs":[],"source":["\nspark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/event_month=2019-01/').filter(\"review_id='gp:AOqpTOHQ9qg91CJhYS7vTwxcJfBnLAr1dzOTDcruejCHxrsuI33EjSzLglUghYCivJ2lqj5H452DXKhrsNnEZw'\").orderBy(\"process_date\",ascending=False).show()"]},{"cell_type":"code","execution_count":0,"id":"20200208-021000_1652492230","metadata":{},"outputs":[],"source":["\n\n\ndup_list = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2019-01/').select('review_id','process_date','process_hour','time').distinct().groupBy(\"review_id\").agg({\"*\":\"count\"}).where(\"count(1)>1\").orderBy(\"count(1)\",ascending = False).show(50,False)"]},{"cell_type":"code","execution_count":0,"id":"20200208-021937_381668751","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql.functions import count, avg\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2019-01\").select('review_id','time').groupBy(\"review_id\",\"time\").agg(count(\"time\")).alias(\"count_time\").groupBy(\"review_id\",\"count_time.count(time)\").withColumn('time_order', agg(count(\"count_time.count(time)\"))).show(50,False)"]},{"cell_type":"code","execution_count":0,"id":"20200208-032024_1964035040","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\n\nreview_df = spark.read.parquet(\n    \"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2018-01\")\nreview_df.registerTempTable(\"reviews\")\nios_samples_df = spark.sql(\"SELECT * FROM reviews WHERE market_code = 'apple-store' and user_review_url is not null LIMIT 10\").cache()\nios_samples_df.count()\nandroid_samples_df = spark.sql(\"SELECT * FROM reviews WHERE market_code = 'google-play' LIMIT 10\").cache()\nandroid_samples_df.count()\nsamples = []\nfor idx, df in [\n    (\"int-ss-review_v1-apple-store-ios-all-201701\", ios_samples_df),\n    (\"int-ss-review_v1-google-play-android-all-201701\", android_samples_df)]:\n    print df.columns\n    rows = (\n        df\n            .withColumn(\"date\", F.date_format(\"time\", \"yyyy-MM-dd\"))\n            .drop(\"time\")\n            .withColumn(\"chinese_content\",\n                        F.when((df.content_language == \"zh\") | (df.content_language == \"zh_Hant\"), df.content))\n            .withColumn(\"chinese_reply\",\n                        F.when((df.reply_language == \"zh\") | (df.reply_language == \"zh_Hant\"), df.reply))\n            .withColumn(\"chinese_title\",\n                        F.when((df.title_language == \"zh\") | (df.title_language == \"zh_Hant\"), df.title))\n            .withColumn(\"english_content\", F.when(df.content_language == \"en\", df.content))\n            .withColumn(\"english_reply\", F.when(df.reply_language == \"en\", df.reply))\n            .withColumn(\"english_title\", F.when(df.title_language == \"en\", df.title))\n            .withColumn(\"japanese_content\", F.when(df.content_language == \"ja\", df.content))\n            .withColumn(\"japanese_reply\", F.when(df.reply_language == \"ja\", df.reply))\n            .withColumn(\"japanese_title\", F.when(df.title_language == \"ja\", df.title))\n            .withColumn(\"korean_content\", F.when(df.content_language == \"ko\", df.content))\n            .withColumn(\"korean_reply\", F.when(df.reply_language == \"ko\", df.reply))\n            .withColumn(\"korean_title\", F.when(df.title_language == \"ko\", df.title))\n            .withColumn(\"other_language_content\", F.when(\n            (df.content_language != \"zh\") & (df.content_language != \"zh_Hant\") & (df.content_language != \"en\") &\n            (df.content_language != \"ja\") & (df.content_language != \"ko\"), df.content))\n            .withColumn(\"other_language_reply\", F.when(\n            (df.reply_language != \"zh\") & (df.reply_language != \"zh_Hant\") & (df.reply_language != \"en\") &\n            (df.reply_language != \"ja\") & (df.reply_language != \"ko\"), df.reply))\n            .withColumn(\"other_language_title\", F.when(\n            (df.title_language != \"zh\") & (df.title_language != \"zh_Hant\") & (df.title_language != \"en\") &\n            (df.title_language != \"ja\") & (df.title_language != \"ko\"), df.title))\n            .collect()\n    )\n    samples.append((idx, rows))\nprint samples\n"]},{"cell_type":"code","execution_count":0,"id":"20200208-032055_1229338952","metadata":{},"outputs":[],"source":["% pyspark\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\n\ndef compare(doc, row):\n    keys = [u\"review_id\", u'_identifier', u'app_id', u'chinese_content', u'chinese_reply', u'chinese_title', u'content',\n            u'content_language', u'country_code', u'date', u'device_code', u'english_content', u'english_reply',\n            u'english_title', u'japanese_content', u'japanese_reply', u'japanese_title', u'korean_content',\n            u'korean_reply', u'korean_title', u'language', u'market_code', u'other_language_content',\n            u'other_language_reply', u'other_language_title', u'product_version', u'rating', u'reply', u'reply_date',\n            u'reply_language', u'title', u'title_language', u'user_device', u'user_id', u'user_language', u'user_name',\n            u'user_purchased']\n    row_dict = row.asDict()\n    for key in keys:\n        if key in (\"app_id\", \"_identifier\"):\n            assert str(doc[key]) == str(row_dict[key]), \"{} is diffrent\".format(key)\n        else:\n            assert doc[key] == row_dict[key], \"{} is diffrent\".format(key)\n\n\ndef es_doc(index, doc_id, routing_id):\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"***\", \"***\")\n\n    url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/{}/_doc/{}?routing={}\".format(\n        index, doc_id, routing_id)\n    resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    doc = json.loads(resp.text)\n    source = doc['_source']\n    source['review_id'] = doc[\"_id\"]\n    return source\n\n\nfor idx, rows in samples:\n    for row in rows:\n        doc = es_doc(idx, row.review_id, row.app_id)\n        compare(doc, row)\n"]},{"cell_type":"code","execution_count":0,"id":"20200208-032055_1821845428","metadata":{},"outputs":[],"source":["\n# review_df = spark.read.parquet(\n#     \"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2018-01\")\n# review_df.registerTempTable(\"reviews\")\nios_samples_df = spark.sql(\"SELECT review_id,time, process_date, process_hour FROM reviews WHERE market_code = 'google-play' group by review_id , time , process_date, process_hour  LIMIT 10\")\nprint ios_samples_df.show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200208-004038_1000666291","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import desc\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_load_temp/review_load_temp_v4/\", depth_is_1=True)\nprint path_list[107]\nfor path in path_list[107:]:\n    dup_list = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/{}/'.format(path.split(\"/\")[3])).select('review_id','process_date','process_hour').distinct().groupBy(\"review_id\").agg({\"*\":\"count\"}).where(\"count(1)>1\").orderBy(\"count(1)\",ascending = False).take(3)\n\n    for x in dup_list:\n        result1 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/{}'.format(path.split(\"/\")[3])).filter(\"review_id='{}'\".format(x[\"review_id\"])).groupBy(\"process_date\",\"process_hour\",\"time\").agg({\"*\":\"count\"}).orderBy(desc(\"time\"),desc(\"process_date\"),desc(\"process_hour\")).head(1)\n        result2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v1/{}\".format(path.split(\"/\")[3])).filter(\"review_id='{}'\".format(x[\"review_id\"])).head(1)\n        # print result1,result2\n        if result1[0][\"process_date\"]==result2[0][\"process_date\"] and result1[0][\"process_hour\"]==result2[0][\"process_hour\"] and result1[0][\"time\"]==result2[0][\"time\"]  :\n            print 'pass', path\n        else:\n            print 'failed!!!',result1, result2,x"]},{"cell_type":"code","execution_count":0,"id":"20200208-003853_1162366435","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import desc\n\ndup_list = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2019-09/').select('review_id','process_date','process_hour').distinct().groupBy(\"review_id\").agg({\"*\":\"count\"}).where(\"count(1)>1\").orderBy(\"count(1)\",ascending = False).take(10)\n\nfor x in dup_list:\n    result1 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2019-09').filter(\"review_id='{}'\".format(x[\"review_id\"])).groupBy(\"process_date\",\"process_hour\",\"time\").agg({\"*\":\"count\"}).orderBy(desc(\"time\"),desc(\"process_date\"),desc(\"process_hour\")).take(1)\n    result2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v1/event_month=2019-09\").filter(\"review_id='{}'\".format(x[\"review_id\"])).take(1)\n    print result1,result2\n    if result1[0][\"process_date\"]==result2[0][\"process_date\"] and result1[0][\"process_hour\"]==result2[0][\"process_hour\"] and result1[0][\"time\"]==result2[0][\"time\"]  :\n        print 'pass'\n    else:\n        print 'failed!!!',result1, result2,x"]},{"cell_type":"code","execution_count":0,"id":"20200207-153603_1267619881","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_load_temp/review_load_temp_v4/\", depth_is_1=True)\nfor path in path_list[107:]:\n    count_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).select(\"review_id\").distinct().count()\n    count_2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/{}\".format(path.split(\"/\")[3])).select(\"review_id\").count()\n    if count_1 != count_2:\n        print \"not equal!!!!!!\", count_1 , count_2, path\n    else:\n        print 'pass', path"]},{"cell_type":"code","execution_count":0,"id":"20200205-022947_1767706589","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.fs.driver import unified\nfrom aadatapipelinecore.core.utils.identifier import virtual_id\nfrom aadatapipelinecore.core.urn import Urn\n\n\ndef urn(namespace):\n    from aadatapipelinecore.core.urn import Urn\n    from aadatapipelinecore.core.pipeline import type_\n    urn = Urn(\n        identifier=virtual_id(),\n        event=type_.EventType.TRANSFORM,\n        manipulation=type_.ManipulationType.QUERY,\n        namespace=namespace,\n        data_type=\"fact\"\n    )\n    return urn\ntest_urn = urn(\"int-ss.review.v1\")\n\nunified.read(spark, test_urn).urn\n# print unified.read(spark, test_urn,sql_where=\"process_granularity='monthly' and process_date=2017-01-31'\").data_dto_list\n\n# take(1)[0]\n"]},{"cell_type":"code","execution_count":0,"id":"20200205-023421_813540349","metadata":{},"outputs":[],"source":["%%sh\ncurl -u bdp_qa:6trmJcZBJrKWdtN2 -Get http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v2-apple-store-ios-all-201703/_search?pretty"]},{"cell_type":"code","execution_count":0,"id":"20200205-035808_1813711086","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import countDistinct\n# spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/MONTHLY/2017-01-31/gp/\",sep=\"\\t\").filter(\"_c0='gp:AOqpTOHUKdAGUV_C94UVZHgqaXl9iqnCCdE-8Uqp3vdNW89Qt6UV9ADIOCo9P73sEEXBRFnDmALiiklCpHj9Gr4'\").show(20,False,vertical=False)\n\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").filter(\"market_code='google-play' and _identifier='220200118161416198'\").cache().agg(countDistinct(\"review_id\")).collect()\nunified:30102239 \ndb:30191242"]},{"cell_type":"code","execution_count":0,"id":"20200205-082157_1726607782","metadata":{},"outputs":[],"source":["%%sh%pyspark\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.storagelevel import StorageLevel\nfrom pyspark.sql import functions as F\nstart_month = datetime.strptime(\"2017-06\",\"%Y-%m\")\nend_month = datetime.strptime(\"2020-01\",\"%Y-%m\")\na_month = relativedelta(months=1)\nnow = start_month\nwhile now <= end_month:\n    path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={weekly,daily,hourly}/process_date=%s-*/\" % now.strftime(\"%Y-%m\")\n    print \"%s started\" % path \n    review_df = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").parquet(path).where(\"process_granularity in ('daily', 'weekly', 'hourly') and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\")\n    review_df = review_df.withColumn(\"event_month\",F.from_unixtime(F.unix_timestamp(\"time\", \"yyyy-MM-dd\"), \"yyyy-MM\")).withColumn(\"event_date\",F.from_unixtime(F.unix_timestamp(\"time\", \"yyyy-MM-dd\"), \"yyyy-MM-dd\")).repartition(1000, \"event_date\").drop(\"event_date\")\n    review_df.write.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v2/\", mode=\"append\", partitionBy=[\"event_month\"])\n    now += a_month\n    print \"%s ended\" % path\n\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}