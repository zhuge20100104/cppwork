{"cells":[{"cell_type":"code","execution_count":0,"id":"20200515-035233_610916587","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                   'usage.basic-kpi.v1/fact/granularity={v1_granularity}/date={v1_date}/'\n    unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                   'usage.basic-kpi.v5/fact/granularity={v3_granularity}/date={v3_date}/'\n    for month in date_list:\n        sample_index = random.randint(0, len(month[1])-1)\n        date = month[1][sample_index]\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1 = (\n            unified_v1\n                .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                                  unified_v1['est_average_session_duration'] * unified_v1['est_average_session_per_user']))\n                .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n                .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n                .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n                .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n                .withColumn('est_install_base',\n                            unified_v1['est_install_penetration'] *\n                            unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n                .withColumn('est_population', unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n                .withColumn('est_share_of_device_mb', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_device_session', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_device_time', lit(None).cast(DoubleType()))\n                .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n                .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n                .withColumn('est_total_sessions',\n                            unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n                .withColumn('est_total_time',\n                            unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n        )\n        unified_v1 = unified_v1.na.fill(0)\n        unified_v1.createOrReplaceTempView(\"df\")\n        agg_df = spark.sql(\"\"\"\n                      select\n                       app_id, country_code,\n                       case \n                           when device_code in ('ios-phone', 'ios-tablet') then 'ios-all' \n                           else 'android-all' \n                       end as device_code,\n                       SUM(est_average_active_days * est_average_active_users) / SUM(est_average_active_users) as est_average_active_days,\n                       SUM(est_average_session_duration * est_average_active_users) / SUM(est_average_active_users) AS est_average_session_duration, \n                       SUM(est_average_session_per_user * est_average_active_users) / SUM(est_average_active_users) AS est_average_session_per_user,\n                       SUM(est_average_time_per_user * est_average_active_users) / SUM(est_average_active_users) AS est_average_time_per_user,\n                       SUM(est_average_bytes_per_session * est_average_active_users) / SUM(est_average_active_users) AS est_average_bytes_per_session,\n                       SUM(est_average_bytes_per_user * est_average_active_users) / SUM(est_average_active_users) AS est_average_bytes_per_user,\n                       SUM(est_mb_per_second * est_population) / SUM(est_population) AS est_mb_per_second,\n                       SUM(est_percent_of_wifi_total * est_population) / SUM(est_population) AS est_percent_of_wifi_total, \n                       SUM(est_percentage_active_days * est_average_active_users) / SUM(est_average_active_users) AS est_percentage_active_days,\n                       SUM(est_average_active_users) AS est_average_active_users, \n                       SUM(est_install_penetration * est_population) / SUM(est_population) AS est_install_penetration,\n                       SUM(est_installs*est_population) / SUM(est_population) AS est_installs,\n                       SUM(est_usage_penetration * est_population) / SUM(est_population) AS est_usage_penetration,\n                       SUM(est_install_base) AS est_install_base,\n                       mean(est_population) as est_population,\n                       SUM(est_total_time) AS est_total_time, \n                       SUM(est_share_of_category_time*est_population) / SUM(est_population) AS est_share_of_category_time,\n                       SUM(est_share_of_device_time*est_population) / SUM(est_population) AS est_share_of_device_time, \n                       SUM(est_share_of_category_session*est_population) / SUM(est_population) AS est_share_of_category_session,\n                       SUM(est_share_of_device_session*est_population) / SUM(est_population) AS est_share_of_device_session, \n                       SUM(est_share_of_category_bytes*est_population) / SUM(est_population) AS est_share_of_category_bytes,\n                       SUM(est_share_of_device_mb*est_population)/SUM(est_population) AS est_share_of_device_mb,\n                       SUM(est_panel_size*est_population) / SUM(est_population) AS est_panel_size\n                       from df \n                       group by app_id, country_code, \n                       case \n                           when device_code in ('ios-phone', 'ios-tablet') then 'ios-all' \n                           else 'android-all' \n                       end\n            \"\"\")\n        agg_df.createOrReplaceTempView(\"agg_df\")\n        agg_more_df = spark.sql(\"\"\"\n            select \n                agg_df.*,\n                agg_df.est_usage_penetration/agg_df.est_install_penetration AS est_open_rate, \n                agg_df.est_average_active_users * agg_df.est_average_session_per_user AS est_total_sessions, \n                agg_df.est_install_base/ww.est_install_base as est_share_of_installs, \n                agg_df.est_average_active_users/ww.est_average_active_users as est_share_of_users\n            from agg_df left join \n                (select device_code, app_id, country_code \n                 est_install_base, est_average_active_users \n                 from agg_df where country_code ='WW'\n                 ) AS ww\n            on (agg_df.app_id=ww.app_id) and (agg_df.device_code = ww.device_code)\n            \"\"\")\n        unified_v1_agg = agg_more_df.union(unified_v1.drop(\"_identifier\").select(agg_more_df.columns))\n        unified_v1_agg = unified_v1_agg.na.fill(0)\n        unified_v1_agg.createOrReplaceTempView(\"unified_v1_agg\")\n\n        unified_v3 = spark.read.parquet(unified_v3_path_parse).drop(\"_identifier\").distinct()\n        unified_v3.createOrReplaceTempView(\"unified_v3\")\n        \n        subtract_df = spark.sql(\"\"\"\n            (select * from unified_v1_agg) EXCEPT (select * from unified_v3)\n        \"\"\")\n        subtract_df.show()\n        # print unified_v1_agg.columns\n        # print unified_v1_agg.filter(\"app_id=20600000770868 and country_code='KR' and device_code='android-all'\").collect()\n        # print unified_v3.filter(\"app_id=20600000770868 and country_code='KR' and device_code='android-all'\").collect()\n        subtract_count = unified_v1_agg.select(unified_v3.columns).subtract(unified_v3).count()\n        if subtract_count != 0:\n            print 'Accuracy Test FAIL!!!! subtract count: {}, date: {}'.format(subtract_count, date)\n        else:\n            print 'date: {} test PASS, subtract count: {}'.format(date, subtract_count)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200515-035311_1289866929","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_accuracy_0515/daily/\")\ndf.show()"]},{"cell_type":"code","execution_count":0,"id":"20200515-070243_794168970","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef check_not_empty(df):\n    empty_count = df.filter(\"est_average_active_users is null\").count()\n    if empty_count != 0:\n        print \"Accuracy Test Fail!!! AU is Null!!!\"\n    else:\n        print \"AU is Not Empty Check Pass!\"\n\n\ndef check_percentage_accuracy(df):\n    illegal_percentage_count = df.filter(\n        \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n        \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n        \"or est_share_of_category_time>1 or est_share_of_device_mb>1 or est_share_of_device_session>1 \"\n        \"or est_share_of_device_time>1 or est_share_of_installs>1 or est_share_of_users>1 \"\n        \"or est_usage_penetration>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Accuracy Test Fail!!! Percentage > 1!!!\"\n    else:\n        print \"Percentage>1 Check Pass!\"\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                   'usage.basic-kpi.v1/fact/granularity={v1_granularity}/date={v1_date}/'\n    unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                   'usage.basic-kpi.v5/fact/granularity={v3_granularity}/date={v3_date}/'\n    for month in date_list:\n        sample_index = random.randint(0, len(month[1])-1)\n        date = month[1][sample_index]\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1 = (\n            unified_v1\n                .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                                  unified_v1['est_average_session_duration'] * unified_v1['est_average_session_per_user']))\n                .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n                .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n                .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n                .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n                .withColumn('est_install_base',\n                            unified_v1['est_install_penetration'] *\n                            unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n                .withColumn('est_population', unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n                .withColumn('est_share_of_device_mb', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_device_session', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_device_time', lit(None).cast(DoubleType()))\n                .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n                .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n                .withColumn('est_total_sessions',\n                            unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n                .withColumn('est_total_time',\n                            unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n        )\n        unified_v1 = unified_v1.na.fill(0)\n        unified_v1.createOrReplaceTempView(\"df\")\n        agg_df = spark.sql(\"\"\"\n                      select\n                       app_id, country_code,\n                       case \n                           when device_code in ('ios-phone', 'ios-tablet') then 'ios-all' \n                           else 'android-all' \n                       end as device_code,\n                       sum(est_average_active_days * est_average_active_users) / sum(est_average_active_users) as est_average_active_days,\n                       sum(est_average_session_duration * est_average_active_users) / sum(est_average_active_users) AS est_average_session_duration, \n                       SUM(est_average_session_per_user * est_average_active_users) / SUM(est_average_active_users) AS est_average_session_per_user,\n                       SUM(est_average_time_per_user * est_average_active_users) / SUM(est_average_active_users) AS est_average_time_per_user,\n                       SUM(est_average_bytes_per_session * est_average_active_users) / SUM(est_average_active_users) AS est_average_bytes_per_session,\n                       SUM(est_average_bytes_per_user * est_average_active_users) / SUM(est_average_active_users) AS est_average_bytes_per_user,\n                       SUM(est_mb_per_second * est_population) / SUM(est_population) AS est_mb_per_second,\n                       SUM(est_percent_of_wifi_total * est_population) / SUM(est_population) AS est_percent_of_wifi_total, \n                       SUM(est_percentage_active_days * est_average_active_users) / SUM(est_average_active_users) AS est_percentage_active_days,\n                       SUM(est_average_active_users) AS est_average_active_users, \n                       SUM(est_install_penetration * est_population) / SUM(est_population) AS est_install_penetration,\n                       SUM(est_installs*est_population) / SUM(est_population) AS est_installs,\n                       SUM(est_usage_penetration * est_population) / SUM(est_population) AS est_usage_penetration,\n                       SUM(est_install_base) AS est_install_base,\n                       mean(est_population) as est_population,\n                       SUM(est_total_time) AS est_total_time, \n                       SUM(est_share_of_category_time*est_population) / SUM(est_population) AS est_share_of_category_time,\n                       SUM(est_share_of_device_time*est_population) / SUM(est_population) AS est_share_of_device_time, \n                       SUM(est_share_of_category_session*est_population) / SUM(est_population) AS est_share_of_category_session,\n                       SUM(est_share_of_device_session*est_population) / SUM(est_population) AS est_share_of_device_session, \n                       SUM(est_share_of_category_bytes*est_population) / SUM(est_population) AS est_share_of_category_bytes,\n                       SUM(est_share_of_device_mb*est_population)/SUM(est_population) AS est_share_of_device_mb,\n                       SUM(est_panel_size*est_population) / SUM(est_population) AS est_panel_size\n                       from df \n                       group by app_id, country_code, \n                       case \n                           when device_code in ('ios-phone', 'ios-tablet') then 'ios-all' \n                           else 'android-all' \n                       end\n            \"\"\")\n        agg_df.createOrReplaceTempView(\"agg_df\")\n        agg_more_df = spark.sql(\"\"\"\n            select \n                agg_df.*,\n                agg_df.est_usage_penetration/agg_df.est_install_penetration AS est_open_rate, \n                agg_df.est_average_active_users * agg_df.est_average_session_per_user AS est_total_sessions, \n                agg_df.est_install_base/ww.est_install_base as est_share_of_installs, \n                agg_df.est_average_active_users/ww.est_average_active_users as est_share_of_users\n            from agg_df left join \n                (select device_code, app_id, country_code \n                 est_install_base, est_average_active_users \n                 from agg_df where country_code ='WW'\n                 ) AS ww\n            on (agg_df.app_id=ww.app_id) and (agg_df.device_code = ww.device_code)\n            \"\"\")\n        unified_v1_agg = agg_more_df.union(unified_v1.drop(\"_identifier\").select(agg_more_df.columns))\n        unified_v1_agg = unified_v1_agg.na.fill(0)\n\n        unified_v3 = spark.read.parquet(unified_v3_path_parse).distinct().drop('_identifier')\n        \n        check_percentage_accuracy(unified_v3)\n        check_not_empty(unified_v3)\n\n        subtract_count = unified_v1_agg.select(unified_v3.columns).subtract(unified_v3).count()\n        subtract_count_reverse = unified_v3.select(unified_v1_agg.columns).subtract(unified_v1_agg).count()\n        if subtract_count != 0 or subtract_count_reverse != 0:\n            print 'Accuracy Test FAIL!!!! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        else:\n            print 'Accuracy Test PASS! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        test_result.append((_granularity, max(subtract_count, subtract_count_reverse), date))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'subtract_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_accuracy_0515/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200528-055246_1946649119","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2015-12-29/')\ndf.select('app_id', 'device_code', 'country_code', 'est_install_penetration', 'est_open_rate', 'est_percent_of_wifi_total', 'est_percentage_active_days', 'est_share_of_category_bytes', 'est_share_of_category_session', 'est_share_of_category_time', 'est_share_of_device_mb', 'est_share_of_device_session', 'est_share_of_device_time',\n'est_share_of_installs', 'est_share_of_users', 'est_usage_penetration').filter(\n    \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n    \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n    \"or est_share_of_category_time>1 or est_share_of_device_mb>1 or est_share_of_device_session>1 \"\n    \"or est_share_of_device_time>1 or est_share_of_installs>1 \"\n    \"or est_usage_penetration>1\").select('est_open_rate', 'est_share_of_users').show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200528-061638_123860752","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2015-12-29/')\ndf.select('app_id', 'device_code', 'country_code', 'est_average_active_users', 'est_share_of_users').filter(\"app_id=326251330 and device_code='ios-all'\").show()\ndf2 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2015-12-29/')\ndf2.select('app_id', 'device_code', 'country_code', 'est_average_active_users', 'est_average_active_users_country_share').filter(\"app_id=326251330 and device_code='ios-phone'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200609-034748_1941792577","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_not_empty(df, date):\n    empty_count = df.filter(\"est_average_active_users is null\").count()\n    if empty_count != 0:\n        print \"AU is Not Empty Test Fail!!! empty_count: {}, date: {}\".format(empty_count, date)\n    else:\n        print \"AU is Not Empty Test Pass! date: {}\".format(date)\n\n\ndef check_percentage_accuracy(df, date):\n    illegal_percentage_count = df.filter(\n        \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n        \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n        \"or est_share_of_category_time>1 or est_share_of_installs>1 or est_share_of_users>1 \"\n        \"or est_usage_penetration>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Percentage<1 Test Fail!!! illegal_percentage_count: {}, date: {}\".format(illegal_percentage_count, date)\n    else:\n        print \"Percentage<1 Test Pass! date: {}\".format(date)\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/'\n        sample_index = random.randint(0, len(month[1]) - 1)\n        date = month[1][sample_index]\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] * \n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population', unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v1 = spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v1 = unified_v1.withColumnRenamed('app_id', 'product_id')\n        unified_v1 = unified_v1.na.fill(0).drop('_identifier')\n        \n        unified_v3 = spark.read.format(\"delta\").load(unified_v3_path_parse).drop('_identifier', 'date', 'granularity')\n\n        check_percentage_accuracy(unified_v3, date)\n        check_not_empty(unified_v3, date)\n\n        subtract_count = unified_v1.select(unified_v3.columns).subtract(unified_v3).count()\n        subtract_count_reverse = unified_v3.select(unified_v1.columns).subtract(unified_v1).count()\n        if subtract_count != 0 or subtract_count_reverse != 0:\n            print 'Accuracy Test FAIL!!!! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        else:\n            print 'Accuracy Test PASS! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200609-034804_1138717535","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_not_empty(df, date):\n    empty_count = df.filter(\"est_average_active_users is null\").count()\n    if empty_count != 0:\n        print \"AU is Not Empty Test Fail!!! empty_count: {}, date: {}\".format(empty_count, date)\n    else:\n        print \"AU is Not Empty Test Pass! date: {}\".format(date)\n\n\ndef check_percentage_accuracy(df, date):\n    illegal_percentage_count = df.filter(\n        \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n        \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n        \"or est_share_of_category_time>1 or est_share_of_installs>1 or est_share_of_users>1 \"\n        \"or est_usage_penetration>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Percentage<1 Test Fail!!! illegal_percentage_count: {}, date: {}\".format(illegal_percentage_count, date)\n    else:\n        print \"Percentage<1 Test Pass! date: {}\".format(date)\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/'\n        sample_index = random.randint(0, len(month[1]) - 1)\n        date = month[1][sample_index]\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] * \n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population', unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v1 = spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v1 = unified_v1.na.fill(0).drop('_identifier')\n        \n        unified_v3 = spark.read.format(\"delta\").load(unified_v3_path_parse).drop('_identifier', 'date', 'granularity')\n\n        check_percentage_accuracy(unified_v3, date)\n        check_not_empty(unified_v3, date)\n\n        subtract_count = unified_v1.select(unified_v3.columns).subtract(unified_v3).count()\n        subtract_count_reverse = unified_v3.select(unified_v1.columns).subtract(unified_v1).count()\n        if subtract_count != 0 or subtract_count_reverse != 0:\n            print 'Accuracy Test FAIL!!!! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        else:\n            print 'Accuracy Test PASS! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n\n\ngranularity_list = [\"weekly\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200609-035243_674124522","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/granularity=daily/date=2020-04-13/')\ndf.filter(\"app_id=20600001688061 and device_code='android-phone' and country_code='SG'\").select('est_install_penetration').show()"]},{"cell_type":"code","execution_count":0,"id":"20200609-065153_2107193600","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_not_empty(df, date):\n    empty_count = df.filter(\"est_average_active_users is null\").count()\n    if empty_count != 0:\n        print \"AU is Not Empty Test Fail!!! empty_count: {}, date: {}\".format(empty_count, date)\n    else:\n        print \"AU is Not Empty Test Pass! date: {}\".format(date)\n\n\ndef check_percentage_accuracy(df, date):\n    illegal_percentage_count = df.filter(\n        \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n        \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n        \"or est_share_of_category_time>1 or est_installs_country_share>1 or est_average_active_users_country_share>1 \"\n        \"or est_usage_penetration>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Percentage<1 Test Fail!!! illegal_percentage_count: {}, date: {}\".format(illegal_percentage_count, date)\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/'\n        for date in month[1]:\n            unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n            unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n            unified_v1 = spark.read.parquet(unified_v1_path_parse)\n\n            check_percentage_accuracy(unified_v1, date)\n            check_not_empty(unified_v1, date)\n\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200609-073924_1854286004","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_not_empty(df, date):\n    empty_count = df.filter(\"est_average_active_users is null\").count()\n    if empty_count != 0:\n        print \"AU is Not Empty Test Fail!!! empty_count: {}, date: {}\".format(empty_count, date)\n    else:\n        print \"AU is Not Empty Test Pass! date: {}\".format(date)\n\n\ndef check_percentage_accuracy(df, date):\n    illegal_percentage_count = df.filter(\n        \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n        \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n        \"or est_share_of_category_time>1 or est_share_of_installs>1 or est_share_of_users>1 \"\n        \"or est_usage_penetration>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Percentage<1 Test Fail!!! illegal_percentage_count: {}, date: {}\".format(illegal_percentage_count, date)\n    else:\n        print \"Percentage<1 Test Pass! date: {}\".format(date)\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/'\n        sample_index = random.randint(0, len(month[1]) - 1)\n        date = month[1][sample_index]\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] * \n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population', unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v1 = spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v1 = unified_v1.na.fill(0).drop('_identifier')\n        \n        unified_v3 = spark.read.format(\"delta\").load(unified_v3_path_parse).drop('_identifier', 'date', 'granularity')\n\n        check_percentage_accuracy(unified_v3, date)\n        check_not_empty(unified_v3, date)\n\n        subtract_count = unified_v1.select(unified_v3.columns).subtract(unified_v3).count()\n        subtract_count_reverse = unified_v3.select(unified_v1.columns).subtract(unified_v1).count()\n        if subtract_count != 0 or subtract_count_reverse != 0:\n            print 'Accuracy Test FAIL!!!! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        else:\n            print 'Accuracy Test PASS! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n\n\ngranularity_list = [\"monthly\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200609-074712_445231673","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/"]},{"cell_type":"code","execution_count":0,"id":"20200609-075003_760982301","metadata":{},"outputs":[],"source":["\nunified_v3 = spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date=2020-05-01/')\nrow_list = unified_v3.collect()\ncolumns = unified_v3.columns\nfor col in columns:\n    print col, '\\t', row_list[0][col]\n    "]},{"cell_type":"code","execution_count":0,"id":"20200609-080829_1077744735","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_not_empty(df, date):\n    empty_count = df.filter(\"est_average_active_users is null\").count()\n    if empty_count != 0:\n        print \"AU is Not Empty Test Fail!!! empty_count: {}, date: {}\".format(empty_count, date)\n    else:\n        print \"AU is Not Empty Test Pass! date: {}\".format(date)\n\n\ndef check_percentage_accuracy(df, date):\n    illegal_percentage_count = df.filter(\n        \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n        \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n        \"or est_share_of_category_time>1 or est_share_of_installs>1 or est_share_of_users>1 \"\n        \"or est_usage_penetration>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Percentage<1 Test Fail!!! illegal_percentage_count: {}, date: {}\".format(illegal_percentage_count, date)\n    else:\n        print \"Percentage<1 Test Pass! date: {}\".format(date)\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/'\n        sample_index = random.randint(0, len(month[1]) - 1)\n        date = month[1][sample_index]\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] *\n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population',\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v1 = spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v1 = unified_v1.na.fill(0).drop('_identifier')\n\n        unified_v3 = spark.read.format(\"delta\").load(unified_v3_path_parse).drop('_identifier', 'date', 'granularity')\n\n        check_percentage_accuracy(unified_v3, date)\n        check_not_empty(unified_v3, date)\n\n        subtract_count = unified_v1.select(unified_v3.columns).subtract(unified_v3).count()\n        subtract_count_reverse = unified_v3.select(unified_v1.columns).subtract(unified_v1).count()\n        if subtract_count != 0 or subtract_count_reverse != 0:\n            print 'Accuracy Test FAIL!!!! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        else:\n            print 'Accuracy Test PASS! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200617-064402_417648179","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2013, 1, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_not_empty(df, date):\n    empty_count = df.filter(\"est_average_active_users is null\").count()\n    if empty_count != 0:\n        print \"AU is Not Empty Test Fail!!! empty_count: {}, date: {}\".format(empty_count, date)\n    else:\n        print \"AU is Not Empty Test Pass! date: {}\".format(date)\n\n\ndef check_percentage_accuracy(df, date):\n    illegal_percentage_count = df.filter(\n        \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n        \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n        \"or est_share_of_category_time>1 or est_share_of_installs>1 or est_share_of_users>1 \"\n        \"or est_usage_penetration>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Percentage<1 Test Fail!!! illegal_percentage_count: {}, date: {}\".format(illegal_percentage_count, date)\n    else:\n        print \"Percentage<1 Test Pass! date: {}\".format(date)\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/'\n        sample_index = random.randint(0, len(month[1]) - 1)\n        date = month[1][sample_index]\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] *\n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population',\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v1 = spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v1 = unified_v1.na.fill(0).drop('_identifier')\n\n        unified_v3 = spark.read.format(\"delta\").load(unified_v3_path_parse).drop('_identifier', 'date', 'granularity')\n\n        check_percentage_accuracy(unified_v3, date)\n        check_not_empty(unified_v3, date)\n\n        subtract_count = unified_v1.select(unified_v3.columns).subtract(unified_v3).count()\n        subtract_count_reverse = unified_v3.select(unified_v1.columns).subtract(unified_v1).count()\n        if subtract_count != 0 or subtract_count_reverse != 0:\n            print 'Accuracy Test FAIL!!!! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        else:\n            print 'Accuracy Test PASS! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n\n\ngranularity_list = [\"weekly\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200619-074945_1425970930","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2013, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2013, 1, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_not_empty(df, date):\n    empty_count = df.filter(\"est_average_active_users is null\").count()\n    if empty_count != 0:\n        print \"AU is Not Empty Test Fail!!! empty_count: {}, date: {}\".format(empty_count, date)\n    else:\n        print \"AU is Not Empty Test Pass! date: {}\".format(date)\n\n\ndef check_percentage_accuracy(df, date):\n    illegal_percentage_count = df.filter(\n        \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n        \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n        \"or est_share_of_category_time>1 or est_share_of_installs>1 or est_share_of_users>1 \"\n        \"or est_usage_penetration>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Percentage<1 Test Fail!!! illegal_percentage_count: {}, date: {}\".format(illegal_percentage_count, date)\n    else:\n        print \"Percentage<1 Test Pass! date: {}\".format(date)\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/'\n        sample_index = random.randint(0, len(month[1]) - 1)\n        date = month[1][sample_index]\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] *\n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population',\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v1 = spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v1 = unified_v1.na.fill(0).drop('_identifier')\n\n        unified_v3 = spark.read.format(\"delta\").load(unified_v3_path_parse).drop('_identifier', 'date', 'granularity')\n\n        check_percentage_accuracy(unified_v3, date)\n        check_not_empty(unified_v3, date)\n\n        subtract_count = unified_v1.select(unified_v3.columns).subtract(unified_v3).count()\n        subtract_count_reverse = unified_v3.select(unified_v1.columns).subtract(unified_v1).count()\n        if subtract_count != 0 or subtract_count_reverse != 0:\n            print 'Accuracy Test FAIL!!!! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        else:\n            print 'Accuracy Test PASS! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n\n\ngranularity_list = [\"monthly\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200619-093510_648067124","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}