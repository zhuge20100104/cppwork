{"cells":[{"cell_type":"code","execution_count":0,"id":"20200306-030138_1941037504","metadata":{},"outputs":[],"source":["\n\ndf1 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='w' and date='2020-02-29' and device_code='ap' and country_code='ae'\")\n\ndf2 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='w' and date='2020-03-14' and device_code='ap' and country_code='ae'\")\n\ndf1.createOrReplaceTempView(\"new_table\")\ndf2.createOrReplaceTempView(\"old_table\")\n\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='w' and date='2020-02-29' and device_code='ap' and country_code='ae'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200306-083207_2014366250","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \nselect * from plproxy.execute_select(\\$proxy\\$select date, app_id, city_id, est_average_active_users ,est_average_bytes_per_user\n              from mu.city_app_w where city_id='31248510' and app_id='20600003074048' and country_code='ae' and date='2020-03-14'\n               \\$proxy\\$) t (date date, \n              app_id bigint, city_id bigint, est_average_active_users real , est_average_bytes_per_user real) limit 8;\nselect sum(cnt) from plproxy.execute_select(\\$proxy\\$select count(1) as cnt from mu.city_app_w where date='2020-03-14' and device_code='ap'  \\$proxy\\$) t (cnt bigint);\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200309-064149_647216320","metadata":{},"outputs":[],"source":["\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='w' and date='2020-03-14' and device_code='ap'\").count()\n# "]},{"cell_type":"code","execution_count":0,"id":"20200319-125841_622316851","metadata":{},"outputs":[],"source":["\n\n\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='w' and date='2020-03-14' and device_code='ap' and city_id='31248510'\").show()\n\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/city_level/report/routine/version=1.0.0/granularity=weekly/date=2020-03-14/country_code=AE/device_code=android-phone/metric_name=est_average_active_users/part-00000-097c28f4-3b7b-4b9a-8e38-ad16a5b7da53.c000.snappy.parquet\").filter(\"city_id='31248510'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200319-123417_694712910","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/granularity=m/\n# aws s3 ls  s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v1/fact/granularity=w/date=2020-03-07/device_code=ap/country_code=ae/\n# aws s3 ls  s3://aardvark-prod-pdx-mdm-to-int/city_level/report/routine/version=1.0.0/granularity=weekly/date=2020-03-14/country_code=AE/device_code=android-phone/metric_name=est_average_active_users/part-00000-097c28f4-3b7b-4b9a-8e38-ad16a5b7da53.c000.snappy.parquet\n\n\n\n\n\n\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200306-101707_994524834","metadata":{},"outputs":[],"source":["\ndef get_compared_data(date):\n    df_new = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='w' and date='{}'\".format(date)).filter(\"device_code='ap'\")\n    df_old = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v1/fact/granularity=w/date={}/device_code=ap/\".format(date))\n\n    list1= df_new.sample(False, 0.001, seed=0).limit(20).take(20)\n    return list1, df_old\n\ntest_list, df_old = get_compared_data('2020-02-29')\n\n\n\ndef compare(test_list, df_old):\n    columns_list = df_old.columns\n\n    for x in range(20):\n        list2 = df_old.filter(\"app_id='{}' and city_id='{}'\".format(test_list[x][\"app_id\"],test_list[x][\"city_id\"])).collect()\n        # print list2\n        for column in columns_list:\n            if test_list[x][column] != list2[0][column]:\n                print 'not equal',column, test_list[x][column], list2[0][column]\n    print 'Finished'           \n\ncompare(test_list, df_old)\n\n# print spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='m' and date='2020-02-29'\").filter(\"device_code='ap'\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200306-074734_684815690","metadata":{},"outputs":[],"source":["\n\ndf1 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='w' and date='2020-02-29' and device_code='ap' and country_code='ae'\")\n\ndf2 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='w' and date='2020-02-22' and device_code='ap' and country_code='ae'\")\n\ndf1.createOrReplaceTempView(\"new_table\")\ndf2.createOrReplaceTempView(\"old_table\")\n\nspark.sql(\"select city_id, app_id from (select city_id, app_id from old_table intersect select city_id, app_id from new_table) as prod\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200226-080528_279969614","metadata":{},"outputs":[],"source":["\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline.type_ import DataType, EventType, ManipulationType\nfrom aadatapipelinecore.core.fs.driver import unified\n\n# namespace='aa.city-level-usage.v1'\nurn = Urn(\n        namespace=\"aa.city-level-usage.v2\",\n        event=\"transform\",\n        data_type=DataType.FACT\n    )\n\ndf = unified.read(spark, urn,  sql_where=\"granularity='m' and date='2018-09-30'\").data_dto_list[0].dataframe\nv2_data_collect = df.filter(\"app_id='20600007997953'\").collect()\nprint v2_data_collect\n# print data.take(1)\n# t =data.take(2)\n# print [x for x in schema]\n# keys = {\"city_id\", \"app_id\",\"est_average_active_users\",\"est_usage_penetration\", \"est_average_session_per_user\",\"est_average_time_per_user\",\"est_average_bytes_per_user\",\"est_total_time\",\"est_average_bytes_per_session\",\"est_average_session_duration\",\"est_affinity\",\"device_code\",\"country_code\"}\n# print t[0]\n\n# for x in keys:\n#     print t[0][x]\n"]},{"cell_type":"code","execution_count":0,"id":"20200225-091308_1504242757","metadata":{},"outputs":[],"source":["\n\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v1/fact/granularity=m/date=2018-09-30/\").cache()\nv1_data_collect= df2.filter(\"app_id='20600007997953'\").collect()\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200226-084137_1210573058","metadata":{},"outputs":[],"source":["\n# data_v2=changed_data.take(1)\n\nfor x in changed_schema:\n    if x not in schema:\n        print x\n\nfor x in range(0,len(v1_data_collect)):\n    for k in keys:\n        if v2_data_collect[x][k] != v1_data_collect[x][k]:\n            print  v2_data_collect[x][k] , v1_data_collect[x][k]\n"]},{"cell_type":"code","execution_count":0,"id":"20200225-093430_655280362","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\n# from aadatapipelinecore.core.fs.device import meta_bucket\n# df2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v1/fact/granularity=m/date=2018-09-30/\").cache()\n\n\n# s3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-usage'))\n# path = s3_bucket_list.all(prefix=\"unified/city-level-usage.v1/fact/granularity=m\", depth_is_1=True)\nprint path[0].split(\"=\")[-1].strip(\"/\")\ntest = [(x.split(\"=\")[-1].strip(\"/\"),'a') for x in path ]\ntest.extend([(u'2018-01-31', 'c'), (u'2018-02-28', 'ca'), (u'2018-03-31', 'a'), (u'2018-04-30', 'a'), (u'2018-05-31', 'a'), (u'2018-06-30', 'a'), (u'2018-07-31', 'a'), (u'2018-08-31', 'a'), (u'2018-09-30', 'a'), (u'2018-10-31', 'a'), (u'2018-11-30', 'a'), (u'2018-12-31', 'a'), (u'2019-01-31', 'a'), (u'2019-02-28', 'a'), (u'2019-03-31', 'a'), (u'2019-04-30', 'a'), (u'2019-05-31', 'a'), (u'2019-06-30', 'a'), (u'2019-07-31', 'a'), (u'2019-08-31', 'ac'), (u'2019-09-30', 'ac'), (u'2019-10-31', 'ac'), (u'2019-11-30', 'a'), (u'2019-12-31', 'a'), (u'2020-01-31', 'a')])\nprint len(test)"]},{"cell_type":"code","execution_count":0,"id":"20200227-030107_1960238842","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline.type_ import DataType, EventType, ManipulationType\nfrom aadatapipelinecore.core.fs.driver import unified\n\ngranularity_list = ['m','w']\n\ndef get_path_list(granularity):\n    s3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-usage'))\n    path = s3_bucket_list.all(prefix=\"unified/city-level-usage.v1/fact/granularity={}\".format(granularity), depth_is_1=True)\n    # print path[0].split(\"=\")[-1].strip(\"/\")\n    # print [x for x in path[x].split(\"=\")[-1].strip(\"/\") ]\n    date_list = [(granularity, x.split(\"=\")[-1].strip(\"/\")) for x in path ]\n    return date_list\n    \n\n\ndef get_delta_lake_version(granularity, date):\n    urn = Urn(\n            namespace=\"aa.city-level-usage.v2\",\n            event=\"transform\",\n            data_type=DataType.FACT\n        )\n\n    df_v2 = unified.read(spark, urn,  sql_where=\"granularity='{}' and date='{}'\".format(granularity, date)).data_dto_list[0].dataframe\n    v2_data_count = df_v2.count()\n    return v2_data_count\n    \ndef get_old_version(granularity, date):\n    df_v1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v1/fact/granularity={}/date={}/\".format(granularity, date))\n    return df_v1.count()\n    \ndef compare(granularity, date):\n    count1 = get_delta_lake_version(granularity, date)\n    count2 = get_old_version(granularity, date)\n    if count1 != count2:\n        print 'Not equal!!!!! v1 : {}, v2 : {}, granularity: {}, date: {}'.format(count1, count2, granularity, date)\n    else:\n        print 'v1 : {}, v2 : {}, granularity: {}, date: {}'.format(count1, count2, granularity, date)\n \ntest_path = []\nfor g in granularity_list:\n    test_path.extend(get_path_list(g))\n\nfor case in test_path:\n    print case\n    compare(case[0],case[1])\n"]},{"cell_type":"code","execution_count":0,"id":"20200225-091339_1543855215","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.utils.runtime import is_prod\nfrom aadatapipelinecore.core.pipeline.type_ import DataType, EventType, ManipulationType\n\nfrom aadatapipelinecore.core.fs.driver import unified\nurn = Urn(\n        namespace=\"city-level-usage-v1.usage\",\n        event=\"transform\",\n        data_type=DataType.FACT\n    )\nresult =  unified.read(spark, urn, \"\")\n\n\nprint result.urn"]},{"cell_type":"code","execution_count":0,"id":"20200225-093823_1575763024","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}