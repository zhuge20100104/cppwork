{"cells":[{"cell_type":"code","execution_count":0,"id":"20211122-083453_1772023510","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls  s3://b2c-prod-dca-store-estimates/iap_revenue/date=2021-12-12/report_type=FIANL/"]},{"cell_type":"code","execution_count":0,"id":"20211209-065619_1423096806","metadata":{},"outputs":[],"source":["\niap_path = \"s3://b2c-prod-dca-store-estimates/iap_revenue/version=3.0.0/\"\n# spark.read.parquet(iap_path).createOrReplaceTempView(\"test\")\n\n# spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/iap_revenue/\").parquet(\"s3://b2c-prod-dca-store-estimates/iap_revenue/date=*/report_type=*\").filter(\"date between '2021-11-20' and '2021-12-04' and report_type='PREVIEW' \").where(\"isSubscription ='true'\").show(100, False)\nspark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/iap_revenue/version=3.0.0/\").parquet(\"s3://b2c-prod-dca-store-estimates/iap_revenue/version=3.0.0/date=*/report_type=*\").filter(\"date = '2022-02-20' and report_type='FINAL' \").createOrReplaceTempView(\"test\")\n# spark.sql(\"select count(1) from test where app_id=284882215 and country_code='US' limit 10\").show(10, False)\n# spark.sql(\"select country_code,sku,app_id,count(distinct(isSubscription)) as co from test group by country_code,sku,app_id  order by  co desc limit 10\").show(100, False)\nspark.sql(\"select * from test where sku='com.facebook.Facebook.fan_funding.10155900505621075' limit 100\").show(100, False)\n# spark.sql(\"select sum(est_iap_revenue), sum(est_iap_vol) from test where app_id =1455487903  and country_code = 'US'  limit 100\").show(100, False)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20211122-083749_1289916102","metadata":{},"outputs":[],"source":["\niap_path = \"s3://b2c-prod-dca-store-estimates/iap_revenue/date=2021-11-15/report_type=FINAL/\"\n# spark.read.parquet(iap_path).createOrReplaceTempView(\"test\")\n# spark.sql(\"select app_id, sum(est_iap_revenue) as iap_revenue, sum(est_iap_vol) as iap_vol from test where  app_id = 284862083 and country_code = 'GB' group by app_id limit 100\").show(100,  False)\n# spark.sql(\"select * from test limit 1\").show(10, False)\n# spark.sql(\"select count(1) from test where app_id=284862083 and country_code='GB'  limit 100\").show(100, False)\n# spark.sql(\"select count(1) from test limit 10\").show(10)\n\n\n\n\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n    \n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    # date_granularity_mapping_list = {\n    #     \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\")\n    # }\n    date_granularity_mapping_list = {\n        \"daily\": get_date_list(begin_date, end_date, \"D\")\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2022-02-23\", '%Y-%m-%d')\n# end_date = datetime.datetime.strptime(\"2021-11-15\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2022-02-26\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\n# range_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\n\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:        \n        path =  \"s3://b2c-prod-dca-store-estimates/iap_revenue/version=3.0.0/date={date}/report_type=PREVIEW/\".format(date=date)\n        spark.read.parquet(path).createOrReplaceTempView(\"test\")\n        result = spark.sql(\"SELECT count(1) FROM test \").collect()\n        # result = spark.sql(\"SELECT sum(est_iap_revenue), sum(est_iap_vol)  FROM test \").collect()\n        # result = spark.sql(\"SELECT date, count(distinct unified_product_id,country_code,device_code,market_code,granularity,date)  FROM test group by date order by date desc \").collect()\n        print(date, result[0][0])\n        "]},{"cell_type":"code","execution_count":0,"id":"20211201-070743_428758008","metadata":{},"outputs":[],"source":[" \n\nfrom pyspark.sql.functions import *\nfrom data_lib.api import spark_loader\nfrom data_lib.api.data_op import compare_columns_and_get_result_df\nfrom data_lib.api import Conv\n\n## Load wide df & Then convert to Apache arrow df and do some comparison\n\nloader = spark_loader(spark)\np_path = \"s3://b2c-prod-data-pipeline-qa/fredric/retention_data_2021_10_31_wide/\"\n\n## this one is raw data in S3\nwide_df = loader.load_parquet_as_table(p_path, \"retention_wide\")\n\n\ndef get_store_iap_est(date):\n    path =  \"s3://b2c-prod-dca-store-estimates/iap_revenue/version=3.0.0/date={date}/report_type=FINAL/\".format(date=date)\n    spark.read.parquet(path).createOrReplaceTempView(\"test\")\n    result = spark.sql(\"SELECT count(1) FROM test \").collect()\n    return date, result[0][0]\n    \ndef get_snowflake_data(date, table_name):\n    sf_df = loader.load_sf_as_table(\"select count(1) from {table_name} where date={date} \".format(table_name=table_name, date=date), \"snowflake_table\")\n    return sf_df.count()\n\n## Snowfake data here\nsf_df = loader.load_sf_as_table(\"select count(1) from AA_INTELLIGENCE_PRODUCTION.ADL_STORE_PAID.FACT_STORE_IAP_EST_RESTATE_V1_CLUSTER_BY_PRODUCT_KEY where date='2021-11-15' \", \"snowflake_table\")\nprint(sf_df.count())"]},{"cell_type":"code","execution_count":0,"id":"20220223-113144_1477080691","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}