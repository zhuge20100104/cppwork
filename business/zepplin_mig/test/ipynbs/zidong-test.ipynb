{"cells":[{"cell_type":"code","execution_count":0,"id":"20200304-070929_743257168","metadata":{},"outputs":[],"source":["%md\n# Topic"]},{"cell_type":"code","execution_count":0,"id":"20200304-070304_138961839","metadata":{},"outputs":[],"source":["%%sh\n# Unified data\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/\""]},{"cell_type":"code","execution_count":0,"id":"20200304-072157_1098250030","metadata":{},"outputs":[],"source":["%%sh\n# Delta lake data\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\""]},{"cell_type":"code","execution_count":0,"id":"20200304-072617_305349393","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom collections import defaultdict\n\nhistorical_data_s3_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/\"\nmigrated_data_s3_path =  \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\"\nhistorical_topic_df = spark.read.parquet(historical_data_s3_path)\nmigrated_topic_df = spark.read.format('delta').load(migrated_data_s3_path)\n\nstart_date = datetime.date(2019, 7, 1)\nend_date = datetime.date(2019, 7, 2)\nstart_date_str = start_date.strftime(\"%Y-%m-%d\")\nend_date_str = end_date.strftime(\"%Y-%m-%d\")\ndate_range = end_date - start_date\ndate_range = date_range.days + 1  # int\n\n# main\nfor _ in range(date_range):\n    process_date_str = start_date.strftime(\"%Y-%m-%d\")\n    # process_date = start_date\n\n    historical_count = historical_topic_df.where(\"process_date='{}'\".format(process_date_str)).distinct().count()\n    migrated_count = migrated_topic_df.where(\"process_date='{}'\".format(process_date_str)).distinct().count()\n\n    if not historical_count == migrated_count:\n        print \"historical_count = {}, migrated_count = {}\".format(historical_count, migrated_count)\n    \n    start_date += datetime.timedelta(days=1)\n    \nprint \"SUCCESS!\"\n"]},{"cell_type":"code","execution_count":0,"id":"20200306-060504_383807345","metadata":{},"outputs":[],"source":["\ndef sample_test(num, process_date_str, process_date, migrated_topic_df, historical_topic_df):\n    column_list = migrated_topic_df.columns\n    # ['_identifier', 'country_code', 'date', 'device_code', 'language', 'process_date', 'product_id', 'product_version', 'rating', 'review_id', 'topic_ids']\n\n    migrated_50_rows = migrated_topic_df.where(\"process_date='{}'\".format(process_date_str)).head(num) # Row list\n\n    fails_id = defaultdict(list)\n    \n    for i in range(num):\n        review_id = migrated_50_rows[i][\"review_id\"]\n        historical_row = historical_topic_df.where(\"process_date='{}' and review_id='{}'\".format(process_date, review_id)).collect()\n        # print historical_row\n        for attr in column_list:\n            if attr == \"process_date\":\n                continue\n            if not migrated_50_rows[i][attr] == historical_row[0][attr]:\n                print \"FAILED\"\n                fails_id[\"process_date={}, review_id={}\".format(process_date_str, review_id)].append(attr)\n    \n    return fails_id\n\n# process_date = start_date\n# process_date_str = start_date.strftime(\"%Y-%m-%d\")\n# res = sample_test(2, process_date_str, process_date, migrated_topic_df, historical_topic_df)\n# print res\n\nstart_date = datetime.date(2019, 7, 1)\nend_date = datetime.date(2020, 2, 11)\nstart_date_str = start_date.strftime(\"%Y-%m-%d\")\nend_date_str = end_date.strftime(\"%Y-%m-%d\")\ndate_range = end_date - start_date\ndate_range = date_range.days + 1  # int\n\nfor _ in range(date_range):\n    process_date = start_date\n    process_date_str = start_date.strftime(\"%Y-%m-%d\")\n    \n    res = sample_test(10, process_date_str, process_date, migrated_topic_df, historical_topic_df)\n    \n    start_date += datetime.timedelta(days=1)\n    \nprint \"SUCCESS!\""]},{"cell_type":"code","execution_count":0,"id":"20200306-022230_1103413362","metadata":{},"outputs":[],"source":["\nprocess_date = \"2019-07-01\"\nnum = 2\n\nprint migrated_topic_df.columns\nprint historical_topic_df.columns\n\nmigrated_50_rows = migrated_topic_df.where(\"process_date='{}'\".format(process_date)).head(num) # Row list\n# migrated_reivew_id_list = [migrated_50_rows[i][\"review_id\"] for i in range(num)]\n# historical_50_rows = [historical_topic_df.where(\"process_date='{}' and review_id='{}'\".format(process_date, review_id)).collect() for review_id in migrated_reivew_id_list]\n\nprint migrated_50_rows\n# print migrated_reivew_id_list\n# print historical_50_rows\n\nreview_id = migrated_50_rows[0][\"review_id\"]\nhistorical_row = historical_topic_df.where(\"process_date='{}' and review_id='{}'\".format(process_date, review_id)).collect()\n\nprint historical_row"]},{"cell_type":"code","execution_count":0,"id":"20200304-083825_1801603","metadata":{},"outputs":[],"source":["\n# num = 200\ndef sample_test(num, process_date):\n    migrated_column_list = migrated_topic_df.columns\n    historical_column_list = historical_topic_df.columns\n    # process_date = \"2020-02-02\"\n    migrated_50_rows = migrated_topic_df.where(\"process_date='{}'\".format(process_date)).head(num) # Row list\n    historical_50_rows = historical_topic_df.where(\"process_date='{}'\".format(process_date)).head(num)\n    \n    # print \"historical\", historical_50_rows[1]\n    # print \"migrated\", migrated_50_rows[1]\n    \n    for i in range(num):\n        for attr in migrated_column_list:\n            if attr == \"process_date\":\n                continue\n            if not migrated_50_rows[i][attr] == historical_50_rows[i][attr]:\n                print \"FAILED\"\n                print \"attr = {}\".format(attr)\n\nnum = 50\nstart_date = datetime.date(2019, 7, 1)\nend_date = datetime.date(2020, 2, 11)\ndate_range = end_date - start_date\ndate_range = date_range.days + 1  # int\n\nfor _ in range(date_range):\n    process_date = start_date.strftime(\"%Y-%m-%d\")\n    sample_test(num, process_date)\n    \n    start_date += datetime.timedelta(days=1)\nprint \"FINISHED\"\n# print migrated_column_list\n# print historical_column_list\n\n# print migrated_topic_df.printSchema()\n# migrated_pandas_df = migrated_topic_df.toPandas()\n# historical_pandas_df = historical_df.toPandas()\n# historical_all_label = []\n# migrated_all_label = []\n# for label, _ in migrated_pandas_df.items():\n#     migrated_all_label.append(label)\n\n# for label, _ in historical_pandas_df.items():\n#     historical_all_label.append(label)\n\n# print migrated_all_label\n# print historical_all_label"]},{"cell_type":"code","execution_count":0,"id":"20200305-022039_148916097","metadata":{},"outputs":[],"source":["%md\n# Term"]},{"cell_type":"code","execution_count":0,"id":"20200304-085006_1724717428","metadata":{},"outputs":[],"source":["%%sh\n# Unified data\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/\""]},{"cell_type":"code","execution_count":0,"id":"20200304-085115_1909165705","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/\""]},{"cell_type":"code","execution_count":0,"id":"20200309-041551_1565623292","metadata":{},"outputs":[],"source":["\ntest_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/process_date=2020-02-03/\")\ntest_df.show()"]},{"cell_type":"code","execution_count":0,"id":"20200309-041903_313048216","metadata":{},"outputs":[],"source":["\ntest_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/process_date=2020-02-03/\")\ntest_df.show()"]},{"cell_type":"code","execution_count":0,"id":"20200304-084930_740955112","metadata":{},"outputs":[],"source":["\nimport datetime\n\nhistorical_data_s3_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/\"\nmigrated_data_s3_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/\"\nhistorical_df = spark.read.parquet(historical_data_s3_path)\nmigrated_df = spark.read.format('delta').load(migrated_data_s3_path)\n\nstart_date = datetime.date(2019, 1, 1)\nend_date = datetime.date(2019, 1, 2)\nstart_date_str = start_date.strftime(\"%Y-%m-%d\")\nend_date_str = end_date.strftime(\"%Y-%m-%d\")\ndate_range = end_date - start_date\ndate_range = date_range.days + 1  # int\n# date_list = [base_date - datetime.timedelta(days=x) for x in range(numdays)]\n\nfor _ in range(date_range):\n    process_date = start_date.strftime(\"%Y-%m-%d\")\n\n    historical_count = historical_df.where(\"process_date='{}'\".format(process_date)).distinct().count()\n    migrated_count = migrated_df.where(\"process_date='{}'\".format(process_date)).distinct().count()\n\n    if not historical_count == migrated_count:\n        print \"process_date={}, historical_count = {}, migrated_count = {}\".format(process_date, historical_count, migrated_count)\n    \n    start_date += datetime.timedelta(days=1)\n    \nprint \"SUCCESS!\"\n"]},{"cell_type":"code","execution_count":0,"id":"20200306-084350_523946418","metadata":{},"outputs":[],"source":["\nstart_date = datetime.date(2019, 1, 1)\nend_date = datetime.date(2020, 2, 11)\nstart_date_str = start_date.strftime(\"%Y-%m-%d\")\nend_date_str = end_date.strftime(\"%Y-%m-%d\")\ndate_range = end_date - start_date\ndate_range = date_range.days + 1  # int\n# date_list = [base_date - datetime.timedelta(days=x) for x in range(numdays)]\n\ndef sample_test(num, process_date_str, process_date, migrated_topic_df, historical_topic_df):\n    column_list = migrated_topic_df.columns\n    # ['_identifier', 'country_code', 'date', 'device_code', 'language', 'process_date', 'product_id', 'product_version', 'rating', 'review_id', 'topic_ids']\n\n    migrated_50_rows = migrated_topic_df.where(\"process_date='{}'\".format(process_date_str)).head(num) # Row list\n\n    fails_id = defaultdict(list)\n    \n    for i in range(num):\n        review_id = migrated_50_rows[i][\"review_id\"]\n        historical_row = historical_topic_df.where(\"process_date='{}' and review_id='{}'\".format(process_date, review_id)).collect()\n        if not historical_row:\n            print \"Historical_row is empty\", historical_row\n            return\n\n        for attr in column_list:\n            if attr == \"process_date\":\n                continue\n            if not migrated_50_rows[i][attr] == historical_row[0][attr]:\n                print \"FAILED\"\n                print \"review_id=\", review_id\n                temp = fails_id[\"process_date={}, review_id={}\".format(process_date_str, review_id)]\n                temp.append(migrated_50_rows)\n                temp.append(historical_row)\n                break\n    \n    return fails_id\n\n# process_date = start_date\n# process_date_str = start_date.strftime(\"%Y-%m-%d\")\n# res = sample_test(2, process_date_str, process_date, migrated_topic_df, historical_topic_df)\n# print res\nres = []\nfor _ in range(date_range):\n    process_date = start_date\n    process_date_str = start_date.strftime(\"%Y-%m-%d\")\n    \n    temp = sample_test(1, process_date_str, process_date, migrated_df, historical_df)\n    if temp:\n        res.append(temp)\n    \n    start_date += datetime.timedelta(days=1)\n    \nprint \"SUCCESS!\"\n"]},{"cell_type":"code","execution_count":0,"id":"20200309-052346_1134779134","metadata":{},"outputs":[],"source":["\nprint res\nprint historical_df.where(\"review_id=5419093684\").collect()"]},{"cell_type":"code","execution_count":0,"id":"20200309-052042_884128216","metadata":{},"outputs":[],"source":["\nmigrated_df.where(\"review_id=5419093684\").collect()"]},{"cell_type":"code","execution_count":0,"id":"20200306-120818_770952285","metadata":{},"outputs":[],"source":["\nprint res\ndef sample_test(num, process_date_str, process_date, migrated_topic_df, historical_topic_df):\n    column_list = migrated_topic_df.columns\n    # ['_identifier', 'country_code', 'date', 'device_code', 'language', 'process_date', 'product_id', 'product_version', 'rating', 'review_id', 'topic_ids']\n\n    migrated_50_rows = migrated_topic_df.where(\"process_date='{}'\".format(process_date_str)).head(num) # Row list\n    # print migrated_50_rows\n    fails_id = defaultdict(dict)\n    \n    for i in range(num):\n        review_id = migrated_50_rows[i][\"review_id\"]\n        historical_row = historical_topic_df.where(\"process_date='{}' and review_id='{}'\".format(process_date, review_id)).collect()\n        # historical_row = historical_topic_df.where(\"review_id='{}'\".format(review_id)).collect()\n        # print historical_row\n        for attr in column_list:\n            if attr == \"process_date\":\n                continue\n            # print i\n            # print historical_row\n            if not migrated_50_rows[i][attr] == historical_row[0][attr]:\n                print \"FAILED\"\n                print \"review_id=\", review_id\n                temp = fails_id[\"process_date={}, review_id={}\".format(process_date_str, review_id)]\n                temp[\"migrated_50_rows\"] = migrated_50_rows\n                temp[\"historical_row\"] = historical_row\n                break\n    \n    return fails_id\nres2 = sample_test(5, \"2020-02-03\", datetime.date(2020, 2, 3), migrated_df, historical_df)\nprint res2"]},{"cell_type":"code","execution_count":0,"id":"20200309-054026_1765377409","metadata":{},"outputs":[],"source":["\nfor x, y in res2.items():\n    print \"date&id:{}  migrated:{}  historical:{}\".format(x, y[\"migrated_50_rows\"][0][\"_identifier\"], y[\"historical_row\"][0][\"_identifier\"])"]},{"cell_type":"code","execution_count":0,"id":"20200305-033836_1064499985","metadata":{},"outputs":[],"source":["\nnum = 50\nmigrated_column_list = migrated_df.columns\nhistorical_column_list = historical_df.columns\nprocess_date = \"2020-02-02\"\nmigrated_50_rows = migrated_df.where(\"process_date='{}'\".format(process_date)).head(num) # Row list\nhistorical_50_rows = historical_df.where(\"process_date='{}'\".format(process_date)).head(num)\n\n# print \"historical\", historical_50_rows[1]\n# print \"migrated\", migrated_50_rows[1]\n\nfor i in range(num):\n    for attr in migrated_column_list:\n        if attr == \"process_date\":\n            continue\n        if not migrated_50_rows[i][attr] == historical_50_rows[i][attr]:\n            print \"FAILED\"\n            print \"attr = {}\".format(attr)\nprint \"SUCCESS\""]},{"cell_type":"code","execution_count":0,"id":"20200304-101812_1548593624","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/\" --recursive --human --summarize | tail -3"]},{"cell_type":"code","execution_count":0,"id":"20200304-091512_1964364014","metadata":{},"outputs":[],"source":["\nmigrated_pandas_df = migrated_df.toPandas()\n# historical_pandas_df = historical_df.toPandas()\n# historical_all_label = []\n# migrated_all_label = []\n# for label, _ in migrated_pandas_df.items():\n#     migrated_all_label.append(label)\n\n# for label, _ in historical_pandas_df.items():\n#     historical_all_label.append(label)\n\n# print migrated_all_label\n# print historical_all_label"]},{"cell_type":"code","execution_count":0,"id":"20200304-080902_1083307875","metadata":{},"outputs":[],"source":["\nimport datetime\n\na = datetime.date(2019, 1, 3)\nb = a - datetime.date(2019, 1, 1)\na = a.strftime(\"%Y-%m-%d\")\n# b = b.strftime(\"%d\")\nprint a.__repr__(), b.days.__repr__()"]},{"cell_type":"code","execution_count":0,"id":"20200304-091508_404790531","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20200304-070803_1761698840","metadata":{},"outputs":[],"source":["\nunified_topic_df = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\")"]},{"cell_type":"code","execution_count":0,"id":"20200304-070421_608147204","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/process_date=2020-02-07/\""]},{"cell_type":"code","execution_count":0,"id":"20200304-065513_147343281","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline.type_ import DataType, EventType, ManipulationType\nfrom aadatapipelinecore.core.fs.driver import unified\n\ngranularity_list = ['m','w']\ndef get_path_list(granularity):\n    s3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-usage'))\n    path = s3_bucket_list.all(prefix=\"unified/city-level-usage.v1/fact/granularity={}\".format(granularity), depth_is_1=True)\n    # print path[0].split(\"=\")[-1].strip(\"/\")\n    # print [x for x in path[x].split(\"=\")[-1].strip(\"/\") ]\n    date_list = [(granularity, x.split(\"=\")[-1].strip(\"/\")) for x in path ]\n    return date_list\n### here you may need to update it with spark.read.format(\"delta\").......instead of unified.read.\ndef get_delta_lake_version(granularity, date):\n    urn = Urn(\n            namespace=\"aa.city-level-usage.v2\",\n            event=\"transform\",\n            data_type=DataType.FACT\n        )\n    df_v2 = unified.read(spark, urn,  sql_where=\"granularity='{}' and date='{}'\".format(granularity, date)).data_dto_list[0].dataframe\n    v2_data_count = df_v2.count()\n    return v2_data_count\ndef get_old_version(granularity, date):\n    df_v1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v1/fact/granularity={}/date={}/\".format(granularity, date))\n    return df_v1.count()\ndef compare(granularity, date):\n    count1 = get_delta_lake_version(granularity, date)\n    count2 = get_old_version(granularity, date)\n    if count1 != count2:\n        print 'Not equal!!!!! v1 : {}, v2 : {}, granularity: {}, date: {}'.format(count1, count2, granularity, date)\n    else:\n        print 'v1 : {}, v2 : {}, granularity: {}, date: {}'.format(count1, count2, granularity, date)\ntest_path = []\nfor g in granularity_list:\n    test_path.extend(get_path_list(g))\nfor case in test_path:\n    print case\n    compare(case[0],case[1])\n"]},{"cell_type":"code","execution_count":0,"id":"20200221-104045_1351327643","metadata":{},"outputs":[],"source":["%angular\n<h1>DB Check<h1>"]},{"cell_type":"code","execution_count":0,"id":"20200225-090037_1825628063","metadata":{},"outputs":[],"source":["%md\n## Unified Review With Topic"]},{"cell_type":"code","execution_count":0,"id":"20200226-111055_573517395","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(\n    spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-topic-join-review\"\n)\n# reload dependencies from temp\n# spark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200306-065624_1702015458","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\"\n"]},{"cell_type":"code","execution_count":0,"id":"20200306-065803_331263829","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2019 App Annie Inc. All rights reserved.\n\n\"\"\"\nAdvanced Review Database Check:\n    1. The number of data in \"unified layer\" is equal to elasticsearch\n\"\"\"\n\nfrom collections import defaultdict\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.table_common_info import urn\nfrom aadatapipelinecore.core.loader import es\n\n\nclass AdvancedReviewUnifiedData(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/\" \\\n                       \"unified/advancedreview.topic.v1/fact/process_date={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date):\n        \"\"\"\n        :param date: process_date in s3 bucket path\n        :type date: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(date))\n        return unified_df\n\n\nclass AdvancedReviewDBData(object):\n    \"\"\"\n    Get data from database (ElasticSearch)\n    \"\"\"\n    _common_config = {\"database\": \"adv_review\"}\n    _index_config = \"int-ss-advancedreview_v1*\"\n\n    def get(self, country_code, device_code, identifier):\n        \"\"\"\n        :return: elasticsearch result\n        :rtype: dict\n        \"\"\"\n        query_body = {\n            \"track_total_hits\": \"true\",\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\"match\": {\"country_code\": country_code}},\n                        {\"match\": {\"device_code\": device_code}},\n                        {\"match\": {\"_identifier\": identifier}}\n                    ]\n                }\n            }\n        }\n        es_conn = es.connection(urn, self._common_config)\n        es_data = es_conn.search(\n            index=self._index_config,\n            body=query_body)\n        return es_data\n\n\nclass TestTopicJoinReview(PipelineTest):\n    \"\"\"\n    Compare ES data amount with unified review data\n    \"\"\"\n    trigger_date_config = (\"10 10 * * *\", 2)\n\n    def setUp(self):\n        self.country_code_list = [\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n        self.device_code_list = [\"ios-all\", \"android-all\"]\n        self.granularity = 'daily'\n        self.failed_ids = defaultdict(list)\n\n    def _compare_diff(self, device_code, country_code, unified_df, identifier):\n        unified_count = unified_df.where(\n            \"country_code='{}' AND device_code='{}'\".format(country_code, device_code)).count()\n        es_data = AdvancedReviewDBData().get(country_code, device_code, identifier)\n        es_total = es_data[\"hits\"][\"total\"][\"value\"]\n        if not es_total == unified_count:\n            self.failed_ids[device_code].append(country_code)\n\n    def test_advanced_review_topic_join_review(self):\n        unified_df = AdvancedReviewUnifiedData(self.spark).get(self.check_date_str)\n        identifier = unified_df.collect()[0][\"_identifier\"]\n        for device_code in self.device_code_list:\n            for country_code in self.country_code_list:\n                self._compare_diff(device_code, country_code, unified_df, identifier)\n\n        self.assertTrue(len(self.failed_ids) == 0, self.failed_ids)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200303-103050_1392027548","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2019 App Annie Inc. All rights reserved.\n\n\"\"\"\nAdvanced Review Database Check:\n    1. The number of data in \"unified layer\" is equal to elasticsearch\n\"\"\"\nimport unittest\nfrom collections import defaultdict\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import etl_skip\nfrom applications.db_check_v1.common.table_common_info import urn\nfrom aadatapipelinecore.core.loader import es\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\n\n\nclass AdvancedReviewUnifiedData(object):\n    \"\"\"\n    Get data from unified s3 bucket\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/\" \\\n                      \"unified/advancedreview.topic.v1/fact/process_date={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date):\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(date))\n        return unified_df\n\n\nclass AdvancedReviewDBData(object):\n    _common_config = {\"database\": \"adv_review\"}\n    _index_config = \"int-ss-advancedreview_v1*\"\n\n    def __init__(self, country, device, identifier):\n        self.country_code = country\n        self.device_code = device\n        self.identifier = identifier\n\n    def get(self):\n        \"\"\"\n        :return: elasticsearch result\n        \"\"\"\n        query_body = {\n            \"track_total_hits\": \"true\",\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\"match\": {\"country_code\": self.country_code}},\n                        {\"match\": {\"device_code\": self.device_code}},\n                        {\"match\": {\"_identifier\": self.identifier}}\n                    ]\n                }\n            }\n        }\n        es_conn = es.connection(urn, self._common_config)\n        es_data = es_conn.search(\n            index=self._index_config,\n            body=query_body)\n        return es_data\n\n\nclass TestTopicJoinReview(PipelineTest):\n    \"\"\"\n    Compare ES data amount with unified review data\n    \"\"\"\n    # Only check today's data\n    trigger_date_config = (\"0 4 * * *\", 1)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.country_code_list = [\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n        self.device_code_list = [\"ios-all\", \"android-all\"]\n        self.granularity = 'daily'\n        self.unified_df = AdvancedReviewUnifiedData(self.spark).get(self.check_date)\n        self.identifier = self.unified_df.collect()[0][\"_identifier\"]\n        self.failed_ids = defaultdict(list)\n\n    def _compare_diff(self, device_code, country_code, unified_df):\n        unified_count = unified_df.where(\n            \"country_code='{}' AND device_code='{}'\".format(country_code, device_code)).count()\n        print \"unified_count\", unified_count\n        es_data = AdvancedReviewDBData(country_code, device_code, self.identifier).get()\n        es_total = es_data[\"hits\"][\"total\"][\"value\"]\n        if not es_total == unified_count:\n            self.failed_ids[device_code].append(country_code)\n\n    def test_topic_join_review(self):\n        \"\"\"\n        :return: None\n        \"\"\"\n        unified_df = AdvancedReviewUnifiedData(self.spark).get(self.check_date)\n        for device_code in self.device_code_list:\n            for country_code in self.country_code_list:\n                self._compare_diff(device_code, country_code, unified_df)\n\n        self.assertTrue(len(self.failed_ids) == 0, self.failed_ids)\n\n\nsuite = unittest.TestSuite()\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestTopicJoinReview))\n"]},{"cell_type":"code","execution_count":0,"id":"20200302-100003_1886202038","metadata":{},"outputs":[],"source":["\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\n\ntrigger_date_config = (\"0 4 * * *\", 1)\ncheck_date = _get_date_from_refresh_routing_config(trigger_date_config)\nprint check_date\nunified_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date={}\".format(check_date))\nprint unified_df.distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200306-035128_514042195","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\""]},{"cell_type":"code","execution_count":0,"id":"20200303-115007_1884762300","metadata":{},"outputs":[],"source":["\ntype(unified_df)"]},{"cell_type":"code","execution_count":0,"id":"20200303-111011_1673269270","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom collections import defaultdict\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import etl_skip\nfrom applications.db_check_v1.common.table_common_info import urn\nfrom aadatapipelinecore.core.loader import es\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\n\nclass AdvancedReviewDBData(object):\n    _common_config = {\"database\": \"adv_review\"}\n    _index_config = \"int-ss-advancedreview_v1*\"\n\n    def __init__(self, device, identifier):\n        self.country_code = country\n        self.device_code = device\n        self.identifier = identifier\n\n    def get(self):\n        \"\"\"\n        :return: elasticsearch result\n        \"\"\"\n        query_body = {\n            \"track_total_hits\": \"true\",\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\"match\": {\"device_code\": self.device_code}},\n                        {\"match\": {\"_identifier\": self.identifier}}\n                    ]\n                }\n            }\n        }\n        es_conn = es.connection(urn, self._common_config)\n        es_data = es_conn.search(\n            index=self._index_config,\n            body=query_body)\n        return es_data\n\nprint AdvancedReviewDBData()"]},{"cell_type":"code","execution_count":0,"id":"20200302-123257_1595483286","metadata":{},"outputs":[],"source":["\nimport unittest\n\nclass TestNotebook(unittest.TestCase):\n\n    def test_add(self):\n        self.assertEqual(add(2, 2), 4)\n\n\nunittest.main(argv=[''], verbosity=2)"]},{"cell_type":"code","execution_count":0,"id":"20200303-043354_900568031","metadata":{},"outputs":[],"source":["\nspark = create_spark()\nunified_review = spark.read.format(\"delta\").load(\n    \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\")\n# Only check today's data\ntrigger_date_config = (\"0 4 * * *\", 1)\ncheck_date = _get_date_from_refresh_routing_config(trigger_date_config)\ndata_num_at_process_date = unified_review.where(\n    \"process_date='{}'\".format(check_date)).distinct().count()\n# if data_num_at_process_date == 0:\n#     trigger_date_config = (\"0 4 * * *\", 2)\n#     check_date = _get_date_from_refresh_routing_config(trigger_date_config)\nes_identifier = unified_review.where(\n    \"process_date='{}'\".format(check_date)).collect()[0]._identifier\n"]},{"cell_type":"code","execution_count":0,"id":"20200303-043444_1293467183","metadata":{},"outputs":[],"source":["\nprint check_date, data_num_at_process_date, es_identifier"]},{"cell_type":"code","execution_count":0,"id":"20200302-083821_1821557334","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2019 App Annie Inc. All rights reserved.\n\n\"\"\"\nAdvanced Review Database Check:\n    1. The number of data in \"unified layer\" is equal to elasticsearch\n\"\"\"\n\nimport unittest\nfrom collections import defaultdict\nfrom aadatapipelinecore.core.utils.spark import create_spark\nfrom applications.db_check_v1.common.table_common_info import urn\nfrom aadatapipelinecore.core.loader import es\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\n\n\nclass TestESdata(unittest.TestCase):\n    \"\"\"\n    Compare ES data amount with unified review data\n    \"\"\"\n    spark = create_spark()\n    unified_review = spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\")\n    # Only check today's data\n    trigger_date_config = (\"0 4 * * *\", 1)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n    data_num_at_process_date = unified_review.where(\n        \"process_date='{}'\".format(check_date)).distinct().count()\n    if data_num_at_process_date == 0:\n        trigger_date_config = (\"0 4 * * *\", 2)\n        check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.country_code_list = [\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n        self.device_code_list = [\"ios-all\", \"android-all\"]\n        self.granularity = 'daily'\n        self.es_identifier = self.unified_review.where(\n            \"process_date='{}'\".format(self.check_date)).collect()[0][\"_identifier\"]\n\n    def test_es_data(self):\n        \"\"\"\n        :return: None\n        \"\"\"\n\n        failed_ids = defaultdict(list)\n\n        for device_code in self.device_code_list:\n            for country_code in self.country_code_list:\n                unified_count = self.unified_review.where(\n                    \"process_date='{}' AND country_code='{}' AND device_code='{}'\".format(\n                        self.check_date, country_code, device_code)).count()\n                query_body = {\n                    \"track_total_hits\": \"true\",\n                    \"query\": {\n                        \"bool\": {\n                            \"must\": [\n                                {\"match\": {\"country_code\": country_code}},\n                                {\"match\": {\"device_code\": device_code}},\n                                {\"match\": {\"_identifier\": self.es_identifier}}\n                            ]\n                        }\n                    }\n                }\n                es_data = self.get_es_db(query_body)\n                es_count = es_data[\"hits\"][\"total\"][\"value\"]\n                if not es_count == unified_count:\n                    failed_ids[device_code].append(country_code)\n\n        self.assertTrue(len(failed_ids) == 0, failed_ids)\n\n    def get_es_db(self, query_body):\n        \"\"\"\n        :param query_body: elasticsearch query script\n        :return: elasticsearch result\n        \"\"\"\n        common_config = {\"database\": \"adv_review\"}\n        es_conn = es.connection(urn, common_config)\n        es_data = es_conn.search(\n            index=\"int-ss-advancedreview_v1*\",\n            body=query_body)\n        return es_data\n\n\n\nunittest.main(argv=[''], verbosity=2, exit=False)"]},{"cell_type":"code","execution_count":0,"id":"20200224-070847_137026908","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date=2020-02-12/"]},{"cell_type":"code","execution_count":0,"id":"20200228-102659_1084460281","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join/process_date=2020-02-25/\""]},{"cell_type":"code","execution_count":0,"id":"20200228-103047_565486925","metadata":{},"outputs":[],"source":["\ntry:\n    df_test = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join/process_date=2020-02-25/\")\nexcept:\n    pass\nprint df_test.collect()\n"]},{"cell_type":"code","execution_count":0,"id":"20200303-094334_763226873","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/\"\n"]},{"cell_type":"code","execution_count":0,"id":"20200303-094027_137390055","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date=2020-02-26/\"\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join/\""]},{"cell_type":"code","execution_count":0,"id":"20200309-021412_1617192903","metadata":{},"outputs":[],"source":["\nnot_join_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join/process_date=2020-03-07/\")\nnot_join_df.count()"]},{"cell_type":"code","execution_count":0,"id":"20200227-040038_1573745493","metadata":{},"outputs":[],"source":["\nunified_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date=2020-02-26\").toPandas()\nunified_df = unified_df.drop([\"_identifier\"], axis=1)\nprint unified_df"]},{"cell_type":"code","execution_count":0,"id":"20200224-115108_39603451","metadata":{},"outputs":[],"source":["\ndf_unified_review.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200309-021127_1946876613","metadata":{},"outputs":[],"source":["%md\n# ADV REVIEW DB CHECK\n"]},{"cell_type":"code","execution_count":0,"id":"20200224-092416_1412312811","metadata":{},"outputs":[],"source":["\n\"\"\"\n@param: df_unified_review DataFrame\n\"\"\"\n# adv_review = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/process_date=2019-07-01/\")\ndf_unified_review = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\")\ntype(df_unified_review)\n"]},{"cell_type":"code","execution_count":0,"id":"20200225-093312_383805792","metadata":{},"outputs":[],"source":["\ndf_unified_review.where(\"review_id='5580841379'\").show()\n# df_unified_review.select(df_unified_review.review_id, df_unified_review._identifier.between(120200225075429899, 120200225075430000)).where(\"process_date='2020-02-24'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200227-101258_1315971102","metadata":{},"outputs":[],"source":["\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\nimport datetime\n\n# 2 days less than now\n# process_date = datetime.datetime.today() - datetime.timedelta(days=2)\n\n# print \"process_date\", process_date.__repr__()\n\n# print \"year=\", process_date.year\n\n# Only check today's data\nprocess_date_config = (\"10 10 * * *\", 1)\n# es_identifier_date_config = (\"0 4 * * *\", 1)\nprocess_date = _get_date_from_refresh_routing_config(process_date_config)\nprint \"Process Date =\\t\\t\", process_date.__repr__()\n\n# Number of data at process_date\ndate_num_at_process_date = df_unified_review.where(\"process_date='{}'\".format(process_date)).distinct().count()\nprint \"Unified Review Count =\\t\", date_num_at_process_date.__repr__()\n\n\n# es_identifier_date = _get_date_from_refresh_routing_config(es_identifier_date_config)\n# print \"ES Identifier Date =\\t\", es_identifier_date.__repr__()\n\nes_identifier = df_unified_review.where(\"process_date='{}'\".format(process_date)).collect()[0]._identifier\n\nprint \"es_identifier =\\t\\t\", es_identifier\n\n# check _identifier\n# print df_unified_review.groupby(\"_identifier\").collect().show()\n\n# es_identifier_date_digit = \"\"\n# for i in es_identifier_date:\n#     if i.isdigit():\n#         es_identifier_date_digit += i\n# es_identifier_lower = \"1{}000000000\".format(es_identifier_date_digit)\n# es_identifier_upper = \"1{}999999999\".format(es_identifier_date_digit)\n\n# print es_identifier_lower.__repr__()\n# print es_identifier_upper.__repr__()\n# {\n#   \"track_total_hits\": \"true\", \n#   \"query\": {\n#     \"bool\": {\n#       \"must\": [\n#         {\"match\": {\"country_code\": country_code}},\n#         {\"match\": {\"device_code\": device_code}}\n#       ],\n#       \"filter\": {\"range\": {\n#         \"_identifier\": {\n#           \"gte\": es_identifier_lower,\n#           \"lte\": es_identifier_upper\n#         }\n#       }}\n#     }\n#   }\n# }"]},{"cell_type":"code","execution_count":0,"id":"20200227-095945_1980737889","metadata":{},"outputs":[],"source":["\nfrom collections import defaultdict\nfrom applications.db_check_v1.common.table_common_info import urn\nfrom aadatapipelinecore.core.loader import es\n\n\nclass TestESdata():\n\n    def es_data_check(self):\n        country_code_list = [\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n        device_code_list = [\"ios-all\", \"android-all\"]\n        failed_es_data_list = []\n        failed_list = []\n        sum = 0\n        \n        for device_code in device_code_list:\n            for country_code in country_code_list:\n                unified_count = df_unified_review.where(\"process_date='{}' AND country_code='{}' AND device_code='{}'\".format(process_date, country_code, device_code)).count()\n                query_body = {\n                              \"track_total_hits\": \"true\", \n                              \"query\": {\n                                \"bool\": {\n                                  \"must\": [\n                                    {\"match\": {\"country_code\": country_code}},\n                                    {\"match\": {\"device_code\": device_code}},\n                                    {\"match\": {\"_identifier\": es_identifier}}\n                                  ]\n                                }\n                              }\n                            }\n                res = self.get_es_db(country_code, query_body)\n                es_count = res[\"hits\"][\"total\"][\"value\"]\n                if not es_count == unified_count:\n                    failed_list.append((country_code, device_code))\n                    failed_es_data_list.append(res)\n                sum += es_count\n        \n        return sum, failed_list, failed_es_data_list\n\n    def get_es_db(self, country_code, query_body):\n        common_config = { \"database\": \"adv_review\" }\n        es_conn = es.connection(urn, common_config)\n        res = es_conn.search(\n            index=\"int-ss-advancedreview_v1*\",\n            body=query_body)\n        return res\n        \n\nes_total_count, failed_list, es_data = TestESdata().es_data_check()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200227-075951_489396392","metadata":{},"outputs":[],"source":["\nprint es_total_count, failed_list\nprint len(es_data)\nprint \"diff = \", date_num_at_process_date - es_total_count\n\n# ('US', 'ios-all') ==> es_data[0]\n# TODO: loop\nreview_id_dic = {}\nfor i in range(len(failed_list)):\n    review_id_list = df_unified_review.where(\"country_code='{}' AND device_code='{}' AND process_date='{}'\".format(failed_list[i][0], failed_list[i][1], process_date)).collect()\n    review_id_list = [row[\"review_id\"] for row in review_id_list]\n    review_id_dic[failed_list[i]] = review_id_list\n# print review_id_dic[('GB', 'ios-all')][:10]\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200225-094608_1791584943","metadata":{},"outputs":[],"source":["\nfrom collections import defaultdict\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\nfrom applications.db_check_v1.common.table_common_info import urn\nfrom aadatapipelinecore.core.loader import es\n\nclass FetchData():\n\n    trigger_date_config = (\"10 10 * * *\", 2)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n    print check_date\n\n    granularity = 'daily'\n\n    def get_es_data(self, review_id):\n        # review_id = 5571421762\n        query_body = {\n                      \"query\": {\n                        \"bool\": {\n                          \"must\": [\n                                      { \"match\": { \"country_code\": \"US\" } },\n                                      { \"match\": { \"device_code\": \"ios-all\"}}\n                            ]\n                        }\n                      }\n                    }\n        res = self.connect_es(query_body)\n        review_count = len(res[\"hits\"][\"hits\"])\n        if review_count == 1:\n            return True\n        return \"debug: count: {}, res: {}\".format(review_count, res)\n\n    def connect_es(self, query_body):\n        common_config = { \"database\": \"app_ss\" }\n        es_conn = es.connection(urn, common_config)\n        res = es_conn.search(\n            index=\"int-ss-advancedreview_v1*\",\n            # index=\"_all\",\n            body=query_body)\n        return res\n\n\nes_raw_data = FetchData().get_es_data(5571421762)\nes_raw_data\n\ndef test():\n    review_ids = [row[\"review_id\"] for row in df_unified_review.where(\"process_date='2020-02-24'\").collect()]\n    # review_ids = review_ids[:100]\n    for review_id in review_ids:\n        in_es = FetchData().get_es_data(review_id)\n        if not in_es:\n            return in_es\n    return \"Success\"\n\ntest_res = test()"]},{"cell_type":"code","execution_count":0,"id":"20200225-033403_104375604","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2019 App Annie Inc. All rights reserved.\n\n\"\"\"\nTest Advanced Review's tables\n\"\"\"\n\nimport unittest\nfrom collections import defaultdict\nfrom applications.db_check_v1.common.constants import query, citus_settings\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\n\n\nclass TestAdvancedReview(unittest.TestCase):\n\n    trigger_date_config = (\"* * * * *\", 2)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.table_list = [\"topic\", \"term\"]\n        self.granularity = 'd'\n        self.countries_list = [\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n\n    def check_advanced_review(self):\n        for table in self.table_list:\n            self.advanced_review_db_check_test(table)\n\n    def advanced_review_db_check_test(self, table):\n        device_code = 'ios-all'\n\n        failed_ids = defaultdict(list)\n        for country_code in self.countries_list:\n            advanced_review_db = self.get_advanced_review_date(table, country_code)\n            if not advanced_review_db[0][0]:\n                failed_ids[device_code].append(country_code)\n        self.assertTrue(len(failed_ids) == 0, failed_ids)\n\n    def get_advanced_review_date(self, table, country_code):\n        sql = '''\n        select count(1) from advancedreview.advancedreview_{table}_fact_v3 where date = '{date}'\n                and country_code = '{country_code}';\n        '''.format(table=table, date=self.check_date, country_code=country_code)\n        result = query(citus_settings(\"aa\"), sql)\n        return result\n\n    def test_advanced_review_daily(self):\n        self.check_advanced_review()\n\nunittest.main(argv=[''], verbosity=2)"]},{"cell_type":"code","execution_count":0,"id":"20200303-061419_332754553","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom applications.db_check_v1.cases.advanced_review.test_topic_join_review import TestTopicJoinReview\n\ndef suite():\n    suite = unittest.TestSuite()\n    suite.addTest(TestTopicJoinReview('test_advanced_review_topic_join_review'))\n    return suite\n\nif __name__ == '__main__':\n    runner = unittest.TextTestRunner(verbosity=2)\n    runner.run(suite())"]},{"cell_type":"code","execution_count":0,"id":"20200311-022754_2090935024","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom applications.db_check_v1.cases.advanced_review.test_topic_join_review import TestTopicJoinReview\n\ndef suite():\n    suite = unittest.TestSuite()\n    suite.addTest(TestTopicJoinReview('test_advanced_review_topic_join_review'))\n    return suite\n\nif __name__ == '__main__':\n    runner2 = unittest.TextTestRunner(verbosity=2)\n    runner2.run(suite())"]},{"cell_type":"code","execution_count":0,"id":"20200311-023647_2093777331","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}