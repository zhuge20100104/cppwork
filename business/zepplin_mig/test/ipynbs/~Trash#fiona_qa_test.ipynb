{"cells":[{"cell_type":"code","execution_count":0,"id":"20190904-070503_1072752255","metadata":{},"outputs":[],"source":["\n#raw data\n\ndf1=spark.read.parquet(\"s3://prod.appannie.mktint.data/oss/SEARCH_VOLUME/routine/granularity=daily/date=2019-10-23/platform=1/\").filter(\"search_volume>=100 and country_code='US'\")#.filter(\"keyword_id=5198892 and country_code='NL' and platform=1 and device_id=0 \")#.show(20,False)\ndf1.show()\n# df2=spark.read.parquet(\"s3://prod.appannie.mktint.data/oss/KEYWORD_DIFFICULTY/routine/granularity=daily/date=2018-07-07/platform=1/\").filter(\"keyword_id='4999617' and country_code='US'\")#.filter(\"keyword_id=5198892 and country_code='NL' and platform=1 and device_id=0 \")#.show(20,False)\n# df2.show()\n# result=df1.join(df2, df1.keyword_id==df2.keyword_id, 'outer').show()\n\n\n# df2=spark.read.parquet(\"s3://prod.appannie.mktint.data/oss/KEYWORD_DIFFICULTY/routine/granularity=daily/date=2018-07-07/platform=1/\").filter(\"keyword_id='4999617' and country_code='US'\")#.filter(\"keyword_id=5198892 and country_code='NL' and platform=1 and device_id=0 \")#.show(20,False)\n\n# print result\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-aso/unified/app-tech.aso.keyword-difficulty-search-volume.v1/fact/granularity=daily/date=2017-07-06\").filter(\"device_code='android-all' and dev\").show(20,False)\n\n\n\n# df2=spark.read.parquet(\"s3://prod.appannie.mktint.data/oss/KEYWORD_DIFFICULTY/routine/granularity=daily/date=2016-10-11/\").filter(\"country_code='US' and platform=2 and device_id=1 and difficulty is null \").count()\n# print df2\n\n\n# df1=spark.read.parquet(\"s3://prod.appannie.mktint.data/oss/SEARCH_VOLUME/routine/granularity=daily/date=2017-07-06/platform=1/\").filter(\"country_code='us'\").show()#.filter(\"keyword_id=5198892 and country_code='NL' and platform=1 and device_id=0 \")#.show(20,False)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20190916-053537_790189680","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom random import choice, randint\n\nfrom aaintdatapipeline.application.app_qa.common.utils import get_month_start_end_date\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import IOS_PHONE, IOS_TABLET, \\\n    ANDROID_ALL, ASO_TS_RAW_DATA_HEAD\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.pysparktest import PySparkTest\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_random_date, get_aso_ts_raw_df, get_aso_svi_raw_df, get_aso_kd_raw_df,\n    check_parquet_exist, get_date_list_from_granularity, get_aso_ts_unified_data_path)\nfrom aaintdatapipeline.core.fs.device import S3Bucket\nfrom aaintdatapipeline.core.conf import Conf\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_aso_citus_connection, datetime_to_string)\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.pysparktest import PySparkTest\n\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import \\\n    urn\nfrom decimal import Decimal\nimport decimal\n\nmkt_new_bucket = \"b2c-prod-int-data-pipeline-unified-paid-aso\"\nts_unified_path = \\\n    \"s3://{}/unified/app-tech.market.aso-keyword-trafficshare.v1/fact/granularity=daily/date={}/device_code={}/\"\nsvi_kd_unified_path = \"s3://{}/unified/app-tech.market.aso-keyword-difficulty-search-volume.v1/fact/\" \\\n                      \"granularity=monthly/date={}/device_code={}/\"\n\n\ndate = \"2018-06-01\"\nplatform = \"1\"\ndevice_code=\"ios-tablet\"\n\naso_sv_kd_raw_bucket = \"prod.appannie.mktint.data\"\naso_sv_raw_bucket_path = \"s3://{}/oss/SEARCH_VOLUME/routine/granularity=daily/date={}/platform={}/\"\naso_kd_raw_bucket_path = \"s3://{}/oss/KEYWORD_DIFFICULTY/routine/granularity=daily/date={}/platform={}\"\n\n\naso_sv_kd_unified_bucket = \"b2c-prod-data-pipeline-unified-aso\"\naso_sv_kd_unified_bucket_path = \"s3://{}/unified/app-tech.aso.keyword-difficulty-search-volume.v1/fact/granularity=daily/date={}/device_code={}/\"\n\n\n\n\ndef get_aso_sv_raw_df_root(path, aso_sv_raw_bucket, date, platform):\n    return path.format(aso_sv_raw_bucket, date, platform)\n\ndef get_aso_kd_raw_df_root(path, aso_kd_raw_bucket, date, platform):\n    return path.format(aso_kd_raw_bucket, date, platform)\n\ndef get_aso_sv_kd_unified_df_root(path, aso_sv_kd_unified_bucket,  date, device_code):\n    return path.format(aso_sv_kd_unified_bucket, date,  device_code)\n\n\naso_sv_raw_parquet_path = get_aso_sv_raw_df_root(aso_sv_raw_bucket_path, aso_sv_kd_raw_bucket,\n                                                           date,\n                                                           platform)\naso_kd_raw_parquet_path = get_aso_kd_raw_df_root(aso_kd_raw_bucket_path, aso_sv_kd_raw_bucket,\n                                                           date,\n                                                           platform)\n\naso_sv_kd_unified_parquet_path = get_aso_sv_kd_unified_df_root(aso_sv_kd_unified_bucket_path,\n                                                                   aso_sv_kd_unified_bucket,\n                                                                   date,\n                                                                   device_code\n                                                                   )\ndef test_android_sv_kd_keyword_count():\n    print 'test android all sv keyword count with different country'\n    country_iterate=\"US\"\n    print country_iterate\n    expected_sv_count = spark.read.parquet(aso_sv_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=2 and device_id=1 and country_code='{}'\".format(\n            country_iterate)) #.count()\n    expected_kd_count = spark.read.parquet(aso_kd_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=2 and device_id=1 and country_code='{}'\".format(\n            country_iterate)) #.count()\n    \n    result=expected_sv_count.join(expected_kd_count, expected_sv_count.keyword_id==expected_kd_count.keyword_id, 'outer').select(expected_kd_count.keyword_id).collect()\n    print len(result)\n    unified_count = spark.read.parquet(aso_sv_kd_unified_parquet_path.split(\"device_code\")[0]).filter(\n        \"country_code='{}'and device_code='ios-phone'\".format(\n            country_iterate)).select(\n        \"keyword_id\").distinct().count()\n        \n    print unified_count\n    \n    \ndef test_android_sv_kd_keyword_count_citus():\n    country_code_android_list=['US']\n    print 'test android all sv-kd keyword count with different country'\n    for country_iterate in country_code_android_list:\n        print country_iterate\n        expected_sv_count = spark.read.parquet(aso_sv_raw_parquet_path.split(\"platform\")[0]).filter(\n            \"platform=1 and device_id=0 and country_code='{}'\".format(\n                country_iterate))\n        expected_kd_count = spark.read.parquet(aso_kd_raw_parquet_path.split(\"platform\")[0]).filter(\n            \"platform=1 and device_id=0 and country_code='{}'\".format(\n                country_iterate))\n        print expected_kd_count\n\n        result = expected_sv_count.join(expected_kd_count,\n                                        expected_sv_count.keyword_id == expected_kd_count.keyword_id,\n                                        'outer').select(expected_kd_count.keyword_id).collect()\n        print len(result)\n        sql = \"select count(distinct keyword_id) from aso.aso_sv_kd_fact_v1 where device_code='{}' and country_code='{}' and date='{}'\".format('android-all',country_iterate,date)\n        db_result = get_aso_citus_connection(urn, sql)\n\n        # unified_count = .spark.read.parquet(.aso_sv_kd_unified_parquet_path.split(\"device_code\")[0]).filter(\n        #     \"country_code='{}' and device_code='android-all'\".format(\n        #         country_iterate)).select(\n        #     \"keyword_id\").distinct().count()\n        print db_result\n    \n    # selassertNotEqual(expected_sv_count or expected_kd_count, 0)\n    # self.assertEqual(max(expected_sv_count, expected_kd_count), unified_count,\n    #                  msg='{}~{}'.format(max(expected_sv_count, expected_kd_count), unified_count))\n\nprint \"########################\"\n\nkeyword_id=1387\ndef test_android_sv_kd_value():\n    print 'test android all sv-kd keyword count with different country'\n    country_code = 'US'\n    expected_sv_count = spark.read.parquet(aso_sv_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=1 and device_id=0 and country_code='{}' and keyword_id={}\".format(\n            country_code,keyword_id))\n    expected_kd_count = spark.read.parquet(aso_kd_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=1 and device_id=0 and country_code='{}' and keyword_id={}\".format(\n        country_code, keyword_id))\n\n    result = expected_sv_count.join(expected_kd_count,\n                                    expected_sv_count.keyword_id == expected_kd_count.keyword_id,\n                                    'outer').collect()[:]\n\n    unified_result = spark.read.parquet(aso_sv_kd_unified_parquet_path.split(\"device_code\")[0]).filter(\n        \"country_code='{}' and device_code='android-all' and keyword_id={}\".format(\n            country_code, keyword_id)).collect()[:]\n            \n    print result\n    print unified_result\n    print result[0][\"search_volume\"]\n    print unified_result[0][\"search_volume\"]\n    print result[0][\"difficulty\"]\n    print unified_result[0][\"difficulty\"]\n    \nkeyword_id=1387\ndef test_android_sv_kd_value_citus():\n    print 'test android all sv-kd keyword count with different country'\n    sql = \"select search_volume, difficulty from aso.aso_sv_kd_fact_v1 where device_code='{}' and country_code='{}' and date='{}' and keyword_id='{}'\".format('android-all','US', date,keyword_id)\n\n    country_code = 'US'\n    expected_sv_count = spark.read.parquet(aso_sv_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=1 and device_id=0 and country_code='{}' and keyword_id={}\".format(\n            country_code,keyword_id))\n    expected_kd_count = spark.read.parquet(aso_kd_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=1 and device_id=0 and country_code='{}' and keyword_id={}\".format(\n        country_code, keyword_id))\n\n    result = expected_sv_count.join(expected_kd_count,\n                                    expected_sv_count.keyword_id == expected_kd_count.keyword_id,\n                                    'outer').collect()[:]\n    print result\n    print result[0][\"search_volume\"]\n    print type(result[0][\"search_volume\"])\n    print result[0][\"difficulty\"]\n\n    db_result = get_aso_citus_connection(urn, sql)\n    print db_result[0]\n\n    \n    print abs(decimal.Decimal(round(result[0][\"search_volume\"],6))-db_result[0][0])<0.01\n    print abs(decimal.Decimal(round(result[0][\"difficulty\"],1))-db_result[0][1])<0.01\n\n\n\n\nmkt_new_bucket = \"b2c-prod-data-pipeline-unified-aso\"\nconf = Conf(\n    bucket_name=mkt_new_bucket\n)\n\nstart_date = '2016-10-11'\nend_date = '2019-06-30'\n\ndef test_android_ts_date_period_completeness():\n    date_list = get_date_list_from_granularity(start_date, end_date, 'daily')\n    print date_list\n    path_fail, data_fail = dict(), dict()\n    path_fail[device_code], data_fail[device_code] = list(), list()\n    for date in date_list:\n        unified_parquet_path = \"unified/app-tech.aso.keyword-difficulty-search-volume.v1/fact/granularity=daily/date={}\".format(date) #get_aso_ts_unified_data_path(date, device_code)\n        files = S3Bucket(conf=conf).all(unified_parquet_path)\n        if files:\n            if check_parquet_exist(files):\n                data_fail[device_code].append(date)\n        else:\n            path_fail[device_code].append(date)\n    if not path_fail[device_code]:\n        path_fail.pop(device_code)\n    if not data_fail[device_code]:\n        data_fail.pop(device_code)\n        \n    print data_fail\n    print path_fail\n\n\ndef test_android_ts_date_period_completeness_citus():\n    date_list = get_date_list_from_granularity(start_date, end_date, 'daily')\n    #print date_list\n    path_fail, data_fail = dict(), dict()\n    path_fail[device_code], data_fail[device_code] = list(), list()\n    \n    sql = \"select distinct date from aso.aso_sv_kd_fact_v1 where device_code='android-all'\"\n    db_result = get_aso_citus_connection(urn, sql)\n\n    acutal_db_result=[]\n    for item in db_result:\n        acutal_db_result.append(str(item[0]))\n        \n        \n    print list(set(date_list).difference(set(acutal_db_result)))\n\n\ndef test_kd_do_not_have_zero():\n    print 'test no Zero value in keyword diffculity'\n    sql = \"select * from aso_sv_kd_fact_v1 where device_code = 'android-all' and difficulty = 0\"\n    db_result = get_aso_citus_connection(urn, sql)\n    print db_result\n\n# test_android_sv_kd_keyword_count()\n# test_android_sv_kd_keyword_value()\n# test_android_ts_date_period_completeness()\ntest_kd_do_not_have_zero()\n"]},{"cell_type":"code","execution_count":0,"id":"20190904-103101_1541713437","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom random import choice, randint\n\nfrom aaintdatapipeline.application.app_qa.common.utils import get_month_start_end_date\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import IOS_PHONE, IOS_TABLET, \\\n    ANDROID_ALL, ASO_TS_RAW_DATA_HEAD\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.pysparktest import PySparkTest\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_random_date, get_aso_ts_raw_df, get_aso_svi_raw_df, get_aso_kd_raw_df,\n    check_parquet_exist, get_date_list_from_granularity, get_aso_ts_unified_data_path)\nfrom aaintdatapipeline.core.fs.device import S3Bucket\nfrom aaintdatapipeline.core.conf import Conf\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_aso_citus_connection, datetime_to_string)\n\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import \\\n    urn\n\n\nmkt_new_bucket = \"b2c-prod-int-data-pipeline-unified-paid-aso\"\nts_unified_path = \\\n    \"s3://{}/unified/app-tech.market.aso-keyword-trafficshare.v1/fact/granularity=daily/date={}/device_code={}/\"\nsvi_kd_unified_path = \"s3://{}/unified/app-tech.market.aso-keyword-difficulty-search-volume.v1/fact/\" \\\n                      \"granularity=monthly/date={}/device_code={}/\"\n\n\ndate = \"2018-06-01\"\nplatform = \"1\"\ndevice_code=\"ios-tablet\"\n\naso_sv_kd_raw_bucket = \"prod.appannie.mktint.data\"\naso_sv_raw_bucket_path = \"s3://{}/oss/SEARCH_VOLUME/routine/granularity=daily/date={}/platform={}/\"\naso_kd_raw_bucket_path = \"s3://{}/oss/KEYWORD_DIFFICULTY/routine/granularity=daily/date={}/platform={}\"\n\n\naso_sv_kd_unified_bucket = \"b2c-prod-data-pipeline-unified-aso\"\naso_sv_kd_unified_bucket_path = \"s3://{}/unified/app-tech.aso.keyword-difficulty-search-volume.v1/fact/granularity=daily/date={}/device_code={}/\"\n\n\n\n\ndef get_aso_sv_raw_df_root(path, aso_sv_raw_bucket, date, platform):\n    return path.format(aso_sv_raw_bucket, date, platform)\n\ndef get_aso_kd_raw_df_root(path, aso_kd_raw_bucket, date, platform):\n    return path.format(aso_kd_raw_bucket, date, platform)\n\ndef get_aso_sv_kd_unified_df_root(path, aso_sv_kd_unified_bucket,  date, device_code):\n    return path.format(aso_sv_kd_unified_bucket, date,  device_code)\n\n\naso_sv_raw_parquet_path = get_aso_sv_raw_df_root(aso_sv_raw_bucket_path, aso_sv_kd_raw_bucket,\n                                                           date,\n                                                           platform)\naso_kd_raw_parquet_path = get_aso_kd_raw_df_root(aso_kd_raw_bucket_path, aso_sv_kd_raw_bucket,\n                                                           date,\n                                                           platform)\n\naso_sv_kd_unified_parquet_path = get_aso_sv_kd_unified_df_root(aso_sv_kd_unified_bucket_path,\n                                                                   aso_sv_kd_unified_bucket,\n                                                                   date,\n                                                                   device_code\n                                                                   )\ndef test_android_sv_kd_keyword_count():\n    print 'test android all sv keyword count with different country'\n    country_iterate=\"US\"\n    print country_iterate\n    expected_sv_count = spark.read.parquet(aso_sv_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=2 and device_id=1 and country_code='{}'\".format(\n            country_iterate)) #.count()\n    expected_kd_count = spark.read.parquet(aso_kd_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=2 and device_id=1 and country_code='{}'\".format(\n            country_iterate)) #.count()\n    \n    result=expected_sv_count.join(expected_kd_count, expected_sv_count.keyword_id==expected_kd_count.keyword_id, 'outer').select(expected_kd_count.keyword_id).collect()\n    print len(result)\n    unified_count = spark.read.parquet(aso_sv_kd_unified_parquet_path.split(\"device_code\")[0]).filter(\n        \"country_code='{}'and device_code='ios-phone'\".format(\n            country_iterate)).select(\n        \"keyword_id\").distinct().count()\n        \n    print unified_count\n    \n    \ndef test_android_sv_kd_keyword_count_citus():\n    country_code_android_list=['US']\n    print 'test android all sv-kd keyword count with different country'\n    for country_iterate in country_code_android_list:\n        print country_iterate\n        expected_sv_count = spark.read.parquet(aso_sv_raw_parquet_path.split(\"platform\")[0]).filter(\n            \"platform=1 and device_id=0 and country_code='{}'\".format(\n                country_iterate))\n        expected_kd_count = spark.read.parquet(aso_kd_raw_parquet_path.split(\"platform\")[0]).filter(\n            \"platform=1 and device_id=0 and country_code='{}'\".format(\n                country_iterate))\n        print expected_kd_count\n\n        result = expected_sv_count.join(expected_kd_count,\n                                        expected_sv_count.keyword_id == expected_kd_count.keyword_id,\n                                        'outer').select(expected_kd_count.keyword_id).collect()\n        print len(result)\n        sql = \"select count(distinct keyword_id) from aso.aso_sv_kd_fact_v1 where device_code='{}' and country_code='{}' and date='{}'\".format('android-all',country_iterate,date)\n        db_result = get_aso_citus_connection(urn, sql)\n\n        # unified_count = .spark.read.parquet(.aso_sv_kd_unified_parquet_path.split(\"device_code\")[0]).filter(\n        #     \"country_code='{}' and device_code='android-all'\".format(\n        #         country_iterate)).select(\n        #     \"keyword_id\").distinct().count()\n        print db_result\n    \n    # selassertNotEqual(expected_sv_count or expected_kd_count, 0)\n    # self.assertEqual(max(expected_sv_count, expected_kd_count), unified_count,\n    #                  msg='{}~{}'.format(max(expected_sv_count, expected_kd_count), unified_count))\n\nprint \"########################\"\n\nkeyword_id=1387\ndef test_android_sv_kd_keyword_value():\n    print 'test android all sv-kd keyword count with different country'\n    country_code = 'US'\n    expected_sv_count = spark.read.parquet(aso_sv_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=1 and device_id=0 and country_code='{}' and keyword_id={}\".format(\n            country_code,keyword_id))\n    expected_kd_count = spark.read.parquet(aso_kd_raw_parquet_path.split(\"platform\")[0]).filter(\n        \"platform=1 and device_id=0 and country_code='{}' and keyword_id={}\".format(\n        country_code, keyword_id))\n\n    result = expected_sv_count.join(expected_kd_count,\n                                    expected_sv_count.keyword_id == expected_kd_count.keyword_id,\n                                    'outer').collect()[:]\n\n    unified_result = spark.read.parquet(aso_sv_kd_unified_parquet_path.split(\"device_code\")[0]).filter(\n        \"country_code='{}' and device_code='android-all' and keyword_id={}\".format(\n            country_code, keyword_id)).collect()[:]\n            \n    print result\n    print unified_result\n    print result[0][\"search_volume\"]\n    print unified_result[0][\"search_volume\"]\n    print result[0][\"difficulty\"]\n    print unified_result[0][\"difficulty\"]\n\nmkt_new_bucket = \"b2c-prod-data-pipeline-unified-aso\"\nconf = Conf(\n    bucket_name=mkt_new_bucket\n)\n\nstart_date = '2016-10-11'\nend_date = '2019-06-30'\n\ndef test_android_ts_date_period_completeness():\n    date_list = get_date_list_from_granularity(start_date, end_date, 'daily')\n    print date_list\n    path_fail, data_fail = dict(), dict()\n    path_fail[device_code], data_fail[device_code] = list(), list()\n    for date in date_list:\n        unified_parquet_path = \"unified/app-tech.aso.keyword-difficulty-search-volume.v1/fact/granularity=daily/date={}\".format(date) #get_aso_ts_unified_data_path(date, device_code)\n        files = S3Bucket(conf=conf).all(unified_parquet_path)\n        if files:\n            if check_parquet_exist(files):\n                data_fail[device_code].append(date)\n        else:\n            path_fail[device_code].append(date)\n    if not path_fail[device_code]:\n        path_fail.pop(device_code)\n    if not data_fail[device_code]:\n        data_fail.pop(device_code)\n        \n    print data_fail\n    print path_fail\n        \n# test_android_sv_kd_keyword_count()\n# test_android_sv_kd_keyword_value()\n# test_android_ts_date_period_completeness()\ntest_android_sv_kd_keyword_count_citus()"]},{"cell_type":"code","execution_count":0,"id":"20190904-070634_942803095","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://prod_appannie_appletv/country-ranks/\n\n# aws s3 ls s3://prod.appannie.mktint.data/oss/SEARCH_VOLUME/routine/granularity=daily/date=2019-10-23/platform=1/\n# aws s3 ls s3://prod.appannie.mktint.data/oss/SEARCH_VOLUME/routine/granularity=daily/date=2019-07-01/platform=1/\n\n# # aws s3 ls s3://prod.appannie.mktint.data/oss/KEYWORD_DIFFICULTY/routine/granularity=daily/date=2019-07-01/platform=1/\n \n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-aso/unified/app-tech.aso.keyword-difficulty-search-volume.v1/fact/granularity=daily/date=2019-10-23/device_code=android-all/\naws s3 ls s3://b2c-prod-data-pipeline-raw-aso/unified/app-tech.aso.keyword-difficulty-search-volume.v1/fact/granularity=daily/date=2019-10-23/device_code=android-all/\n\n\n# # %%sh\n# PGPASSWORD='rn*Wh%osCl2C' psql -h internal-aa-citus-mkt-elb-1511434527.us-east-1.elb.amazonaws.com  -U citus_mkt_qa -d aa_mkt_db -p 7432 << EOF \n# set search_path=aso;\n# select * from aso_sv_kd_fact_v1 where device_code='android-all' and date='2019-10-23'\n\n# EOF \n\n"]},{"cell_type":"code","execution_count":0,"id":"20190821-033349_43207708","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import round\n# raw data:app_type=SINGLE_APP/\n# aws s3 ls s3://prod.appannie.mktint.data/oss/SEARCH_ADS_KEYWORD_SOV/routine/granularity=daily/date=2019-07-26/\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-aso/unified/app-tech.aso.sov-search-ads-keyword-fact.v1/metric/granularity=daily/date=2019-07-25/\n\n\n# aws s3 ls s3://prod.appannie.mktint.data/oss/SEARCH_ADS_KEYWORD_SOV/routine/granularity=daily/date=2019-08-08/platform=1/app_type=SINGLE_APP/\n \n# aws s3 ls s3://prod.appannie.aso.paid.product/oss/DAY/2018-09-27/TS/2/\n\nspark.read.csv(\"s3://prod.appannie.aso.paid.product/oss/DAY/2018-10-27/TS/2/\",sep=\"\\t\").filter(\"_c2='UA' and  _c1=1 and _c0=2\").select(\"_c3\" , round('_c6',6).alias('r')).filter(\"r!=0\").select(\"_c3\").distinct().show()\n\n\nspark.read.parquet(\"s3://b2c-prod-int-data-pipeline-unified-paid-aso/unified/app-tech.market.aso-keyword-trafficshare.v1/fact/granularity=daily/date=2018-10-27/device_code=ios-phone/\").filter(\"country_code='UA'\").show(20,False)\n\n# aws s3 ls s3://b2b-prod-int-data-pipeline-metastore/meta/staging-events/app-tech.aso.sov-search-ads-keyword-fact.v1/2019/08/21/08/\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20190905-085808_1344307605","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom random import choice, randint\nfrom aaintdatapipeline.application.app_qa.common.utils import get_date_list\n\nfrom aaintdatapipeline.application.app_qa.common.utils import get_month_start_end_date\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import IOS_PHONE, IOS_TABLET, \\\n    ANDROID_ALL, ASO_TS_RAW_DATA_HEAD\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import \\\n    urn\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.pysparktest import PySparkTest\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_random_date, get_aso_ts_raw_df, get_aso_svi_raw_df, get_aso_kd_raw_df,\n    check_parquet_exist, get_date_list_from_granularity, get_aso_ts_unified_data_path, get_aso_est_unified_data_path,\n    get_aso_citus_connection,datetime_to_string)\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import \\\nAPP_STORE_RANK_TABLE_NAME, urn, COUNTRY_CODE_CHECK_LIST, ios_devices\n\nfrom aaintdatapipeline.core.conf import Conf\nfrom aaintdatapipeline.core.fs.device import S3Bucket\nmkt_new_bucket = \"b2c-prod-int-data-pipeline-unified-paid-aso\"\nts_unified_path = \\\n    \"s3://{}/unified/app-tech.market.aso-keyword-trafficshare.v1/fact/granularity=daily/date={}/device_code={}/\"\nsvi_kd_unified_path = \"s3://{}/unified/app-tech.market.aso-keyword-difficulty-search-volume.v1/fact/\" \\\n                      \"granularity=monthly/date={}/device_code={}/\"\nconf = Conf(\n    bucket_name=mkt_new_bucket\n)\n\ntraffic_share_table=\"aso.aso_keyword_trafficshare_fact_v1\"\nstart_date = '2017-01-01'\nend_date = '2018-12-31'\ndate = get_random_date(start_date, end_date)\nrandom_monthly_date = get_random_date(start_date, end_date)\n_, monthly_date = get_month_start_end_date(random_monthly_date)\ncountry_code = 'IN'\nraw_ts_data_path = get_aso_ts_raw_df(ASO_TS_RAW_DATA_HEAD, date, 1)\n\ndef test_android_ts_metric_est():\n    expected_prodID_ts = spark.read.csv(raw_ts_data_path, sep=\"\\t\").filter(\n        \"_c1 = 0 and _c2 = '{}' and _c3 = {}\".format(\n            country_code, keyword_id)).select('_c4', '_c6').collect()\n    sql = \"select app_id_list,traffic_share_list from {} where country_code='{}' and keyword_id={} and date='{}' and device_code='{}'\".format(\n        traffic_share_table, country_code, keyword_id, date, ANDROID_ALL)\n    db_result = get_aso_citus_connection(urn, sql)\n    print 'test_android_ts_metric_est'\n\n    db_count = db_result[0][0] or db_result[0][1] if len(db_result) else 0\n    #assertNotEqual(len(db_count), 0)\n    #assertEqual(len(db_result[0][0]),len(db_result[0][1]))\n    print db_count\n    print \"expected_prodID_ts\"\n    print expected_prodID_ts\n    print \"db_result\"\n    print db_result\n    for data in range(0, len(expected_prodID_ts)):\n        product_id_index = list(db_result[0][0]).index(long(expected_prodID_ts[data][0]))\n        print round(float(expected_prodID_ts[data][1]),6)\n        print db_result[0][1][product_id_index]\n\n#test_android_ts_metric_est()\n\ndef test_ios_date_period_completeness():\n    expected_date_list = get_date_list(start_date, end_date)\n    date_fail = dict()\n    actual_date_list = []\n    country_code_list=\"'US', 'FR', 'DE', 'GB', 'AU', 'BE', 'FI', 'GR', 'JP', 'IT'\"\n    for device_code in ios_devices:\n        date_fail[device_code] = list()\n        sql = \"select distinct(date) from {} where granularity = 'daily' and device_code = '{}' \" \\\n              \"and country_code in ({}) and date between '{}' and '{}'\".format(traffic_share_table, device_code,\n                                                                              country_code_list, start_date,\n                                                                              end_date)\n\n        date_list = get_aso_citus_connection(urn, sql)\n        for item in date_list:\n            actual_date_list.append(datetime_to_string(item[0]))\n        print 'actual_date_list'\n        print actual_date_list\n        print 'expected_date_list'\n        print expected_date_list\n        for date in expected_date_list:\n            if str(date) not in actual_date_list:\n                date_fail[device_code].append(date)\n        if not date_fail[device_code]:\n            date_fail.pop(device_code)\n    print date_fail\n    # self.assertFalse(date_fail, msg='{}'.format(date_fail))\n\ntest_ios_date_period_completeness()"]},{"cell_type":"code","execution_count":0,"id":"20190924-070648_703608869","metadata":{},"outputs":[],"source":["%%sh\n"]},{"cell_type":"code","execution_count":0,"id":"20190924-070647_427532921","metadata":{},"outputs":[],"source":["%%sh\n"]},{"cell_type":"code","execution_count":0,"id":"20190924-070639_2007256281","metadata":{},"outputs":[],"source":["%%sh\n"]},{"cell_type":"code","execution_count":0,"id":"20190924-070636_704786887","metadata":{},"outputs":[],"source":["%%sh\n"]},{"cell_type":"code","execution_count":0,"id":"20190910-031929_780776383","metadata":{},"outputs":[],"source":["\n\nimport datetime\nfrom collections import defaultdict\nfrom elasticsearch import Elasticsearch\n\nfrom aaintdatapipeline.application.app_qa.common.utils import get_random_day_in_last_month, datetime_to_string, \\\n    get_random_day_in_last_week, get_week_start_end_date, send_data_source_date_email, get_month_start_end_date, \\\n    string_to_datetime\n\nfrom aaintdatapipeline.core.loader import es\nfrom aaintdatapipeline.core.urn import Urn\nfrom aaintdatapipeline.core.utils.spark import canned_spark, stop\nfrom dateutil.relativedelta import relativedelta\n\n# country_code_list=[\"us\",\"gb\",\"jp\",\"fr\",\"au\"]\n# device_code_list=[\"ios-phone\",\"android-all\",\"ios-tablet\"]\n# today = datetime.date.today()\n# date = today #- datetime.timedelta(days=1)\n\nurn = Urn(\n    namespace='app-qa.db-check.v1',\n    owner='app_qa'\n)\n\n\ndef check_aso_sov_data():\n    params={\"project_name\":\"All_Products\"}\n    country_code_list = [\"us\", \"gb\", \"jp\", \"fr\", \"au\"]\n    device_code_list = [\"ios-phone\", \"android-all\", \"ios-tablet\"]\n    today = datetime.date.today() - datetime.timedelta(days=5)\n    date = today - datetime.timedelta(days=2)\n    failed_ids = defaultdict(list)\n    project_name = params['project_name']\n    if project_name == 'All_Products':\n        # check daily\n        for device_code in device_code_list:\n            for country_code in country_code_list:\n                query_body = {\n                    \"query\": {\"bool\": {\"must\": [{\"match\": {\"device_code\": device_code}}, {\"match\": {\"date\": date}}]}},\n                    \"sort\": [{\"date\": {\"order\": \"desc\", \"mode\": \"max\"}}]}\n                res = check_aso_sov_db(country_code, query_body)\n                print \"{} - {} - {}\".format(date,country_code,res[\"hits\"][\"total\"])\n                if not res[\"hits\"][\"total\"]:\n                    failed_ids[\"daily {} {}\".format(date, device_code)].append(country_code)\n                    \n        # check weekly and monthly            \n        sov_dict_list = get_granularity_date(today)\n        print sov_dict_list\n        if sov_dict_list:\n            for granularity, granularity_date in sov_dict_list.iteritems():\n                print granularity\n                print granularity_date\n                for device_code in device_code_list:\n                    for country_code in country_code_list:\n                        query_body = {\n                            \"query\": {\"bool\": {\n                                \"must\": [{\"match\": {\"device_code\": device_code}}, {\"match\": {\"date\": granularity_date}}]}},\n                            \"sort\": [{\"date\": {\"order\": \"desc\", \"mode\": \"max\"}}]}\n                        res = check_aso_sov_db(country_code, query_body, granularity=granularity)\n                        print res\n                        if not res[\"hits\"][\"total\"]:\n                            failed_ids[\"{} {} {} - \".format(granularity,granularity_date, device_code)].append(country_code)\n        return failed_ids\n\n\ndef check_aso_sov_db(country_code, query_body,granularity=\"daily\"):\n    common_config = {\n        \"database\": \"aso_sov\"\n    }\n    es_conn = es.connection(urn, common_config)\n    res = es_conn.search(index=\"aso-kpi-sov_search_ads_keyword_fact_v1_{}_{}_*\".format(granularity,country_code.lower()),\n                         body=query_body)\n    return res\n\ndef get_granularity_date(today):\n    print today\n    granularity_dic= {}\n    if today.isoweekday() == 5:\n        week_date = today - datetime.timedelta(days=6)\n        granularity_dic[\"weekly\"]=week_date\n    if today.day == 6:\n        month_date = datetime.date(today.year, today.month, 1) - relativedelta(days=1)\n        granularity_dic[\"monthly\"]=month_date\n    print granularity_dic\n    return granularity_dic\n\ncheck_aso_sov_data()\n"]},{"cell_type":"code","execution_count":0,"id":"20190821-032711_2112903970","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://prod.appannie.mktint.data/oss/SEARCH_VOLUME/routine/granularity=daily/date=2017-07-06/platform=1/\naws s3 ls s3://prod.appannie.mktint.data/oss/KEYWORD_DIFFICULTY/routine/granularity=daily/date=2017-07-06/platform=1/\naws s3 ls s3://b2c-prod-data-pipeline-unified-aso/unified/app-tech.aso.keyword-difficulty-search-volume.v1/fact/granularity=daily/date=2017-07-06/device_code=android-all/ \n\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}