{"cells":[{"cell_type":"code","execution_count":0,"id":"20201015-064420_1002539345","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_name=app/granularity=monthly/date=2020-02-29/"]},{"cell_type":"code","execution_count":0,"id":"20201017-032017_2062244876","metadata":{},"outputs":[],"source":["\nsegment_by_product_sql = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=monthly/month=2019-03/\"\n# after_transform=spark.read.format(\"delta\").load(usage_basic_unified_path)\nsegment_by_product_sql=spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact\").parquet(segment_by_product_sql)\nsegment_by_product_sql.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20201015-060818_1443688902","metadata":{},"outputs":[],"source":["\nusage_basic_unified_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_name=domain/granularity=daily/\"\n# df1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact\").parquet(usage_basic_unified_path).createOrReplaceTempView(\"test\")\nstart_str=\"2020-05-01\"\nend_str=\"2020-05-02\"\nafter_transform=spark.read.format(\"delta\").load(usage_basic_unified_path).where(\"date>='{}'\".format(start_str)).where(\"date<='{}'\".format(end_str))\n# after_transform=spark.read.format(\"delta\").load(usage_basic_unified_path)\nafter_transform.groupBy(\"date\").count().orderBy(\"date\",ascending=False).show()\nafter_transform=after_transform.createOrReplaceTempView(\"test_unified\")\n\n# spark.sql(\"select count(distinct(product_id)) from test_unified  where est_active_users is not null and est_active_users <> 0\").show()\n# spark.sql(\"select * from test_unified  where est_active_users is not null and est_active_users <> 0 and unified_category_key=800000 limit 1\").show()\n\nafter_transform_avg=spark.sql(\"\"\"\n    select distinct product_id,sum(est_usage_penetration)/2 AS est_usage_penetration\n    from test_unified  \n    where est_active_users is not null \n    and est_active_users <> 0 \n    and country_code = 'US' \n    and device_code = 'ios-phone' \n    and unified_category_key=800000\n    group by product_id\n    order by est_usage_penetration desc\n    \"\"\")\n# after_transform_avg=spark.sql(\"\"\"\n#     select product_id,est_usage_penetration\n#     from test_unified  \n#     where est_active_users is not null \n#     and est_active_users <> 0 \n#     and country_code = 'US' \n#     and device_code = 'ios-phone' \n#     and unified_category_key=800000\n#     order by est_usage_penetration desc\n#     \"\"\")\nafter_transform_avg=after_transform_avg.createOrReplaceTempView(\"after_transform_avg\")    \n    \n\nspark.sql(\"\"\"\n    select product_id, est_usage_penetration\n    from after_transform_avg \n    order by est_usage_penetration desc\n    \"\"\").show(1000, False)\n\n# spark.sql(\"select distinct country_code,device_code,date,product_id,market_code,granularity from test where est_active_users is not null and est_active_users <> 0\").createOrReplaceTempView(\"unified_remove_category\")\n# spark.sql(\"select count(*) as line_count from unified_remove_category\").show()\n# spark.sql(\"select count(distinct country_code) as country_count from unified_remove_category limit 1\").show()\n# spark.sql(\"select * from test limit 10\").show()"]},{"cell_type":"code","execution_count":0,"id":"20201015-060847_560101128","metadata":{},"outputs":[],"source":["%%sh\n# mw.category_m \n# select  app_id, rank, kpi, estimate \n#             from plproxy.execute_select(\\$proxy\\$\n#                 SELECT app_id, rank, kpi, estimate \n#                 FROM mu.category_monthly\n#                 where date = '2020-01-31' \n#                 and rank <= 1000 \n#                 and category_id = 36\n#                 and store_id = 143441 \n#                 and device_id = 2001 \\$proxy\\$)\n#  t (app_id bigint, rank integer, kpi smallint, estimate double precision) order by rank asc;\n\n# select  domain_id, rank_est_usage_penetration,est_usage_penetration \n#         from plproxy.execute_select(\\$proxy\\$\n#             SELECT domain_id, rank_est_usage_penetration,est_usage_penetration \n#             FROM mw.category_m \n#             where date = '2020-01-31' \n#             and rank_est_usage_penetration <= 1000 \n#             and category_id = 800000\n#             and country_code = 'us'\n#             and device_code = 'ip' \\$proxy\\$)\n# t (domain_id bigint, rank_est_usage_penetration integer, est_usage_penetration real) order by rank_est_usage_penetration asc;\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n# internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-usage-plproxy-internal-1640809782.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\n            select  domain_id, rank_est_usage_penetration,est_usage_penetration \n            from plproxy.execute_select(\\$proxy\\$\n                SELECT domain_id, rank_est_usage_penetration,est_usage_penetration \n                FROM mw.category_d \n                where date = '2020-06-02' \n                and rank_est_usage_penetration <= 1000 \n                and category_id = 800000\n                and country_code = 'us'\n                and device_code = 'ip' \\$proxy\\$)\n t (domain_id bigint, rank_est_usage_penetration integer, est_usage_penetration real) order by rank_est_usage_penetration asc;\n\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20201016-081531_9064397","metadata":{},"outputs":[],"source":["\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2019, 1, 1)\nend_date = datetime(2020, 9, 30)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\nDATE_GRANULARITY_MAPPINGLIST[\"weekly\"].reverse()\n\n# print(DATE_GRANULARITY_MAPPINGLIST[\"monthly\"])\n\nsql_template_a_x_a = \"\"\"\nselect kpi, sum(count_a) as app_count, sum(sum_e) as sum_e from plproxy.execute_select_nestloop($proxy$ \n    select kpi, count(estimate) as count_a, sum(estimate) as sum_e\n    from ca.app_monthly\n    where \n        date ='{date}'\n    group by kpi order by kpi desc\n$proxy$) tpl (kpi smallint, count_a bigint, sum_e double precision ) group by kpi order by kpi desc ;\n\"\"\"\n\nsql_template = \"\"\"\nselect sum(count) as count, sum(affinity_sum) as affinity_sum,  sum(up_sum) as up_sum from plproxy.execute_select_nestloop($proxy$ \n    select count(*) as count, sum(est_cross_product_affinity) as affinity_sum,  sum(est_cross_product_usage_penetration) as up_sum\n    from mw.{db_name}_m\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint, affinity_sum real,up_sum real ) ;\n\n\"\"\"\n\ncity_level_sql = \"\"\"select sum(count) as count, sum(af_sum_value) as af_sum from plproxy.execute_select_nestloop($proxy$ \n    select count(*) as count, sum(est_affinity) as af_sum_value\n    from mu.city_app_w\n    where \n        date ='{date}' and  est_average_active_users <> 0 and est_average_active_users is not null\n$proxy$) tpl (count bigint, af_sum_value real) ;\n\"\"\"\n\ndomain_basic_sql = \"\"\"\nselect  domain_id,sum \n            from plproxy.execute_select($proxy$\n                SELECT domain_id,sum(est_usage_penetration)/2 AS sum\n                FROM mw.category_w \n                where date >= '2020-05-02'\n                and date <= '2020-05-09'\n                and category_id = 800000\n                and country_code = 'us'\n                and device_code = 'ip' \n                group by domain_id$proxy$)\nt (domain_id bigint, sum double precision);\n\"\"\"\n\nfrom pyspark.sql.types import StructType, StructField, LongType\n\ndef get_plproxy_result():\n    plproxy_result = []\n    result = query(PLPROXY_DSN, domain_basic_sql)\n    # distinct_domain_id = result[1][0]\n    # for _r in result:\n    #     plproxy_result.append(_r[0])\n        \n    df_data = [Row(domain_id=long(r[0]),sum=r[1]) for r in result]\n    # print df_data[1]\n    _schema =StructType([StructField(\"domain_id\", LongType(), False),\n                        StructField(\"sum\", DoubleType(), False)])\n    df_plproxy = spark.createDataFrame(data=df_data, schema=_schema)\n    return df_plproxy\n    \n# def get_unified_data():\n#     unified_source_path = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity=w/month=202005/date=2020-05-16\"\n#     spark.read.format(\"delta\").load(unified_source_path).createOrReplaceTempView(\"test_unified\")\n#     spark.sql(\"select distinct domain_id from test_unified where  est_average_active_users <> 0 and est_average_active_users is not null order by domain_id asc\").createOrReplaceTempView(\"unified_df_new\")\n\n    \ndef compare():\n    df_plproxy=get_plproxy_result()\n    get_unified_data()\n    df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n    spark.sql(\"\"\"select * \n                from plproxy_df_new\n                order by sum desc\"\"\").createOrReplaceTempView(\"domain_diff\")\n    #spark.sql(\"select count(1) from domain_diff\").show(10, False)\n    spark.sql(\"select * from domain_diff\").show(1000, False)\n    \n    \n\n# unified_source_path = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity=w/date=2020-05-16\"\n# spark.read.format(\"delta\").load(unified_source_path).show(10)\n\ncompare()\n"]},{"cell_type":"code","execution_count":0,"id":"20201015-060921_27520321","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}