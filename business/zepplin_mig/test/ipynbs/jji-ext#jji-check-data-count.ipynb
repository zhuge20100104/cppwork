{"cells":[{"cell_type":"code","execution_count":0,"id":"20200430-063315_177884470","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\nselect date, sum(count_a) from plproxy.execute_select_nestloop(\\$proxy\\$\n    select date, count(*) as count_a\n    from mu.app_daily\n    where\n        date between '2020-05-24' and '2020-06-20'\n    group by date\n\\$proxy\\$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200708-100259_83589312","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=usage;\nselect date, count(1) from usage_basic_kpi_fact_v6 where granularity='daily' and date between '2020-05-24' and '2020-06-20' group by date order by date asc;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200618-122919_440904612","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\nselect count(*) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select app_id, device_id, store_id, min(estimate) estimate\n    from mu.app_weekly\n    where \n        date='2015-01-10' and kpi=3\n    group by app_id, device_id, store_id\n\\$proxy\\$) tpl (app_id BIGINT, device_id SMALLINT,store_id INT, estimate FLOAT);\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200430-075604_622743837","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-05/platform=1/')\nprint df.select('app_id').count()\ndf.show(5)\ndf.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200430-072530_1300504369","metadata":{},"outputs":[],"source":["\nprint spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2019-10-05/').select('_identifier').count()"]},{"cell_type":"code","execution_count":0,"id":"20200430-110403_24542842","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-05/platform=1/')\ndf2 = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-05/platform=2/')\ndf_raw = df1.union(df2)\ndf_unified = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2019-10-05/')\ndf_new = df_raw.select('app_id').subtract(df_unified.select('app_id'))\nprint df_new.count()\ndf_new.show()"]},{"cell_type":"code","execution_count":0,"id":"20200502-063710_313041708","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-12/platform=1/')\ndf2 = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-12/platform=2/')\ndf_raw = df1.union(df2)\ndf_unified = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2019-10-12/')\ndf_new = df_raw.select('app_id').subtract(df_unified.select('app_id'))\nprint df_new.count()\ndf_new.show()"]},{"cell_type":"code","execution_count":0,"id":"20200502-072836_165523010","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-05/platform=2/')\ndf.show()\ndf.distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200506-022847_134563432","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-12/platform=1/')\ndf2 = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-12/platform=2/')\ndf_raw = df1.union(df2)\nprint df_raw.distinct().count()\ndf_raw.registerTempTable('table')\nresult = spark.sql('select * from table where app_id in (20600010300112,20600000036872,20600006186679,20600000237273,20600002170299,20600006014768,20600009972296,20600004046874,20600000552245,20600000616062,20600004877234,20600008864314,20600007109354,20600001814348)')\nresult.show()"]},{"cell_type":"code","execution_count":0,"id":"20200506-031201_402709905","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-05/platform=1/')\ndf2 = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-05/platform=2/')\ndf_raw = df1.union(df2)\nprint df_raw.distinct().count()\ndf_raw.registerTempTable('table')\nresult = spark.sql('select * from table where app_id in (20600003093415,20600006284220,20600000014288,20600005719194,20600000616062,20600004081760,20600008235304,20600009903623,20600004334505,2060000752482720600007291487,20600001814348,20600000004495,20600010356851)')\nresult.show()"]},{"cell_type":"code","execution_count":0,"id":"20200506-103445_682168648","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=WEEK/date=2019-10-05/platform=1/')\ndf.filter(\"country='WW' and AAD is null and ADU is null and AFU is null and MBS is null \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200508-012748_2094834486","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-10-30/')\ndf.printSchema()\nprint df.count()\nprint df.select('app_id').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200508-013320_852105888","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2020-03-28/')\ndf.printSchema()\ndf2 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2020-02-15/')\ndf2.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200512-062938_1135579626","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2020-02-15/')\ndf2 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2020-02-15/')\nprint df1.count()\nprint df2.count()"]},{"cell_type":"code","execution_count":0,"id":"20200513-015113_896919576","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_daily_count_0511/daily/')\nresult = df.distinct().groupBy('date').agg(sum('dump').alias('dump_count'),sum('unified_v1').alias('unified_v1_count')).orderBy('date').collect()\nfor row in result:\n    print row['date'],'\\t',row['dump_count'],'\\t',row['unified_v1_count']"]},{"cell_type":"code","execution_count":0,"id":"20200513-024511_2022580474","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_daily_count_0511/weekly/')\nresult = df.distinct().groupBy('date').agg(sum('dump').alias('dump_count'),sum('unified_v1').alias('unified_v1_count')).orderBy('date').collect()\nfor row in result:\n    print row['date'],'\\t',row['dump_count'],'\\t',row['unified_v1_count']"]},{"cell_type":"code","execution_count":0,"id":"20200513-035602_1730768514","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0616/weekly/')\nresult = df.distinct().groupBy('date').agg(sum('dump_count').alias('dump_count'),sum('unified_v1_count').alias('unified_v1_count')).orderBy('date').collect()\nfor row in result:\n    print row['date'],'\\t',row['dump_count'],'\\t',row['unified_v1_count']"]},{"cell_type":"code","execution_count":0,"id":"20200513-073924_1422010953","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all', 'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2015-12-27/')\ndf = df.withColumn('device_code', functions.UserDefinedFunction(lambda x: device_code_agg_mapping[x])(df['device_code']))\ndf2 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date=2015-12-27/')\nprint df.count() + df.select('app_id', 'device_code', 'country_code').distinct().count()\nprint df2.count()"]},{"cell_type":"code","execution_count":0,"id":"20200513-100227_2060614658","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.10.254 -p 5432 -U citus_bdp_prod_app_int_qa -d aa_store_db << EOF\nSET search_path=store;\nselect COLUMN_NAME from information_schema.COLUMNS where table_name = 'usage_basic_kpi_fact_v1_p_201701';\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200514-023718_1638696742","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all', 'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2015-12-27/')\ndf = df.withColumn('device_code', functions.UserDefinedFunction(lambda x: device_code_agg_mapping[x])(df['device_code']))\ndf2 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2015-12-27/')\nprint df.count() + df.select('app_id', 'device_code', 'country_code').distinct().count()\nprint df2.distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200514-035955_651436132","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n\nselect date, sum(count_a) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select date, count(*) as count_a\n    from mu.app_weekly\n    where \n        date between '2015-6-12' AND '2015-12-12'\n    group by date\n\\$proxy\\$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\n\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200514-074002_1852478946","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n\nselect device_id,store_id,date,app_id,kpi,estimate from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select device_id,store_id,date,app_id,kpi,estimate\n    from mu.app_weekly\n    where \n        date = '2015-07-25'\n\\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT) limit 3;\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200514-094525_1767709259","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n\nselect date, sum(count_a) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select date, count(*) as count_a\n    from mu.app_weekly\n    where \n        date between '2019-12-12' AND '2020-5-01'\n    group by date\n\\$proxy\\$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\n\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200518-092810_1350657515","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2020-03-28/')\nprint df.select('AU').filter(\"AU is null\").count()\nprint df.select('IP', 'MBWFT', 'OR', 'PAD', 'UP', 'SOI', 'SOU').filter(\n        \"IP>1 or MBWFT>1 or OR>1 or PAD>1 or UP>1 or SOI>1 or SOU>1\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200519-121244_924304610","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        date_list = [('2015-12', '2015-12-27'), ('2016-06', '2016-06-27'),\n                     ('2016-12', '2016-12-27'), ('2017-06', '2017-06-27'),\n                     ('2017-12', '2017-12-27')]\n    if granularity == 'weekly':\n        date_list = [('2013-12', '2013-12-28'), ('2014-12', '2014-12-27'),\n                     ('2015-12', '2015-12-26'), ('2016-12', '2016-12-24'),\n                     ('2017-12', '2017-12-23')]\n    if granularity == 'monthly':\n        date_list = [('2013-12', '2013-12-31'), ('2014-12', '2014-12-31'),\n                     ('2015-12', '2015-12-31'), ('2016-12', '2016-12-31'),\n                     ('2017-12', '2017-12-31')]\n    return date_list\n\n\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select device_id,store_id,date,app_id,kpi,estimate from plproxy.execute_select_nestloop($proxy$ \n    select device_id,store_id,date,app_id,kpi,estimate\n    from mu.app_{}\n    where \n        date = '{}'\n$proxy$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT) limit 50;\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_plproxy_dump_accuracy(date_list, graularity):\n    for date in date_list:\n        result = query(aa_dsn, sql.format(graularity, date[1]))\n        plproxy_df = spark.createDataFrame(result, schema=['device_id', 'store_id', 'date', 'app_id', 'kpi', 'estimate'])\n\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={}/month={}/\"\n        dump_df = spark.read.parquet(dump_path.format(graularity, date[0]))\n\n        join_df_count = dump_df.filter(\"date='{}'\".format(date[1])).join(\n            plproxy_df, on=[plproxy_df.device_id == dump_df.device_id, plproxy_df.store_id == dump_df.store_id,\n                            plproxy_df.app_id == dump_df.app_id, plproxy_df.kpi == dump_df.kpi,\n                            plproxy_df.estimate == dump_df.estimate], how='inner').count()\n        if join_df_count != 50:\n            print \"Accuracy Test Fail!!!! date: {}\".format(date[1])\n        else:\n            print \"Accuracy Test Pass date : {}\".format(date[1])\n\n\ngraularity_list = [\"weekly\", \"monthly\"]\nfor graularity in graularity_list:\n    check_plproxy_dump_accuracy(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200520-030624_1116249110","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\nselect count(distinct store_id) from plproxy.execute_select_nestloop(\\$proxy\\$\n    select store_id\n    from mu.app_monthly\n    where \n        date = '2015-12-27'\n    and\n        device_id>2000\n    and\n        kpi in (24,25)\n\\$proxy\\$) t (store_id INT);\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200520-085833_1392695572","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2020-02-14/\")\nprint df1.count()\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2020-02-14/\")\nprint df2.count()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200521-074944_2012609236","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql.functions import lit\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.types import LongType\nfrom pyspark.sql import functions\nfrom pyspark.sql import Row\n\n\ndevice_code_dict = {1: {'1': 'Android-phone', '2': 'Android-tablet'}, 2: {'1': 'ios-phone', '2': 'ios-tablet'}}\n\n\ndef check_not_empty(df):\n    empty_count = df.select('AU').filter(\"AU is null\").count()\n    if empty_count != 0:\n        print \"Accuracy Test Fail!!! AU is Null!!!\"\n\n\ndef check_percentage_accuracy(df):\n    illegal_percentage_count = df.select('IP', 'MBWFT', 'OR', 'PAD', 'UP', 'SOI', 'SOU').filter(\n        \"IP>1 or MBWFT>1 or OR>1 or PAD>1 or UP>1 or SOI>1 or SOU>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Accuracy Test Fail!!! Percentage > 1!!!\"\n\n\nroutine_df = spark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2020-02-14/\")\nv1_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2020-02-14/\").drop('_identifier')\ncheck_not_empty(routine_df)\ncheck_percentage_accuracy(routine_df)\nroutine_df = (\n                routine_df\n                .withColumn('device_code', functions.UserDefinedFunction(\n                lambda x, y: device_code_dict[x][y])(routine_df['platform'], routine_df['device_type']))\n                .withColumnRenamed('country', 'country_code')\n                .withColumn('app_id', routine_df['app_id'].cast(LongType()))\n                .withColumnRenamed('AU', 'est_average_active_users')\n                .withColumnRenamed('AFU', 'est_average_session_per_user')\n                .withColumnRenamed('ADU', 'est_average_session_duration')\n                .withColumnRenamed('IP', 'est_install_penetration')\n                .withColumnRenamed('AAD', 'est_average_active_days')\n                .withColumnRenamed('PAD', 'est_percentage_active_days')\n                .withColumnRenamed('MBPU', 'est_average_bytes_per_user')\n                .withColumnRenamed('ATU', 'est_average_time_per_user')\n                .withColumnRenamed('UP', 'est_usage_penetration')\n                .withColumnRenamed('OR', 'est_open_rate')\n                .withColumnRenamed('MBPS', 'est_average_bytes_per_session')\n                .withColumnRenamed('MBWFT', 'est_percent_of_wifi_total')\n                .withColumnRenamed('MBS', 'est_mb_per_second')\n                .withColumnRenamed('IS', 'est_installs')\n                .withColumnRenamed('SOU', 'est_average_active_users_country_share')\n                .withColumnRenamed('SOI', 'est_installs_country_share')\n                .withColumn('est_share_of_category_time', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_category_session', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_category_bytes', lit(None).cast(DoubleType()))\n                .withColumn('est_panel_size', lit(None).cast(DoubleType()))\n                .drop('device_type')\n                .drop('platform')\n        )\nsubtract_count = routine_df.select(v1_df.columns).subtract(v1_df).count()\nsubtract_count_reverse = v1_df.select(routine_df.columns).subtract(routine_df).count()\nif subtract_count != 0 or subtract_count_reverse != 0:\n    print 'Accuracy Test Fail!!!!', subtract_count, subtract_count_reverse\nelse:\n    print 'Accuracy Test PASS'"]},{"cell_type":"code","execution_count":0,"id":"20200527-021122_1706976402","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\ntest_result = []\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 29)\n    start = datetime.date(2019, 10, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 03, 28)\n    start = datetime.date(2019, 10, 05)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 03, 28)\n    start = datetime.date(2019, 11, 01)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_routine_v1_completeness(_granularity, date_list):\n    v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                   'usage.basic-kpi.v1/fact/granularity={unified_granularity}/date={unified_date}/'\n    routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/' \\\n               'range_type={raw_granularity}/date={raw_date}/'\n    for date in date_list:\n        v1_path_parse = v1_path.format(unified_granularity=_granularity, unified_date=date[1])\n        routine_path_parse = routine_path.format(raw_granularity=raw_granularity_dict[_granularity], raw_date=date[1])\n        routine_count = spark.read.parquet(routine_path_parse).count()\n        v1_count = spark.read.parquet(v1_path_parse).count()\n        if routine_count != v1_count:\n            print 'Completeness Test Fail!!! routine data: {}, unified data: {}, date: {}'.format(\n                    routine_count, v1_count, date[1])\n        else:\n            print 'Completeness Test Pass! date: {} '.format(date[1])\n        test_result.append((_granularity, routine_count, v1_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'routine_count', 'v1_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0514/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_routine_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200527-100037_2048060001","metadata":{},"outputs":[],"source":["\n# 2 4 5\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2015-12-28/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'ios%' and est_average_active_users!=0 and est_usage_penetration!=0 and est_install_penetration is not null and est_installs is not null and est_open_rate is not null\").select('country_code').distinct().count()\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2014-01-11/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'ios%' and est_average_active_users is not null and est_usage_penetration is not null and est_install_penetration is not null and est_installs is not null and est_open_rate is not null\").select('country_code').distinct().count()\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2014-01-31/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'ios%' and est_average_active_users is not null and est_usage_penetration is not null and est_install_penetration is not null and est_installs is not null and est_open_rate is not null\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200529-091012_1343950687","metadata":{},"outputs":[],"source":["\n# 3 6\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2015-12-27/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'ios%' and est_average_active_users_country_share is not null and est_installs_country_share is not null\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2014-01-11/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'ios%' and est_average_active_users_country_share is not null and est_installs_country_share is not null\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2014-01-31/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'ios%' and est_average_active_users_country_share is not null and est_installs_country_share is not null\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200529-092624_688493871","metadata":{},"outputs":[],"source":["\n# 7 8\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2018-06-03/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'ios%' and est_average_session_per_user!=0 and est_average_session_duration!=0 and est_average_time_per_user!=0\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2018-06-09/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'ios%' and est_average_session_per_user!=0 and est_average_session_duration!=0 and est_average_time_per_user!=0\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2018-06-30/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'ios%' and est_average_session_per_user!=0 and est_average_session_duration!=0 and est_average_time_per_user!=0\").select('country_code').distinct().count()\n\nv1_df_us_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2014-01-11/')\nprint 'weekly', v1_df_us_weekly.filter(\"device_code like 'ios%' and est_average_session_per_user is not null and est_average_session_duration is not null and est_average_time_per_user is not null\").select('country_code').distinct().count()\n\nv1_df_us_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2014-01-31/')\nprint 'monthly', v1_df_us_monthly.filter(\"device_code like 'ios%' and est_average_session_per_user!=0 and est_average_session_duration!=0 and est_average_time_per_user!=0\").select('country_code').distinct().count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200529-093116_1719059511","metadata":{},"outputs":[],"source":["\n# 9 11 15 20\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2015-12-27/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'and%' and est_average_active_users is not null and est_usage_penetration is not null and est_average_active_users_country_share is not null and est_install_penetration is not null and est_installs is not null and est_open_rate is not null and est_installs_country_share is not null and est_average_session_per_user is not null and est_average_session_duration!=0 and est_average_time_per_user!=0 and est_average_bytes_per_session is not null and est_average_bytes_per_user is not null\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2013-01-12/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and est_average_active_users!=0 and est_usage_penetration!=0 and est_average_active_users_country_share!=0 and est_average_session_per_user!=0 and est_average_session_duration!=0 and est_average_time_per_user!=0 and est_average_bytes_per_session!=0 and est_average_bytes_per_user!=0\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2013-01-31/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'and%' and est_average_active_users!=0 and est_usage_penetration!=0 and est_average_active_users_country_share!=0 and est_average_session_per_user!=0 and est_average_session_duration!=0 and est_average_time_per_user!=0 and est_average_bytes_per_session!=0 and est_average_bytes_per_user!=0\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200529-095104_580406915","metadata":{},"outputs":[],"source":["\n# 12 14\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2015-12-27/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'and%' and est_install_penetration!=0 and est_installs!=0 and est_open_rate!=0 and est_installs_country_share!=0\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2015-05-23/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and est_install_penetration!=0 and est_installs!=0 and est_open_rate!=0 and est_installs_country_share!=0\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2015-06-30/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'and%' and est_install_penetration!=0 and est_installs!=0 and est_open_rate!=0 and est_installs_country_share!=0\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200601-072133_1453280267","metadata":{},"outputs":[],"source":["\n# 12 14\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2015-05-23/')\nprint 'weekly', v1_df_weekly.select('app_id', 'country_code', 'device_code', 'est_install_penetration', 'est_installs', 'est_open_rate', 'est_installs_country_share').filter(\"device_code like 'and%'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200601-073742_1899580037","metadata":{},"outputs":[],"source":["\n# 10\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2017-05-14/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'and%' and est_install_penetration is not null and est_installs is not null and est_open_rate is not null\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2017-05-20/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and est_install_penetration is not null and est_installs is not null and est_open_rate is not null\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2017-06-30/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'and%' and est_install_penetration is not null and est_installs is not null and est_open_rate is not null\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200601-072607_1776326219","metadata":{},"outputs":[],"source":["\n# 13\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2017-12-31/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'and%' and est_average_active_users is not null and est_usage_penetration is not null\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2018-01-06/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and est_average_active_users is not null and est_usage_penetration is not null\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2018-01-31/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'and%' and est_average_active_users is not null and est_usage_penetration is not null\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200601-075747_784448262","metadata":{},"outputs":[],"source":["\n# 16\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2017-07-30/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'and%' and est_average_session_per_user is not null and est_average_session_duration!=0 and est_average_time_per_user!=0\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2017-07-29/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and est_average_session_per_user is not null and est_average_session_duration!=0 and est_average_time_per_user!=0\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2017-08-31/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'and%' and est_average_session_per_user is not null and est_average_session_duration!=0 and est_average_time_per_user!=0\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200601-075938_1636868313","metadata":{},"outputs":[],"source":["\n# 17 19\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2017-07-30/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'and%' and est_average_active_days is not null and est_percentage_active_days!=0\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2013-01-12/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and est_average_active_days is not null and est_percentage_active_days!=0\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2013-01-31/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'and%' and est_average_active_days is not null and est_percentage_active_days!=0\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200601-080400_1592107543","metadata":{},"outputs":[],"source":["\n# 18\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2017-07-30/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'and%' and est_average_active_days is not null and est_percentage_active_days!=0\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2017-05-20/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and est_average_active_days is not null and est_percentage_active_days!=0\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2017-06-30/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'and%' and est_average_active_days is not null and est_percentage_active_days!=0\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200601-083513_1003919849","metadata":{},"outputs":[],"source":["\n# 21\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2017-05-14/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'and%' and est_average_bytes_per_session!=0 and est_average_bytes_per_user!=0\").select('country_code').distinct().count()\n\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2017-07-29/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and est_average_bytes_per_session!=0 and est_average_bytes_per_user!=0\").select('country_code').distinct().count()\n\nv1_df_monthly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2017-08-31/')\nprint 'monthly', v1_df_monthly.filter(\"device_code like 'and%' and est_average_bytes_per_session!=0 and est_average_bytes_per_user!=0\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200601-083809_1024393927","metadata":{},"outputs":[],"source":["\n# 21\nv1_df_daily = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2017-05-14/')\nprint 'daily', v1_df_daily.filter(\"device_code like 'and%' and est_average_bytes_per_session!=0 and est_average_bytes_per_user!=0\").select('country_code').filter(\"country_code='CN'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200601-083935_1855415988","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2015-12-29/')\ndf.select('app_id', 'device_code', 'country_code', 'est_install_penetration', 'est_open_rate', 'est_percent_of_wifi_total', 'est_percentage_active_days', 'est_share_of_category_bytes', 'est_share_of_category_session', 'est_share_of_category_time', 'est_share_of_device_mb', 'est_share_of_device_session', 'est_share_of_device_time',\n'est_share_of_installs', 'est_share_of_users', 'est_usage_penetration').filter(\n    \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n    \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n    \"or est_share_of_category_time>1 or est_share_of_device_mb>1 or est_share_of_device_session>1 \"\n    \"or est_share_of_device_time>1 or est_share_of_installs>1 \"\n    \"or est_usage_penetration>1\").select('app_id', 'device_code', 'country_code', 'est_open_rate').show(2)"]},{"cell_type":"code","execution_count":0,"id":"20200601-085452_749453039","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2015-12-29/')\ndf.select('app_id', 'device_code', 'country_code', 'est_average_active_users', 'est_share_of_users').filter(\"app_id=326251330 and device_code='ios-all' and (country_code='CN' or country_code='WW')\").distinct().show()\ndf2 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2015-12-29/')\ndf2.select('app_id', 'device_code', 'country_code', 'est_average_active_users', 'est_average_active_users_country_share').filter(\"app_id=326251330 and device_code='ios-phone' and (country_code='CN' or country_code='WW')\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200601-090129_480011673","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2019-10-01/')\ncolumns = df.columns\nfor Row in df.filter(\"app_id=20600007997522\").collect():\n    for col in columns:\n        print col, '\\t', Row[col]\n    break"]},{"cell_type":"code","execution_count":0,"id":"20200602-025136_1698493269","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2019-10-01/')\ncolumns = df.columns\nfor Row in df.filter(\"app_id=20600007997522 and device_code='android-phone'\").collect():\n    for col in columns:\n        print col, '\\t', Row[col]\n    break"]},{"cell_type":"code","execution_count":0,"id":"20200603-094615_84658220","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\nselect count(distinct store_id) from plproxy.execute_select_nestloop(\\$proxy\\$\n    select store_id\n    from mu.app_weekly\n    where \n        date = '2015-05-23'\n    and\n        device_id<2000\n    and\n        kpi in (4,23,10,25)\n\\$proxy\\$) t (store_id INT);\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200603-110143_1984217316","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\nselect device_id,store_id,date,app_id,kpi,estimate from plproxy.execute_select_nestloop(\\$proxy\\$\n    select device_id,store_id,date,app_id,kpi,estimate\n    from mu.app_weekly\n    where \n        date = '2015-05-23'\n    and\n        device_id<2000\n    and\n        kpi in (4,23,10,25)\n    and\n        app_id=20600000014024\n\\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT) limit 50;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200603-110443_803705614","metadata":{},"outputs":[],"source":["\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2015-05-23/')\nprint 'weekly', v1_df_weekly.select('app_id', 'country_code', 'device_code', 'est_install_penetration', 'est_installs', 'est_open_rate', 'est_installs_country_share').filter(\"device_code like 'and%' and app_id=20600000014024\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200603-110805_2056641724","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=weekly/month=2015-05/').filter(\"date='2015-05-23' and app_id=20600000014024 and device_id<2000\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200603-111145_1426276806","metadata":{},"outputs":[],"source":["\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-01-01/')\nprint v1_df_weekly.filter(\"device_code='CN' and app_id=20600007997522\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200604-054653_226781025","metadata":{},"outputs":[],"source":["\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-01-01/')\nprint v1_df_weekly.filter(\"device_code='CN' and app_id=20600007997522\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200604-094054_1997069577","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0616/weekly/').orderBy('date').collect()\nfor row in df:\n    print row['date'],'\\t',int(row['raw_count']),'\\t',row['unified_count']"]},{"cell_type":"code","execution_count":0,"id":"20200605-050729_123704233","metadata":{},"outputs":[],"source":["\ndf = spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_daily_count_0616/daily/').orderBy('date').collect()\nfor row in df:\n    print row['date'],'\\t',int(row['dump_count']),'\\t',row['unified_v1_count']"]},{"cell_type":"code","execution_count":0,"id":"20200605-050906_1215743287","metadata":{},"outputs":[],"source":["\ndf =spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_plproxy_dump_count_0605/weekly/').orderBy('date').collect()\nfor row in df:\n    print row['date'],'\\t',int(row['plproxy_count']),'\\t',row['dump_count']"]},{"cell_type":"code","execution_count":0,"id":"20200618-075941_689296661","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\ndf =spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0616/weekly/')\nresult = df.distinct().groupBy('date').agg(sum('dump_count').alias('dump_count'),sum('unified_v1_count').alias('unified_v1_count')).orderBy('date').collect()\nfor row in result:\n    print row['date'],'\\t',row['dump_count'],'\\t',row['unified_v1_count']"]},{"cell_type":"code","execution_count":0,"id":"20200605-055649_687554026","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2019-11-30/')\n# df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-10-31/\")\nprint df.filter('AU is not null').count()"]},{"cell_type":"code","execution_count":0,"id":"20200608-061334_2053076975","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\nselect kpi, sum(kpi_count) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select kpi, count(app_id) as kpi_count\n    from mu.app_daily\n    where \n        date = '2019-11-30'\n    group by kpi\n\\$proxy\\$) t (kpi SMALLINT, kpi_count BIGINT) group by kpi order by kpi asc;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200608-062241_726402658","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\nselect count(app_id) from plproxy.execute_select_nestloop(\\$proxy\\$\n    select app_id\n    from mu.app_daily\n    where\n        date = '2019-11-30' and kpi=1\n\\$proxy\\$) t (app_id BIGINT);\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200608-062332_761639268","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nkpi_mapping_dict = {1: 'AU', 2: 'AFU', 3: 'ADU', 4: 'IP', 5: 'AAD', 6: 'PAD', 7: 'MBPU',\n                    8: 'ATU', 9: 'UP', 10: 'OR', 17: 'MBPS', 23: 'IS', 24: 'SOU', 25: 'SOI'}\ngranularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select kpi, sum(kpi_count) from plproxy.execute_select_nestloop($proxy$ \n    select kpi, count(app_id) as kpi_count\n    from mu.app_{}\n    where \n        date = '{}'\n    group by kpi\n$proxy$) t (kpi SMALLINT, kpi_count BIGINT) group by kpi order by kpi asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_routine_consistency(_granularity, date_list):\n    date_list = ['2019-12-02']\n    routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type={}/date={}/'\n    for date in date_list:\n        plproxy_kpi_count_list = query(aa_dsn, sql.format(_granularity, date))\n        routine_path_parse = routine_path.format(granularity_dict[_granularity], date)\n        routine_df = spark.read.parquet(routine_path_parse)\n        for plproxy_kpi_count in plproxy_kpi_count_list:\n            routine_kpi_count = routine_df.filter(\n                \"{} is not null\".format(kpi_mapping_dict[plproxy_kpi_count[0]])).count()\n            if plproxy_kpi_count[1] != routine_kpi_count:\n                print \"Consistency Test Fail!!!! date: {}, plproxy_count: {}, routine_count: {}, kpi: {}\".format(\n                    date, plproxy_kpi_count[1], routine_kpi_count, kpi_mapping_dict[plproxy_kpi_count[0]]\n                )\n            else:\n                print \"Consistency Test Pass! date: {}, plproxy_count: {}, routine_count: {}, kpi: {}\".format(\n                    date, plproxy_kpi_count[1], routine_kpi_count, kpi_mapping_dict[plproxy_kpi_count[0]]\n                )\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_routine_consistency(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200608-070500_1248032744","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200608-070544_2038812876","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n\nselect device_id,store_id,date,app_id,kpi,estimate from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select device_id,store_id,date,app_id,kpi,estimate\n    from mu.app_daily\n    where\n        date = '2019-01-29' and app_id=20600001429645 and device_id=1001 and kpi=1\n\\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT);\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200609-065838_64822796","metadata":{},"outputs":[],"source":["\ndate_list = ['2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01', '2020-01-01']\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/granularity=daily/date=2020-01-01/')\ndf.filter(\"\")"]},{"cell_type":"code","execution_count":0,"id":"20200609-091013_1451910614","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact' \\\n                          '/granularity={v3_granularity}/date={v3_date}/'\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n        unified_v3_count = spark.read.format(\"delta\").load(unified_v3_path_parse).count()\n\n        if unified_v1_count != unified_v3_count:\n            print 'Completeness Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        else:\n            print 'Completeness Test Pass! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        test_result.append((_granularity, unified_v1_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0609/weekly/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"weekly\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200609-091016_1908473276","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact' \\\n                          '/granularity={v3_granularity}/date={v3_date}/'\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n        unified_v3_count = spark.read.format(\"delta\").load(unified_v3_path_parse).count()\n\n        if unified_v1_count != unified_v3_count:\n            print 'Completeness Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        else:\n            print 'Completeness Test Pass! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        test_result.append((_granularity, unified_v1_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0609/monthly/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"monthly\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200609-091554_1704524086","metadata":{},"outputs":[],"source":["\nroutine_df = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type=DAY/date=2020-03-31')\nroutine_df.filter(\"app_id=100 and country_code='RO' and device_code='ios-phone'\").show()\nplproxy_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/granularity=daily/date=2020-03-31/\")\nplproxy_df.filter(\"app_id=100 and country_code='RO' and device_code='ios-phone'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200611-081345_686567196","metadata":{},"outputs":[],"source":["\nplproxy_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2020-03-31/\")\nplproxy_df.filter(\"app_id=100 and country_code='RO' and device_code='ios-phone'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200611-081631_810375917","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/granularity=quarterly/date=2010-09-30/"]},{"cell_type":"code","execution_count":0,"id":"20200616-101439_1244441207","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/granularity=quarterly/date=2010-09-30/')\ndf.show()"]},{"cell_type":"code","execution_count":0,"id":"20200617-081217_1586474286","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}