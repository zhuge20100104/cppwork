{"cells":[{"cell_type":"code","execution_count":0,"id":"20200323-080421_116512768","metadata":{},"outputs":[],"source":["%%sh\n# \n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/temp/ --human --summarize\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v2/dimension/\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v4/dimension/date=2020-03-21/"]},{"cell_type":"code","execution_count":0,"id":"20200326-104816_481328370","metadata":{},"outputs":[],"source":["%%sh\n\n\nPGPASSWORD='2mHdFW6%#REu' psql -h 10.2.26.136  -U app_bdp_usage_qa -d dna -p 6432 << EOF \n\\c aa\nselect count(*),DATE(update_time) AS ForDate\nfrom aa_domain_metadata where DATE(update_time) between '2020-03-24' and '2020-03-31' group by DATE(update_time) order by DATE(update_time) desc;\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200401-025834_1070073897","metadata":{},"outputs":[],"source":["%%sh\n\n\nPGPASSWORD='2mHdFW6%#REu' psql -h 10.2.26.136  -U app_bdp_usage_qa -d dna -p 6432 << EOF \n\\c aa\nselect  * from aa_domain_metadata where DATE(update_time)='2020-04-01' limit 10;\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200326-040825_1956730853","metadata":{},"outputs":[],"source":["\nfrom datetime import timedelta, date, datetime\nfrom random import randint\nimport psycopg2\nfrom contextlib import closing\n\n\ndbinfo = {\n    'NAME': 'aa',\n    'USER': 'app_bdp_usage_qa',\n    'PASSWORD': '2mHdFW6%#REu',\n    'HOST': '10.2.26.136',\n    'PORT': 6432,\n    \n}\nconn = psycopg2.connect(\"\"\"\n            dbname={dbname} user={user} host={host} port={port} password={passwd}\n            \"\"\".format(dbname=dbinfo['NAME'], user=dbinfo['USER'], passwd=dbinfo['PASSWORD'],\n                       host=dbinfo['HOST'], port=dbinfo['PORT']))\n\ndef select(stmt, params):\n    with closing(conn.cursor()) as cur:\n        cur.execute(stmt, params)\n        rows = cur.fetchall()\n        cnt = cur.rowcount\n    return rows, cnt\n\n# sql = \"\"\"select cnt from plproxy.execute_select($$select count(1) as cnt from aa_domain_metadata\"\n#         \"where date = '2019-03-07'$$)t(\n#                 cnt bigint);\"\"\"\n\nsql = \"\"\"select count(1) from aa_domain_metadata;\"\"\"\n            \nrows, _ = select(sql, None)\nprint len(rows)\nprint rows"]},{"cell_type":"code","execution_count":0,"id":"20200327-033905_109525469","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\n\nPG_AA_HOSTS = [('10.2.26.136', 6432)]\nPG_AA_NAME = 'aa'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql= \"\"\"select count(distinct name) from aa_domain_metadata;\"\"\"\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\nprint query(aa_dsn, sql)\n\n# res = pg_settings(urn, dis, None, 'aa')"]},{"cell_type":"code","execution_count":0,"id":"20200325-082244_290643207","metadata":{},"outputs":[],"source":["\nimport json\nimport random\nimport time\n\ndef time_cost(func):\n    def test_wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print \"{} cost {} second\".format(func.__name__, round(end - start,2))\n        return result\n    return test_wrapper\n\n@time_cost\ndef test_missing_date():\n    date_list = ['2019-03-19', '2019-03-22', '2019-03-26', '2019-04-27', '2019-05-04', '2019-05-11', '2019-05-18', '2019-05-25', '2019-05-31']\n    for _date in date_list:\n        d = spark.read.text(\"s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/temp/{}.deflate/\".format(_date)).collect()\n        _json = json.loads(d[0].value)\n        raw_dic = _json[0]['source'][0]['raw_data']\n        random_key = random.sample(raw_dic.keys(), 5)\n    # test count\n        raw_count = len(raw_dic.keys())\n        unified_count = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v4/dimension/date={}/\".format(_date)).distinct().count()\n        try:\n            assert raw_count == unified_count, \"Count mismatched in {}\".format(_date)\n        except AssertionError as count_msg:\n            print count_msg\n    \n        for _k in random_key:\n            # print _k,raw_dic[_k]\n            for _detail_col, _detail in raw_dic[_k].items():\n              raw_detail = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v4/dimension/date={}/\".format(_date)).filter(\"name='{}'\".format(_k)).collect()[0][_detail_col]\n              try:\n                  assert raw_detail == _detail, \"Detail {}'s {} mismatched in {}\".format(_k, _detail_col, _date)\n              except AssertionError as msg:\n                  print msg\n    print \"ALL DONE\"\n\n\n\ntest_missing_date()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200323-080623_1850679681","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v2/dimension/date=2019-05-31/\").filter(\"name='letrasdemusicas.fm'\").drop(\"_identifier\").distinct()\n\n\n\ndf2 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v4/dimension/date=2019-05-31/\").filter(\"name='letrasdemusicas.fm'\").drop(\"_identifier\",\"date\").distinct()\n\ndf1.show()\ndf2.show()\n\n\n# df3 = df1.join(df2,[\"name\", \"description\", \"original_icon_url\", \"icon_url\", \"site_url\", \"path\"],how='inner')\n# df3.show()\n# cnt = df3.count()\n# print cnt"]},{"cell_type":"code","execution_count":0,"id":"20200324-030529_813195037","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nfrom collections import defaultdict\nimport re\nimport time\nimport random\n# Test Count\n# v1:2018.09.18～2019.03.15\n# v2:2019.05.31～2020.03.07\n\ndef time_cost(func):\n    def test_wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print \"{} cost {} second\".format(func.__name__, round(end - start,2))\n        return result\n    return test_wrapper\n\nORIGIN_PATH = \"s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.{key}/dimension/date={date}/\"\nTARGET_PATH = \"s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v4/dimension/date={date}/\"\ndef _parse_string(path):\n    pattern = re.compile(\"unified.*?date=(.*?)/\")\n    return pattern.search(path).groups()[0]\n\ndef get_bucket_date_range():\n    date_range = defaultdict(list)\n    \n    s3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-mobileweb-free'))\n    for _v in [\"v1\", \"v2\"]:\n        path_list = s3_bucket_list.all(prefix=\"unified/mobileweb.domain.{}/dimension/\".format(_v), depth_is_1=True)\n        for _path in path_list:\n           date_range[_v].append(_parse_string(_path))\n    return date_range\n    \ndef _prepare_name(key, date):\n    name_list = spark.read.parquet(ORIGIN_PATH.format(key=key, date=date)).select(\"name\").collect()\n    return random.sample(name_list, 1000)\n    \n@time_cost\ndef test_mobile():\n    all_date = get_bucket_date_range()\n    for _k, _v in all_date.items():\n        for date_ in _v:\n            start_time = time.time()\n            unified_df_cnt = spark.read.parquet(ORIGIN_PATH.format(key=_k, date=date_)).count()\n            delta_df_cnt = spark.read.format(\"delta\").load(TARGET_PATH.format(date=date_)).count()\n            assert unified_df_cnt == delta_df_cnt, \"Count Not Equal in {}\".format(date_)\n            \n            # check detail\n            _test_details(_k, date_)\n            # print(\"{} cost {} seconds\".format(date_, (time.time() - start_time)))\n\n\n\n# @time_cost\ndef _test_details(key, date):\n    V1_COLUMN = [\"description\", \"icon_url\", \"name\", \"original_icon_url\", \"site_url\"]\n    V2_COLUMN = [\"description\", \"icon_url\", \"name\", \"original_icon_url\", \"path\", \"site_url\" ]\n    col_name_list = _prepare_name(key, date)\n    for _name in col_name_list:\n        df_1 = spark.read.parquet(ORIGIN_PATH.format(key=key, date=date)).filter(\"name='{}'\".format(_name[0])).drop(\"_identifier\").distinct()\n        df_2 = spark.read.format(\"delta\").load(TARGET_PATH.format(date=date)).filter(\"name='{}'\".format(_name[0])).drop(\"date\",\"_identifier\").distinct()\n        if key == 'v1':\n            df_3 = df_1.join(df_2, V1_COLUMN, how = 'inner')\n        elif key == 'v2':\n            df_3 = df_1.join(df_2, V2_COLUMN, how = 'inner')\n        # print \"{} is under asserting ...\".format(_name[0])\n        # df_1.show()\n        # df_2.show()\n        # df_3.show()\n        # print df_3.count()\n        try:\n            assert df_1.count() == 1, \"Name: '{}' in {} raw data is not unified\".format(_name[0], date)\n            assert df_1.count() == 1, \"Name: '{}' in {} delta lake data is not unified\".format(_name[0], date)\n            assert df_3.count() == 1, \"Name: '{}' mismatched in {}\".format(_name[0], date)\n        except AssertionError as msg:\n            print msg\n\n\n\n# test_mobile()\n# test_mobile()\n# start_time = time.time()\n# _test_details(\"v2\", \"2019-05-31\")\n# print(\"{} cost {} seconds\".format('2018-09-18', (time.time() - start_time)))\n_prepare_name(\"v1\", \"2018-09-18\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200324-041509_2066332976","metadata":{},"outputs":[],"source":["%%sh\n\n\nPGPASSWORD='2mHdFW6%#REu' psql -h 10.2.26.136  -U app_bdp_usage_qa -d dna -p 6432 << EOF \n\\c aa\n\\d aa_domain_metadata\nselect  * from aa_domain_metadata where DATE(update_time)='2020-04-01' limit 10;\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200325-022128_256058118","metadata":{},"outputs":[],"source":["\nr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v4/dimension/date=2020-03-21/\").filter(\"name='{NAME}'\".format(NAME='couples.jp')).select(\"description\", \"original_icon_url\", \"site_url\").collect()\nprint r[0][0]\nDETAIN_SQL = \"\"\"SELECT description,original_icon_url,site_url FROM aa_domain_metadata WHERE name='couples.jp';\"\"\"\nres = query(aa_dsn, DETAIN_SQL)\nprint res[0][0]"]},{"cell_type":"code","execution_count":0,"id":"20200402-025805_2016778555","metadata":{},"outputs":[],"source":["\n\n\nDETAIN_SQL = \"\"\"SELECT description,original_icon_url,site_url FROM aa_domain_metadata WHERE name='{}';\"\"\"\n\nfor _name in _prepare_name(\"v4\", \"2020-03-21\"):\n    # print _name[0]\n    db_res = query(aa_dsn, DETAIN_SQL.format(_name[0]))\n    des,icon,sit_u =  (db_res[0][0]), (db_res[0][1]), (db_res[0][2])\n    un_res = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v4/dimension/date=2020-03-21/\").filter(\"name='{NAME}'\".format(NAME=_name[0])).select(\"description\", \"original_icon_url\", \"site_url\").collect()\n    try:\n        assert icon == un_res[0][1], \"{}, {}\".format(des, un_res[0][1])\n        assert sit_u == un_res[0][2], \"{}, {}\".format(des, un_res[0][2])\n    except AssertionError as m:\n        print m\n        \n        \n    \n\n    \n"]},{"cell_type":"code","execution_count":0,"id":"20200402-114618_1847255138","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}