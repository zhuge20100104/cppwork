{"cells":[{"cell_type":"code","execution_count":0,"id":"20211011-013815_36456617","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-bigdata-mps/interface/MPS_BRANDED_KPI_V2/version=2.1.0/range_type=WEEK/\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20220104-020852_642131427","metadata":{},"outputs":[],"source":["\nbranded_kpi = \"s3://b2c-prod-bigdata-mps/interface/MPS_MARKET_BENCHMARK_AGG_V2/version=2.1.0/range_type=WEEK/date=2021-01-09/\"\nspark.read.parquet(branded_kpi).createOrReplaceTempView(\"test\")\nspark.sql(\"SELECT genre_id, count(1) as co FROM test where country_code = 'US' group by genre_id order by co desc  limit 100 \").show(100, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20211011-013908_1127707178","metadata":{},"outputs":[],"source":["\nbin_path = \"s3://b2c-prod-bigdata-mps/interface/MPS_BIN_KPI_V2/version=2.0.0/range_type=WEEK/date=2021-05-08/\"\nattribute_kpi =  \"s3://b2c-prod-bigdata-mps/interface/MPS_ATTRIBUTE_KPI_V2/version=2.0.0/range_type=WEEK/date=2021-05-08/\"\nbranded_kpi = \"s3://b2c-prod-bigdata-mps/interface/MPS_BRANDED_KPI_V2/version=2.0.0/range_type=WEEK/date=2021-05-08/\"\nmarket_benchmark = \"s3://b2c-prod-bigdata-mps/interface/MPS_MARKET_BENCHMARK_AGG/version=2.0.0/range_type=WEEK/date=2021-05-08/\"\nmsov = \"s3://b2c-prod-bigdata-mps/interface/MPS_MONETIZATION_SOV/version=1.1.0/range_type=WEEK/date=2021-05-08/\"\nrating_kpi = \"s3://b2c-prod-bigdata-mps/interface/MPS_RATING_KPI/version=1.1.0/range_type=WEEK/date=2021-05-08/\"\ntotal_downloads = \"s3://b2c-prod-bigdata-mps/interface/MPS_TOTAL_DOWNLOADS/version=1.1.0/range_type=WEEK/date=2021-05-08/\"\ntmp= \"s3://b2c-prod-bigdata-mps/interface/MPS_RATING_KPI/version=1.1.0/range_type=WEEK/date=2021-05-08/\"\n\n\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n    \n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\")\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-01-09\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2022-04-02\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\n# range_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\n\n# App IQ need to check\n\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:        \n        branded_kpi =  \"s3://b2c-prod-bigdata-mps/interface/MPS_MARKET_BENCHMARK_AGG_V2/version=2.1.0/range_type=WEEK/date={date}/\".format(date=date)\n        spark.read.parquet(branded_kpi).createOrReplaceTempView(\"test\")\n        result = spark.sql(\"SELECT date, count(1)  FROM test group by date order by date desc \").collect()\n        # result = spark.sql(\"SELECT date, count(distinct unified_product_id,country_code,device_code,market_code,granularity,date,genre_id)  FROM test group by date order by date desc \").collect()\n        print(date, result[0][1])\n\n\n# branded kpi\n# spark.sql(\"SELECT date, count(distinct unified_product_id, country_code,genre_id,device_code,market_code,granularity,date)  FROM test where country_code = 'WW' group by date order by date desc \").show(10)\n# spark.sql(\"SELECT date, count(1) FROM test  where country_code = 'WW' group by date order by date desc \").show(10)\n# spark.read.parquet(market_benchmark).createOrReplaceTempView(\"test\") \n# spark.sql(\"SELECT * FROM test where country_code='WW' and date = '2021-05-08' and unified_product_id = 1000600000699388 limit 10 \").show(100)\n\n# market_benchmark\n# spark.sql(\"SELECT date, count(distinct unified_product_id,country_code,genre_id,device_code,market_code,granularity,date)  FROM test group by date order by date desc \").show(10)\n# spark.sql(\"SELECT * FROM test where country_code='CN' and date = '2021-05-08' and unified_product_id = 1000600000619545 limit 10 \").show(100)\n# spark.sql(\"SELECT * FROM test where country_code='WW' and date = '2021-05-08' and unified_product_id = 1000600000567541 limit 10 \").show(100)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20211104-015110_1675966917","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2013, 1, 1)\nend_date = datetime(2014, 12, 31)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\n# DATE_GRANULARITY_MAPPINGLIST[\"monthly\"].reverse()\n# DATE_GRANULARITY_MAPPINGLIST[\"weekly\"].reverse()\n# DATE_GRANULARITY_MAPPINGLIST[\"daily\"].reverse()\n\nprint (DATE_GRANULARITY_MAPPINGLIST[\"monthly\"])\nprint (DATE_GRANULARITY_MAPPINGLIST[\"weekly\"])\nprint (DATE_GRANULARITY_MAPPINGLIST[\"daily\"])"]},{"cell_type":"code","execution_count":0,"id":"20211230-020513_458289799","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"s3://b2c-prod-dca-bdp-data/BDP-PROD-APP-INT-QA/bdp/user_data/application_name=aa_data_test_app/application_code/version=latest/code/bdp_resources/usr/lib/spark/python/dependencies.zip\")\nfrom data_lib.api import spark_loader\nloader = spark_loader(spark)\nstu_df = loader.load_aurora(\"select date,count(1) from adl_mps_paid.fact_branded_mps_kpi_genre_au_v1 group by date order by  date desc\").show(10)\n"]},{"cell_type":"code","execution_count":0,"id":"20220412-094649_865573996","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20211018-070816_1594045737","metadata":{},"outputs":[],"source":["%python\nimport snowflake.connector\ndef execute(cs, query):\n    cs.execute(query)\n    result = cs.fetchall()\n    return result\nwarehouse = \"AA_STORE_PRODUCT_LOCALIZED_SUMMARY_V1\"\nctx = snowflake.connector.connect(\n    user='htian',\n    password='P2osNW6X8Xn&VCJ@',\n    account='APPANNIE_AA_INT_PROD.us-east-1',\n    role='DEV_DATA_TECH',\n    session_parameters={\n        'QUERY_TAG': 'load store adl sample {}',\n    }\n)\nschema = \"ADL_MASTER\"\ncs = ctx.cursor()\nexecute(cs, \"use warehouse {};\".format(warehouse))\nexecute(cs,\"USE database aa_intelligence_production;\")\nexecute(cs,\"use role AA_UDW_WRITER;\")\nexecute(cs,\"USE schema {};\".format(schema))\n# print(execute(cs,sql_1))\nprint(execute(cs,\"\"\"ALTER TABLE ADL_MASTER.DIM_PRODUCT_LOCALIZED_DETAIL_V3_CLUSTER_BY_PRODUCT_KEY RENAME TO ADL_MASTER.DIM_PRODUCT_LOCALIZED_DETAIL_V1_CLUSTER_BY_PRODUCT_KEY \"\"\"))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}