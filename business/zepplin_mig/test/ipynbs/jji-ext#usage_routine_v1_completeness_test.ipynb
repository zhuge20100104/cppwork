{"cells":[{"cell_type":"code","execution_count":0,"id":"20200527-075943_470505459","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\ntest_result = []\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 29)\n    start = datetime.date(2019, 10, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 03, 28)\n    start = datetime.date(2019, 10, 05)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 03, 28)\n    start = datetime.date(2019, 11, 22)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_routine_v1_completeness(_granularity, date_list):\n    v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                   'usage.basic-kpi.v1/fact/granularity={unified_granularity}/date={unified_date}/'\n    routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/' \\\n               'range_type={raw_granularity}/date={raw_date}/'\n    for date in date_list:\n        v1_path_parse = v1_path.format(unified_granularity=_granularity, unified_date=date[1])\n        routine_path_parse = routine_path.format(raw_granularity=raw_granularity_dict[_granularity], raw_date=date[1])\n        routine_count = spark.read.parquet(routine_path_parse).count()\n        v1_count = spark.read.parquet(v1_path_parse).count()\n        if routine_count != v1_count:\n            print 'Completeness Test Fail!!! routine data: {}, unified data: {}, date: {}'.format(\n                    routine_count, v1_count, date[1])\n        else:\n            print 'Completeness Test Pass! date: {} '.format(date[1])\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_routine_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200527-080021_1268370222","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/"]},{"cell_type":"code","execution_count":0,"id":"20200527-100518_628865357","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2019-11-23/')\nprint df.count()"]},{"cell_type":"code","execution_count":0,"id":"20200528-014903_753667473","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2019-11-24/')\nprint df.count()"]},{"cell_type":"code","execution_count":0,"id":"20200528-021502_1845241510","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\ntest_result = []\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 29)\n    start = datetime.date(2019, 10, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 03, 28)\n    start = datetime.date(2019, 10, 05)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 14)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_routine_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                  'usage.basic-kpi.v5/fact/granularity={unified_granularity}/date={unified_date}/'\n        routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/' \\\n                       'range_type={raw_granularity}/date={raw_date}/'\n        v1_path_parse = v1_path.format(unified_granularity=_granularity, unified_date=date[1])\n        routine_path_parse = routine_path.format(raw_granularity=raw_granularity_dict[_granularity], raw_date=date[1])\n        routine_count = spark.read.parquet(routine_path_parse).count()\n        v1_count = spark.read.parquet(v1_path_parse).count()\n        if routine_count != v1_count:\n            print 'Completeness Test Fail!!! routine data: {}, unified data: {}, date: {}'.format(\n                routine_count, v1_count, date[1])\n        else:\n            print 'Completeness Test Pass! date: {} '.format(date[1])\n        test_result.append((_granularity, routine_count, v1_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'routine_count', 'v1_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_routine_v1_count_0629/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_routine_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200629-091836_2053432341","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/\n"]},{"cell_type":"code","execution_count":0,"id":"20200629-092112_506572920","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\ntest_result = []\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 29)\n    start = datetime.date(2019, 10, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 20)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 14)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_routine_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                  'usage.basic-kpi.v5/fact/granularity={unified_granularity}/date={unified_date}/'\n        routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/' \\\n                       'range_type={raw_granularity}/date={raw_date}/'\n        v1_path_parse = v1_path.format(unified_granularity=_granularity, unified_date=date[1])\n        routine_path_parse = routine_path.format(raw_granularity=raw_granularity_dict[_granularity], raw_date=date[1])\n        routine_count = spark.read.parquet(routine_path_parse).count()\n        v1_count = spark.read.parquet(v1_path_parse).count()\n        if routine_count != v1_count:\n            print 'Completeness Test Fail!!! routine data: {}, unified data: {}, date: {}'.format(\n                routine_count, v1_count, date[1])\n        else:\n            print 'Completeness Test Pass! date: {} '.format(date[1])\n        test_result.append((_granularity, routine_count, v1_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'routine_count', 'v1_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_routine_v1_count_0629/weekly/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"weekly\"]\nfor granularity in granularity_list:\n    check_usage_routine_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200702-123246_6346844","metadata":{},"outputs":[],"source":["\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndf = spark.read.format(\"delta\").load('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type=WEEK/date=2020-06-20/')\nprint df.count()"]},{"cell_type":"code","execution_count":0,"id":"20200702-122639_1111443513","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_routine_v1_count_0629/weekly/')\ndf.orderBy('date').show()"]},{"cell_type":"code","execution_count":0,"id":"20200629-093838_779299901","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\ntest_result = []\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2020, 05, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 03, 28)\n    start = datetime.date(2019, 10, 05)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 14)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_routine_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                  'usage.basic-kpi.v5/fact/granularity={unified_granularity}/date={unified_date}/'\n        routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/' \\\n                       'range_type={raw_granularity}/date={raw_date}/'\n        v1_path_parse = v1_path.format(unified_granularity=_granularity, unified_date=date[1])\n        routine_path_parse = routine_path.format(raw_granularity=raw_granularity_dict[_granularity], raw_date=date[1])\n        routine_count = spark.read.parquet(routine_path_parse).count()\n        v1_count = spark.read.format(\"delta\").load(v1_path_parse).count()\n        print v1_count, routine_count\n        if routine_count != v1_count:\n            print 'Completeness Test Fail!!! routine data: {}, unified data: {}, date: {}'.format(\n                routine_count, v1_count, date[1])\n        else:\n            print 'Completeness Test Pass! date: {} '.format(date[1])\n        test_result.append((_granularity, routine_count, v1_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'routine_count', 'v1_count'])\n\n\n\ngranularity_list = [\"monthly\"]\nfor granularity in granularity_list:\n    check_usage_routine_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200709-070551_243652098","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\ntest_result = []\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 29)\n    start = datetime.date(2019, 10, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 03, 28)\n    start = datetime.date(2019, 10, 05)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 27)\n    start = datetime.date(2020, 05, 24)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_routine_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                  'usage.basic-kpi.v5/fact/granularity={unified_granularity}/date={unified_date}/'\n        routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/' \\\n                       'range_type={raw_granularity}/date={raw_date}/'\n        v1_path_parse = v1_path.format(unified_granularity=_granularity, unified_date=date[1])\n        routine_path_parse = routine_path.format(raw_granularity=raw_granularity_dict[_granularity], raw_date=date[1])\n        routine_count = spark.read.parquet(routine_path_parse).count()\n        v1_count = spark.read.format(\"delta\").load(v1_path_parse).count()\n        if routine_count != v1_count:\n            print 'Completeness Test Fail!!! routine data: {}, unified data: {}, date: {}'.format(\n                routine_count, v1_count, date[1])\n        else:\n            print 'Completeness Test Pass! date: {} '.format(date[1])\n        test_result.append((_granularity, routine_count, v1_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'routine_count', 'v1_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_routine_v1_count_0709/daily/\",\n            mode=\"overwrite\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_routine_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200709-074752_800646296","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\ntest_result = []\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 29)\n    start = datetime.date(2019, 10, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 27)\n    start = datetime.date(2020, 05, 30)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 05, 24)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_routine_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                  'usage.basic-kpi.v5/fact/granularity={unified_granularity}/date={unified_date}/'\n        routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/' \\\n                       'range_type={raw_granularity}/date={raw_date}/'\n        v1_path_parse = v1_path.format(unified_granularity=_granularity, unified_date=date[1])\n        routine_path_parse = routine_path.format(raw_granularity=raw_granularity_dict[_granularity], raw_date=date[1])\n        routine_count = spark.read.parquet(routine_path_parse).count()\n        v1_count = spark.read.format(\"delta\").load(v1_path_parse).count()\n        if routine_count != v1_count:\n            print 'Completeness Test Fail!!! routine data: {}, unified data: {}, date: {}'.format(\n                routine_count, v1_count, date[1])\n        else:\n            print 'Completeness Test Pass! date: {} '.format(date[1])\n        test_result.append((_granularity, routine_count, v1_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'routine_count', 'v1_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_routine_v1_count_0709/weekly/\",\n            mode=\"overwrite\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"weekly\"]\nfor granularity in granularity_list:\n    check_usage_routine_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200709-075205_814674502","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\ntest_result = []\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 27)\n    start = datetime.date(2020, 05, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 05, 30)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 05, 24)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_routine_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                  'usage.basic-kpi.v5/fact/granularity={unified_granularity}/date={unified_date}/'\n        routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/' \\\n                       'range_type={raw_granularity}/date={raw_date}/'\n        v1_path_parse = v1_path.format(unified_granularity=_granularity, unified_date=date[1])\n        routine_path_parse = routine_path.format(raw_granularity=raw_granularity_dict[_granularity], raw_date=date[1])\n        routine_count = spark.read.parquet(routine_path_parse).count()\n        v1_count = spark.read.format(\"delta\").load(v1_path_parse).count()\n        if routine_count != v1_count:\n            print 'Completeness Test Fail!!! routine data: {}, unified data: {}, date: {}'.format(\n                routine_count, v1_count, date[1])\n        else:\n            print 'Completeness Test Pass! date: {} '.format(date[1])\n        test_result.append((_granularity, routine_count, v1_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'routine_count', 'v1_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_routine_v1_count_0709/monthly/\",\n            mode=\"overwrite\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"monthly\"]\nfor granularity in granularity_list:\n    check_usage_routine_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200709-073104_866327455","metadata":{},"outputs":[],"source":["\ndf = spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_routine_v1_count_0709/monthly/')\nresult = df.distinct().orderBy('v1_count').collect()\nfor row in result:\n    print row['date'], '\\t', row['routine_count'], '\\t', row['v1_count']"]},{"cell_type":"code","execution_count":0,"id":"20200709-071300_673784509","metadata":{},"outputs":[],"source":["\nstart = '2020-05-24'\nend = '2020-06-20'\ndf = spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/').where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndf.createOrReplaceTempView(\"df\")\nresult = spark.sql(\"select date, count(app_id) as count from df group by date order by date asc\").collect()\nfor r in result:\n    print r['date'], '\\t', r['count']"]},{"cell_type":"code","execution_count":0,"id":"20200709-085708_1755145052","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200709-084211_1093868327","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select count(uniqlo_id) from plproxy.execute_select_nestloop($proxy$ \n    select max(app_id) as uniqlo_id\n    from mu.app_{}\n    where \n        date='{}'\n    group by\n    app_id,\n    store_id,\n    device_id\n$proxy$) t (uniqlo_id BIGINT);\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 27)\n    start = datetime.date(2020, 05, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 27)\n    start = datetime.date(2020, 05, 30)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 27)\n    start = datetime.date(2020, 05, 24)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = collect_date\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_dump_completeness(_granularity, date_list):\n    for date in date_list:\n        plproxy_count = query(aa_dsn, sql.format(_granularity, date[0]))\n        print date[0], '\\t', plproxy_count[0][0]\n\n\ngranularity_list = [\"daily\", 'weekly', 'monthly']\nfor granularity in granularity_list:\n    print granularity\n    check_usage_plproxy_dump_completeness(granularity, get_path_date_list(granularity))\n    print 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200709-072443_360088348","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=usage;\nselect date, count(1) from usage_basic_kpi_fact_v6 where granularity='monthly' and date between '2020-05-24' and '2020-06-27' group by date order by date asc;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200713-012817_1996145986","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=usage;\nselect * from usage_basic_kpi_fact_v6 where granularity='daily' and date between '2020-07-03' and '2020-07-03' limit 3;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200713-013340_1080975799","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \nselect count(uniqlo_id) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select max(app_id) as uniqlo_id\n    from mu.app_daily\n    where \n        date='2020-07-03'\n    group by\n    app_id,\n    store_id,\n    device_id\n\\$proxy\\$) t (uniqlo_id BIGINT);\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200709-073005_1932863257","metadata":{},"outputs":[],"source":["\nroutine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type=DAY/date=2020-07-03/'\ndf = spark.read.parquet(routine_path)\nprint df.filter(\"platform=2\").count()\nprint df.filter(\"platform=1\").count()\nprint df.count()"]},{"cell_type":"code","execution_count":0,"id":"20200709-073842_1244874906","metadata":{},"outputs":[],"source":["\nstart = '2020-06-20'\nend = '2020-06-20'\ndf = spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/').where(\"granularity='weekly' and date between '{}' and '{}'\".format(start, end))\nprint df.filter(\"device_code like 'ios%'\").count()\nprint df.filter(\"device_code like 'android%'\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200709-074018_1284653093","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/"]},{"cell_type":"code","execution_count":0,"id":"20200710-092352_969819090","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}