{"cells":[{"cell_type":"code","execution_count":0,"id":"20191115-023239_551534035","metadata":{},"outputs":[],"source":["%%sh\nexport PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH\npython  ~/bdp/application/aaintdatapipeline/core/pipeline/doctor.py readavro -k raw/app-int.advancedreview.topic.v1/insert/_default_partition=2019-12/aidpc896494dfa40e6cc993824da9404b451.avro\n"]},{"cell_type":"code","execution_count":0,"id":"20191128-074849_1612979276","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-advanced-review/oss/ADVRVW_INHOUSE_TOPIC_PREDICTIONS/\n# ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/process_date=2020-04-24/   --recursive --human --summarize | tail -5\n\n  #  --recursive --human --summarize | tail -5\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date=2020-04-29/   --recursive --human --summarize | tail -5\n\n\n \n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/  --recursive --human --summarize | tail -5\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/process_date=2020-05-12/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date=2020-05-12/\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/process_date=2020-07-11/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date=2020-07-11/\n"]},{"cell_type":"code","execution_count":0,"id":"20200210-055739_345862121","metadata":{},"outputs":[],"source":["\n\n\n\n# spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=2.0.3/platform=2/process_date=2020-08-19/\").where(\"country in ('IN') and date_format(review_datetime, 'yyyy-MM-dd') ='2020-08-19' \").show(2)\n"]},{"cell_type":"code","execution_count":0,"id":"20191127-070523_2142072647","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-raw-advancedreview/raw/advancedreview.topic.v1/insert/_default_partition=2020-10/ | sort -n\n"]},{"cell_type":"code","execution_count":0,"id":"20191206-030923_693329908","metadata":{},"outputs":[],"source":["\n\ndf = spark.read.option(\"basePath\", \"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2\").load(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/process_date=2020-09-2{4,5,6}\").cache()\ndf.createOrReplaceTempView(\"temp\")\nspark.sql(\"select * from temp where app_id = 284882215 and country = 'JP'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20191031-094211_1597480381","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\nfrom datetime import datetime,timedelta\nimport datetime\n\n# test_date = ['2019-07-0'+ str(a) for a in xrange(1,10)] + ['2019-07-1'+str(a) for a in xrange(0,6)]\nnumdays=122\nbase = datetime.date.today()- datetime.timedelta(days=23)\ndate_list = [str(base - datetime.timedelta(days=x)) for x in range(numdays)]\n\n                            \n#and review_datetime='2019-07-14T00:00:00.000000Z'\ndef check_data_in_process_date_group_by(pd):\n    print pd\n    country_list=[\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n    language_list=[\"en\",\"ja\",\"ko\"]\n\n    raw_process_date_path = \"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2\" \\\n                        \"/process_date={}/\".format(pd)\n    unified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview\" \\\n                            \".topic.v1/\"\n    for language in language_list:\n        for country in country_list:\n\n            t = unittest.TestCase('run')\n            raw_result = spark.read.parquet(raw_process_date_path).filter(\"country='{}' and language='{}'\".format(country,language))\n            raw_result = raw_result.withColumn(\"record_date\", raw_result['review_datetime'].cast(DateType()))\n            raw_result = raw_result.groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\"))\n            raw_data_row = raw_result.groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\")).collect()\n\n            unified_result = spark.read.parquet(unified_process_date_path)\n            unified_result = unified_result.withColumn(\"record_date\", unified_result['date'].cast(DateType()))\n            unified_data_row = unified_result.filter(\"process_date='{}' and country_code='{}' and language='{}'\".format(pd,country,language)).collect()\n\n            range_len = len(raw_data_row)\n            range_len_unified=len(unified_data_row)\n            review_id_list = []\n            for x in range(0,range_len):\n                review_id_list.append(raw_data_row[x][1])\n            # print len(review_id_list)\n\n            review_id_list_unified = []\n            # print range_len_unified\n            for x in range(0,range_len_unified):\n                review_id_list_unified.append(unified_data_row[x][9])\n            # print len(review_id_list_unified)\n            l= [x for x  in review_id_list if x not in review_id_list_unified]\n            # print '!!!', len(l) , \"different is : \" , l \n            for x in l:\n                row = unified_result.filter(\"review_id={}\".format(x)).select(\"process_date\").collect()[0]\n                _date = datetime.datetime.strptime(pd, \"%Y-%m-%d\").date()\n                if(row[0]<_date):\n                    print 'compare current process date' , pd , ' ~~~ unified process date',  row\n        print '~~~~~~~~~~~'\n\n    \n    \n    \n    # print list(set(review_id_list) & set(review_id_list_unified)) \n\n    print 'no error'\n                                                                                   \nfor x in date_list:\n    check_data_in_process_date_group_by(x)"]},{"cell_type":"code","execution_count":0,"id":"20191101-015926_906675321","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import array_contains\nfrom pyspark.sql.functions import col\n\n\nunified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/date=2019-10-17/part-20191017.c000.gz.parquet\"\ndf=spark.read.parquet(unified_process_date_path)\ndf.printSchema()\n\ndf=df.withColumn(\"contains_1\",df.select(array_contains(col(\"topic_ids\"),1)))\ndf.show()\n"]},{"cell_type":"code","execution_count":0,"id":"20191111-032021_306873163","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132 -p 5432 -U citus_bdp_usage_qa -d aa_citus_db << EOF\nSET search_path=advancedreview;\n--\\d advancedreview_topic_fact_v3;\n--\\d advancedreview_term_fact_v3;\nBEGIN;\n-- SELECT count(*) FROM  advancedreview.advancedreview_topic_fact_v2 where country_code='US' and language='en' and array_length(topic_ids, 1)>=2 and topic_ids@>'{1}';\n--SELECT * FROM  advancedreview.advancedreview_topic_fact_v2 where country_code='IN' and review_id='4395695885';\n-- select * from advancedreview_topic_fact_v2 where  cardinality(topic_ids)=0 order by date desc limit 3;\n-- select * from advancedreview_topic_fact_v2 where topic_ids <@ ARRAY[10]::smallint[] limit 10 ;\n-- select _identifier, count(*) from advancedreview_topic_fact_v2 group by _identifier order by _identifier ;\n-- select date, count(*) from advancedreview_term_fact_v3 where date in ('2020-05-09', '2020-05-10', '2020-05-11','2020-05-12') group by date;\n-- select date, count(*) from advancedreview_topic_fact_v3 where date in ('2020-05-10','2020-05-11','2020-05-12') group by date;\nselect date, count(1), country_code from advancedreview_topic_fact_v3 where date in ('2020-12-14','2020-12-13', '2020-12-12', '2020-12-11') group by country_code, date order by country_code, date ;\n--select date, count(1), country_code from advancedreview_term_fact_v3 where date in ('2020-12-13', '2020-12-12', '2020-12-11')  group by country_code, date order by country_code, date ;\n\nCOMMIT;\nEOF \n\n"]},{"cell_type":"code","execution_count":0,"id":"20191031-094315_304282779","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\nimport datetime\n\nraw_process_date_path = \"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2\" \\\n                        \"/process_date={}/count/lang\"\nunified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview\" \\\n                            \".topic.v1/fact/process_date={}/\"\n                            \n\ndef check_data_in_process_date_group_by():\n    t = unittest.TestCase('run')\n    country_list=[\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n    language_list=[\"en\",\"ja\",\"ko\"]\n    # test_date = datetime.date.today()- datetime.timedelta(days=2)\n    numdays=116\n    start_date='2019-07-01'\n    end_date='2019-10-24'\n    base = datetime.date.today()- datetime.timedelta(days=32)\n    expected_date_list = [str(base - datetime.timedelta(days=x)) for x in range(numdays)]\n    for test_date in expected_date_list:\n        for country in country_list:\n            for language in language_list:\n                print 'started to test: ', country, language , test_date\n                raw_path=raw_process_date_path.format(test_date)\n                raw_result = spark.read.parquet(raw_path).filter(\"country='{}' and language='{}'\".format(country,language))\n                raw_result = raw_result.withColumn(\"record_date\", raw_result['review_datetime'].cast(DateType()))\n                raw_result = raw_result.groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\"))\n                raw_data_row = sorted(raw_result.groupBy(\"record_date\").agg(count(\"count_review_id\").alias(\"total_review\")).collect())\n\n            # print raw_data_row\n                unified_path=unified_process_date_path.format(test_date)\n                unified_result = spark.read.parquet(unified_path)\n                unified_result = unified_result.withColumn(\"record_date_unified\", unified_result['date'].cast(DateType()))\n                unified_data_row = sorted(unified_result.filter(\"country_code='{}' and language='{}'\".format(country,language)).groupBy(\"record_date_unified\").count().collect())\n\n            # print unified_data_row\n            \n            \n                range_len = len(raw_data_row)\n                print range_len\n                for x in range(0, range_len):\n                # print raw_data_row[x][0], type(raw_data_row[x][0])\n                # print unified_data_row[x][0], type(unified_data_row[x][0])\n                    t.assertEqual(raw_data_row[x][0], unified_data_row[x][0],\n                     'date is not equals {} ~ {}, debug info {} \\n {} '.format(raw_data_row[x][0],\n                                                                              unified_data_row[x][0],\n                                                                              raw_path,\n                                                                              unified_path))\n                # print raw_data_row[x][1], type(raw_data_row[x][1])\n                # print unified_data_row[x][1], type(unified_data_row[x][1])\n                    t.assertEqual(raw_data_row[x][1], unified_data_row[x][1],\n                     'number is not equals {} ~ {}, debug info {} \\n {} '.format(raw_data_row[x][1],\n                                                                              unified_data_row[x][1],\n                                                                              raw_path,\n                                                                              unified_path))\n    print 'no error'\n                           \n                           \n\ncheck_data_in_process_date_group_by()"]},{"cell_type":"code","execution_count":0,"id":"20191125-071228_92301157","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-071312_2058962943","metadata":{},"outputs":[],"source":["\n# raw_path=\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/process_date=2019-10-25/\"\npath=\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date=2020-02-03\"\nspark.read.parquet(path).filter(\"review_id='5481813221'\").show(20,False)\n\npath=\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/process_date=2020-02-03\"\nspark.read.parquet(path).filter(\"review_id='5481813221'\").show(20,False)\n\n\npath1=\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/process_date=2020-02-03\"\nspark.read.parquet(path1).filter(\"review_id='5481813221'\").show(20,False)\n\n\n# path2=\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/process_date=2020-02-03\"\n# spark.read.parquet(path2).filter(\"review_id='5121372801'\").show()\n\n#  review_id  | product_id | product_version | country_code | language | rating |    date    | topic_ids | device_code |    _identifier     | _disable_idx_4_query \n# ------------+------------+-----------------+--------------+----------+--------+------------+-----------+-------------+--------------------+----------------------\n#  5121372801 | 1461479554 | 1.4.6           | US           | en       |      3 | 2019-11-11 | {2}       | ios-all     | 220191128041020689 |                    0\n\n# print c1 \n# print c2"]},{"cell_type":"code","execution_count":0,"id":"20200207-020127_1948444393","metadata":{},"outputs":[],"source":["\ndf1=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/process_date=2020-02-03\").filter(\"review_id=5458214386\").show()\ndf1=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/process_date=2020-02-03\").filter(\"review_id=5458214386\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-022355_1399415301","metadata":{},"outputs":[],"source":["\nimport datetime\nnumdays=116\nstart_date='2019-07-01'\nend_date='2019-12-20'\nbase = datetime.date.today()- datetime.timedelta(days=150)\nexpected_date_list = [str(base - datetime.timedelta(days=x)) for x in range(numdays)]\nprint expected_date_list"]},{"cell_type":"code","execution_count":0,"id":"20200206-031537_1000717850","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v2/fact/\n"]},{"cell_type":"code","execution_count":0,"id":"20191101-053239_2108596613","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.functions import count\n\ndef count_review_id_test():\n    t = unittest.TestCase('run')\n    country_list=[\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n    language_list=[\"en\",\"ja\",\"ko\"]\n    for language in language_list:\n        for country in country_list:\n            print country\n            result = spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/process_date=2020-02-03/\")\n            raw_count = result.filter(\"country='{}' and language='{}'\".format(country,language)).select(\"review_id\").distinct().count()\n            result1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date=2020-02-03\")\n            unified_count = result1.filter(\"country_code='{}' and language='{}'\".format(country,language)).select(\"review_id\").distinct().count()\n            print 'raw count is : ', raw_count\n            print 'unified_count is : ', unified_count\n            t.assertEqual(raw_count,unified_count, \"raw data is not equals with unifed data {} - {} \".format(raw_count,unified_count))\n    \ncount_review_id_test()"]},{"cell_type":"code","execution_count":0,"id":"20191101-020708_718055960","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\n\n\nunified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/\"\ndef check_rating_is_1_to_5():\n    country_list=[\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n\n    for country in country_list:\n        print 'test country : ' , country\n        t = unittest.TestCase('run')\n        unified_result = spark.read.parquet(unified_process_date_path)\n        unified_data_row = unified_result.filter(\n            \"process_date='2020-02-03' and country_code='{}' and language='en'\".format(country)).collect()\n\n        range_len = len(unified_data_row)\n        for x in range(0, range_len):\n            result= unified_data_row[x].asDict()[\"rating\"]\n            t.assertTrue(result in [1,2,3,4,5], \"rating value is not correct {}\".format(result))\n    print 'pass'\ncheck_rating_is_1_to_5()\n"]},{"cell_type":"code","execution_count":0,"id":"20191101-053849_164984846","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\n\nunified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\"\ndef check_is_topic_is_1_to_11():\n\n    unittest.TextTestRunner(verbosity=2)\n    t = unittest.TestCase('run')\n    unified_result = spark.read.parquet(unified_process_date_path)\n    unified_result = unified_result.withColumn(\"record_date\", unified_result['date'].cast(DateType()))\n    unified_data_row = unified_result.filter(\n        \"process_date='2020-02-03' and country_code='US' and language='en'\").collect()\n\n    range_len = len(unified_data_row)\n    for x in range(0, range_len):\n        array_result= unified_data_row[x].asDict()[\"topic_ids\"]\n        for i in array_result:\n            t.assertTrue(i in range(1,12), \"topic_ids value is not correct {} \".format(i))\n    print 'pass'\ncheck_is_topic_is_1_to_11()\n"]},{"cell_type":"code","execution_count":0,"id":"20191115-080306_871587455","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\nfrom datetime import datetime,timedelta\nimport datetime\nfrom aaintdatapipeline.core.conf import Conf\nfrom aaintdatapipeline.core.fs.device import S3Bucket\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import get_mkt_unified_path, check_parquet_exist\n\nconf = Conf(\n    bucket_name=\"b2c-prod-data-pipeline-unified-advancedreview\"\n)\n\ndef test_date_completeness():\n    device_code='ios-all'\n    t = unittest.TestCase('run')\n    numdays=136\n    base = datetime.date.today()- datetime.timedelta(days=2)\n    date_list = [str(base - datetime.timedelta(days=x)) for x in range(numdays)]\n    path_fail, data_fail = dict(), dict()\n    path_fail[device_code], data_fail[device_code] = list(), list()\n    for date in date_list:\n        unified_parquet_path = \"unified/app-int.advancedreview.topic.v1/fact/process_date={}/\".format(date)\n        files = S3Bucket(conf=conf).all(unified_parquet_path)\n        if files:\n            if check_parquet_exist(files):\n                data_fail[device_code].append(date)\n        else:\n            path_fail[device_code].append(date)\n    if not path_fail[device_code]:\n        path_fail.pop(device_code)\n    if not data_fail[device_code]:\n        data_fail.pop(device_code)\n    t.assertFalse(data_fail or path_fail, msg='{}~{}'.format(data_fail, path_fail))\n    print 'passed'\ntest_date_completeness()\n"]},{"cell_type":"code","execution_count":0,"id":"20191115-090443_190192199","metadata":{},"outputs":[],"source":["\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_aso_citus_connection)\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import urn\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_citus_connection)\nimport unittest\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\n\ndef test_count():\n    t = unittest.TestCase('run')\n    country_list=[\"US\"]#, \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n    language_list=[\"en\"]#,\"ja\",\"ko\"]\n    sql=\"SELECT distinct (review_id) FROM  advancedreview.advancedreview_topic_fact_v2  where device_code='ios-all';\"\n    raw_path_1=\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.1_dedupe/platform=2/\"\n    raw_path_2=\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/\"\n\n    for language in language_list:\n        for country in country_list:\n            result_1 = spark.read.parquet(raw_path_1).select(\"review_id\").distinct()\n            result_2 = spark.read.parquet(raw_path_2).filter(\"process_date>='2019-11-12' and process_date<='2019-11-26'\").select(\"review_id\").distinct()\n            result = result_1.union(result_2).select(\"review_id\").distinct()\n            db_result = get_citus_connection(urn, sql)\n            rows_result=[]\n            for row in db_result:\n                rows_result.extend(Row(review_id_from_db=row))\n            new_df=spark.createDataFrame(rows_result)\n            # print 'raw count is : ', result\n            # print 'db is : ', db_result[0][0]\n            result.select(\"review_id\").subtract(new_df).show()\n\n\n# s1=x1.select(\"review_id\")\n# s2=x2.select(\"review_id\")\n# s1.union(s2).subtract(s1.intersect(s2)).show()\n\ntest_count()"]},{"cell_type":"code","execution_count":0,"id":"20200213-132840_1932288491","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/process_date=2020-02-12/"]},{"cell_type":"code","execution_count":0,"id":"20191127-075243_2064923100","metadata":{},"outputs":[],"source":["\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_aso_citus_connection)\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import urn\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_citus_connection)\nimport unittest\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\n\ndef test_count():\n    t = unittest.TestCase('run')\n    country_list=[\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n    language_list=[\"en\",\"ja\",\"ko\"]\n    # sql=\"SELECT count(distinct review_id) FROM  advancedreview.advancedreview_topic_fact_v3  where device_code='ios-all';\"\n    sql=\"SELECT count(distinct review_id) FROM  advancedreview.advancedreview_topic_fact_v3  where country_code='{}' and language='{}' and device_code='ios-all';\"\n\n    raw_path=\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/\"\n    for language in language_list:\n        for country in country_list:\n            result = spark.read.parquet(raw_path).filter(\"country='{}' and language='{}'\".format(country,language)).select(\"review_id\").distinct().count()\n            # result = spark.read.parquet(raw_path).select(\"review_id\").distinct().count()\n            print language, country\n            # db_result = get_citus_connection(urn, sql)\n            db_result = get_citus_connection(urn, sql.format(country,language))\n            print 'raw count is : ', result\n            \n            # path2=\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/\"\n            # print 'another part row...: ' ,spark.read.parquet(path2).filter(\"process_date>='2019-11-12' and process_date<='2019-11-25'\").select(\"review_id\").distinct().count()\n\n            print 'db is : ', db_result[0][0]\n            t.assertEqual(result,db_result[0][0], \"raw data is not equals with unifed data {} - {} \".format(result,db_result[0][0]))\n\n    print 'pass'\n\ntest_count()"]},{"cell_type":"code","execution_count":0,"id":"20191119-033201_403748981","metadata":{},"outputs":[],"source":["\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_aso_citus_connection)\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import urn\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_citus_connection)\nimport unittest\nfrom pyspark.sql.functions import count\nimport datetime\n\ndef datetime_to_string(date_time, convert_format=\"%Y-%m-%d\"):\n    return datetime.datetime.strftime(date_time, convert_format)\n\ndef check_date_completeness():\n    numdays=148\n    start_date='2019-07-01'\n    end_date='2019-11-27'\n    base = datetime.date.today()- datetime.timedelta(days=2)\n    expected_date_list = [str(base - datetime.timedelta(days=x)) for x in range(numdays)]\n    # print expected_date_list\n    t = unittest.TestCase('run')\n\n    date_fail = dict()\n    actual_date_list = []\n    country_code_list = ['US', 'GB', 'CA', 'AU', 'IN', 'JP', 'KR']\n    language_list=[\"en\",\"ja\",\"ko\"]\n    device_code='ios-all'\n    for country_code in country_code_list:\n        print country_code\n        for language in language_list:\n            date_fail[device_code] = list()\n            sql = \"SELECT distinct date FROM  advancedreview.{} where country_code='{}' and language='{}' and device_code='ios-all' and date between '{}' and '{}'\".format(\"advancedreview_topic_fact_v3\", country_code, language, start_date, end_date)\n            date_list = get_citus_connection(urn, sql)\n            for item in date_list:\n                actual_date_list.append(datetime_to_string(item[0]))\n            for date in expected_date_list:\n                if str(date) not in actual_date_list:\n                    date_fail[device_code].append(date)\n            if not date_fail[device_code]:\n                date_fail.pop(device_code)\n            t.assertFalse(date_fail, msg='{}'.format(date_fail))\n    print 'pass'\n\ncheck_date_completeness()\n"]},{"cell_type":"code","execution_count":0,"id":"20191119-060200_2135186931","metadata":{},"outputs":[],"source":["\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_aso_citus_connection)\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import urn\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_citus_connection)\nimport unittest\nfrom pyspark.sql.functions import count\n\ndef check_topic_mutually_exclusive_count():\n    t = unittest.TestCase('run')\n    country_list=[\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n    language_list=[\"en\",\"ja\",\"ko\"]\n    sql=\"SELECT count(*) FROM  advancedreview.advancedreview_topic_fact_v3 where country_code='{}' and language='{}' and array_length(topic_ids, 1)>=2 and topic_ids@>'{{1}}';\"\n    for language in language_list:\n        for country in country_list:\n            print country, language\n            db_result = get_citus_connection(urn, sql.format(country,language))\n            print 'db is : ', db_result[0][0]\n            t.assertEqual(0,db_result[0][0], \"find topic contains 1 and others - {} \".format(db_result[0][0]))\n    print 'pass'\n\ncheck_topic_mutually_exclusive_count()"]},{"cell_type":"code","execution_count":0,"id":"20191119-073210_1423107056","metadata":{},"outputs":[],"source":["\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_aso_citus_connection)\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import urn\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_citus_connection)\nimport unittest\nfrom pyspark.sql.functions import count\n\ndef check_topic_id_list_is_empty():\n    t = unittest.TestCase('run')\n    sql=\"select count(1) from advancedreview.advancedreview_topic_fact_v3 where cardinality(topic_ids)=0;\"\n    db_result = get_citus_connection(urn, sql)\n    t.assertEqual(0,db_result[0][0], \"find empty topic list - {} \".format(db_result[0][0]))\n    print 'pass'\n\ncheck_topic_id_list_is_empty()"]},{"cell_type":"code","execution_count":0,"id":"20191127-071725_1423503026","metadata":{},"outputs":[],"source":["\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_aso_citus_connection)\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import urn\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_citus_connection)\nimport unittest\nfrom pyspark.sql.functions import count\n\ndef check_topic_id_list_is_empty():\n    t = unittest.TestCase('run')\n    sql=\"select * from advancedreview.advancedreview_topic_fact_v3 where product_id='1318292856' and country_code='US' order by date desc limit 3\"\n    db_result = get_citus_connection(urn, sql)\n    print db_result\n\ncheck_topic_id_list_is_empty()"]},{"cell_type":"code","execution_count":0,"id":"20200114-032500_194144855","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/\n# spark.read.parquet(path2).filter(\"review_id='5121372801'\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200114-032742_1534740337","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\nimport datetime\npath=\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/\"\nraw_result=spark.read.parquet(path).filter(\"app_id='1318292856' and country='US' \")\nraw_result = raw_result.withColumn(\"record_date\", raw_result['review_datetime'].cast(DateType()))\nraw_result = raw_result.groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\"))\nraw_data_row = sorted(raw_result.groupBy(\"record_date\").agg(count(\"count_review_id\").alias(\"total_review\")).collect())\nprint raw_data_row\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200114-033335_1042431346","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\nimport datetime\n\npath=\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/\"\nunified_data=spark.read.parquet(path).filter(\"app_id='1318292856'\")\nunified_result = unified_data.groupBy(\"review_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\"))\nraw_data_row = sorted(raw_result.groupBy(\"record_date\").agg(count(\"count_review_id\").alias(\"total_review\")).collect())\nprint raw_data_row"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}