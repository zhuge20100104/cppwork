{"cells":[{"cell_type":"code","execution_count":0,"id":"20200730-071243_1903238895","metadata":{},"outputs":[],"source":["\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\").where(\"process_date = '2020-07-28'\").createOrReplaceTempView(\"adv_data\")\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(\"process_date = '2020-07-28'\").createOrReplaceTempView(\"review_data\")\nspark.sql(\"\"\"\nselect * from adv_data a left join review_data rv\non a.review_id = rv.review_id \nwhere rv.review_id is null\n\"\"\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200730-071719_1405200110","metadata":{},"outputs":[],"source":["%%sh\ndate\n\naws s3 ls s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2020-07-28/23/ios/528194791 --human \n"]},{"cell_type":"code","execution_count":0,"id":"20200730-072122_1999664362","metadata":{},"outputs":[],"source":["\n\nspark.read.format(\"csv\").load(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2020-07-28/23/ios/526096699.gz\", sep=\"\\t\").show(20,False)\n"]},{"cell_type":"code","execution_count":0,"id":"20200229-060957_1659107691","metadata":{},"outputs":[],"source":["%python\nimport gzip\nimport base64\nfrom aadatapipelinecore.core.fs import Conf\nfrom aadatapipelinecore.core.fs.device import S3Bucket\nfrom aadatapipelinecore.core.fs.device.bucket import raw_bucket, specified_bucket\npath = \"app-ss-review/2020/07/28/00/b2c-prod-data-pipeline-stream-firehose-review-4-2020-03-02-00-00-03-73ff853f-ec91-4c2b-99db-b44ad553b09d\"\nSAFE_SEPARATOR = \"S-AIDPSEPA-E\"\nconf = Conf()\nconf.bucket_name = \"b2c-prod-data-pipeline-buffer-review\"\nconf.bucket_class = S3Bucket\nbuffer_s3 = specified_bucket(conf)\ncontent = buffer_s3.get(path)\nb64_records = content.split(SAFE_SEPARATOR)\nfor b64_rec in b64_records:\n    with gzip.GzipFile(fileobj=BytesIO(base64.b64decode(b64_rec)), mode='rb') as fh:\n        print fh.read()\n"]},{"cell_type":"code","execution_count":0,"id":"20200303-060722_386293127","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-buffer-review/app-ss-review/2020/07/28/\n# aws s3 ls s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2020-03-02/23/gp/ |sort -n | head -20\n# aws s3 ls  s3://b2c-prod-data-pipeline-qa/aa.review.count/"]},{"cell_type":"code","execution_count":0,"id":"20200303-060500_63743896","metadata":{},"outputs":[],"source":["\n# from pyspark.sql import Row\nimport gzip\nimport base64\nimport json\nfrom aadatapipelinecore.core.fs import Conf\nfrom aadatapipelinecore.core.fs.device import S3Bucket\nfrom aadatapipelinecore.core.fs.device.bucket import raw_bucket, specified_bucket\n# b2 = lack.select(\"review_id\").collect()\n# bbb = [str(bb['review_id']) for bb in b2]\n#path = \"app-ss-review/2020/03/01/16/b2c-prod-data-pipeline-stream-firehose-review-2-2020-01-09-02-44-06-e78e4371-7fd4-4c4a-974b-a59217f54e85\"\nSAFE_SEPARATOR = \"S-AIDPSEPA-E\"\nconf = Conf()\nconf.bucket_name = \"b2c-prod-data-pipeline-buffer-review\"\nconf.bucket_class = S3Bucket\nbuffer_s3 = specified_bucket(conf)\npaths = buffer_s3.all(\"app-ss-review/2020/07/28/\")\n# paths2 = buffer_s3.list(\"app-ss-review/2020/02/19\")\npaths =[path.replace(\"s3://b2c-prod-data-pipeline-buffer-review/\",\"\") for path in  paths]\n# review_id_prodcut_id = []\n# paths=[\"app-ss-review/2020/02/24/00/b2c-prod-data-pipeline-stream-firehose-review-2-2020-02-24-00-00-18-40d6a90c-404d-4a76-919d-3868a0d0a1cc\"]\n\nprint len(paths)\npaths[0]\nreview_row=set()\nfor path in paths:\n    content = buffer_s3.get(path)\n    b64_records = content.split(SAFE_SEPARATOR)[:-1]\n    for b64_rec in b64_records:\n        with gzip.GzipFile(fileobj=BytesIO(base64.b64decode(b64_rec)), mode='rb') as fh:\n            json_record = json.loads(fh.read())\n            for ss in json_record[\"source\"]:\n\n                if  ss[\"process_granularity\"] == 'DAILY' :\n                    # review_row.append(Row(id=ss[\"id\"],product_id=ss[\"product_id\"],process_date=ss[\"process_date\"], granularity=ss[\"process_granularity\"],platform=ss[\"platform\"],process_hour= ss[\"process_hour\"]))\n                    review_row.add(str(ss[\"id\"]))\n                    # print ss\n                    # print len(review_row)\n                    # break\n# rdd1 = sc.parallelize(list(review_row), 5)\n# row_rdd = rdd1.map(lambda x: Row(x))\n# df = spark.createDataFrame(row_rdd, ['review_id'])\n# df.show()\n\n# df.write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review.count/process_date=2020-03-02/\")\n\nprint len(review_row)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200730-073357_1629153477","metadata":{},"outputs":[],"source":["\n# print  dir(review_row)\n# |6246671330|526096699|3.5|null|null|English|null|null|https://itunes.ap...|  1|  US|2020-07-27T00:00:...|  en|       It’s just bad|       it just bad|Doesn’t work well...|doesn’t work well...|null|null|null|null|\n# |6249681885|526096699|3.5|null|null|English|null|null|https://itunes.ap...|  3|  US|2020-07-28T00:00:...|  en|Buying more after...|bui more after sub|Recently got a di...|recent got a digi...|null|null|null|null|\n\n# l = list(review_row)\n\nprint l[0]\nif \"gp:AOqpTOHrBrnxSgW6jX1Zn4x8EvtsObyfkwN-azEi84_PTJzL47pWGChKqOXLJYFeFXzdRmK8eFLDzYN812WKnA\" in l:\n    print 'yes'\n\n# l1 = [ str(x) for x in l]"]},{"cell_type":"code","execution_count":0,"id":"20200304-055013_796585424","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\ngranularity = 'daily'\npath_ = \"process_granularity='daily' and process_date='2020-03-02' and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\"\nunified_df1 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(path_)\nunified_df1.createOrReplaceTempView(\"temp_unified1\")\n\n# spark.sql(\"select count(review_id) from temp_unified1 where  \").show()\n\nspark.sql(\"select COUNT(DISTINCT review_id) from temp_unified1\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200224-115410_1933695371","metadata":{},"outputs":[],"source":["\n\nfrom ast import literal_eval\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\ndef compare_two_values(key, raw_value, unified_value):\n    try:\n        if raw_value ==\"\" :\n            if unified_value != None:\n                print \"NOT EQUAL\"\n        elif isinstance(unified_value,int):\n            if int(raw_value) == unified_value:\n                pass\n            else:\n                print 'NOT EQUAL!' , key , raw_value,unified_value\n        elif str(raw_value) == str(unified_value.decode(\"utf-8\")):\n            pass\n        else:\n            print 'NOT EQUAL!' , key , raw_value,unified_value\n    except UnicodeEncodeError as e :\n        print key, raw_value, unified_value\n        print e\n\ndef get_sample_test_data():\n    result_df = spark.read.format(\"avro\").load(\"s3://b2c-prod-data-pipeline-raw-review/raw/review.v1/insert/process_granularity=DAILY/process_date=2020-02-16/process_hour=23/platform=ios/aidpad927e6761e9cafd7fb8bb8f5d593cc1.avro/\")\n    print result_df.show()\n    result = result_df.limit(1).select(\"_identifier\",\"source\").take(1)\n    print result \n    raw_data_source = literal_eval(result[0][\"source\"])[0]\n    return raw_data_source\n\ndef get_unified_test_data(raw_result_df):\n    return spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2020-02-16/process_hour=23/device_code=ios-all/market_code=apple-store/\").filter(\"review_id='{}'\".format(raw_result_df[\"id\"])).take(10)\n\nraw_result_df=get_sample_test_data()\nprint 'sample data :', raw_result_df\n\n\nunified_result_df = get_unified_test_data(raw_result_df)\n\ndef compare_df_value(raw_result_df, unified_result_df):\n    \n    print raw_result_df, unified_result_df\n    print 'start to compare'\n    for key,value in raw_result_df.items():\n        if key == 'userreview_url':\n            compare_two_values(key, value, unified_result_df[0]['user_review_url'])\n        elif key == 'title_t' or key =='process_date' or key == 'content_t' or key=='process_granularity' or key=='reply_t' or key=='process_hour' or key=='platform':\n            pass\n        elif key == 'id' :\n            compare_two_values(key, value, unified_result_df[0]['review_id'])\n        elif key == 'platform' :\n            compare_two_values(key, platforms[value][\"device_code\"], unified_result_df[0][\"device_code\"])\n            compare_two_values(key, platforms[value][\"market_code\"], unified_result_df[0][\"market_code\"])\n\n        elif key == 'date' :\n            compare_two_values(key, value, unified_result_df[0]['time'])\n\n        elif key == 'product_id' :\n            compare_two_values(key, value, unified_result_df[0]['app_id'])\n\n        elif key == 'country' :\n            compare_two_values(key, value, unified_result_df[0]['country_code'])\n\n        else:\n            compare_two_values(key, value, unified_result_df[0][key])\n        \ncompare_df_value(raw_result_df, unified_result_df)        \n \n\n"]},{"cell_type":"code","execution_count":0,"id":"20200224-092346_619868838","metadata":{},"outputs":[],"source":["\nimport json\nimport csv\nimport traceback\nfrom gzip import GzipFile\nimport boto3\nfrom StringIO import StringIO\nfrom exceptions import IOError\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n# lines =[(\"prod_appannie_uniform_public_app_store_data\", \"review-uniform/DAILY/2020-02-19/23/ios/1439363292.gz\")]\ndef parse_csv(lines):\n    session = boto3.session.Session()\n    client = session.client(\"s3\")\n    \n    def generate_rows(lines):\n        for bucket, key in lines:\n            items = key.split(\"/\")\n            granularity = items[1].lower()\n            if granularity != \"monthly\":\n                process_date = items[2]\n                process_hour = items[3]\n                platform = items[4]\n            else:\n                process_date = items[2]\n                process_hour = \"23\"\n                platform = items[3]\n            assert platform in platforms, \"Invalid platform %s in %s\" % (platform, key)\n            device_code = platforms[platform][\"device_code\"]\n            market_code = platforms[platform][\"market_code\"]\n            response = client.get_object(Bucket=bucket, Key=key)\n            data = response['Body'].read()\n            gz = GzipFile(None, 'rb', 9, StringIO(data))\n            try:\n                csv_lines = list(csv.reader(gz, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_ALL))\n            except IOError as e:\n                raise\n                \n            for item in csv_lines:\n                row = (\n                        item[0],\n                        item[1],\n                        granularity, process_date, process_hour, device_code, market_code)\n                yield row\n    return generate_rows(lines)\n\n\n# print len(set(parse_csv(lines)))"]},{"cell_type":"code","execution_count":0,"id":"20200224-095222_525592125","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ndef get_bucket_list(granularity, date):\n    s3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\n    #review-uniform/DAILY/2020-02-24/23\n    path_d = s3_bucket_list.all(prefix=\"review-uniform/{}/{}/\".format(granularity, date), depth_is_1=False)\n    print path_d[0]\n    temp_lines = [(\"prod_appannie_uniform_public_app_store_data\", k)  for k in path_d]\n    return temp_lines\n    \n# temp_lines = get_bucket_list(\"DAILY\",\"2020-03-06\")\ntemp_lines = get_bucket_list(\"HOURLY\",\"2020-03-07\")\n\nrdd = spark.sparkContext.parallelize(temp_lines, 120).mapPartitions(parse_csv)\ndf = spark.createDataFrame(rdd).cache()\nprint 'dataframe contains count as: ', df.count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200225-135425_1962708254","metadata":{},"outputs":[],"source":["\n# df.coalesce(1).write.mode(\"overwrite\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/raw_03_07_hourly\")\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/raw_03_07_hourly\")\nprint df.filter(\"_7 in ('apple-store','google-play')\").select(\"_1\").distinct().count()\ndf.createOrReplaceTempView(\"temp_raw\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200225-134901_272886719","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\n# granularity = 'daily'\npath_ = \"process_granularity='hourly' and process_date='2020-03-07' and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\"\nunified_df1 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(path_)\nunified_df1.createOrReplaceTempView(\"temp_unified1\")\n\nunified_df2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=hourly/process_date=2020-03-07\")\nunified_df2.createOrReplaceTempView(\"temp_unified2\")\n\n\n\nspark.sql(\"select COUNT(DISTINCT review_id) from (select _1 as review_id from temp_raw EXCEPT select review_id from temp_unified1) as prod\").show()\nspark.sql(\"select COUNT(DISTINCT app_id)  from (select _2 as app_id from temp_raw EXCEPT select app_id from temp_unified1) as prod\").show()\nmissed_df = spark.sql(\"select  review_id,  from (select _1 as review_id from temp_raw EXCEPT select review_id from temp_unified1) as prod\")\nmissed_df.show()\n\n# missed_df.coalesce(1).write.mode(\"overwrite\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/missed_delta_lake_03_07_hourly\")\n\n\n# spark.sql(\"select COUNT(DISTINCT review_id) from (select _1 as review_id from temp_raw EXCEPT select review_id from temp_unified2) as prod\").show()\n# spark.sql(\"select COUNT(DISTINCT app_id)  from (select _2 as app_id from temp_raw EXCEPT select app_id from temp_unified2) as prod\").show()\n# spark.sql(\"select  review_id from (select _1 as review_id from temp_raw EXCEPT select review_id from temp_unified2) as prod\").show(20,False)\n"]},{"cell_type":"code","execution_count":0,"id":"20200310-063941_1590539884","metadata":{},"outputs":[],"source":["\n\ndf_hourly = spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/missed_delta_lake_03_07\")\ndf_hourly_parquet = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=daily/process_date=2020-03-07\")\n\ndf_hourly.createOrReplaceTempView(\"h1\")\ndf_hourly_parquet.createOrReplaceTempView(\"h2\")\n\nspark.sql(\" select h2._identifier,  count(distinct h1.review_id) from h2 inner join h1 on h1.review_id=h2.review_id group by h2._identifier\").show(20,False)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200224-104242_1215970195","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\n\ngranularity = 'daily'\npath_ = \"process_granularity='daily' and process_date= '2020-02-25' and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\"\nunified_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(path_)\n# unified_df.show(20,False)\n\n\ndf.createOrReplaceTempView(\"temp_raw\")\nunified_df.createOrReplaceTempView(\"temp_unified\")\n\nspark.sql(\"select distinct count (_1) from temp_raw \").show()\n\n\n# result = spark.sql(\"select app_id,count(review_id) as count_review_id from (select _1 as review_id ,  _2 as app_id from temp_raw intersect select review_id , app_id from temp_unified ) as join_view group by app_id\")\n# print type(result)\n\n# print spark.sql(\"select review_id, app_id from (select _1 as review_id ,  _2 as app_id from temp_raw intersect select review_id , app_id from temp_unified ) as join_view sort by app_id\").collect()\n# df_join = df.join(unified_df, df._2==unified_df.app_id, 'left')\n# print df.select(\"app_id\").distinct()\n\nspark.sql(\"select COUNT(DISTINCT review_id) from (select _1 as review_id from temp_raw EXCEPT select review_id from temp_unified) as prod\").show()\nspark.sql(\"select COUNT(DISTINCT app_id)  from (select _2 as app_id from temp_raw EXCEPT select app_id from temp_unified) as prod\").show()\n\n# spark.sql(\"select  count(distinct review_id ) from temp_unified \").show()\nprint unified_df.select(\"review_id\").distinct().count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200224-140520_1112821341","metadata":{},"outputs":[],"source":["\nspark.sql(\"select count(distinct _1)  from temp_raw group by _2 order by 1 desc \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200224-122103_1074108590","metadata":{},"outputs":[],"source":["\n# 1070688 -924404\ndf_left_join = df.join(unified_df, df._2==unified_df.app_id, 'left')\n\ndf_left_join.filter(\"app_id is null\").show(20,False)"]},{"cell_type":"code","execution_count":0,"id":"20200222-100450_669617055","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\ndest_path='s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_by_process_date_streaming_0217.v2/'\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = ['process_granularity={hourly,daily,weekly}/process_date=%s/' % item for item in [\"2020-02-17\"]]\nprint path_list[0]\nfor path in path_list:\n    print path.split(\"/\")[1].split(\"=\")[1]\n    path_ = \"process_granularity in ('hourly','daily','weekly') and process_date>= '2020-02-17' and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\"\n    print path_\n    count_1 = spark.read.format(\"delta\").parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(path_).select(\"review_id\").distinct().count()\n    print count_1\n    df = spark.read.json(dest_path)\n    count_2 = df.filter(\"index is not null\").count()\n    count_3 = df.filter(\"_identifier is not null\").count()\n    print count_1, count_2, count_3\n    if count_1 == count_2 == count_3 :\n        print 'pass', path\n    else:\n        print \"not equal!!!!!!\", count_1 , count_2, count_3,  path\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200220-063058_659655771","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-raw-review'))\npath_d = s3_bucket_list.all(prefix=\"raw/review.v1/insert/process_granularity=DAILY/\", depth_is_1=True)\ndate_list_raw = [x.split(\"/\")[2] for x in path_d]\n\n# s3_bucket_list_unified = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\n# path_d_unified = s3_bucket_list_unified.all(prefix=\"unified/review.v1/fact/process_granularity=hourly/\", depth_is_1=True)\n# date_list_unified = [x.split(\"/\")[4].split(\"=\")[1] for x in path_d_unified]\n"]},{"cell_type":"code","execution_count":0,"id":"20200220-053701_986708466","metadata":{},"outputs":[],"source":["%%sh \naws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=hourly/process_date=2020-02-22/process_hour=0/device_code=android-all/market_code=google-play/\n\n\n # print unified_review_list\n# unified_list=unified_review_list.select(\"_identifier\",\"review_id\").distinct().groupBy(\"_identifier\").agg({\"_identifier\":\"count\"}).collect()\n\n# print len(unified_list)\n# print unified_list\n# unified_review_list = spark.read.format(\"delta\").option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").option(\"versionAsOf\", latest_version[0][0]).\n\n# unified_review_list = spark.read.format(\"delta\").(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v3/fact/\").parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v3/fact/process_granularity=daily/process_date=2020-02-{11,12,13,14,15,16,17,18,19}/process_hour=23/device_code=ios-all/market_code=apple-store/\").filter(\"review_id='5548326002'\").show()\n\n\n# check_review_list = []\n# print unified_review_list\n# for x in unified_review_list:\n#     check_review_list.append(x.review_id)\n# print check_review_list\n\n\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200220-055955_592500557","metadata":{},"outputs":[],"source":["\n\n\nunified_review_list = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(\"process_granularity in ('hourly','daily','weekly') and process_date >='2020-02-20' and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\")\ncount1 = unified_review_list.select(\"review_id\").distinct().count()\nprint 'distinct review', count1\n\ncount2 = unified_review_list.select(\"_identifier\",\"review_id\").distinct().count()\nprint 'distinct review and _identifier',count2\n\nprint 'differencet : ' , count1-count2\n"]},{"cell_type":"code","execution_count":0,"id":"20200223-033210_26221267","metadata":{},"outputs":[],"source":["\nfrom elasticsearch import TransportError\nfrom requests.auth import HTTPBasicAuth\nfrom elasticsearch import Elasticsearch, Urllib3HttpConnection, client, helpers\nhost = \"http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com\"\naccess_id = 'bdp_read'\nsecret_key = 'E9vjhU2qG1bAcM83'\nport = 19200\nnot_use_ssl = host.startswith(\"http://\")\nconn_config = {\n        \"semaphore_name\": \"temp\",\n        \"host\": '{}:{}'.format(host, port),\n        \"access_id\": access_id,\n        \"secret_key\":secret_key,\n        \"connection_capacity\": 5,\n        \"semaphore_timeout\": 100,\n        \"use_ssl\": not not_use_ssl\n    }\nhost = conn_config['host']\naccess_id = conn_config['access_id']\nsecret_key = conn_config['secret_key']\nes = Elasticsearch(\n        hosts=[host],\n        connection_class=Urllib3HttpConnection,\n        http_auth=(\n            \"{user}:{password}\".format(\n                user=access_id,\n                password=secret_key\n            )\n        ),\n        use_ssl=conn_config['use_ssl'],\n        retry_on_timeout=True,\n        max_retries=3,\n        timeout=10,\n)\nfrom aadatapipelinecore.core.loader.es import _read\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nlack = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join_201908_20200211_2/\").cache()\n////load from old review es\n\n \nfrom datetime import datetime\nfrom elasticsearch import helpers\nfrom pyspark.sql import types as T\nSCHEMA = T.StructType(\n    [\n        T.StructField(\"id\", T.StringType(), True),\n        T.StructField(\"product_id\", T.StringType(), True),\n        T.StructField(\"product_version\", T.StringType(), True),\n        T.StructField(\"user_id\", T.StringType(), True),\n        T.StructField(\"user_name\", T.StringType(), True),\n        T.StructField(\"user_language\", T.StringType(), True),\n        T.StructField(\"user_device\", T.StringType(), True),\n        T.StructField(\"user_purchased\", T.StringType(), True),\n        T.StructField(\"userreview_url\", T.StringType(), True),\n        T.StructField(\"rating\", T.ShortType(), True),\n        T.StructField(\"country\", T.StringType(), True),\n        T.StructField(\"date\", T.StringType(), True),\n        T.StructField(\"language\", T.StringType(), True),\n        T.StructField(\"title\", T.StringType(), True),\n        T.StructField(\"title_t\", T.StringType(), True),\n        T.StructField(\"content\", T.StringType(), True),\n        T.StructField(\"content_t\", T.StringType(), True),\n        T.StructField(\"reply_id\", T.StringType(), True),\n        T.StructField(\"reply\", T.StringType(), True),\n        T.StructField(\"reply_t\", T.StringType(), True),\n        T.StructField(\"reply_date\", T.StringType(), True),\n        T.StructField(\"process_granularity\", T.StringType(), True),\n        T.StructField(\"process_date\", T.StringType(), True),\n        T.StructField(\"process_hour\", T.StringType(), True),\n        T.StructField(\"platform\", T.StringType(), True)\n    ]\n)\ncolumns = ['id',\n 'product_id',\n 'product_version',\n 'user_id',\n 'user_name',\n 'user_language',\n 'user_device',\n 'user_purchased',\n 'userreview_url',\n 'rating',\n 'country',\n 'date',\n 'language',\n 'title',\n 'title_t',\n 'content',\n 'content_t',\n 'reply_id',\n 'reply',\n 'reply_t',\n 'reply_date',\n 'process_granularity',\n 'process_date',\n 'process_hour',\n 'platform']\nb2 = lack.select(\"review_id\").collect()\nbbb = [str(bb['review_id']) for bb in b2]\ni = 47104\ndf = None\nwhile(i < len(bbb)):\n    print i,i+1024\n    b = bbb[i:i+1024]\n    res = helpers.scan(es,\n        query={\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\n                            \"terms\": {\n                                \"_id\": b\n                            }\n                        }\n                    ]\n                }\n            }\n        },\n        index=\"int-reviews-ios*\",\n        size=1024\n    )\n    ll = []\n    for value in res:\n        a = value['_source']\n        a[\"id\"] = value['_id']\n        a = {k: v for k, v in a.items() if k in columns}\n        ll.append(a)\n    if len(ll) != 1024:\n        print len(ll)\n    df = spark.createDataFrame(ll, schema=SCHEMA)\n    if(i + 1024 >=len(bbb)):\n        break\n    else:\n        i += 1024\n    df.write.mode(\"append\").partitionBy(\"date\").parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/aa.review_missing_before_event_02_11_yyz_3/\")\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200220-023823_1849677376","metadata":{},"outputs":[],"source":["\nfrom elasticsearch import Elasticsearch\nimport json\n\nfrom pyspark.sql.functions import count, avg\n\n# review_miss_list = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.old_es_0211_before_2/event_month={2019-10,2019-11,2019-12,2020-01}/\").collect()\n# print review_miss_list\nl =[5521202303, 5524091158, 5522336479, 5522731791, 5276117530, 5521920647]\n\n\n\n\n\n# for x in review_miss_list:\n#     l.append(str(x.review_id))\n# print len(l)\nnot_find=[]\ndef es_doc(review_id):\n    es_connection = Elasticsearch([\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200\"],http_auth=(\"bdp\",\"C38vEJEuraCw\"),port=19200)\n    doc = es_connection.search(index=\"int-ss-review_v1-*\", body={\"query\":{\"match\":{\"_id\":\"{}\".format(review_id)}}})\n    if len(doc[\"hits\"][\"hits\"]) < 1:\n        not_find.append(review_id)\n\nfor review_id in check_review_list:\n    es_doc(review_id)\n\nprint not_find"]},{"cell_type":"code","execution_count":0,"id":"20200211-071112_1468224318","metadata":{},"outputs":[],"source":["%%sh\n\n# aws s3 ls s3://b2c-prod-data-pipeline-raw-review/unified/review.v001/fact/process_granularity=hourly/process_date=2020-02-06/process_hour=23/device_code=android-all/market_code=google-play/\naws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v3/fact/process_granularity=daily/process_date=2020-02-12/process_hour=23/device_code=android-all/market_code=google-play/\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200212-015235_172770148","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v001/fact/process_granularity=hourly/process_date=2020-02-06/process_hour=23/device_code=android-all/market_code=google-play/\")\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200211-071500_1645368588","metadata":{},"outputs":[],"source":["\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v001/fact/\").show(60,False,vertical=True)"]},{"cell_type":"code","execution_count":0,"id":"20200211-072148_570577758","metadata":{},"outputs":[],"source":["\n\nfrom ast import literal_eval\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\ndef compare_two_values(key, raw_value, unified_value):\n    try:\n        if raw_value ==\"\" :\n            if unified_value != None:\n                print \"NOT EQUAL\"\n        elif isinstance(unified_value,int):\n            if int(raw_value) == unified_value:\n                pass\n            else:\n                print 'NOT EQUAL!' , key , raw_value,unified_value\n        elif str(raw_value) == str(unified_value.decode(\"utf-8\")):\n            pass\n        else:\n            print 'NOT EQUAL!' , key , raw_value,unified_value\n    except UnicodeEncodeError as e :\n        print key, raw_value, unified_value\n        print e\n\ndef get_sample_test_data():\n    result_df = spark.read.format(\"avro\").load(\"s3://b2c-prod-data-pipeline-raw-review/raw/review.v1/insert/process_granularity=DAILY/process_date=2020-02-16/process_hour=23/platform=ios/aidpad927e6761e9cafd7fb8bb8f5d593cc1.avro/\")\n    print result_df.show()\n    result = result_df.limit(1).select(\"_identifier\",\"source\").take(1)\n    print result \n    raw_data_source = literal_eval(result[0][\"source\"])[0]\n    return raw_data_source\n\ndef get_unified_test_data(raw_result_df):\n    return spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2020-02-16/process_hour=23/device_code=ios-all/market_code=apple-store/\").filter(\"review_id='{}'\".format(raw_result_df[\"id\"])).take(10)\n\nraw_result_df=get_sample_test_data()\nprint 'sample data :', raw_result_df\n\n\nunified_result_df = get_unified_test_data(raw_result_df)\n\ndef compare_df_value(raw_result_df, unified_result_df):\n    \n    print raw_result_df, unified_result_df\n    print 'start to compare'\n    for key,value in raw_result_df.items():\n        if key == 'userreview_url':\n            compare_two_values(key, value, unified_result_df[0]['user_review_url'])\n        elif key == 'title_t' or key =='process_date' or key == 'content_t' or key=='process_granularity' or key=='reply_t' or key=='process_hour' or key=='platform':\n            pass\n        elif key == 'id' :\n            compare_two_values(key, value, unified_result_df[0]['review_id'])\n        elif key == 'platform' :\n            compare_two_values(key, platforms[value][\"device_code\"], unified_result_df[0][\"device_code\"])\n            compare_two_values(key, platforms[value][\"market_code\"], unified_result_df[0][\"market_code\"])\n\n        elif key == 'date' :\n            compare_two_values(key, value, unified_result_df[0]['time'])\n\n        elif key == 'product_id' :\n            compare_two_values(key, value, unified_result_df[0]['app_id'])\n\n        elif key == 'country' :\n            compare_two_values(key, value, unified_result_df[0]['country_code'])\n\n        else:\n            compare_two_values(key, value, unified_result_df[0][key])\n        \ncompare_df_value(raw_result_df, unified_result_df)        \n"]},{"cell_type":"code","execution_count":0,"id":"20200221-064659_1742920526","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}