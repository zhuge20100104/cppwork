{"cells":[{"cell_type":"code","execution_count":0,"id":"20200422-014548_1607970962","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nselect  app_id,feed_id,store_id,estimate,category_id ,device_id from plproxy.execute_select_nestloop(\\$proxy\\$ \nselect  distinct app_id,feed_id,store_id,estimate,category_id ,device_id\n    from aa.app_store_daily_estimate_1\n    where \n      date = '2013-03-28'  order by estimate  asc limit 5 \\$proxy\\$) tbl \n      (app_id BIGINT,feed_id SMALLINT,store_id INT, estimate INT, category_id INT, device_id SMALLINT ) order by estimate  asc limit 5 ;\n\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200420-144355_1823945042","metadata":{},"outputs":[],"source":["\nimport datetime\nstart = \"2010-07-04\"\nend = \"2020-04-09\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\n\nmonth_day=list()\nfor days in xrange(date_range.days):\n    month_day.append(real_date1 + datetime.timedelta(days))\n\n\ntest_list= sorted(list(set([ d.strftime(\"%Y-%m-%d\")[:7] for d in month_day ])))\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200420-144411_144704204","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\n# s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2020-03-28\ndef check_diff(month):\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}-*/\".format(month))\n    df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=weekly/date={}-*/\".format(month))\n\n    df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=weekly/date={}-*/\".format(month))\n\n    df_1.createOrReplaceTempView(\"daily_rank\")\n    df_2.createOrReplaceTempView(\"daily_est\")\n    df_3.createOrReplaceTempView(\"monthly\")\n    \n    print \"compare app id: \" , month\n    # spark.sql(\"select distinct app_id from monthly except all select distinct app_id from  daily \").show(2)\n    # spark.sql(\"select distinct app_id from daily except all select distinct app_id from  monthly \").show(2)\n    spark.sql('''select daily_est.app_id, daily_est.free_app_download, daily_est.country_code, daily_est.paid_app_download, daily_est.revenue, daily_est.device_code,daily_rank.category_id from daily_rank \n                join daily_est \n                on daily_rank.country_code= daily_est.country_code\n                and daily_rank.device_code=daily_est.device_code\n                and daily_rank.app_id = daily_est.app_id ''').createOrReplaceTempView(\"sum_category_daily\")\n\n    spark.sql(\n        \"select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download , sum(revenue) as revenue, device_code ,category_id from sum_category_daily group by country_code,device_code,app_id, category_id \").createOrReplaceTempView(\n        \"sum_daily\")\n\n    spark.sql(\n        '''select app_id, country_code, free_app_download, paid_app_download,  revenue, device_code, category_id, \n        RANKNUM() OVER (ORDER BY free_app_download) free_app_download,\n        RANKNUM() OVER (ORDER BY paid_app_download) paid_app_download,\n        RANKNUM() OVER (ORDER BY revenue) revenue,\n         from sum_daily  ''').createOrReplaceTempView(\n        \"order_daily\")\n\n\n    # spark.sql(\"select app_id, country_code, free_app_download , paid_app_download , revenue, device_code,category_id from daily group by country_code,device_code,app_id,category_id \").createOrReplaceTempView(\"sum_daily\")\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from sum_daily except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from monthly\").show(2)\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from monthly except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from sum_daily\").show(2)\n\n    except_1 = spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from sum_daily except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from monthly\").withColumn(\"date\", F.lit(month) ).withColumn(\"type\",F.lit(\"daily_monthly\"))\n    except_2 = spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from monthly except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code ,category_id from sum_daily\").withColumn(\"date\", F.lit(month)).withColumn(\"type\",F.lit(\"monthly_daily\"))\n\n    df_write_result = except_2.union(except_1)\n    df_write_result.show()\n    #if df_write_result.rdd.isEmpty():\n    #    print 'pass'\n#    else:\n #       print 'failed!!!!!!!' , month\n\n    # from aadatapipelinecore.core.utils.retry import retry\n    # def write_test_result(df_write_result):\n    #     df_write_result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_monthly_category_count/\",\n    #                                       mode=\"append\",\n    #                                       partitionBy=[\"date\"])\n    # retry(write_test_result,(df_write_result,),{},interval=10)\n\nsc.parallelize(map(check_diff, test_list), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200420-144816_949954199","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2020-01-01/\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200421-054532_1340015178","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=monthly/date=2020-01-31/\").where(\"app_id=640073731 and country_code='MT' and device_code='ios-phone'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200421-055005_894040223","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=monthly/date=2020-01-31/device_code=android-all/ --human\n"]},{"cell_type":"code","execution_count":0,"id":"20200421-075957_1396476817","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}