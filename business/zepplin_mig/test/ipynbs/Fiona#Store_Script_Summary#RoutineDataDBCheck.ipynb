{"cells":[{"cell_type":"code","execution_count":0,"id":"20200715-112333_448708888","metadata":{},"outputs":[],"source":["\n\"\"\"\nDB Check modules\n\"\"\"\n\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\n\nfrom collections import defaultdict\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\ndef get_store_db_data(sql):\n    citus_dsn_ = (\n        \"dbname='{db}' user='{user}' password='{password}' \"\n        \"host='{host}' port='{port}'\".format(\n            db=\"aa_store_db\",\n            user=\"citus_bdp_prod_app_int_qa\",\n            host=\"10.2.6.141\",\n            password=\"wZw8cfBuuklIskVG\",\n            port=5432\n        )\n    )\n    db_data = query(citus_dsn_, sql)\n    return db_data\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n# def get_store_db_data(sql):\n#     result = query(citus_settings(\"store\"), sql)\n#     return result\n\n\ndef get_start_end_date_list(granularity, date):\n    end = date\n    if granularity == 'weekly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(weeks=1) + relativedelta(\n            days=1)\n        start = datetime.datetime.strftime(start, '%Y-%m-%d')\n    elif granularity == 'monthly':\n        start = date[:7] + str('-01')\n    elif granularity == 'quarterly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(months=2)\n        start = datetime.datetime.strftime(start, '%Y-%m') + str('-01')\n    elif granularity == 'yearly':\n        start = date[:4] + str('-01-01')\n    return start, end\n\n\ndef get_check_date_granularity(date):\n    granularity_list = list()\n    quarterly_date_list = ['03-31', '06-30', '09-30', '12-31']\n    yearly_date_list = ['12-31']\n    check_date = datetime.datetime.strptime(date, '%Y-%m-%d')\n    if check_date.isoweekday() == 6:\n        granularity_list.append(\"weekly\")\n    if last_day_of_month(check_date) == date:\n        granularity_list.append(\"monthly\")\n    if date[-5:] in quarterly_date_list:\n        granularity_list.append(\"quarterly\")\n    if date[-5:] in yearly_date_list:\n        granularity_list.append(\"yearly\")\n\n    return granularity_list\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return (next_month - datetime.timedelta(days=next_month.day)).strftime(\"%Y-%m-%d\")\n\n\n\n\n\nclass TestStoreDownloadRevenue(unittest.TestCase):\n    trigger_date_config = (\"9 * * * *\", 2)\n\n    sql_daily_category = '''SELECT \n                               Sum(est_free_app_download), \n                               Sum(est_paid_app_download), \n                               Sum(est_revenue) \n                        FROM   (SELECT app_id, \n                                       device_code, \n                                       country_code, \n                                       category_id, \n                                       Sum(est_free_app_download) AS est_free_app_download, \n                                       Sum(est_paid_app_download) AS est_paid_app_download, \n                                       Sum(est_revenue)           AS est_revenue \n                                FROM   store.store_est_category_fact_v1 \n                                WHERE  date BETWEEN '{}' AND '{}' \n                                GROUP  BY app_id, \n                                          device_code, \n                                          country_code, \n                                          category_id) AS t;  '''\n\n    sql_daily_est = '''SELECT \n                               Sum(est_free_app_download), \n                               Sum(est_paid_app_download), \n                               Sum(est_revenue) \n                        FROM   (SELECT app_id, \n                                       device_code, \n                                       country_code, \n                                       Sum(est_free_app_download) AS est_free_app_download, \n                                       Sum(est_paid_app_download) AS est_paid_app_download, \n                                       Sum(est_revenue)           AS est_revenue \n                                FROM   store.store_est_fact_v2 \n                                WHERE  date BETWEEN '{}' AND '{}' \n                                GROUP  BY app_id, \n                                          device_code, \n                                          country_code) AS t;  '''\n\n    sql_pre_agg_category='''SELECT  \n                                   Sum(est_free_app_download), \n                                   Sum(est_paid_app_download), \n                                   Sum(est_revenue) \n                            FROM   store.store_est_category_t_{}_fact_v1 \n                            WHERE  date BETWEEN '{}' AND  '{}' \n                            AND    granularity='{}' and country_code='US' '''\n\n\n    sql_pre_agg_est='''SELECT \n                               Sum(est_free_app_download), \n                               Sum(est_paid_app_download), \n                               Sum(est_revenue) \n                        FROM   store.store_est_t_{}_fact_v2 \n                        WHERE  date BETWEEN '{}' AND  '{}' \n                        AND    granularity='{}' and country_code='US' '''\n\n    def setUp(self):\n        self.failed_ids = defaultdict(list)\n\n    def test_store_download_revenue_pre_agg(self):\n        trigger_date_config = (\"9 * * * *\", 5)\n        check_date_str, _ = _get_date_from_refresh_routing_config(trigger_date_config,\"test\")\n        check_date_str = str(check_date_str)\n        check_list = [{x: get_start_end_date_list(x, check_date_str)} for x in get_check_date_granularity(check_date_str)]\n        print check_date_str\n        print check_list\n        for agg_data in check_list:\n            for key, value in agg_data.items():\n                print key,value\n                granularity = key\n                start_date = value[0]\n                end_date = value[1]\n                est_daily_agg_result = get_store_db_data(self.sql_daily_est.format(start_date, end_date))[0]\n                est_pre_agg_result = get_store_db_data(self.sql_pre_agg_est.format(granularity[0], start_date, end_date, granularity))[0]\n\n                category_daily_agg_result = get_store_db_data(self.sql_daily_category.format(start_date, end_date))[0]\n                category_pre_agg_result = get_store_db_data(self.sql_pre_agg_category.format(granularity[0], start_date, end_date, granularity))[0]\n\n                # print est_daily_agg_result\n                # print est_pre_agg_result\n                est_metric = ['est_free_app_download', 'est_paid_app_download', 'est_revenue_download']\n                result_est = [ {est_metric: (long(daily_est) , long(pre_agg_est) )} for daily_est, pre_agg_est, est_metric in zip(est_daily_agg_result , est_pre_agg_result, est_metric) ]\n                # print result_est\n                [ type(x) for x in result_est ]\n\n                failed_ids = [ (k,v) for k, v in x.iteritems() if v[0] != v[1] for x in result_est ]\n                self.assertTrue(len(failed_ids) == 0, failed_ids)\n                \n                \n                category_metric = ['category_free_app_download', 'category_paid_app_download', 'category_revenue_download']\n                category_result = [ {category_metric: (long(daily_category) , long(pre_agg_category) )} for daily_category, pre_agg_category, category_metric in zip(category_daily_agg_result , category_pre_agg_result, category_metric) ]\n                [ type(x) for x in category_result ]\n                failed_ids =  [(k,v) for k, v in x.iteritems() if v[0] != v[1] for x in category_result ]\n                self.assertTrue(len(failed_ids) == 0, \"pre_agg metric %s failed\" % failed_ids)\n\n                \n\n\nunittest.main(argv=[''], verbosity=2, exit=False)"]},{"cell_type":"code","execution_count":0,"id":"20200717-121900_1127050936","metadata":{},"outputs":[],"source":["\n\"\"\"\nget date:  [ [month, [days]], [month, [days]], [month, [days]], ....... ]\n\"\"\"\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\nprint get_date_list(\"2010-01-01\",\"2010-03-01\")"]},{"cell_type":"code","execution_count":0,"id":"20200717-122812_1071335469","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(\n    spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"qa-data-db-check-fiona\"\n)"]},{"cell_type":"code","execution_count":0,"id":"20200716-022351_129885153","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef get_granularity(date):\n    granularity_list = list()\n    \n    quarterly_date_list = ['03-31', '06-30', '09-30', '12-31']\n    yearly_date_list = ['12-31']\n\n    check_date = datetime.datetime.strptime(date, '%Y-%m-%d')\n    if check_date.isoweekday() == 6:\n        granularity_list.append(\"weekly\")\n    if last_day_of_month(check_date) == date:\n        granularity_list.append(\"monthly\")\n    if date[-5:] in quarterly_date_list:\n        granularity_list.append(\"quarterly\")\n    if date[-5:] in yearly_date_list:\n        granularity_list.append(\"yearly\")\n\n        \n    return granularity_list\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return (next_month - datetime.timedelta(days=next_month.day)).strftime(\"%Y-%m-%d\")\n\n\nprint get_granularity(\"2020-02-29\")\n\n\n\ndef get_date_list(granularity, date):\n    end = date\n    if granularity == 'weekly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(weeks=1) + relativedelta(\n            days=1)\n        start = datetime.datetime.strftime(start, '%Y-%m-%d')\n    elif granularity == 'monthly':\n        start = date[:7] + str('-01')\n    elif granularity == 'quarterly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(months=2)\n        start = datetime.datetime.strftime(start, '%Y-%m') + str('-01')\n    elif granularity == 'yearly':\n        start = date[:4] + str('-01-01')\n    return start, end\n\nt = [ {x :get_date_list(x, '2016-12-31' )} for x in get_granularity(\"2016-12-31\")]\nfor x in t:\n    for key,value in x.items():\n        print key,value"]},{"cell_type":"code","execution_count":0,"id":"20200715-065359_889381302","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom collections import defaultdict\nfrom dateutil.relativedelta import relativedelta\n\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\nfrom applications.db_check_v1.common.base_test import PipelineTest\n\nfrom collections import defaultdict\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\n\ndef get_store_db_data(sql):\n    citus_dsn_ = (\n        \"dbname='{db}' user='{user}' password='{password}' \"\n        \"host='{host}' port='{port}'\".format(\n            db=\"aa_store_db\",\n            user=\"citus_bdp_prod_app_int_qa\",\n            host=\"10.2.6.141\",\n            password=\"wZw8cfBuuklIskVG\",\n            port=5432\n        )\n    )\n    db_data = query(citus_dsn_, sql)\n    return db_data\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\ndef get_start_end_date_list(granularity, date):\n    end = date\n    if granularity == 'weekly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(weeks=1) + relativedelta(\n            days=1)\n        start = datetime.datetime.strftime(start, '%Y-%m-%d')\n    elif granularity == 'monthly':\n        start = date[:7] + str('-01')\n    elif granularity == 'quarterly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(months=2)\n        start = datetime.datetime.strftime(start, '%Y-%m') + str('-01')\n    elif granularity == 'yearly':\n        start = date[:4] + str('-01-01')\n    return start, end\n\n\ndef get_check_date_granularity(date):\n    granularity_list = list()\n    quarterly_date_list = ['03-31', '06-30', '09-30', '12-31']\n    yearly_date_list = ['12-31']\n    check_date = datetime.datetime.strptime(date, '%Y-%m-%d')\n    if check_date.isoweekday() == 6:\n        granularity_list.append(\"weekly\")\n    if last_day_of_month(check_date) == date:\n        granularity_list.append(\"monthly\")\n    if date[-5:] in quarterly_date_list:\n        granularity_list.append(\"quarterly\")\n    if date[-5:] in yearly_date_list:\n        granularity_list.append(\"yearly\")\n\n    return granularity_list\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return (next_month - datetime.timedelta(days=next_month.day)).strftime(\"%Y-%m-%d\")\n\n\nclass TestStoreDownloadRevenuePreAgg(PipelineTest):\n    trigger_date_config = (\"13 * * * *\", 6)\n\n    sql_daily_category = \"SELECT Sum(est_free_app_download), Sum(est_paid_app_download), Sum(est_revenue) \" \\\n                         \"FROM (SELECT app_id, device_code, country_code, category_id, \" \\\n                         \"Sum(est_free_app_download) AS est_free_app_download, \" \\\n                         \"Sum(est_paid_app_download) AS est_paid_app_download, \" \\\n                         \"Sum(est_revenue) AS est_revenue \" \\\n                         \"FROM store.store_est_category_fact_v1 \" \\\n                         \"WHERE date BETWEEN '{}' AND '{}' \" \\\n                         \"GROUP BY app_id, device_code, country_code, category_id) AS t;\"\n\n    sql_daily_est = \"SELECT Sum(est_free_app_download), Sum(est_paid_app_download), Sum(est_revenue) \" \\\n                    \"FROM (SELECT app_id, device_code, country_code, \" \\\n                    \"Sum(est_free_app_download) AS est_free_app_download, \" \\\n                    \"Sum(est_paid_app_download) AS est_paid_app_download, \" \\\n                    \"Sum(est_revenue) AS est_revenue FROM store.store_est_fact_v2 \" \\\n                    \"WHERE date BETWEEN '{}' AND '{}' GROUP BY app_id, device_code, country_code) AS t;\"\n\n    sql_pre_agg_category = \"SELECT Sum(est_free_app_download), Sum(est_paid_app_download), \" \\\n                           \"Sum(est_revenue) FROM store.store_est_category_t_{}_fact_v1 \" \\\n                           \"WHERE date BETWEEN '{}' AND '{}' AND granularity='{}'\"\n\n    sql_pre_agg_est = \"SELECT Sum(est_free_app_download), Sum(est_paid_app_download), Sum(est_revenue) \" \\\n                      \"FROM store.store_est_t_{}_fact_v2 \" \\\n                      \"WHERE date BETWEEN '{}' AND '{}' AND granularity='{}'\"\n\n    failed_ids = defaultdict(list)\n\n    def test_store_download_revenue_pre_agg(self):\n\n        check_list = [{x: get_start_end_date_list(x, self.check_date_str)} for x in\n                      get_check_date_granularity(self.check_date_str)]\n\n        print check_list\n        for agg_data in check_list:\n            for key, value in agg_data.items():\n                print key, value\n                granularity = key\n                start_date = value[0]\n                end_date = value[1]\n                est_daily_agg_result = get_store_db_data(self.sql_daily_est.format(start_date, end_date))[0]\n                est_pre_agg_result = get_store_db_data(\n                    self.sql_pre_agg_est.format(granularity[0], start_date, end_date, granularity))[0]\n\n                category_daily_agg_result = \\\n                    get_store_db_data(self.sql_daily_category.format(start_date, end_date))[0]\n                category_pre_agg_result = get_store_db_data(\n                    self.sql_pre_agg_category.format(granularity[0], start_date, end_date, granularity))[0]\n\n                est_metric = ['est_free_app_download', 'est_paid_app_download', 'est_revenue_download']\n                result_est = [{est_metric: (long(daily_est), long(pre_agg_est))} for\n                              daily_est, pre_agg_est, est_metric in\n                              zip(est_daily_agg_result, est_pre_agg_result, est_metric)]\n                print [type(x) for x in result_est]\n\n                failed_ids = [(k, v) for k, v in x.iteritems() if v[0] != v[1] for x in result_est]\n                self.assertTrue(len(failed_ids) == 0, failed_ids)\n\n                category_metric = ['category_free_app_download', 'category_paid_app_download',\n                                   'category_revenue_download']\n                category_result = [{category_metric: (long(daily_category), long(pre_agg_category))} for\n                                   daily_category, pre_agg_category, category_metric in\n                                   zip(category_daily_agg_result, category_pre_agg_result, category_metric)]\n                print [type(x) for x in category_result]\n                failed_ids = [(k, v) for k, v in x.iteritems() if v[0] != v[1] for x in category_result]\n                self.assertTrue(len(failed_ids) == 0, \"pre_agg metric %s failed\" % failed_ids)\n                \n                \n                \nclass TestStoreDownloadRevenue(PipelineTest):\n    trigger_date_config = (\"9 * * * *\", 2)\n    hot_country_list = [ 'AU', 'BR', 'CN', 'FR', 'DE', 'IN', 'JP', 'KR', 'GB', 'US', 'WW']\n\n    sql_est_count = '''SELECT Count(1) FROM store.store_est_fact_v2 WHERE  date = '{}' '''\n\n    sql_category_count = \"SELECT Count(1) FROM store.store_est_category_fact_v1 WHERE date = '{}'\"\n\n    sql_est_country_count = \"SELECT DISTINCT device_code, Count(country_code) \" \\\n                            \"FROM (SELECT DISTINCT device_code, country_code \" \\\n                            \"FROM store.store_est_fact_v2 WHERE date = '{}' And country_code in ('{}') ) AS prod \" \\\n                            \"GROUP BY device_code ORDER By device_code\"\n\n    sql_category_country_count = \"SELECT DISTINCT device_code, Count(country_code) \" \\\n                                 \"FROM (SELECT DISTINCT device_code, country_code \" \\\n                                 \"FROM store.store_est_category_fact_v1 WHERE date = '{}' And country_code in ('{}') ) AS prod \" \\\n                                 \"GROUP BY device_code ORDER By device_code\"\n\n    sql_download_att_init_value = \"SELECT Sum(est_free_app_download) + Sum(est_paid_app_download) \" \\\n                                  \"AS est_app_download, Sum(est_organic_download) AS est_organic_download, \" \\\n                                  \"Sum(est_paid_download) AS est_paid_download \" \\\n                                  \"FROM store.store_est_fact_v2 WHERE date = '{}'\"\n\n    failed_ids = defaultdict(list)\n\n    def test_store_download_revenue(self):\n\n        db_est_count = get_store_db_data(self.sql_est_count.format(self.check_date_str))[0][0]\n        db_category_count = get_store_db_data(self.sql_category_count.format(self.check_date_str))[0][0]\n        db_download_attr_init = get_store_db_data(self.sql_download_att_init_value.format(self.check_date_str))[0]\n        db_est_country_count = get_store_db_data(self.sql_est_country_count.format(self.check_date_str, \"','\".join(self.hot_country_list)))\n        db_category_country_count = get_store_db_data(self.sql_category_country_count.format(self.check_date_str,\"','\".join(self.hot_country_list)))\n\n\n\n        self.assertNotEqual(db_est_count, 0, \"est data is not ready for date {}\".format(self.check_date_str))\n        self.assertNotEqual(db_category_count, 0,\n                            \"category data is not ready for date {}\".format(self.check_date_str))\n\n\n        self.assertEqual(db_download_attr_init[0], db_download_attr_init[1],\n                         \"download attr organic paid init value != free_app_download + paid_app_download\")\n        self.assertEqual(db_download_attr_init[2], 0,\n                         \"download attribution paid download init data is not equals to 0\")\n                         \n\n        self.assertTrue(db_est_country_count[0][1]==db_est_country_count[0][1]==db_est_country_count[2][1]  )\n        self.assertEqual(db_est_country_count[0][1], len(self.hot_country_list) )\n\n        self.assertTrue(db_category_country_count[0][1]==db_category_country_count[0][1]==db_category_country_count[2][1]  )\n        self.assertEqual(db_category_country_count[0][1], len(self.hot_country_list) )\n\n\n\nclass DnaChangeLog(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.mapping-log.v1/\" \\\n                       \"dimension/update_date={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date):\n        \"\"\"\n        :param date:  update date in s3 bucket path\n        :type date: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(\"2020-07-20\")).select(\"app_id\",\n                                                                                        \"company_id\",\n                                                                                        \"parent_company_id\",\n                                                                                        \"publisher_id\").distinct()\n        return unified_df\n\n\nclass StoreEstDaily(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date):\n        \"\"\"\n        :param date:  update date in s3 bucket path\n        :type date: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.format('delta').load(self._unified_s3_path).where(\"granularity='daily' and date='{}' \".format(date)).select(\"app_id\",\n                                                                                        \"company_id\",\n                                                                                        \"parent_company_id\",\n                                                                                        \"publisher_id\").distinct()\n        return unified_df\n\nclass TestStoreDownloadRevenue_new(PipelineTest):\n    trigger_date_config = (\"0 9 * * *\", 1)\n\n    def test_store_download_revenue_dna_change_log_accuracy(self):\n        dna_unified = DnaChangeLog(self.spark).get(self.check_date)\n        est_unified = StoreEstDaily(self.spark).get(self.check_date)\n        dna_unified.createOrReplaceTempView(\"dna_unified\")\n        est_unified.createOrReplaceTempView(\"est_unified\")\n        self.spark.sql(\n            \"select app_id, publisher_id, coalesce(company_id, 0) as company_id, coalesce(parent_company_id, 0) as parent_company_id  from est_unified \"\n            \"except select app_id, publisher_id,  coalesce(company_id, 0) as company_id, coalesce(parent_company_id, 0) as parent_company_id from dna_unified\").show()\n            \n        self.spark.sql(\n            \"select * from ( select app_id, publisher_id, coalesce(company_id, 0) as company_id, coalesce(parent_company_id, 0) as parent_company_id  from est_unified \"\n            \"except select app_id, publisher_id,  coalesce(company_id, 0) as company_id, coalesce(parent_company_id, 0) as parent_company_id from dna_unified ) where publisher_id !=0 \").show()\n    \n        # self.spark.sql(\n        #     \"select app_id, publisher_id, company_id, parent_company_id from dna_unified \"\n        #     \"except select app_id, publisher_id, company_id, parent_company_id from est_unified\").show()\n\nsend_message()\n"]},{"cell_type":"code","execution_count":0,"id":"20200716-113758_1111798376","metadata":{},"outputs":[],"source":["%%sh\n\ncat /tmp/db_check.log\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200721-023655_1441483555","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\n\neject_all_caches(spark)\n"]},{"cell_type":"code","execution_count":0,"id":"20200715-065413_2063081061","metadata":{},"outputs":[],"source":["\n\"\"\"\nTest DNA tag/hq/unified_app tables\n\"\"\"\nimport unittest\nimport datetime\nfrom collections import defaultdict\n\n\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.html_report_test_runner import HTMLTestRunner\n\n\n\nfrom aadatapipelinecore.core.monitoring.pipeline_monitor import running, fail, task_success\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.utils.identifier import package_id\nfrom aadatapipelinecore.core.utils.spark import canned_spark, stop\n\n\nfrom applications.db_check_v1.common.html_report_test_runner import HTMLTestRunner\nfrom applications.db_check_v1.common.utils import send_db_check_email\n\nimport psycopg2\nimport datetime\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\ndef get_dna_db_data(sql):\n    citus_dsn_ = (\n        \"dbname='{db}' user='{user}' password='{password}' \"\n        \"host='{host}' port='{port}'\".format(\n            db=\"aa_store_db\",\n            user=\"citus_bdp_prod_app_int_qa\",\n            host=\"10.2.6.141\",\n            password=\"wZw8cfBuuklIskVG\",\n            port=5432\n        )\n    )\n    db_data = query(citus_dsn_, sql)\n    return db_data\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\nclass DnaHQUnifiedData(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.company-hq-mapping.v1/\" \\\n                       \"dimension/_update_date={}/_update_hour={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, hour):\n        \"\"\"\n        :param date:  update date in s3 bucket path\n        :type date: string\n        :param hour:  update hour in s3 bucket\n        :type hour: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(date, hour))\n        return unified_df\n\n\nclass DnaTagUnifiedData(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.app-tag-mapping.v1/\" \\\n                       \"dimension/_update_date={}/_update_hour={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, hour):\n        \"\"\"\n        :param date:  update date in s3 bucket path\n        :type date: string\n        :param hour:  update hour in s3 bucket\n        :type hour: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(date, hour))\n        return unified_df\n\n\nclass DnaUnifiedAppSingleAppData(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-dna/unified/\" \\\n                       \"dna.in-unified-product-product-mapping.v1/dimension/_update_date={}/_update_hour={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, hour):\n        \"\"\"\n        :param date:  update date in s3 bucket path\n        :type date: string\n        :param hour:  update hour in s3 bucket\n        :type hour: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(date, hour)).where(\"status=1\")\n        return unified_df\n\n\n# def get_dna_db_data(sql):\n#     result = query(citus_settings(\"store\"), sql)\n#     return result\n\n\n# class TestDNAHQ(PipelineTest):\n#     trigger_date_config = (\"0 2,14 * * *\", 0)\n\n#     sql_hq = \"select count(1) from dna.dna_company_hq_dim_v1\"\n\n#     def setUp(self):\n#         super\n\n#     def test_dna_hq_completeness(self):\n#         self.check_date = self._get_check_date_from_routing_config(self.trigger_datetime)\n#         print type(self.check_date)\n#         day = self.check_date_str\n#         hour = self.check_date.strftime(\"%-H\")\n#         unified_df = DnaHQUnifiedData(self.spark).get(day, hour)\n#         unified_hq_count = unified_df.count()\n#         self.assertNotEqual(unified_hq_count, 0, \"DNA HQ unified data is not generated!\")\n#         db_hq_count = get_dna_db_data(self.sql_hq)[0][0]\n#         print unified_hq_count, db_hq_count\n#         self.assertEqual(unified_hq_count, db_hq_count,\n#                          \"DNA HQ unified data is not equals to db data! unified is {}, db is {}\".format(\n#                              unified_hq_count, db_hq_count))\n\n\nclass TestDNAUnifiedSingleAPP(PipelineTest):\n    trigger_date_config = (\"0 1 * * *\", 0)\n    sql_unified_single_app = \"select count(1) from dna.dna_app_unified_dim_v2\"\n\n    def test_dna_unified_app_single_app(self):\n        # self.check_date = self._get_check_date_from_routing_config(self.trigger_datetime)\n        print type(self.check_date)\n        day = self.check_date_str\n        hour = self.check_date.strftime(\"%-H\")\n        unified_df = DnaUnifiedAppSingleAppData(self.spark).get(day, hour)\n        unified_app_count = unified_df.count()\n        self.assertNotEqual(unified_app_count, 0, \"unified data is not generated!\")\n        db_unified_app_count = get_dna_db_data(self.sql_unified_single_app)[0][0]\n        self.assertEqual(unified_app_count, db_unified_app_count,\n                         \"Unified/single app data is not equals to db data! unified is {}, db is {}\".format(\n                             unified_app_count, db_unified_app_count))\n\n\n\nsend_message()\n"]},{"cell_type":"code","execution_count":0,"id":"20200717-122844_1233097528","metadata":{},"outputs":[],"source":["\n\nimport unittest\nimport datetime\nfrom collections import defaultdict\nfrom dateutil.relativedelta import relativedelta\n\n\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.html_report_test_runner import HTMLTestRunner\n\n\n\nfrom aadatapipelinecore.core.monitoring.pipeline_monitor import running, fail, task_success\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.utils.identifier import package_id\nfrom aadatapipelinecore.core.utils.spark import canned_spark, stop\n\n\nfrom applications.db_check_v1.common.html_report_test_runner import HTMLTestRunner\nfrom applications.db_check_v1.common.utils import send_db_check_email\n\nimport psycopg2\nimport datetime\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\n# def get_store_db_data(sql):\n#     citus_dsn_ = (\n#         \"dbname='{db}' user='{user}' password='{password}' \"\n#         \"host='{host}' port='{port}'\".format(\n#             db=\"aa_store_db\",\n#             user=\"citus_bdp_prod_app_int_qa\",\n#             host=\"10.2.6.141\",\n#             password=\"wZw8cfBuuklIskVG\",\n#             port=5432\n#         )\n#     )\n#     db_data = query(citus_dsn_, sql)\n#     return db_data\n   \n   \ndef get_store_db_data(sql):\n    citus_dsn_ = (\n        \"dbname='{db}' user='{user}' password='{password}' \"\n        \"host='{host}' port='{port}'\".format(\n            db=\"metadb\",\n            user=\"app_meta_qa\",\n            host=\"b2b-prod-uds-storage-meta-db-new.crlexxwtzodp.us-east-1.rds.amazonaws.com\",\n            password=\"IEdwiF2hmqzdKK43\",\n            port=5432\n        )\n    )\n    db_data = query(citus_dsn_, sql)\n    return db_data\n \n    \ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef get_start_end_date_list(granularity, date):\n    end = date\n    if granularity == 'weekly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(weeks=1) + relativedelta(\n            days=1)\n        start = datetime.datetime.strftime(start, '%Y-%m-%d')\n    elif granularity == 'monthly':\n        start = date[:7] + str('-01')\n    elif granularity == 'quarterly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(months=2)\n        start = datetime.datetime.strftime(start, '%Y-%m') + str('-01')\n    elif granularity == 'yearly':\n        start = date[:4] + str('-01-01')\n    return start, end\n    \n    \n    \ndef get_date_list(start_date, end_date, freq=\"D\"):\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in\n                 list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_start_end_date_list(granularity, date):\n    end = date\n    if granularity == 'weekly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(weeks=1) + relativedelta(\n            days=1)\n        start = datetime.datetime.strftime(start, '%Y-%m-%d')\n    elif granularity == 'monthly':\n        start = date[:7] + str('-01')\n    elif granularity == 'quarterly':\n        start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(months=2)\n        start = datetime.datetime.strftime(start, '%Y-%m') + str('-01')\n    elif granularity == 'yearly':\n        start = date[:4] + str('-01-01')\n    return start, end\n\n\ndef get_check_date_granularity(date):\n    granularity_list = list()\n    quarterly_date_list = ['03-31', '06-30', '09-30', '12-31']\n    yearly_date_list = ['12-31']\n    check_date = datetime.datetime.strptime(date, '%Y-%m-%d')\n    if check_date.isoweekday() == 6:\n        granularity_list.append(\"weekly\")\n    if last_day_of_month(check_date) == date:\n        granularity_list.append(\"monthly\")\n    if date[-5:] in quarterly_date_list:\n        granularity_list.append(\"quarterly\")\n    if date[-5:] in yearly_date_list:\n        granularity_list.append(\"yearly\")\n\n    return granularity_list\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return (next_month - datetime.timedelta(days=next_month.day)).strftime(\"%Y-%m-%d\")\n\n\n\nclass TestStoreDownloadAttributionPreAgg(PipelineTest):\n    trigger_date_config = (\"0 8 * * 4\", 5)\n    sql_paid_download_count = '''SELECT distinct date FROM store.store_est_fact_v2 WHERE  date between '{}' and\n                      '{}' and est_paid_app_download != 0 order by date desc'''\n\n    def test_download_attribution_db_pre_agg_timeleness(self):\n        print self.check_date\n        self.check_date = self._get_check_date_from_routing_config(self.trigger_datetime)\n        if self.check_date.isoweekday() == 7:\n            start, end = get_start_end_date_list(\"weekly\", self.check_date_str)\n            print start, end\n            db_est_count = get_store_db_data(self.sql_paid_download_count.format(start, end))\n            date_list = [d[0].strftime('%Y-%m-%d') for d in db_est_count]\n            self.assertEqual(len(date_list), 8,\n                             \"Miss days in download attr {}\".format(' ,'.join(map(str, date_list))))\n\n\nclass UnifedDownloadAttribution(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v4/\" \\\n                       \"fact/\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, start, end):\n        \"\"\"\n        :param start:  update date in s3 bucket path\n        :type start: string\n        :param end:  update hour in s3 bucket\n        :type end: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.format(\"delta\").load(self._unified_s3_path).where(\n            \"granularity='daily' and  date between '{}' and '{}'  \".format(start, end))\n        return unified_df\n\n\nclass TestStoreDownloadAttributionUnified(PipelineTest):\n    trigger_date_config = (\"0 8 * * 4\", 9)\n\n    def test_download_attribution_(self):\n        print self.check_date\n        self.check_date = self._get_check_date_from_routing_config(self.trigger_datetime)\n        if self.check_date.isoweekday() == 6:\n            start, end = get_start_end_date_list(\"weekly\", self.check_date_str)\n            UnifedDownloadAttribution(self.spark).get(start, end).createOrReplaceTempView(\"dwonlaod_attribution_share\")\n            date_list = self.spark.sql(\"select distinct date from dwonlaod_attribution_share\").collect()\n            date_list = [d[0].strftime('%Y-%m-%d') for d in date_list]\n            self.assertEqual(len(date_list), 7, \"Miss days in download attr {}\".format(' ,'.join(map(str, date_list))))\n\n\n\n\n\n\n\n\nclass TestStoreDownloadRevenuePreAgg(PipelineTest):\n    trigger_date_config = (\"0 11 * * 4\", 5)\n\n    sql_download_att_daily = \"SELECT Sum(est_free_app_download) + Sum(est_paid_app_download) , Sum(est_revenue) \" \\\n                                  \"AS est_app_download, Sum(est_organic_download) AS est_organic_download, \" \\\n                                  \"Sum(est_paid_download) AS est_paid_download \" \\\n                                  \"FROM store.store_est_fact_v2 WHERE date between '{}' and '{}' \"\n\n\n    sql_pre_agg_est = \"SELECT Sum(est_free_app_download) + Sum(est_paid_app_download), Sum(est_revenue), \" \\\n                      \"sum(est_organic_download), sum(est_paid_download) FROM store.store_est_t_{}_fact_v2 \" \\\n                      \"WHERE date BETWEEN '{}' AND '{}' AND granularity='{}'\"\n\n    failed_ids = defaultdict(list)\n\n    def test_store_download_attr_pre_agg(self):\n\n        check_list = [{x: get_start_end_date_list(x, self.check_date_str)} for x in\n                      get_check_date_granularity(self.check_date_str)]\n\n        print check_list\n        for agg_data in check_list:\n            for key, value in agg_data.items():\n                print key, value\n                granularity = key\n                start_date = value[0]\n                end_date = value[1]\n                est_daily_agg_result = get_store_db_data(self.sql_download_att_daily.format(start_date, end_date))[0]\n                est_pre_agg_result = get_store_db_data(\n                    self.sql_pre_agg_est.format(granularity[0], start_date, end_date, granularity))[0]\n\n                print 'est_daily_agg_result', est_daily_agg_result\n                print 'est_daily_agg_result', est_daily_agg_result\n\n\n                est_metric = ['est_free_app_download', 'est_paid_app_download', 'est_revenue_download']\n                result_est = [{est_metric: (long(daily_est), long(pre_agg_est))} for\n                              daily_est, pre_agg_est, est_metric in\n                              zip(est_daily_agg_result, est_pre_agg_result, est_metric)]\n                print [type(x) for x in result_est]\n\n                failed_ids = [(k, v) for k, v in x.iteritems() if v[0] != v[1] for x in result_est]\n                self.assertTrue(len(failed_ids) == 0, failed_ids)\n\n\n# def get_store_db_data(sql):\n#     result = query(citus_settings(\"metadb\"), sql)\n#     return result\n\n\nclass TestStoreMetaDB(PipelineTest):\n    trigger_date_config = (\"13 * * * *\", 6)\n\n    sql_meta_db = \"select table_name, app_stats -> 'latest_available_date' from meta where table_name like '%store_est_t_{}_fact%' \" \\\n                  \"and is_active=True limit 2;\"\n\n    failed_ids = defaultdict(list)\n\n    def test_store_meta_db_accuracy(self):\n\n        check_list = [{x: get_start_end_date_list(x, self.check_date_str)} for x in\n                      get_check_date_granularity(self.check_date_str)]\n\n        print 'meta db ', check_list\n        for agg_data in check_list:\n            for key, value in agg_data.items():\n                print key, value\n                granularity = key\n                end_date = value[1]\n                est_daily_agg_result = get_store_db_data(self.sql_meta_db.format(granularity[0]))[0]\n                print est_daily_agg_result\n                self.assertEquals(end_date, est_daily_agg_result[1],\n                                  \"meta db is not updated!, it should be {}, but current value is {}\".format(\n                                      end_date, est_daily_agg_result[1]))\n                print 'pass'\n\nclass TestStoreDownloadAttributionDB(PipelineTest):\n    trigger_date_config = (\"0 8 * * 4\", 5)\n    # sql_paid_download_count = '''SELECT distinct date FROM store.store_est_fact_v2 WHERE  date between '{}' and\n    #                   '{}' and est_paid_app_download != 0 order by date desc  '''\n\n    def test_download_attribution_db_timeleness(self):\n        print self.check_date\n        self.check_date = self._get_check_date_from_routing_config(self.trigger_datetime)\n        if self.check_date.isoweekday() == 6:\n            print 'need to check startuday'\n        #     start, end = get_start_end_date_list(\"weekly\", self.check_date_str)\n        #     print start, end\n        #     db_est_count = get_store_db_data(self.sql_paid_download_count.format(start, end))\n        #     date_list = [d[0].strftime('%Y-%m-%d') for d in db_est_count]\n        #     self.assertEqual(len(date_list), 8, \"Miss days in download attr {}\".format(' ,'.join(map(str, date_list))))\n\nsend_message()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200731-064435_411746840","metadata":{},"outputs":[],"source":["\nclass TestStoreMetaDB(PipelineTest):\n    trigger_date_config = (\"13 * * * *\", 122)\n    \n    def test_store_meta_db(self):\n        print self.check_date\nsend_message()\n"]},{"cell_type":"code","execution_count":0,"id":"20200731-041429_476878879","metadata":{},"outputs":[],"source":["%%sh\n\ncat /tmp/db_check.log\n"]},{"cell_type":"code","execution_count":0,"id":"20200717-025631_519174718","metadata":{},"outputs":[],"source":["\nfrom applications.db_check_v1.common.html_report_test_runner import HTMLTestRunner\n\ndef send_message():\n    log_file = \"/tmp/db_check.log\"\n    with open(log_file, \"w\") as html_file:\n        suite = unittest.TestSuite()\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestStoreMetaDB))\n\n        runner = HTMLTestRunner(\n            stream=html_file,\n            title='DB Test Report',\n            description='This db_check the report output by Tech Team.'\n        )\n\n        failed_count = 0\n        result_list = runner.run(suite).result\n        for result in result_list:\n            if result[0] == 1 or result[0] == 2:\n                failed_count += 1\n\n    with open(log_file, 'r') as html_file:\n        today = datetime.date.today()\n        str_today = today.strftime(\"%Y-%m-%d\")\n\n        title = \"Data Refresh Multiple Check Report - \" + str_today + \" - \"\n        if failed_count == 0:\n            title += \"Passed\"\n        else:\n            title += \"Failed\"\n        print html_file.read()\n        # send_db_check_email(title, html_file.read())\n        \n\n"]},{"cell_type":"code","execution_count":0,"id":"20200731-041425_52987330","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20200717-075053_5445551","metadata":{},"outputs":[],"source":["\nl = [Row(date=datetime.date(2020, 7, 11)), Row(date=datetime.date(2020, 7, 6)), Row(date=datetime.date(2020, 7, 5)), Row(date=datetime.date(2020, 7, 9)), Row(date=datetime.date(2020, 7, 10)), Row(date=datetime.date(2020, 7, 7)), Row(date=datetime.date(2020, 7, 8))]\n\nprint len(l)"]},{"cell_type":"code","execution_count":0,"id":"20200717-031432_1020673991","metadata":{},"outputs":[],"source":["# Copyright (c) 2019 App Annie Inc. All rights reserved.\n\n\"\"\"\nTest DNA tag/hq/unified_app tables\n\"\"\"\n\nimport datetime\nfrom collections import defaultdict\n\nfrom applications.db_check_v1.common.constants import query, citus_settings\n\nfrom applications.db_check_v1.common.base_test import PipelineTest\n\n\nclass DnaHQUnifiedData(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.company-hq-mapping.v1/\" \\\n                       \"dimension/_update_date={}/_update_hour={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, hour):\n        \"\"\"\n        :param date:  update date in s3 bucket path\n        :type date: string\n        :param hour:  update hour in s3 bucket\n        :type hour: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(date, hour))\n        return unified_df\n\n\nclass DnaTagUnifiedData(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.app-tag-mapping.v1/\" \\\n                       \"dimension/_update_date={}/_update_hour={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, hour):\n        \"\"\"\n        :param date:  update date in s3 bucket path\n        :type date: string\n        :param hour:  update hour in s3 bucket\n        :type hour: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(date, hour))\n        return unified_df\n\n\nclass DnaUnifiedAppSingleAppData(object):\n    \"\"\"\n    Get data from unified s3 bucket, then return a DataFrame\n    \"\"\"\n    _unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-dna/unified/\" \\\n                       \"dna.in-unified-product-product-mapping.v1/dimension/_update_date={}/_update_hour={}\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, hour):\n        \"\"\"\n        :param date:  update date in s3 bucket path\n        :type date: string\n        :param hour:  update hour in s3 bucket\n        :type hour: string\n        :return: unified_df\n        :rtype: DataFrame\n        \"\"\"\n        unified_df = self.spark.read.parquet(self._unified_s3_path.format(date, hour)).where(\"status=1\")\n        return unified_df\n\n\ndef get_dna_db_data(sql):\n    result = query(citus_settings(\"store\"), sql)\n    return result\n\n\nclass TestDNAHQ(PipelineTest):\n    trigger_date_config = (\"0 2,14 * * *\", 0)\n\n    sql_hq = \"select count(1) from dna.dna_company_hq_dim_v1\"\n\n    def setUp(self):\n        super(TestDNAHQ, self).setUp()\n\n    def test_dna_hq_completeness(self):\n        self.check_date = self._get_check_date_from_routing_config(self.trigger_datetime)\n        print type(self.check_date)\n        day = self.check_date_str\n        hour = self.check_date.strftime(\"%-H\")\n        unified_df = DnaHQUnifiedData(self.spark).get(day, hour)\n        unified_hq_count = unified_df.count()\n        self.assertNotEqual(unified_hq_count, 0, \"DNA HQ unified data is not generated!\")\n        db_hq_count = get_dna_db_data(self.sql_hq)[0][0]\n        print unified_hq_count, db_hq_count\n        self.assertEqual(unified_hq_count, db_hq_count,\n                         \"DNA HQ unified data is not equals to db data! unified is {}, db is {}\".format(\n                             unified_hq_count, db_hq_count))\n\n\nclass TestDNATAG(PipelineTest):\n    trigger_date_config = (\"0 1,7,13,19 * * *\", 0)\n    sql_tag = \"select tag_type_code, count(1) from dna.dna_app_tag_dim_v1 \" \\\n              \"where tag_type_code in ('genre', 'modifier') group by tag_type_code\"\n\n    def setUp(self):\n        super(TestDNATAG, self).setUp()\n\n    def test_dna_tag(self):\n        self.check_date = self._get_check_date_from_routing_config(self.trigger_datetime)\n        print type(self.check_date)\n        day = self.check_date_str\n        hour = self.check_date.strftime(\"%-H\")\n        unified_df = DnaTagUnifiedData(self.spark).get(day, hour)\n        unified_tag_genre_count = unified_df.where(\"tag_type_code = 'genre'\").count()\n        unified_tag_modifier_count = unified_df.where(\"tag_type_code = 'modifier'\").count()\n\n        self.assertNotEqual(unified_tag_genre_count, 0, \"DNA tag genre unified data is not generated!\")\n        self.assertNotEqual(unified_tag_modifier_count, 0, \"DNA tag modifier unified data is not generated!\")\n\n        print get_dna_db_data(self.sql_tag)\n        db_tag_modifier_count = get_dna_db_data(self.sql_tag)[0][1]\n        db_tag_genre_count = get_dna_db_data(self.sql_tag)[1][1]\n\n        self.assertEqual(unified_tag_genre_count, db_tag_genre_count,\n                         \"DNA Tag unified data is not equals to db genre data! unified is {}, db is {}\".format(\n                             unified_tag_genre_count, db_tag_genre_count))\n        self.assertEqual(unified_tag_modifier_count, db_tag_modifier_count,\n                         \"DNA Tag unified data is not equals to db modifier data! unified is {}, db is {}\".format(\n                             unified_tag_modifier_count, db_tag_modifier_count))\n\n\nclass TestDNAUnifiedSingleAPP(PipelineTest):\n    trigger_date_config = (\"0 1 * * *\", 0)\n    sql_unified_single_app = \"select count(1) from dna.dna_app_unified_dim_v1\"\n\n    def test_dna_unified_app_single_app(self):\n        check_date_str, check_hour_str = _get_date_from_refresh_routing_config(trigger_date_config, \"hour\")\n        trigger_datetime = datetime.datetime.strptime(\"2020-07-14 17:00:00\", '%Y-%m-%d %H:%M:%S')\n        self._get_check_date_from_routing_config(trigger_datetime).strftime(\"%Y-%m-%d\")\n        unified_df = DnaUnifiedAppSingleAppData(self.spark).get(check_date_str, check_hour_str)\n        unified_app_count = unified_df.count()\n        self.assertNotEqual(unified_app_count, 0, \"unified data is not generated!\")\n        db_unified_app_count = get_dna_db_data(self.sql_unified_single_app)[0][0]\n        self.assertEqual(unified_app_count, db_unified_app_count,\n                         \"Unified/single app data is not equals to db data! unified is {}, db is {}\".format(\n                             unified_app_count, db_unified_app_count))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}