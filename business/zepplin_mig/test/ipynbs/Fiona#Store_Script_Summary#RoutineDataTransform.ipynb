{"cells":[{"cell_type":"code","execution_count":0,"id":"20200706-035619_860812034","metadata":{},"outputs":[],"source":["%md\nstore download revenue est/category daily \naa.store.app-est.v4\naa.store.app-est-category.v4\n\nstore download revenue preload est/category daily\naa.store.app-est-dna-log.v1\naa.store.app-est-category-load.v4\n\nstore download attr daily\naa.store.download-attribution.v4\n\n\nstore download attr daily\naa.store.download-attribution-dna-log.v2\naa.store.download-attribution-category-load.v4\n"]},{"cell_type":"code","execution_count":0,"id":"20200720-064856_879058647","metadata":{},"outputs":[],"source":["\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date=2020-07-18/\").show()\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/granularity=daily/date=2020-07-18/\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200720-065036_711456903","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date=2020-07-18\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200713-063359_1085928459","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom applications.auto_pipeline.temp_script.utils.base_test import PipelineTest\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nimport aaplproxy\nfrom aadatapipelinecore.core.utils import email\nfrom applications.auto_pipeline.temp_script.utils.html_report_test_runner import HTMLTestRunner\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\nsql_download_attribution_with_est = \"\"\"\n\n    WITH download_attribution AS (\n        select *, CAST(est_non_organic_download_share as decimal(36,20)) as new_est_non_organic_download_share from download_attribution\n    );\n\n    WITH download_attribution_1 AS (\n\n     select device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share, \n        case when device_code='android-phone' THEN 'android-all' \n             when device_code='ios-phone' THEN 'ios-tablet' END AS new_device_code from download_attribution\n    );\n\n    WITH download_attribution_2\n    AS (\n        select distinct * from ( select new_device_code as device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share as est_non_organic_download_share from download_attribution_1 \n        union all\n        select device_code, country_code, granularity, date, product_id, est_non_organic_download_share from download_attribution ) as t1 where device_code!='android-phone'\n\n    );\n\n    WITH union_data AS (\n    select *, store_unified.device_code as unified_device_code , store_unified.country_code as unified_country_code\n    from store_unified left join download_attribution_2\n    on store_unified.device_code=download_attribution_2.device_code and\n    store_unified.country_code=UPPER(download_attribution_2.country_code) and\n    store_unified.app_id=download_attribution_2.product_id\n    where est_non_organic_download_share is not null\n\n    );\n\n\n    WITH calculate_data_prepare AS (\n    select app_id, coalesce(est_free_app_download, 0) as free_app_download,  coalesce(est_paid_app_download, 0 )  as paid_app_download, est_revenue as revenue, unified_device_code, unified_country_code, est_non_organic_download_share from union_data where not (est_free_app_download is null and est_paid_app_download is null) \n    );\n\n\n    WITH caculate_data AS (\n    select app_id, free_app_download, paid_app_download, revenue, unified_device_code, unified_country_code, est_non_organic_download_share, \n    round (( 1 - est_non_organic_download_share ) *(free_app_download+paid_app_download)) as organic_download, ( free_app_download+paid_app_download - ROUND(( 1- est_non_organic_download_share) * (free_app_download+paid_app_download))) as paid_download    from calculate_data_prepare\n    );\n\n\n    WITH compare_data_raw AS (\n    select app_id,free_app_download, paid_app_download, organic_download, paid_download, unified_device_code as device_code, unified_country_code as country_code from caculate_data\n    );\n\n\n    WITH compare_data_unified AS (\n    select app_id,coalesce(est_free_app_download, 0 ) as free_app_download,  coalesce(est_paid_app_download, 0 ) as paid_app_download,  est_organic_download as organic_download, est_paid_download as paid_download, device_code,  country_code from test_unified_download_attribution\n    );\n\n\n\n    \"\"\"\n\n\ndef test_daily_download_attr_with_est(spark, test_date_list):\n    print test_date_list\n    spark.read.option(\"basePath\",\n                      \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\").parquet(\n        \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(\n            test_date_list[-1])).createOrReplaceTempView(\n        \"download_attribution\")\n\n    spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log.v1/fact/\").where(\n        \"granularity='daily' and  date in ('{}') \".format(\"','\".join(\n            test_date_list))).createOrReplaceTempView(\"test_unified_download_attribution\")\n\n    spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/\").where(\n        \"granularity='daily' and  date in ('{}') \".format(\"','\".join(\n            test_date_list))).createOrReplaceTempView(\"store_unified\")\n\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n            }\n\n        ]\n    }\n    run(spark, ingest_msg, sql_download_attribution_with_est)\n    qa_df_1 = spark.sql(\"select * from compare_data_raw where except select * from compare_data_unified\")\n    qa_df_2 = spark.sql(\"select * from compare_data_unified except select * from compare_data_raw\")\n    count_1 = spark.sql(\"select count(*) from compare_data_raw \").take(1)\n    count_2 = spark.sql(\"select count(*) from compare_data_unified \").take(1)\n    qa_df_1.show()\n    qa_df_2.show()\n    # if count_1[0][0] != count_2[0][0]:\n    #     print 'failed!!!!!!!!!!!!!'\n\n    # spark.sql(\"select * from compare_data_unified_add_to_est where diff !=0  \").show()\n    # eject_all_caches(spark)\n    return qa_df_1, qa_df_2, count_1, count_2\n\n\nDEFAULT_RECIPIENTS = \"fzhang@appannie.com\"\n\n\ndef send_db_check_email(title, text_context, key=None):\n    default_recipients = [DEFAULT_RECIPIENTS]\n    email.send(title, text_context, default_recipients, sender='dev-qa-data-quality@appannie.com')\n\n\ndef get_last_week_date(check_date):\n    index = (check_date.isoweekday() + 1) % 7\n    print index\n    if index == 0:\n        end = check_date\n    else:\n        end = check_date - datetime.timedelta(index)\n    start = end + datetime.timedelta(-7)\n    date_range = end - start\n    dates = list()\n    print date_range.days\n    for days in xrange(date_range.days + 1):\n        dates.append(str(start + datetime.timedelta(days)))\n    return dates[1:]\n\n\nclass demoTest(PipelineTest):\n    trigger_date_config = (\"0 12 * * *\", 8)\n\n    def test_daily_unified_est_category_data(self):\n        trigger_datetime = datetime.datetime.strptime(\"2020-07-04\", '%Y-%m-%d')\n\n        raw_except_unified, unified_except_raw, count_1, count_2 = test_daily_download_attr_with_est(\n            self.spark, get_last_week_date(datetime.date(2020, 07, 05)))\n\n        self.assertTrue(raw_except_unified.rdd.isEmpty(), \"raw except unified is not empty\")\n        self.assertTrue(unified_except_raw.rdd.isEmpty(), \"unified except raw is not empty\")\n\n        # currently store_est has duplicate data...\n        self.assertEqual(count_1, count_2, \" count is not same\")\n\n\ndef send_message(spark):\n    log_file = \"/tmp/db_check.log\"\n    with open(log_file, \"w\") as html_file:\n        suite = unittest.TestSuite()\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(demoTest))\n        runner = HTMLTestRunner(\n            stream=html_file,\n            title='Transform Test Report',\n            description='This db_check the report output by Tech Team.'\n        )\n\n        failed_count = 0\n        result_list = runner.run(suite).result\n        for result in result_list:\n            if result[0] == 1 or result[0] == 2:\n                failed_count += 1\n\n    with open(log_file, 'r') as html_file:\n        today = datetime.date.today()\n        str_today = today.strftime(\"%Y-%m-%d\")\n\n        title = \"store daily check - store download attribution daily - \" + str_today + \" - \"\n        if failed_count == 0:\n            title += \"Passed\"\n        else:\n            title += \"Failed\"\n        send_db_check_email(title, html_file.read())\n\nsend_message(spark)"]},{"cell_type":"code","execution_count":0,"id":"20200713-065838_1271810897","metadata":{},"outputs":[],"source":["\ndef get_last_week_date(check_date):\n    index = (check_date.isoweekday() + 1) % 7\n    print index\n    if index == 0:\n        end = check_date\n    else:\n        end = check_date - datetime.timedelta(index)\n    start = end + datetime.timedelta(-7)\n    date_range = end - start\n    dates = list()\n    print date_range.days\n    for days in xrange(date_range.days + 1):\n        dates.append(str(start + datetime.timedelta(days)))\n    return dates\n\nprint get_last_week_date()"]},{"cell_type":"code","execution_count":0,"id":"20200710-072256_1602578206","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom applications.auto_pipeline.temp_script.utils.base_test import PipelineTest\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nimport aaplproxy\nfrom aadatapipelinecore.core.utils import email\nfrom applications.auto_pipeline.temp_script.utils.html_report_test_runner import HTMLTestRunner\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\nsql_download_attribution_with_est = \"\"\"\n\n    WITH download_attribution AS (\n        select *, CAST(est_non_organic_download_share as decimal(36,20)) as new_est_non_organic_download_share from download_attribution\n    );\n\n    WITH download_attribution_1 AS (\n\n     select device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share, \n        case when device_code='android-phone' THEN 'android-all' \n             when device_code='ios-phone' THEN 'ios-tablet' END AS new_device_code from download_attribution\n    );\n\n    WITH download_attribution_2\n    AS (\n        select distinct * from ( select new_device_code as device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share as est_non_organic_download_share from download_attribution_1 \n        union all\n        select device_code, country_code, granularity, date, product_id, est_non_organic_download_share from download_attribution ) as t1 where device_code!='android-phone'\n\n    );\n\n    WITH union_data AS (\n    select *, store_unified.device_code as unified_device_code , store_unified.country_code as unified_country_code\n    from store_unified left join download_attribution_2\n    on store_unified.device_code=download_attribution_2.device_code and\n    store_unified.country_code=UPPER(download_attribution_2.country_code) and\n    store_unified.app_id=download_attribution_2.product_id\n    where est_non_organic_download_share is not null\n\n    );\n\n\n    WITH calculate_data_prepare AS (\n    select app_id, coalesce(est_free_app_download, 0) as free_app_download,  coalesce(est_paid_app_download, 0 )  as paid_app_download, est_revenue as revenue, unified_device_code, unified_country_code, est_non_organic_download_share from union_data where not (est_free_app_download is null and est_paid_app_download is null) \n    );\n\n\n    WITH caculate_data AS (\n    select app_id, free_app_download, paid_app_download, revenue, unified_device_code, unified_country_code, est_non_organic_download_share, \n    round (( 1 - est_non_organic_download_share ) *(free_app_download+paid_app_download)) as organic_download, ( free_app_download+paid_app_download - ROUND(( 1- est_non_organic_download_share) * (free_app_download+paid_app_download))) as paid_download    from calculate_data_prepare\n    );\n    \n\n \n\n\n\n    WITH compare_data_raw AS (\n    select app_id,free_app_download, paid_app_download, unified_device_code as device_code, unified_country_code as country_code from caculate_data\n    );\n\n\n    WITH compare_data_unified AS (\n    select app_id,coalesce(est_free_app_download, 0 ) as free_app_download,  coalesce(est_paid_app_download, 0 ) as paid_app_download,  device_code,  country_code from test_unified_download_attribution\n    );\n\n\n\n    \"\"\"\n\n\nstart = \"2020-06-28\"\nend = \"2020-07-05\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp = list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days): temp})\n\ntest_path = list()\n\nfor x in sar_list:\n    for key, item in x.items():\n        test_path.append(\n            (\n                [\n                    \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(\n                        key)],\n                [i.strftime(\"%Y-%m-%d\") for i in item],\n                [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date={}\".format(\n                        i) for i in item]\n            )\n        )\nprint test_path[0][0][0]\nprint test_path[0][1]\ndef test_daily_download_attr_with_est(spark, test_date):\n    spark.read.option(\"basePath\",\n                      \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\").parquet(\n        test_path[0][0][0]).createOrReplaceTempView(\n        \"download_attribution\")\n\n\n    spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log.v1/fact/\").where(\n        \"granularity='daily' and  date in ('{}') \".format(\"','\".join(\n            test_path[0][1]))).createOrReplaceTempView(\"test_unified_download_attribution\")\n\n    spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/\").where(\n        \"granularity='daily' and  date in ('{}') \".format(\"','\".join(\n            test_path[0][1]))).createOrReplaceTempView(\"store_unified\")\n\n\n    print test_path[0][2][0]\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                # \"data_encoding\": \"parquet\",\n                # \"compression\": \"gzip\",\n                # \"name\": \"store_unified\",\n                # \"path\": test_path[0][2]\n            }\n           \n        ]\n    }\n    run(spark, ingest_msg, sql_download_attribution_with_est)\n    qa_df_1 = spark.sql(\"select * from compare_data_raw where except select * from compare_data_unified\")\n    qa_df_2 = spark.sql(\"select * from compare_data_unified except select * from compare_data_raw\")\n    count_1 = spark.sql(\"select count(*) from compare_data_raw \").take(1)\n    count_2 = spark.sql(\"select count(*) from compare_data_unified \").take(1)\n    qa_df_1.show()\n    qa_df_2.show()\n    # if count_1[0][0] != count_2[0][0]:\n    #     print 'failed!!!!!!!!!!!!!'\n\n    # spark.sql(\"select * from compare_data_unified_add_to_est where diff !=0  \").show()\n    # eject_all_caches(spark)\n    return qa_df_1, qa_df_2, count_1, count_2\n\nDEFAULT_RECIPIENTS = \"fzhang@appannie.com\"\n\ntest_daily_download_attr_with_est(spark, \"2020-06-27\")\ndef send_db_check_email(title, text_context, key=None):\n    default_recipients = [DEFAULT_RECIPIENTS]\n    email.send(title, text_context, default_recipients, sender='dev-qa-data-quality@appannie.com')\n\n\n\ndef get_last_week_date(check_date):\n    idx = (check_date.isoweekday() + 1) % 7 \n    print idx\n    if idx == 0:\n        print check_date\n    else:\n        sat = check_date - datetime.timedelta(idx)\n        print sat\n\n    end = sat\n    start = end + datetime.timedelta(-7)\n\n    date_range = end - start\n    dates = list()\n    sar_list = list()\n    print date_range.days\n    for days in xrange(date_range.days + 1):\n        dates.append(str(start + datetime.timedelta(days)))\n\n\nclass demoTest(PipelineTest):\n    trigger_date_config = (\"0 12 * * *\", 8)\n\n    def test_daily_unified_est_category_data(self):\n        trigger_datetime = datetime.datetime.strptime(\"2020-07-02\", '%Y-%m-%d')\n\n        raw_except_unified, unified_except_raw, count_1, count_2 = test_daily_download_attr_with_est(self.spark, \"2020-06-20\")\n\n        self.assertTrue(raw_except_unified.rdd.isEmpty(), \"raw except unified is not empty\")\n        self.assertTrue(unified_except_raw.rdd.isEmpty(), \"unified except raw is not empty\")\n        self.assertEqual(count_1,count_2, \" count is not same\")\n\n\n\ndef send_message(spark):\n    log_file = \"/tmp/db_check.log\"\n    with open(log_file, \"w\") as html_file:\n        suite = unittest.TestSuite()\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(demoTest))\n        runner = HTMLTestRunner(\n            stream=html_file,\n            title='Transform Test Report',\n            description='This db_check the report output by Tech Team.'\n        )\n\n        failed_count = 0\n        result_list = runner.run(suite).result\n        for result in result_list:\n            if result[0] == 1 or result[0] == 2:\n                failed_count += 1\n\n    with open(log_file, 'r') as html_file:\n        today = datetime.date.today()\n        str_today = today.strftime(\"%Y-%m-%d\")\n\n        title = \"store daily check - store est/category daily - \" + str_today + \" - \"\n        if failed_count == 0:\n            title += \"Passed\"\n        else:\n            title += \"Failed\"\n        send_db_check_email(title, html_file.read())\n\n\nsend_message(spark)\n\n\n\n#   WITH caculate_data AS (\n#     select app_id,free_app_download,paid_app_download,revenue, unified_device_code, unified_country_code, est_non_organic_download_share from calculate_data_prepare\n#     );\n"]},{"cell_type":"code","execution_count":0,"id":"20200713-035004_2014352133","metadata":{},"outputs":[],"source":["\nimport datetime\ncheck_date = datetime.date(2020, 07, 11) #datetime.date.today()\nprint check_date\ndef get_last_week_date(check_date):\n    index = (check_date.isoweekday() + 1) % 7\n    print index\n    if index == 0:\n        end = check_date\n    else:\n        end = check_date - datetime.timedelta(index)\n\n    start = end + datetime.timedelta(-7)\n\n    date_range = end - start\n    dates = list()\n    print date_range.days\n    for days in xrange(date_range.days + 1):\n        dates.append(str(start + datetime.timedelta(days)))\n    return dates[1:]\n    \nprint get_last_week_date(check_date)"]},{"cell_type":"code","execution_count":0,"id":"20200713-020822_2025203544","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from test_unified_download_attribution where app_id=20600006439056 and country_code='WW'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200713-021023_1047477881","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from store_unified where app_id=289560144 and country_code='WW'\").show()\nspark.sql(\"select * from download_attribution where product_id=289560144 \").show()\nspark.sql(\"select * from compare_data_unified where app_id=289560144\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200712-133148_827668480","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from test_unified_download_attribution where app_id=289560144 and country_code='WW'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200710-081557_1384738110","metadata":{},"outputs":[],"source":["\n# spark.sql(\"select * from test_unified_download_attribution limit 2 \").show()\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log.v1/fact/granularity=daily/date=2020-06-21/\").count()\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log.v1/fact/granularity=daily/date=2020-06-21/\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200710-073546_604318907","metadata":{},"outputs":[],"source":["\neject_all_caches(spark)\n"]},{"cell_type":"code","execution_count":0,"id":"20200710-073646_231603355","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v4/fact/granularity=daily/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200709-020337_631045374","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\n\nfrom applications.auto_pipeline.temp_script.utils.base_test import PipelineTest\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.utils import email\nfrom applications.auto_pipeline.temp_script.utils.html_report_test_runner import HTMLTestRunner\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\nd1 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"ios\"))\nd1 = spark.createDataFrame([(0, 'WW', 'Worldwide', 'ios')],\n                           schema=[\"store_id\", \"country_code\", \"_c2\", \"market_code\"]).union(d1)\n\nd2 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"android\"))\ncountry_code_df = d1.union(d2).where(\"country_code is not null\").cache()\ncountry_code_df = country_code_df.withColumnRenamed(\"store_id\", \"country_code_store_id\")\nprint 'country mapping table'\ncountry_code_df.show(2)\ncountry_code_df.createOrReplaceTempView(\"country_code_mapping\")\n\ncategory_mapping_table = spark.read.parquet(\n    \"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\")\ncategory_mapping_table.createOrReplaceTempView(\"category_mapping_deminsion_service\")\n\n\nsql_text_category = '''\n        WITH filter_top_n_raw_data AS \n        ( \n               SELECT * \n               FROM   ( \n                               SELECT   id, \n                                        Sum(est) AS est, \n                                        category_id, \n                                        store_id, \n                                        platform_id, \n                                        feed, \n                                        vertical, \n                                        platform \n                               FROM     ( \n                                                        SELECT DISTINCT d1.id, \n                                                                        d1.est, \n                                                                        d1.store_id, \n                                                                        d1.date, \n                                                                        d1.feed, \n                                                                        d1.vertical, \n                                                                        d1.platform_id, \n                                                                        d1.platform, \n                                                                        d2.category_id \n                                                        FROM            daily_data AS d1 \n                                                        JOIN            daily_data AS d2 \n                                                        ON              d1.id = d2.id \n                                                        AND             d1.store_id = d2.store_id \n                                                        AND             d1.feed = d2.feed \n                                                        AND             d1.vertical = d2.vertical \n                                                        AND             d1.platform_id = d2.platform_id \n                                                        AND             d1.platform = d2.platform \n                                                        WHERE           ( \n                                                                                        d1.rank <= 4000 \n                                                                        AND             d2.rank<=4000 \n                                                                        AND             d1.store_id == 0 \n                                                                        AND             d1.platform = 'ios' )\n                                                        OR              ( \n                                                                                        d1.rank <= 1000 \n                                                                        AND             d2.rank<=1000 \n                                                                        AND             d1.store_id != 0 \n                                                                        AND             d1.platform = 'ios' )\n                                                        OR              ( \n                                                                                        d1.rank <= 4000 \n                                                                        AND             d2.rank<=4000 \n                                                                        AND             d1.store_id == 1000\n                                                                        AND             d1.platform = 'android' )\n                                                        OR              ( \n                                                                                        d1.rank <= 1000 \n                                                                        AND             d2.rank<=1000 \n                                                                        AND             d1.store_id != 1000\n                                                                        AND             d1.platform = 'android' ) ) AS t\n                               WHERE    feed IN ( 0, \n                                                 1, \n                                                 2, \n                                                 101, \n                                                 100, \n                                                 102 ) \n                               GROUP BY id, \n                                        store_id, \n                                        category_id, \n                                        platform_id, \n                                        platform, \n                                        vertical, \n                                        feed ) ); \n        with replace_metric AS \n        ( \n               SELECT * , \n                      CASE \n                             WHEN feed='0' \n                             AND    platform='ios' THEN 'free_app_download' \n                             WHEN feed='1' \n                             AND    platform='ios' THEN 'paid_app_download' \n                             WHEN feed='2' \n                             AND    platform='ios' THEN 'revenue' \n                             WHEN feed='101' \n                             AND    platform='ios' THEN 'free_app_download' \n                             WHEN feed='100' \n                             AND    platform='ios' THEN 'paid_app_download' \n                             WHEN feed='102' \n                             AND    platform='ios' THEN 'revenue' \n                             WHEN feed='0' \n                             AND    platform='android' THEN 'free_app_download' \n                             WHEN feed='1' \n                             AND    platform='android' THEN 'paid_app_download' \n                             WHEN feed='2' \n                             AND    platform='android' THEN 'revenue' \n                      END AS metric \n               FROM   filter_top_n_raw_data); \n        with replace_metric_device_code AS \n        ( \n               SELECT * , \n                      CASE \n                             WHEN feed='0' \n                             AND    platform='ios' THEN 'ios-phone' \n                             WHEN feed='1' \n                             AND    platform='ios' THEN 'ios-phone' \n                             WHEN feed='2' \n                             AND    platform='ios' THEN 'ios-phone' \n                             WHEN feed='101' \n                             AND    platform='ios' THEN 'ios-tablet' \n                             WHEN feed='100' \n                             AND    platform='ios' THEN 'ios-tablet' \n                             WHEN feed='102' \n                             AND    platform='ios' THEN 'ios-tablet' \n                             WHEN feed='0' \n                             AND    platform='android' THEN 'android-all' \n                             WHEN feed='1' \n                             AND    platform='android' THEN 'android-all' \n                             WHEN feed='2' \n                             AND    platform='android' THEN 'android-all' \n                      END AS device_code \n               FROM   replace_metric); \n        with group_by_metric_1 AS \n        ( \n                 SELECT   max(est) AS est, \n                          id, \n                          metric, \n                          device_code, \n                          store_id, \n                          platform, \n                          category_id \n                 FROM     replace_metric_device_code \n                 WHERE    store_id NOT IN (3,4,5,6) \n                 AND      device_code  IN ('ios-phone' , \n                                           'ios-tablet' ) \n                 AND      feed NOT IN (1000, \n                                       1001, \n                                       1002) \n                 GROUP BY id, \n                          store_id, \n                          metric, \n                          device_code, \n                          platform, \n                          category_id ); \n        with group_by_metric_2 AS \n        ( \n                 SELECT   max(est) AS est, \n                          id, \n                          metric, \n                          device_code, \n                          store_id, \n                          platform, \n                          category_id \n                 FROM     replace_metric_device_code \n                 WHERE    store_id NOT IN ( 1003, \n                                           1005, \n                                           1006,1007) \n                 AND      device_code='android-all' \n                 AND      feed NOT IN (1000, \n                                       1001, \n                                       1002) \n                 GROUP BY id, \n                          store_id, \n                          metric, \n                          device_code, \n                          platform, \n                          category_id ); \n        with group_by_metric AS \n        ( \n               SELECT * \n               FROM   group_by_metric_1 \n               UNION ALL \n               SELECT * \n               FROM   group_by_metric_2 ); \n        -- pivot metric column \n        with pivot_metric_raw AS \n        ( \n                        SELECT DISTINCT id AS app_id, \n                                        store_id, \n                                        platform, \n                                        device_code, \n                                        free_app_download, \n                                        revenue, \n                                        paid_app_download, \n                                        category_id AS category_id_pivot \n                        FROM            group_by_metric PIVOT ( max(est) FOR metric IN ('free_app_download',\n                                                                                        'revenue', \n                                                                                        'paid_app_download') ) );\n        -- map raw with category \n        with category_mapping_raw AS \n        ( \n                        SELECT          * \n                        FROM            ( \n                                               SELECT *, \n                                                      'ios' AS mapping_platform \n                                               FROM   category_mapping_deminsion_service \n                                               WHERE  market_code='apple-store' \n                                               UNION ALL \n                                               SELECT *, \n                                                      'android' AS mapping_platform \n                                               FROM   category_mapping_deminsion_service \n                                               WHERE  market_code='google-play' ) AS mapping \n                        FULL OUTER JOIN pivot_metric_raw \n                        ON              mapping.legacy_category_id=pivot_metric_raw.category_id_pivot \n                        AND             mapping.mapping_platform=pivot_metric_raw.platform ); \n        -- union all platform with country_code mapping\n        WITH country_code_mapping AS \n        ( \n               SELECT *, \n                      'android' AS market_code \n               FROM   android_country_mapping \n               UNION ALL \n               SELECT *, \n                      'ios' market_code \n               FROM   ios_country_mapping \n               UNION ALL \n               SELECT 143502, \n                      'VE', \n                      'VESA', \n                      'ios' \n               UNION ALL \n               SELECT 0, \n                      'WW', \n                      'worldwide', \n                      'ios' \n               UNION ALL \n               SELECT 36, \n                      'CZ', \n                      'CZ', \n                      'android' \n               UNION ALL \n               SELECT 5, \n                      'ES', \n                      'ES', \n                      'android' );\n            WITH country_category_mapping_raw AS \n                ( \n                   SELECT     app_id, \n                              country_code, \n                              device_code, \n                              free_app_download, \n                              paid_app_download, \n                              revenue, \n                              category_id \n                   FROM       country_code_mapping \n                   INNER JOIN category_mapping_raw \n                   ON         country_code_mapping.store_id=category_mapping_raw.store_id \n                   AND        country_code_mapping.market_code=category_mapping_raw.platform \n                   WHERE      country_name!='Global' );\n           '''\n\n\nsql_text_est = '''\nWITH filter_top_n_raw_data AS \n( \n                SELECT DISTINCT id, \n                                Sum(est) AS est, \n                                store_id, \n                                platform_id, \n                                feed, \n                                vertical, \n                                platform \n                FROM            ( \n                                                SELECT DISTINCT d1.id, \n                                                                d1.est, \n                                                                d1.store_id, \n                                                                d1.date, \n                                                                d1.feed, \n                                                                d1.vertical, \n                                                                d1.platform_id, \n                                                                d1.platform \n                                                FROM            daily_data AS d1 \n                                                JOIN            daily_data AS d2 \n                                                ON              d1.id = d2.id \n                                                AND             d1.store_id = d2.store_id \n                                                AND             d1.feed = d2.feed \n                                                AND             d1.vertical = d2.vertical \n                                                AND             d1.platform_id = d2.platform_id \n                                                WHERE           ( \n                                                                                d1.rank <= 4000 \n                                                                AND             d2.rank<=4000 \n                                                                AND             d1.store_id == 0 \n                                                                AND             d1.platform = 'ios' )\n                                                OR              ( \n                                                                                d1.rank <= 1000 \n                                                                AND             d2.rank<=1000 \n                                                                AND             d1.store_id != 0 \n                                                                AND             d1.platform = 'ios' )\n                                                OR              ( \n                                                                                d1.rank <= 4000 \n                                                                AND             d2.rank<=4000 \n                                                                AND             d1.store_id == 1000\n                                                                AND             d1.platform = 'android' )\n                                                OR              ( \n                                                                                d1.rank <= 1000 \n                                                                AND             d2.rank<=1000 \n                                                                AND             d1.store_id != 1000\n                                                                AND             d1.platform = 'android' ) ) AS t\n                WHERE           feed IN ( 0, \n                                         1, \n                                         2, \n                                         101, \n                                         100, \n                                         102 ) \n                GROUP BY        id, \n                                store_id, \n                                platform_id, \n                                vertical, \n                                feed, \n                                platform); \nwith replace_metric AS \n( \n       SELECT * , \n              CASE \n                     WHEN feed='0' \n                     AND    platform='ios' THEN 'free_app_download' \n                     WHEN feed='1' \n                     AND    platform='ios' THEN 'paid_app_download' \n                     WHEN feed='2' \n                     AND    platform='ios' THEN 'revenue' \n                     WHEN feed='101' \n                     AND    platform='ios' THEN 'free_app_download' \n                     WHEN feed='100' \n                     AND    platform='ios' THEN 'paid_app_download' \n                     WHEN feed='102' \n                     AND    platform='ios' THEN 'revenue' \n                     WHEN feed='0' \n                     AND    platform='android' THEN 'free_app_download' \n                     WHEN feed='1' \n                     AND    platform='android' THEN 'paid_app_download' \n                     WHEN feed='2' \n                     AND    platform='android' THEN 'revenue' \n              END AS metric \n       FROM   filter_top_n_raw_data);\n       WITH replace_metric_device_code AS \n( \n       SELECT * , \n              CASE \n                     WHEN feed='0' \n                     AND    platform='ios' THEN 'ios-phone' \n                     WHEN feed='1' \n                     AND    platform='ios' THEN 'ios-phone' \n                     WHEN feed='2' \n                     AND    platform='ios' THEN 'ios-phone' \n                     WHEN feed='101' \n                     AND    platform='ios' THEN 'ios-tablet' \n                     WHEN feed='100' \n                     AND    platform='ios' THEN 'ios-tablet' \n                     WHEN feed='102' \n                     AND    platform='ios' THEN 'ios-tablet' \n                     WHEN feed='0' \n                     AND    platform='android' THEN 'android-all' \n                     WHEN feed='1' \n                     AND    platform='android' THEN 'android-all' \n                     WHEN feed='2' \n                     AND    platform='android' THEN 'android-all' \n              END AS device_code \n       FROM   replace_metric); \nWITH group_by_metric_1 AS \n( \n         SELECT   max(est) AS est, \n                  id, \n                  metric, \n                  device_code, \n                  store_id, \n                  platform \n         FROM     replace_metric_device_code \n         WHERE    store_id NOT IN (3,4,5,6) \n         AND      device_code  IN ('ios-phone' , \n                                   'ios-tablet' ) \n         AND      feed NOT IN (1000, \n                               1001, \n                               1002) \n         GROUP BY id, \n                  store_id, \n                  metric, \n                  device_code, \n                  platform );\n                  WITH group_by_metric_2 AS \n( \n         SELECT   Max(est) AS est, \n                  id, \n                  metric, \n                  device_code, \n                  store_id, \n                  platform \n         FROM     replace_metric_device_code \n         WHERE    store_id NOT IN ( 1003, \n                                   1005, \n                                   1006,1007) \n         AND      device_code='android-all' \n         AND      feed NOT IN (1000, \n                               1001, \n                               1002) \n         GROUP BY id, \n                  store_id, \n                  metric, \n                  device_code, \n                  platform ); \nWITH group_by_metric AS \n( \n       SELECT * \n       FROM   group_by_metric_1 \n       UNION ALL \n       SELECT * \n       FROM   group_by_metric_2 ); \n-- pivot metric column\n\nWITH pivot_metric_raw AS \n( \n                SELECT DISTINCT id AS app_id, \n                                store_id, \n                                platform, \n                                device_code, \n                                free_app_download, \n                                revenue, \n                                paid_app_download \n                FROM            group_by_metric PIVOT ( Max(est) FOR metric IN ('free_app_download',\n                                                                                'revenue', \n                                                                                'paid_app_download') ) );\n-- union all platform with country_code mapping \nwith country_code_mapping AS \n( \n       SELECT *, \n              'android' AS market_code \n       FROM   android_country_mapping \n       UNION ALL \n       SELECT *, \n              'ios' market_code \n       FROM   ios_country_mapping \n       UNION ALL \n       SELECT 143502, \n              'VE', \n              'VESA', \n              'ios' \n       UNION ALL \n       SELECT 0, \n              'WW', \n              'worldwide', \n              'ios' \n       UNION ALL \n       SELECT 36, \n              'CZ', \n              'CZ', \n              'android' \n       UNION ALL \n       SELECT 5, \n              'ES', \n              'ES', \n              'android' ); \n-- map raw with country_code\n\nWITH country_est_mapping_raw AS \n( \n           SELECT     app_id, \n                      country_code, \n                      device_code, \n                      free_app_download, \n                      paid_app_download, \n                      revenue \n           FROM       country_code_mapping \n           INNER JOIN pivot_metric_raw \n           ON         country_code_mapping.store_id=pivot_metric_raw.store_id \n           AND        country_code_mapping.market_code=pivot_metric_raw.platform \n           WHERE      country_name!='Global' ); \nWITH filter_top_n_raw_data AS \n( \n                SELECT DISTINCT id, \n                                sum(est) AS est, \n                                store_id, \n                                platform_id, \n                                feed, \n                                vertical, \n                                platform \n                FROM            ( \n                                                SELECT DISTINCT d1.id, \n                                                                d1.est, \n                                                                d1.store_id, \n                                                                d1.date, \n                                                                d1.feed, \n                                                                d1.vertical, \n                                                                d1.platform_id, \n                                                                d1.platform \n                                                FROM            daily_data AS d1 \n                                                JOIN            daily_data AS d2 \n                                                ON              d1.id = d2.id \n                                                AND             d1.store_id = d2.store_id \n                                                AND             d1.feed = d2.feed \n                                                AND             d1.vertical = d2.vertical \n                                                AND             d1.platform_id = d2.platform_id \n                                                WHERE           ( \n                                                                                d1.rank <= 4000 \n                                                                AND             d2.rank<=4000 \n                                                                AND             d1.store_id == 0 \n                                                                AND             d1.platform = 'ios' )\n                                                OR              ( \n                                                                                d1.rank <= 1000 \n                                                                AND             d2.rank<=1000 \n                                                                AND             d1.store_id != 0 \n                                                                AND             d1.platform = 'ios' )\n                                                OR              ( \n                                                                                d1.rank <= 4000 \n                                                                AND             d2.rank<=4000 \n                                                                AND             d1.store_id == 1000\n                                                                AND             d1.platform = 'android' )\n                                                OR              ( \n                                                                                d1.rank <= 1000 \n                                                                AND             d2.rank<=1000 \n                                                                AND             d1.store_id != 1000\n                                                                AND             d1.platform = 'android' ) ) AS t\n                WHERE           feed IN ( 0, \n                                         1, \n                                         2, \n                                         101, \n                                         100, \n                                         102 ) \n                GROUP BY        id, \n                                store_id, \n                                platform_id, \n                                vertical, \n                                feed, \n                                platform); \nwith replace_metric AS \n( \n       SELECT * , \n              CASE \n                     WHEN feed='0' \n                     AND    platform='ios' THEN 'free_app_download' \n                     WHEN feed='1' \n                     AND    platform='ios' THEN 'paid_app_download' \n                     WHEN feed='2' \n                     AND    platform='ios' THEN 'revenue' \n                     WHEN feed='101' \n                     AND    platform='ios' THEN 'free_app_download' \n                     WHEN feed='100' \n                     AND    platform='ios' THEN 'paid_app_download' \n                     WHEN feed='102' \n                     AND    platform='ios' THEN 'revenue' \n                     WHEN feed='0' \n                     AND    platform='android' THEN 'free_app_download' \n                     WHEN feed='1' \n                     AND    platform='android' THEN 'paid_app_download' \n                     WHEN feed='2' \n                     AND    platform='android' THEN 'revenue' \n              END AS metric \n       FROM   filter_top_n_raw_data);WITH replace_metric_device_code AS \n( \n       SELECT * , \n              CASE \n                     WHEN feed='0' \n                     AND    platform='ios' THEN 'ios-phone' \n                     WHEN feed='1' \n                     AND    platform='ios' THEN 'ios-phone' \n                     WHEN feed='2' \n                     AND    platform='ios' THEN 'ios-phone' \n                     WHEN feed='101' \n                     AND    platform='ios' THEN 'ios-tablet' \n                     WHEN feed='100' \n                     AND    platform='ios' THEN 'ios-tablet' \n                     WHEN feed='102' \n                     AND    platform='ios' THEN 'ios-tablet' \n                     WHEN feed='0' \n                     AND    platform='android' THEN 'android-all' \n                     WHEN feed='1' \n                     AND    platform='android' THEN 'android-all' \n                     WHEN feed='2' \n                     AND    platform='android' THEN 'android-all' \n              END AS device_code \n       FROM   replace_metric); \nWITH group_by_metric_1 AS \n( \n         SELECT   max(est) AS est, \n                  id, \n                  metric, \n                  device_code, \n                  store_id, \n                  platform \n         FROM     replace_metric_device_code \n         WHERE    store_id NOT IN (3,4,5,6) \n         AND      device_code  IN ('ios-phone' , \n                                   'ios-tablet' ) \n         AND      feed NOT IN (1000, \n                               1001, \n                               1002) \n         GROUP BY id, \n                  store_id, \n                  metric, \n                  device_code, \n                  platform );WITH group_by_metric_2 AS \n( \n         SELECT   Max(est) AS est, \n                  id, \n                  metric, \n                  device_code, \n                  store_id, \n                  platform \n         FROM     replace_metric_device_code \n         WHERE    store_id NOT IN ( 1003, \n                                   1005, \n                                   1006,1007) \n         AND      device_code='android-all' \n         AND      feed NOT IN (1000, \n                               1001, \n                               1002) \n         GROUP BY id, \n                  store_id, \n                  metric, \n                  device_code, \n                  platform ); \nWITH group_by_metric AS \n( \n       SELECT * \n       FROM   group_by_metric_1 \n       UNION ALL \n       SELECT * \n       FROM   group_by_metric_2 ); \n-- pivot metric column\n\nWITH pivot_metric_raw AS \n( \n                SELECT DISTINCT id AS app_id, \n                                store_id, \n                                platform, \n                                device_code, \n                                free_app_download, \n                                revenue, \n                                paid_app_download \n                FROM            group_by_metric PIVOT ( Max(est) FOR metric IN ('free_app_download',\n                                                                                'revenue', \n                                                                                'paid_app_download') ) );\n-- union all platform with country_code mapping \nwith country_code_mapping AS \n( \n       SELECT *, \n              'android' AS market_code \n       FROM   android_country_mapping \n       UNION ALL \n       SELECT *, \n              'ios' market_code \n       FROM   ios_country_mapping \n       UNION ALL \n       SELECT 143502, \n              'VE', \n              'VESA', \n              'ios' \n       UNION ALL \n       SELECT 0, \n              'WW', \n              'worldwide', \n              'ios' \n       UNION ALL \n       SELECT 36, \n              'CZ', \n              'CZ', \n              'android' \n       UNION ALL \n       SELECT 5, \n              'ES', \n              'ES', \n              'android' ); \n-- map raw with country_code\n\nWITH country_est_mapping_raw AS \n( \n           SELECT     app_id, \n                      country_code, \n                      device_code, \n                      free_app_download, \n                      paid_app_download, \n                      revenue \n           FROM       country_code_mapping \n           INNER JOIN pivot_metric_raw \n           ON         country_code_mapping.store_id=pivot_metric_raw.store_id \n           AND        country_code_mapping.market_code=pivot_metric_raw.platform \n           WHERE      country_name!='Global' );\n'''\n\n\n\n\"\"\"\nget date:  [ [month, [days]], [month, [days]], [month, [days]], ....... ]\n\"\"\"\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in\n                 list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef test_daily_trancate_category_data(spark, test_data):\n    # CSV schema\n    from pyspark.sql import types as T\n    from pyspark.sql import functions as F\n\n    csv_schema = T.StructType(\n        [\n            T.StructField(\"store_id\", T.IntegerType(), True),\n            T.StructField(\"date\", T.DateType(), True),\n            T.StructField(\"platform_id\", T.IntegerType(), True),\n            T.StructField(\"vertical\", T.IntegerType(), True),\n            T.StructField(\"feed\", T.IntegerType(), True),\n            T.StructField(\"id\", T.LongType(), True),\n            T.StructField(\"est\", T.IntegerType(), True),\n            T.StructField(\"category_id\", T.IntegerType(), True),\n            T.StructField(\"rank\", T.IntegerType(), True)\n        ]\n    )\n\n    print test_data\n    month_indicator = test_data\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator < '2012-01-01':\n        df_1 = spark.read.option(\"basePath\",\n                                 \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % (test_data),\n            sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id',\n                                                                  'platform_id', 'vertical', 'rank', 'feed',\n                                                                  'est', 'date', 'platform').cache()\n\n\n    ### 2. only csv\n    elif month_indicator >= '2012-01-01' and month_indicator < '2019-07-14':\n        df_ios = spark.read.option(\"basePath\",\n                                   \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % (test_data),\n            sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id',\n                                                                  'platform_id', 'vertical', 'rank', 'feed',\n                                                                  'est', 'date', 'platform').cache()\n        df_android = spark.read.option(\"basePath\",\n                                       \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/android/sbe_est_app/*/\" % (\n                test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).select('id', 'store_id',\n                                                                                      'category_id',\n                                                                                      'platform_id',\n                                                                                      'vertical', 'rank',\n                                                                                      'feed', 'est', 'date',\n                                                                                      'platform').cache()\n        df_1 = df_ios.union(df_android)\n\n\n    ### 3. only parquet ###\n    else:\n        df_1 = spark.read.option(\"basePath\",\n                                 \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\n            \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" % (\n                test_data)).cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n\n    daily_est_load = spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/granularity=daily/\").where(\n        \"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_est_load.createOrReplaceTempView(\"daily_unified_category\")\n\n    # store_unified , rank_unified\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"category_mapping_deminsion_service\",\n                \"path\": [\"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\"],\n            },\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"ios_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n\n                \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n            },\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"android_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n\n                \"path\": [\n                    \"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n            }\n        ]\n    }\n\n    run(spark, ingest_msg, sql_text_category)\n\n    diff_df1 = spark.sql('''\n                            SELECT app_id, country_code, category_id, device_code, free_app_download, paid_app_download, revenue\n                            FROM  country_category_mapping_raw \n                            EXCEPT ALL \n                            SELECT app_id, \n                                   country_code, \n                                   category_id, \n                                   device_code, \n                                   est_free_app_download AS free_app_download, \n                                   est_paid_app_download AS paid_app_download, \n                                   est_revenue           AS revenue \n                            FROM   daily_unified_category\n                            ''')\n    diff_df2 = spark.sql('''\n                            SELECT app_id, \n                               country_code, \n                               category_id, \n                               device_code, \n                               est_free_app_download AS free_app_download, \n                               est_paid_app_download AS paid_app_download, \n                               est_revenue           AS revenue \n                            FROM   daily_unified_category \n                            EXCEPT ALL \n                            SELECT app_id, country_code, category_id, device_code, free_app_download, paid_app_download, revenue \n                            FROM   country_category_mapping_raw\n                            ''')\n\n    print diff_df1.take(3)\n    print diff_df2.take(3)\n    return diff_df1, diff_df2\n\n\ndef test_daily_truncate_est_data(spark, test_data):\n    # CSV schema\n    from pyspark.sql import types as T\n    from pyspark.sql import functions as F\n\n    csv_schema = T.StructType(\n        [\n            T.StructField(\"store_id\", T.IntegerType(), True),\n            T.StructField(\"date\", T.DateType(), True),\n            T.StructField(\"platform_id\", T.IntegerType(), True),\n            T.StructField(\"vertical\", T.IntegerType(), True),\n            T.StructField(\"feed\", T.IntegerType(), True),\n            T.StructField(\"id\", T.LongType(), True),\n            T.StructField(\"est\", T.IntegerType(), True),\n            T.StructField(\"category_id\", T.IntegerType(), True),\n            T.StructField(\"rank\", T.IntegerType(), True)\n        ]\n    )\n\n    print test_data\n    month_indicator = test_data\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\n                                 \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % (\n                temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id',\n                                                                                        'category_id',\n                                                                                        'platform_id',\n                                                                                        'vertical',\n                                                                                        'rank', 'feed',\n                                                                                        'est', 'date',\n                                                                                        'platform').cache()\n\n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2019-07-14':\n        df_ios = spark.read.option(\"basePath\",\n                                   \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % (\n                test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id',\n                                                                                  'category_id',\n                                                                                  'platform_id',\n                                                                                  'vertical', 'rank',\n                                                                                  'feed', 'est', 'date',\n                                                                                  'platform').cache()\n        df_android = spark.read.option(\"basePath\",\n                                       \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/android/sbe_est_app/*/\" % (\n                test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).select('id', 'store_id',\n                                                                                      'category_id',\n                                                                                      'platform_id',\n                                                                                      'vertical', 'rank',\n                                                                                      'feed', 'est',\n                                                                                      'date',\n                                                                                      'platform').cache()\n        df_1 = df_ios.union(df_android)\n\n\n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\n                                 \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\n            \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" % (\n                test_data)).cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n\n    daily_est_load = spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/\").where(\n        \"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_est_load.createOrReplaceTempView(\"daily_unified_est\")\n\n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"ios_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n\n                \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n            },\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"android_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n\n                \"path\": [\n                    \"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n            }\n        ]\n    }\n\n    run(spark, ingest_msg, sql_text_est)\n\n    diff_df1 = spark.sql(\n        \"select * from country_est_mapping_raw except all select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue from daily_unified_est \")\n    diff_df2 = spark.sql(\n        \"select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue from daily_unified_est  except all select * from country_est_mapping_raw\")\n\n    return diff_df1, diff_df2\n\n\nsql_text_unified_est_category='''\n-- mapping feed as metrc in raw\nWITH feed_metric AS (\nselect *, 'free_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='0' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='1' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric , \"ios-phone\" as device_code from rank_raw where  feed='2' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-tablet\" as device_code from rank_raw where  feed='101' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='100' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='102' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1000' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1001' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1002' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric , \"android-all\" as device_code from rank_raw   where  feed='0' and platform='android'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"android-all\" as device_code from rank_raw  where  feed='1' and platform='android'\nUNION ALL\nselect *, 'revenue' as metric,  \"android-all\" as device_code from rank_raw  where  feed='2' and platform='android'\n);\n\n\n-- select tested column from raw data\nWITH metric_raw_data AS (\nSELECT id, category_id as raw_category_id,rank,store_id as raw_store_id , metric,device_code,date , platform from feed_metric where store_id not in (3,4,5,6, 1002,1003, 1005,1004, 1006,1007)\n);\n\n\n-- group by and count data in raw data\nWITH group_by_raw AS (\nSELECT count(id) AS total_count , raw_category_id, raw_store_id, metric,device_code,date,platform from metric_raw_data where raw_store_id not in (3,4,5,6, 1002,1003, 1004, 1005, 1006,1007) group by raw_category_id, raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_rank_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download, raw_category_id,raw_store_id,device_code, platform,date\nFROM\n      group_by_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n\n-- select tested column from raw data\nWITH metric_raw_store_data AS (\nSELECT distinct id, est, store_id as raw_store_id , metric,device_code, date , platform from feed_metric where store_id not in (3,4,5,6, 1002,1003, 1005,1004, 1006,1007)\n\n);\n\n\n-- group by and count data in raw data\nWITH group_by_store_raw AS (\nSELECT count(est) AS total_count ,raw_store_id, metric,device_code,date,platform from metric_raw_store_data where raw_store_id not in (3,4,5,6,1002,1003, 1004, 1005, 1006,1007) group by raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_store_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download,raw_store_id,device_code, platform, date\nFROM\n      group_by_store_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n-- map raw with category\nWITH category_mapping_raw AS (\n\nSELECT * from ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \nUNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n ) as mapping right join pivot_metric_rank_raw on mapping.legacy_category_id=pivot_metric_rank_raw.raw_category_id and \nmapping.mapping_platform=pivot_metric_rank_raw.platform\n);\n\n\n-- map raw with rank country_code\nWITH country_category_mapping_rank_raw AS (\nselect date, raw_store_id, country_code,device_code,category_id,free_app_download,paid_app_download,revenue from country_code_mapping right join category_mapping_raw on country_code_mapping.country_code_store_id=category_mapping_raw.raw_store_id and country_code_mapping.market_code=category_mapping_raw.platform\n);\n\n\n\n-- map raw with store country_code\nWITH country_mapping_store_raw AS (\nselect date, raw_store_id, country_code,device_code,free_app_download,paid_app_download,revenue from country_code_mapping right join pivot_metric_store_raw on country_code_mapping.country_code_store_id=pivot_metric_store_raw.raw_store_id and country_code_mapping.market_code=pivot_metric_store_raw.platform\n);\n\n\n-- group by unified data\nWITH unified_group_data AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code, category_id as unified_category_id from ( select distinct  app_id, free_app_download_rank as free_app_download , paid_app_download_rank as paid_app_download, revenue_rank as revenue, country_code, device_code, category_id from  rank_unified where data_stage='final' ) as unified\ngroup by  category_id,  country_code,  device_code );\n\n\n-- group by unified data\nWITH unified_group_data_store AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code from  store_unified where data_stage='final' ) as unified\ngroup by   country_code,  device_code );\n\n\n\n\n-- compare raw vs unified data\nWITH compared_data_rank AS (\n    SELECT * from country_category_mapping_rank_raw left join unified_group_data on unified_group_data.unified_country_code==country_category_mapping_rank_raw.country_code and unified_group_data.unified_category_id==country_category_mapping_rank_raw.category_id and unified_group_data.unified_device_code==country_category_mapping_rank_raw.device_code\n);\n\nWITH miss_data_rank AS (\nselect * from compared_data_rank where unified_count_paid_app_download!=paid_app_download or unified_count_free_app_download != free_app_download  or unified_count_revenue != revenue or unified_count_app_id is null\n)\n\n\n\n-- compare raw vs unified data store\nWITH compared_store_data AS (\n    SELECT * from country_mapping_store_raw left join unified_group_data_store on unified_group_data_store.unified_country_code==country_mapping_store_raw.country_code and unified_group_data_store.unified_device_code==country_mapping_store_raw.device_code\n);\n\n\nWITH miss_data_store AS (\nselect * from compared_store_data where free_app_download!=unified_count_free_app_download or paid_app_download!=unified_count_paid_app_download or revenue!=unified_count_revenue or unified_count_app_id is null\n)\n\n'''\n\ndef test_daily_unified_est_category_data(spark, test_date):\n    # df_1 = spark.read.option(\"basePath\",\n    #                          \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\n    #     \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" % (\n    #         test_date)).cache()\n\n    # df_1.createOrReplaceTempView(\"daily_data\")\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"name\": \"rank_raw\",\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"path\": [\n                    \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={}/\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v4/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v4/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }\n        ]\n    }\n    run(spark, ingest_msg, sql_text_unified_est_category)\n    miss_category = spark.sql(\"select * from miss_data_rank\")\n    miss_est= spark.sql(\"select * from miss_data_store\")\n    return miss_category, miss_est\n\n\nDEFAULT_RECIPIENTS = \"fzhang@appannie.com\"\n\n\ndef send_db_check_email(title, text_context, key=None):\n    default_recipients = [DEFAULT_RECIPIENTS]\n    email.send(title, text_context, default_recipients, sender='dev-qa-data-quality@appannie.com')\n\n\n\n\n\nclass demoTest(PipelineTest):\n    trigger_date_config = (\"0 12 * * *\", 8)\n\n\n    # def test_daily_unified_est_category_data(self):\n    #     trigger_datetime = datetime.datetime.strptime(\"2020-07-03\", '%Y-%m-%d')\n    #     self._get_check_date_from_routing_config(trigger_datetime).strftime(\n    #         \"%Y-%m-%d\")\n\n    #     df_category, df_est = test_daily_unified_est_category_data(self.spark, self.check_date)\n\n    #     self.assertTrue(df_category.rdd.isEmpty(), \"category raw is not equal unified\")\n    #     self.assertTrue(df_est.rdd.isEmpty(), \"est raw is not equal to unified\")\n\n    def test_store_daily_category(self):\n        trigger_datetime = datetime.datetime.strptime(\"2020-06-01\", '%Y-%m-%d')\n        check_date_str_actual = self._get_check_date_from_routing_config(trigger_datetime).strftime(\n            \"%Y-%m-%d\")\n\n        df1, df2 = test_daily_trancate_category_data(self.spark, self.check_date)\n\n        self.assertTrue(df1.rdd.isEmpty(), \"category raw is more than unified\")\n        self.assertTrue(df2.rdd.isEmpty(), \"category unified is more than raw\")\n\n    # def test_store_daily_est(self):\n    #     trigger_datetime = datetime.datetime.strptime(\"2020-06-01\", '%Y-%m-%d')\n    #     check_date_str_actual = self._get_check_date_from_routing_config(trigger_datetime).strftime(\n    #         \"%Y-%m-%d\")\n\n    #     df_1, df_2 = test_daily_truncate_est_data(self.spark, check_date_str_actual)\n\n    #     self.assertTrue(df_1.rdd.isEmpty(), \"est raw is more than unified\")\n    #     self.assertTrue(df_2.rdd.isEmpty(), \"est unified is more than raw\")\n\n\ndef send_message(spark):\n    log_file = \"/tmp/db_check.log\"\n    with open(log_file, \"w\") as html_file:\n        suite = unittest.TestSuite()\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(demoTest))\n        runner = HTMLTestRunner(\n            stream=html_file,\n            title='Transform Test Report',\n            description='This db_check the report output by Tech Team.'\n        )\n\n        failed_count = 0\n        result_list = runner.run(suite).result\n        for result in result_list:\n            if result[0] == 1 or result[0] == 2:\n                failed_count += 1\n\n    with open(log_file, 'r') as html_file:\n        today = datetime.date.today()\n        str_today = today.strftime(\"%Y-%m-%d\")\n\n        title = \"store daily check - preload est/category - \" + str_today + \" - \"\n        if failed_count == 0:\n            title += \"Passed\"\n        else:\n            title += \"Failed\"\n        send_db_check_email(title, html_file.read())\n\n\nsend_message(spark)\n"]},{"cell_type":"code","execution_count":0,"id":"20200709-081356_1580609070","metadata":{},"outputs":[],"source":["\n\ndiff_df1 = spark.sql('''\n                        SELECT * \n                        FROM  country_category_mapping_raw \n                        EXCEPT ALL \n                        SELECT app_id, \n                               country_code, \n                               category_id, \n                               device_code, \n                               est_free_app_download AS free_app_download, \n                               est_paid_app_download AS paid_app_download, \n                               est_revenue           AS revenue \n                        FROM   daily_unified_category\n                        ''')\ndiff_df2 = spark.sql('''\n                        SELECT app_id, \n                           country_code, \n                           category_id, \n                           device_code, \n                           est_free_app_download AS free_app_download, \n                           est_paid_app_download AS paid_app_download, \n                           est_revenue           AS revenue \n                        FROM   daily_unified_category \n                        EXCEPT ALL \n                        SELECT * \n                        FROM   country_category_mapping_raw\n                        ''')\n                        \ndiff_df1.show()\ndiff_df2.show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200709-135131_14939011","metadata":{},"outputs":[],"source":["\nspark.sql('''select app_id, country_code, \n                                   category_id, \n                                   device_code, free_app_download, paid_app_download,revenue\n from country_category_mapping_raw where app_id=281736535 and country_code='CA' and category_id=100001 except SELECT app_id, \n                                   country_code, \n                                   category_id, \n                                   device_code, \n                                   est_free_app_download AS free_app_download, \n                                   est_paid_app_download AS paid_app_download, \n                                   est_revenue           AS revenue \n                            FROM   daily_unified_category where app_id=281736535 and country_code='CA' and category_id=100001''').show()\n                            \n                            \nspark.sql('''SELECT app_id, \n                                   country_code, \n                                   category_id, \n                                   device_code, \n                                   est_free_app_download AS free_app_download, \n                                   est_paid_app_download AS paid_app_download, \n                                   est_revenue           AS revenue \n                            FROM   daily_unified_category where app_id=281736535 and country_code='CA' and category_id=100001\n                            except select app_id, country_code, \n                                   category_id, \n                                   device_code, free_app_download, paid_app_download,revenue\n                            from country_category_mapping_raw where app_id=281736535 and country_code='CA' and category_id=100001''').show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200709-034445_258833840","metadata":{},"outputs":[],"source":["\n\nprint spark.read.option(\"basePath\",\n                                 \"s3://b2c-prod-data-pipeline-qa/aa.store/publisher_id/\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/publisher_id/date=2020-0[1,2,3]-01/\").select(\"publisher_id\").distinct().count()\n\n# print spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/publisher_id/date=2020-01-01/\").select(\"publisher_id\").distinct().count()\n# spark.sql(\"select * from rank_unified\").show()\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2020-06-29\").createOrReplaceTempView(\"rank_unified_old\")\n# spark.sql(\"select * from rank_unified_old\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200709-070618_807446515","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from country_category_mapping_rank_raw where country_code='AE' and device_code='ios-phone' and category_id=100000 \").show()\nspark.sql(\"select * from unified_group_data where unified_country_code='AE' and unified_device_code='ios-phone' and unified_category_id=100000\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200709-040149_562531598","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from country_est_mapping_raw where app_id = 20600006432273 and country_code='CA'\").show()\nspark.sql(\"select * from daily_unified_est where app_id = 20600006432273 and country_code='CA'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200709-040616_680803044","metadata":{},"outputs":[],"source":["\ntest_date='2020-07-05'\ndaily_est_load = spark.read.format(\"delta\").load(\n    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/\").where(\n    \"granularity='daily' and date='{}'\".format(test_date)).cache()\ndaily_est_load.createOrReplaceTempView(\"daily_test_est_truncate\")\nspark.sql(\"select * from daily_test_est_truncate where app_id = 20600006432273 and country_code='CA'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200709-035316_262016309","metadata":{},"outputs":[],"source":["\ntest_date='2020-07-03'\ndaily_est_load = spark.read.format(\"delta\").load(\n    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/\").where(\n    \"granularity='daily' and date='{}'\".format(test_date)).cache()\ndaily_est_load.createOrReplaceTempView(\"daily_test_est_truncate\")\nspark.sql(\"select * from daily_test_est_truncate where app_id = 20600006432273 and country_code='CA'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200709-020426_1511634693","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nimport croniter\n\nfrom aadatapipelinecore.core.utils.commandline import env\nfrom aadatapipelinecore.core.utils.encode import activate_system_utf8\nfrom aadatapipelinecore.core.utils.spark import create_spark\n\ncheck_date = '2020-07-01'\ntrigger_date_config = (\"0 12 * * *\", 5)\n\n\n\ntrigger_datetime = datetime.datetime.strptime(\"2020-07-02\", '%Y-%m-%d')\n\nprint trigger_datetime\n\ndef _get_check_date_from_routing_config(trigger_datetime):\n    \"\"\"\n    return the date of : <days_delta> ago from previous scheduled date&time according to <cron_time>.\n    e.g.\n    config = (\"0 9 * * *\", 1), today is 2019-10-27 8:00\n    so previous scheduled date&time is 2019-10-26 9:00\n    will return \"2019-10-25\"\n\n    e.g.\n    config = (\"0 7 * * *\", 1), today is 2019-10-27 8:00\n    so previous scheduled date&time is 2019-10-27 7:00\n    will return \"2019-10-26\"\n\n    Cron Time Format\n    Character\tDescriptor\t        Acceptable values\n    1\t        Minute\t            0 to 59, or * (no specific value)\n    2\t        Hour\t            0 to 23, or * for any value. All times UTC.\n    3\t        Day of the month\t1 to 31, or * (no specific value)\n    4\t        Month\t            1 to 12, or * (no specific value)\n    5\t        Day of the week\t    0 to 7 (0 and 7 both represent Sunday), or * (no specific value)\n\n    :param trigger_datetime: the test trigger date\n    :type trigger_datetime: object\n    :return: date obj of \"%Y-%m-%d\"\n    :type return: object\n    \"\"\"\n    schedule, days_delta = trigger_date_config\n    # here use UTC now\n    cron = croniter.croniter(schedule, trigger_datetime)\n    date = cron.get_prev(datetime.datetime) - datetime.timedelta(days=days_delta)\n    return date\n\n\nprint _get_check_date_from_routing_config(trigger_datetime)\n# def test_store_daily_category(self):\n#     trigger_datetime = datetime.datetime.strptime(\"2020-07-02\", '%Y-%m-%d')\n#     check_date_str_actual = self._get_check_date_from_routing_config(trigger_datetime).strftime(\n#             \"%Y-%m-%d\")\n# df1, df2 = test_daily_trancate_category_data(self.spark, self.check_date)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200709-015833_1628249161","metadata":{},"outputs":[],"source":["\n\n#!/usr/bin/env python\n# Copyright (c) 2017 App Annie Inc. All rights reserved.\n# pylint: disable=E1101,E0633,W9018,W9012,\n# coding=utf-8\n\"\"\"\n\nImplement test cases here\n\"\"\"\nimport unittest\nimport datetime\nimport croniter\n\nfrom aadatapipelinecore.core.utils.commandline import env\nfrom aadatapipelinecore.core.utils.encode import activate_system_utf8\nfrom aadatapipelinecore.core.utils.spark import create_spark\n\n\nclass PipelineTest(unittest.TestCase):\n    trigger_date_config = None\n    trigger_datetime = None\n    prev_etl_datetime = None\n    only_check_in_24hr = False\n\n    def __init__(self, methodName='runTest', trigger_datetime=None):\n        super(PipelineTest, self).__init__(methodName)\n        self.trigger_datetime = trigger_datetime or datetime.datetime.utcnow()\n        self.check_date_str = self._get_check_date_from_routing_config(self.trigger_datetime).strftime(\"%Y-%m-%d\")\n        self.check_date = self.check_date_str  # for compatibility with send email\n        self.prev_etl_datetime = self._get_pre_etl_completed_date()\n\n    def setUp(self):\n        super(PipelineTest, self).setUp()\n        # print \"Triggered Datetime : {}\".format(self.trigger_datetime)\n        # print \"Check date str : {}\".format(self.check_date_str)\n        self._verify_config()\n\n    @classmethod\n    def setUpClass(cls):\n        super(PipelineTest, cls).setUpClass()\n        activate_system_utf8()\n        env(PYTHONIOENCODING='utf8')\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext\n\n    def _verify_config(self):\n        self.assertIsNotNone(self.trigger_date_config)\n        self.assertIsNotNone(self.trigger_datetime)\n        self.assertIsNotNone(self.prev_etl_datetime)\n        self.assertIsNotNone(self.check_date_str)\n        self.assertIsNotNone(self.check_date)\n\n    def _get_check_date_from_routing_config(self, trigger_datetime):\n        \"\"\"\n        return the date of : <days_delta> ago from previous scheduled date&time according to <cron_time>.\n        e.g.\n        config = (\"0 9 * * *\", 1), today is 2019-10-27 8:00\n        so previous scheduled date&time is 2019-10-26 9:00\n        will return \"2019-10-25\"\n\n        e.g.\n        config = (\"0 7 * * *\", 1), today is 2019-10-27 8:00\n        so previous scheduled date&time is 2019-10-27 7:00\n        will return \"2019-10-26\"\n\n        Cron Time Format\n        Character\tDescriptor\t        Acceptable values\n        1\t        Minute\t            0 to 59, or * (no specific value)\n        2\t        Hour\t            0 to 23, or * for any value. All times UTC.\n        3\t        Day of the month\t1 to 31, or * (no specific value)\n        4\t        Month\t            1 to 12, or * (no specific value)\n        5\t        Day of the week\t    0 to 7 (0 and 7 both represent Sunday), or * (no specific value)\n\n        :param trigger_datetime: the test trigger date\n        :type trigger_datetime: object\n        :return: date obj of \"%Y-%m-%d\"\n        :type return: object\n        \"\"\"\n        schedule, days_delta = self.trigger_date_config\n        # here use UTC now\n        cron = croniter.croniter(schedule, trigger_datetime)\n        date = cron.get_prev(datetime.datetime) - datetime.timedelta(days=days_delta)\n        return date\n\n    def _get_pre_etl_completed_date(self):\n        schedule, _ = self.trigger_date_config\n        cron = croniter.croniter(schedule, self.trigger_datetime)\n        date = cron.get_prev(datetime.datetime)\n        return date\n\n\n\n# Copyright (c) 2019 App Annie Inc. All rights reserved.\n# URL: http://tungwaiyip.info/software/HTMLTestRunner.html\n\n\"\"\"\nCREATE HTML TEMPLATE TABLE\n\"\"\"\nimport datetime\nimport StringIO\nimport sys\nimport unittest\nfrom xml.sax import saxutils\n\n\nclass OutputRedirector(object):\n    \"\"\" Wrapper to redirect stdout or stderr \"\"\"\n\n    def __init__(self, fp):\n        self.fp = fp\n\n    def write(self, s):\n        self.fp.write(s)\n\n    def writelines(self, lines):\n        self.fp.writelines(lines)\n\n    def flush(self):\n        self.fp.flush()\n\n\nstdout_redirector = OutputRedirector(sys.stdout)\nstderr_redirector = OutputRedirector(sys.stderr)\n\n\n# ----------------------------------------------------------------------\n# Template\n\nclass Template_mixin(object):\n    STATUS = {\n        0: 'SUCCESS',\n        1: 'FAILED',\n        2: 'ERROR',\n        3: 'SKIP',\n    }\n\n    DEFAULT_TITLE = 'DB Test Report'\n    DEFAULT_DESCRIPTION = ''\n\n    # ------------------------------------------------------------------------\n    # HTML Template\n\n    HTML_TMPL = r\"\"\"\n\n%(heading)s\n%(report)s\n%(ending)s\n\n\"\"\"\n    # variables: (title, generator, stylesheet, heading, report, ending)\n\n    # ------------------------------------------------------------------------\n    # Heading\n    #\n\n    HEADING_TMPL = \"\"\"<div>\n<h1>%(title)s</h1>\n%(parameters)s\n<p>%(description)s</p>\n</div>\n\n\"\"\"\n\n    HEADING_ATTRIBUTE_TMPL = \"\"\"<p><strong>%(name)s:</strong> %(value)s</p>\n\"\"\"\n\n    REPORT_TMPL = \"\"\"\n<table id='result_table' border='1' cellspacing='0' cellpadding='10'>\n<colgroup>\n<col align='left' />\n<col align='right' />\n<col align='right' />\n<col align='right' />\n</colgroup>\n<tr>\n    <th>Test Case</th>\n    <th>Status</th>\n    <th>Check Date</th>\n    <th>Error Information</th>\n</tr>\n%(test_list)s\n</table>\n\"\"\"  # variables: (test_list, count, Pass, fail, error)\n\n    REPORT_CLASS_TMPL = r\"\"\"\n<tr>\n    <td colspan='5' align='center'>%(desc)s</td>\n</tr>\n\"\"\"  # variables: (style, desc, count, Pass, fail, error, cid)\n\n    REPORT_TEST_WITH_OUTPUT_TMPL = r\"\"\"\n<tr>\n    <td>%(desc)s</td>\n    <td bgcolor=%(status_color)s>%(status)s</td>\n    <td>%(check_date)s</td>\n    <td colspan='2' align='left'>\n        <pre>\n        %(script)s\n        </pre>\n    </td>\n</tr>\n\"\"\"  # variables: (tid, Class, style, desc, status)\n\n    REPORT_TEST_NO_OUTPUT_TMPL = r\"\"\"\n<tr>\n    <td>%(desc)s</td>\n    <td bgcolor=%(status_color)s  align='center'>%(status)s</td>\n    <td align='center'>%(check_date)s</td>\n    <td colspan='2' align='center'>\n    </td>\n</tr>\n\"\"\"  # variables: (tid, Class, style, desc, status)\n\n    REPORT_TEST_OUTPUT_TMPL = r\"\"\"\n%(output)s\n\"\"\"  # variables: (id, output)\n\n    # ------------------------------------------------------------------------\n    # ENDING\n    #\n\n    ENDING_TMPL = \"\"\"<div id='ending'>&nbsp;</div>\"\"\"\n\n\n# -------------------- The end of the Template class -------------------\n\n\nTestResult = unittest.TestResult\n\n\nclass _TestResult(TestResult):\n    # note: _TestResult is a pure representation of results.\n    # It lacks the output and reporting ability compares to unittest._TextTestResult.\n\n    def __init__(self, verbosity=1):\n        TestResult.__init__(self)\n        self.outputBuffer = StringIO.StringIO()\n        self.stdout0 = None\n        self.stderr0 = None\n        self.success_count = 0\n        self.failure_count = 0\n        self.error_count = 0\n        self.skipped_count = 0\n        self.verbosity = verbosity\n\n        # result is a list of result in 4 tuple\n        # (\n        #   result code (0: success; 1: fail; 2: error),\n        #   TestCase object,\n        #   Test output (byte string),\n        #   stack trace,\n        # )\n        self.result = []\n\n    def startTest(self, test):\n        TestResult.startTest(self, test)\n        # just one buffer for both stdout and stderr\n        stdout_redirector.fp = self.outputBuffer\n        stderr_redirector.fp = self.outputBuffer\n        self.stdout0 = sys.stdout\n        self.stderr0 = sys.stderr\n        sys.stdout = stdout_redirector\n        sys.stderr = stderr_redirector\n\n    def complete_output(self):\n        \"\"\"\n        Disconnect output redirection and return buffer.\n        Safe to call multiple times.\n        \"\"\"\n        if self.stdout0:\n            sys.stdout = self.stdout0\n            sys.stderr = self.stderr0\n            self.stdout0 = None\n            self.stderr0 = None\n        return self.outputBuffer.getvalue()\n\n    def stopTest(self, test):\n        # Usually one of addSuccess, addError or addFailure would have been called.\n        # But there are some path in unittest that would bypass this.\n        # We must disconnect stdout in stopTest(), which is guaranteed to be called.\n        self.complete_output()\n\n    def addSuccess(self, test):\n        self.success_count += 1\n        TestResult.addSuccess(self, test)\n        output = self.complete_output()\n        self.result.append((0, test, output, ''))\n        if self.verbosity > 1:\n            sys.stderr.write('ok ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('.')\n\n    def addError(self, test, err):\n        self.error_count += 1\n        TestResult.addError(self, test, err)\n        _, _exc_str = self.errors[-1]\n        output = self.complete_output()\n        self.result.append((2, test, output, _exc_str))\n        if self.verbosity > 1:\n            sys.stderr.write('E  ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('E')\n\n    def addFailure(self, test, err):\n        self.failure_count += 1\n        TestResult.addFailure(self, test, err)\n        _, _exc_str = self.failures[-1]\n        output = self.complete_output()\n        self.result.append((1, test, output, _exc_str))\n        if self.verbosity > 1:\n            sys.stderr.write('F  ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('F')\n\n    def addSkip(self, test, reason):\n        self.skipped_count += 1\n        TestResult.addSkip(self, test, reason)\n        _, _exc_str = self.skipped[-1]\n        output = self.complete_output()\n        self.result.append((3, test, output, _exc_str))\n        if self.verbosity > 1:\n            sys.stderr.write('skip  ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('skip')\n\n\nclass HTMLTestRunner(Template_mixin):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, stream=sys.stdout, verbosity=1, title=None, description=None):\n        self.stopTime = None\n        self.stream = stream\n        self.verbosity = verbosity\n        if title is None:\n            self.title = self.DEFAULT_TITLE\n        else:\n            self.title = title\n        if description is None:\n            self.description = self.DEFAULT_DESCRIPTION\n        else:\n            self.description = description\n\n        self.startTime = datetime.datetime.now()\n\n    def run(self, test):\n        \"\"\"Run the given test case or test suite.\"\"\"\n        result = _TestResult(self.verbosity)\n        test(result)\n        self.stopTime = datetime.datetime.now()\n        self.generateReport(result)\n        print >> sys.stderr, '\\nTime Elapsed: %s' % (self.stopTime - self.startTime)\n        return result\n\n    @staticmethod\n    def push_failed_case_to_front(result_list):\n        k = 0\n        for i, result in enumerate(result_list):\n            if result[0] == 1 or result[0] == 2:\n                result_list[i], result_list[k] = result_list[k], result_list[i]\n                k += 1\n        return result_list\n\n    @staticmethod\n    def get_db_check_name(method_name):\n        remove_test_name = method_name[len(\"test_\"):]\n        name_word_list = remove_test_name.split('_')\n        final_name = \"\"\n        for name in name_word_list:\n            final_name += name.capitalize() + \" \"\n        return final_name\n\n    def getReportAttributes(self, result):\n        \"\"\"\n        Return report attributes as a list of (name, value).\n        Override this to add custom attributes.\n        \"\"\"\n        startTime = str(self.startTime)[:19]\n        duration = str(self.stopTime - self.startTime)\n        status = []\n        if result.success_count:\n            status.append('Pass %s' % result.success_count)\n        if result.failure_count:\n            status.append('Failure %s' % result.failure_count)\n        if result.error_count:\n            status.append('Error %s' % result.error_count)\n        if result.skipped_count:\n            status.append('Skip %s' % result.skipped_count)\n        if status:\n            status = ' '.join(status)\n        else:\n            status = 'none'\n        return [\n            ('Start Time', startTime),\n            ('Duration', duration),\n            ('Status', status),\n        ]\n\n    def generateReport(self, result):\n        report_attrs = self.getReportAttributes(result)\n        generator = 'HTMLTestRunner'\n        heading = self._generate_heading(report_attrs)\n        report = self._generate_report(result)\n        ending = self._generate_ending()\n        output = self.HTML_TMPL % dict(\n            title=saxutils.escape(self.title),\n            generator=generator,\n            heading=heading,\n            report=report,\n            ending=ending,\n        )\n        self.stream.write(output.encode('utf8'))\n\n    def _generate_heading(self, report_attrs):\n        a_lines = []\n        for name, value in report_attrs:\n            line = self.HEADING_ATTRIBUTE_TMPL % dict(\n                name=saxutils.escape(name),\n                value=saxutils.escape(value),\n            )\n            a_lines.append(line)\n        heading = self.HEADING_TMPL % dict(\n            title=saxutils.escape(self.title),\n            parameters=''.join(a_lines),\n            description=saxutils.escape(self.description),\n        )\n        return heading\n\n    def _generate_report(self, result):\n        rows = []\n        case_list = self.push_failed_case_to_front(result.result)\n        for case in case_list:\n            n = case[0]  # status number eg. 0, 1, 2, 3\n            t = case[1]  # test case class\n            o = case[2]  # out put\n            e = case[3]  # error message\n\n            self._generate_report_test(rows, n, t, o, e)\n\n        report = self.REPORT_TMPL % dict(\n            test_list=''.join(rows),\n            count=str(result.success_count + result.failure_count + result.error_count),\n            Pass=str(result.success_count),\n            fail=str(result.failure_count),\n            error=str(result.error_count),\n            skip=str(result.skipped_count),\n        )\n        return report\n\n    def _generate_report_test(self, rows, n, t, o, e):\n        # e.g. 'pt1.1', 'ft1.1', etc\n        has_output = bool(o or e)\n        name = t.id().split('.')[-1]\n        check_date = t.check_date\n\n        # test_class_name = t.__class__.__name__[len(\"Test\"):]\n        desc = HTMLTestRunner.get_db_check_name(name)\n        tmpl = (self.REPORT_TEST_WITH_OUTPUT_TMPL if has_output else self.REPORT_TEST_NO_OUTPUT_TMPL)\n\n        # o and e should be byte string because they are collected from stdout and stderr?\n        if isinstance(o, str):\n            # TODO: some problem with 'string_escape': it escape \\n and mess up formating\n            # uo = unicode(o.encode('string_escape'))\n            uo = o.decode('latin-1')\n        else:\n            uo = o\n        if isinstance(e, str):\n            # TODO: some problem with 'string_escape': it escape \\n and mess up formating\n            # ue = unicode(e.encode('string_escape'))\n            ue = e.decode('latin-1')\n        else:\n            ue = e\n\n        script = self.REPORT_TEST_OUTPUT_TMPL % dict(\n            output=saxutils.escape(uo + ue),\n        )\n\n        status_color = \"green\"\n\n        if n == 1:\n            status_color = \"red\"\n        if n == 2:\n            status_color = \"yellow\"\n        if n == 3:\n            status_color = \"grey\"\n\n        row = tmpl % dict(\n            desc=desc,\n            script=script,\n            status=self.STATUS[n],\n            status_color=status_color,\n            check_date=check_date,\n        )\n        rows.append(row)\n        if not has_output:\n            return\n\n    def _generate_ending(self):\n        return self.ENDING_TMPL\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200708-071005_210609679","metadata":{},"outputs":[],"source":["\n\ndaily_est_load = spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/\").where(\n        \"granularity='daily' and date='2020-03-01'\").cache()\ndaily_est_load.createOrReplaceTempView(\"daily_unified_est\")\nspark.sql(\"select * from daily_unified_est\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200708-042112_426979668","metadata":{},"outputs":[],"source":["\n\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nimport aaplproxy\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\nsql_text = \"\"\"\n\n    WITH filter_top_N_raw_data AS(\n    SELECT\n     distinct\n      id,\n      Sum(est) AS est,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      platform_id,\n      vertical,\n      feed,\n      platform);\n\n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n\n\n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n        );\n\n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n    );\n\n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n\n\n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n    -- map raw with country_code\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue \n     from country_code_mapping \n     inner join \n         pivot_metric_raw \n     on \n         country_code_mapping.store_id=pivot_metric_raw.store_id \n     and \n         country_code_mapping.market_code=pivot_metric_raw.platform\n    where country_name!='Global'\n    );\n\n\n      \"\"\"\n\n\n\n\n\"\"\"\nget date:  [ [month, [days]], [month, [days]], [month, [days]], ....... ]\n\"\"\"\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\n\n\n\n\ndef test_daily_pre_data(spark, test_data):\n    # CSV schema\n    from pyspark.sql import types as T\n    from pyspark.sql import functions as F\n\n    csv_schema = T.StructType(\n        [\n            T.StructField(\"store_id\", T.IntegerType(), True),\n            T.StructField(\"date\", T.DateType(), True),\n            T.StructField(\"platform_id\", T.IntegerType(), True),\n            T.StructField(\"vertical\", T.IntegerType(), True),\n            T.StructField(\"feed\", T.IntegerType(), True),\n            T.StructField(\"id\", T.LongType(), True),\n            T.StructField(\"est\", T.IntegerType(), True),\n            T.StructField(\"category_id\", T.IntegerType(), True),\n            T.StructField(\"rank\", T.IntegerType(), True)\n        ]\n    )\n\n    print test_data\n    month_indicator = test_data\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator < '2012-01-01':\n        df_1 = spark.read.option(\"basePath\",\n                                   \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % (test_data),\n            sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id',\n                                                                  'platform_id', 'vertical', 'rank', 'feed',\n                                                                  'est', 'date', 'platform').cache()\n\n\n    ### 2. only csv\n    elif month_indicator >= '2012-01-01' and month_indicator < '2019-07-14':\n        df_ios = spark.read.option(\"basePath\",\n                                   \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % (test_data),\n            sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id',\n                                                                  'platform_id', 'vertical', 'rank', 'feed',\n                                                                  'est', 'date', 'platform').cache()\n        df_android = spark.read.option(\"basePath\",\n                                       \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/android/sbe_est_app/*/\" % (\n                test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).select('id', 'store_id',\n                                                                                      'category_id',\n                                                                                      'platform_id',\n                                                                                      'vertical', 'rank',\n                                                                                      'feed', 'est', 'date',\n                                                                                      'platform').cache()\n        df_1 = df_ios.union(df_android)\n\n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\n                                 \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\n            \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" % (\n                test_data)).cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n\n    daily_est_load = spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\n        \"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_est_load.createOrReplaceTempView(\"daily_unified_est\")\n\n\n\n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"ios_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n\n                \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n            },\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"android_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n\n                \"path\": [\n                    \"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n            }\n        ]\n    }\n\n    run(spark, ingest_msg, sql_text)\n\n    diff_df1 = spark.sql(\n        \"select * from country_category_mapping_raw except all select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue from daily_unified_est \").persist()\n    diff_df2 = spark.sql(\n        \"select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue from daily_unified_est  except all select * from country_category_mapping_raw\").persist()\n\n    # diff_df1.show()\n    # diff_df2.show()\n    print diff_df1.take(3)\n    print diff_df2.take(3)\n    return diff_df1, diff_df2\n\n\n\nDEFAULT_RECIPIENTS = \"fzhang@appannie.com\"\n\ndef send_db_check_email(title, text_context, key=None):\n    default_recipients = [DEFAULT_RECIPIENTS]\n    email.send(title, text_context, default_recipients, sender='dev-qa-data-quality@appannie.com')\n\n\n\n\n\n\nimport unittest\n\n\n\nclass demoTest(unittest.TestCase):\n    check_date = '2020-07-01'\n    df1, df2 = test_daily_pre_data(spark, check_date)\n\n    def test_store_daily_est_category(self):\n        self.assertTrue(self.df1.rdd.isEmpty(), \"raw is more than unified\")\n        self.assertTrue(self.df2.rdd.isEmpty(), \"unified is more than raw\")\n\n\n\ndef send_message():\n    log_file = \"/tmp/db_check.log\"\n    with open(log_file, \"w\") as html_file:\n        suite = unittest.TestSuite()\n        suite.addTests(unittest.TestLoader().loadTestsFromTestCase(demoTest))\n        runner = HTMLTestRunner(\n            stream=html_file,\n            title='DB Test Report',\n            description='This db_check the report output by Tech Team.'\n        )\n\n        failed_count = 0\n        result_list = runner.run(suite).result\n        for result in result_list:\n            if result[0] == 1 or result[0] == 2:\n                failed_count += 1\n\n    with open(log_file, 'r') as html_file:\n        today = datetime.date.today()\n        str_today = today.strftime(\"%Y-%m-%d\")\n\n        title = \"store daily check - unified est/category - \" + str_today + \" - \"\n        if failed_count == 0:\n            title += \"Passed\"\n        else:\n            title += \"Failed\"\n        send_db_check_email(title, html_file.read())\n\n\nsend_message()\n"]},{"cell_type":"code","execution_count":0,"id":"20200708-075643_359328787","metadata":{},"outputs":[],"source":["\n\na = list()\nprint len(a)"]},{"cell_type":"code","execution_count":0,"id":"20200708-070144_1615233394","metadata":{},"outputs":[],"source":["\nimport datetime\nimport StringIO\nimport sys\nimport unittest\nfrom xml.sax import saxutils\n\n\nclass OutputRedirector(object):\n    \"\"\" Wrapper to redirect stdout or stderr \"\"\"\n\n    def __init__(self, fp):\n        self.fp = fp\n\n    def write(self, s):\n        self.fp.write(s)\n\n    def writelines(self, lines):\n        self.fp.writelines(lines)\n\n    def flush(self):\n        self.fp.flush()\n\n\nstdout_redirector = OutputRedirector(sys.stdout)\nstderr_redirector = OutputRedirector(sys.stderr)\n\n\n# ----------------------------------------------------------------------\n# Template\n\nclass Template_mixin(object):\n    STATUS = {\n        0: 'SUCCESS',\n        1: 'FAILED',\n        2: 'ERROR',\n        3: 'SKIP',\n    }\n\n    DEFAULT_TITLE = 'DB Test Report'\n    DEFAULT_DESCRIPTION = ''\n\n    # ------------------------------------------------------------------------\n    # HTML Template\n\n    HTML_TMPL = r\"\"\"\n\n%(heading)s\n%(report)s\n%(ending)s\n\n\"\"\"\n    # variables: (title, generator, stylesheet, heading, report, ending)\n\n    # ------------------------------------------------------------------------\n    # Heading\n    #\n\n    HEADING_TMPL = \"\"\"<div>\n<h1>%(title)s</h1>\n%(parameters)s\n<p>%(description)s</p>\n</div>\n\n\"\"\"\n\n    HEADING_ATTRIBUTE_TMPL = \"\"\"<p><strong>%(name)s:</strong> %(value)s</p>\n\"\"\"\n\n    REPORT_TMPL = \"\"\"\n<table id='result_table' border='1' cellspacing='0' cellpadding='10'>\n<colgroup>\n<col align='left' />\n<col align='right' />\n<col align='right' />\n<col align='right' />\n</colgroup>\n<tr>\n    <th>Test Case</th>\n    <th>Status</th>\n    <th>Check Date</th>\n    <th>Error Information</th>\n</tr>\n%(test_list)s\n</table>\n\"\"\"  # variables: (test_list, count, Pass, fail, error)\n\n    REPORT_CLASS_TMPL = r\"\"\"\n<tr>\n    <td colspan='5' align='center'>%(desc)s</td>\n</tr>\n\"\"\"  # variables: (style, desc, count, Pass, fail, error, cid)\n\n    REPORT_TEST_WITH_OUTPUT_TMPL = r\"\"\"\n<tr>\n    <td>%(desc)s</td>\n    <td bgcolor=%(status_color)s>%(status)s</td>\n    <td>%(check_date)s</td>\n    <td colspan='2' align='left'>\n        <pre>\n        %(script)s\n        </pre>\n    </td>\n</tr>\n\"\"\"  # variables: (tid, Class, style, desc, status)\n\n    REPORT_TEST_NO_OUTPUT_TMPL = r\"\"\"\n<tr>\n    <td>%(desc)s</td>\n    <td bgcolor=%(status_color)s  align='center'>%(status)s</td>\n    <td align='center'>%(check_date)s</td>\n    <td colspan='2' align='center'>\n    </td>\n</tr>\n\"\"\"  # variables: (tid, Class, style, desc, status)\n\n    REPORT_TEST_OUTPUT_TMPL = r\"\"\"\n%(output)s\n\"\"\"  # variables: (id, output)\n\n    # ------------------------------------------------------------------------\n    # ENDING\n    #\n\n    ENDING_TMPL = \"\"\"<div id='ending'>&nbsp;</div>\"\"\"\n\n\n# -------------------- The end of the Template class -------------------\n\n\nTestResult = unittest.TestResult\n\n\nclass _TestResult(TestResult):\n    # note: _TestResult is a pure representation of results.\n    # It lacks the output and reporting ability compares to unittest._TextTestResult.\n\n    def __init__(self, verbosity=1):\n        TestResult.__init__(self)\n        self.outputBuffer = StringIO.StringIO()\n        self.stdout0 = None\n        self.stderr0 = None\n        self.success_count = 0\n        self.failure_count = 0\n        self.error_count = 0\n        self.skipped_count = 0\n        self.verbosity = verbosity\n\n        # result is a list of result in 4 tuple\n        # (\n        #   result code (0: success; 1: fail; 2: error),\n        #   TestCase object,\n        #   Test output (byte string),\n        #   stack trace,\n        # )\n        self.result = []\n\n    def startTest(self, test):\n        TestResult.startTest(self, test)\n        # just one buffer for both stdout and stderr\n        stdout_redirector.fp = self.outputBuffer\n        stderr_redirector.fp = self.outputBuffer\n        self.stdout0 = sys.stdout\n        self.stderr0 = sys.stderr\n        sys.stdout = stdout_redirector\n        sys.stderr = stderr_redirector\n\n    def complete_output(self):\n        \"\"\"\n        Disconnect output redirection and return buffer.\n        Safe to call multiple times.\n        \"\"\"\n        if self.stdout0:\n            sys.stdout = self.stdout0\n            sys.stderr = self.stderr0\n            self.stdout0 = None\n            self.stderr0 = None\n        return self.outputBuffer.getvalue()\n\n    def stopTest(self, test):\n        # Usually one of addSuccess, addError or addFailure would have been called.\n        # But there are some path in unittest that would bypass this.\n        # We must disconnect stdout in stopTest(), which is guaranteed to be called.\n        self.complete_output()\n\n    def addSuccess(self, test):\n        self.success_count += 1\n        TestResult.addSuccess(self, test)\n        output = self.complete_output()\n        self.result.append((0, test, output, ''))\n        if self.verbosity > 1:\n            sys.stderr.write('ok ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('.')\n\n    def addError(self, test, err):\n        self.error_count += 1\n        TestResult.addError(self, test, err)\n        _, _exc_str = self.errors[-1]\n        output = self.complete_output()\n        self.result.append((2, test, output, _exc_str))\n        if self.verbosity > 1:\n            sys.stderr.write('E  ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('E')\n\n    def addFailure(self, test, err):\n        self.failure_count += 1\n        TestResult.addFailure(self, test, err)\n        _, _exc_str = self.failures[-1]\n        output = self.complete_output()\n        self.result.append((1, test, output, _exc_str))\n        if self.verbosity > 1:\n            sys.stderr.write('F  ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('F')\n\n    def addSkip(self, test, reason):\n        self.skipped_count += 1\n        TestResult.addSkip(self, test, reason)\n        _, _exc_str = self.skipped[-1]\n        output = self.complete_output()\n        self.result.append((3, test, output, _exc_str))\n        if self.verbosity > 1:\n            sys.stderr.write('skip  ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('skip')\n\n\nclass HTMLTestRunner(Template_mixin):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, stream=sys.stdout, verbosity=1, title=None, description=None):\n        self.stopTime = None\n        self.stream = stream\n        self.verbosity = verbosity\n        if title is None:\n            self.title = self.DEFAULT_TITLE\n        else:\n            self.title = title\n        if description is None:\n            self.description = self.DEFAULT_DESCRIPTION\n        else:\n            self.description = description\n\n        self.startTime = datetime.datetime.now()\n\n    def run(self, test):\n        \"\"\"Run the given test case or test suite.\"\"\"\n        result = _TestResult(self.verbosity)\n        test(result)\n        self.stopTime = datetime.datetime.now()\n        self.generateReport(result)\n        print >> sys.stderr, '\\nTime Elapsed: %s' % (self.stopTime - self.startTime)\n        return result\n\n    @staticmethod\n    def push_failed_case_to_front(result_list):\n        k = 0\n        for i, result in enumerate(result_list):\n            if result[0] == 1 or result[0] == 2:\n                result_list[i], result_list[k] = result_list[k], result_list[i]\n                k += 1\n        return result_list\n\n    @staticmethod\n    def get_db_check_name(method_name):\n        remove_test_name = method_name[len(\"test_\"):]\n        name_word_list = remove_test_name.split('_')\n        final_name = \"\"\n        for name in name_word_list:\n            final_name += name.capitalize() + \" \"\n        return final_name\n\n    def getReportAttributes(self, result):\n        \"\"\"\n        Return report attributes as a list of (name, value).\n        Override this to add custom attributes.\n        \"\"\"\n        startTime = str(self.startTime)[:19]\n        duration = str(self.stopTime - self.startTime)\n        status = []\n        if result.success_count:\n            status.append('Pass %s' % result.success_count)\n        if result.failure_count:\n            status.append('Failure %s' % result.failure_count)\n        if result.error_count:\n            status.append('Error %s' % result.error_count)\n        if result.skipped_count:\n            status.append('Skip %s' % result.skipped_count)\n        if status:\n            status = ' '.join(status)\n        else:\n            status = 'none'\n        return [\n            ('Start Time', startTime),\n            ('Duration', duration),\n            ('Status', status),\n        ]\n\n    def generateReport(self, result):\n        report_attrs = self.getReportAttributes(result)\n        generator = 'HTMLTestRunner'\n        heading = self._generate_heading(report_attrs)\n        report = self._generate_report(result)\n        ending = self._generate_ending()\n        output = self.HTML_TMPL % dict(\n            title=saxutils.escape(self.title),\n            generator=generator,\n            heading=heading,\n            report=report,\n            ending=ending,\n        )\n        self.stream.write(output.encode('utf8'))\n\n    def _generate_heading(self, report_attrs):\n        a_lines = []\n        for name, value in report_attrs:\n            line = self.HEADING_ATTRIBUTE_TMPL % dict(\n                name=saxutils.escape(name),\n                value=saxutils.escape(value),\n            )\n            a_lines.append(line)\n        heading = self.HEADING_TMPL % dict(\n            title=saxutils.escape(self.title),\n            parameters=''.join(a_lines),\n            description=saxutils.escape(self.description),\n        )\n        return heading\n\n    def _generate_report(self, result):\n        rows = []\n        case_list = self.push_failed_case_to_front(result.result)\n        for case in case_list:\n            n = case[0]  # status number eg. 0, 1, 2, 3\n            t = case[1]  # test case class\n            o = case[2]  # out put\n            e = case[3]  # error message\n\n            self._generate_report_test(rows, n, t, o, e)\n\n        report = self.REPORT_TMPL % dict(\n            test_list=''.join(rows),\n            count=str(result.success_count + result.failure_count + result.error_count),\n            Pass=str(result.success_count),\n            fail=str(result.failure_count),\n            error=str(result.error_count),\n            skip=str(result.skipped_count),\n        )\n        return report\n\n    def _generate_report_test(self, rows, n, t, o, e):\n        # e.g. 'pt1.1', 'ft1.1', etc\n        has_output = bool(o or e)\n        name = t.id().split('.')[-1]\n        check_date = t.check_date\n\n        # test_class_name = t.__class__.__name__[len(\"Test\"):]\n        desc = HTMLTestRunner.get_db_check_name(name)\n        tmpl = (self.REPORT_TEST_WITH_OUTPUT_TMPL if has_output else self.REPORT_TEST_NO_OUTPUT_TMPL)\n\n        # o and e should be byte string because they are collected from stdout and stderr?\n        if isinstance(o, str):\n            # TODO: some problem with 'string_escape': it escape \\n and mess up formating\n            # uo = unicode(o.encode('string_escape'))\n            uo = o.decode('latin-1')\n        else:\n            uo = o\n        if isinstance(e, str):\n            # TODO: some problem with 'string_escape': it escape \\n and mess up formating\n            # ue = unicode(e.encode('string_escape'))\n            ue = e.decode('latin-1')\n        else:\n            ue = e\n\n        script = self.REPORT_TEST_OUTPUT_TMPL % dict(\n            output=saxutils.escape(uo + ue),\n        )\n\n        status_color = \"green\"\n\n        if n == 1:\n            status_color = \"red\"\n        if n == 2:\n            status_color = \"yellow\"\n        if n == 3:\n            status_color = \"grey\"\n\n        row = tmpl % dict(\n            desc=desc,\n            script=script,\n            status=self.STATUS[n],\n            status_color=status_color,\n            check_date=check_date,\n        )\n        rows.append(row)\n        if not has_output:\n            return\n\n    def _generate_ending(self):\n        return self.ENDING_TMPL\n"]},{"cell_type":"code","execution_count":0,"id":"20200708-033220_499534146","metadata":{},"outputs":[],"source":["\nimport unittest\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\nfrom aadatapipelinecore.core.fs import Conf\n\nfrom aadatapipelinecore.core.monitoring.pipeline_monitor import running, fail, task_success\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.utils.identifier import package_id\nfrom aadatapipelinecore.core.utils.spark import canned_spark, stop\nfrom aadatapipelinecore.core.utils import email\n\nios_feed = {1: \"0,1,2,100,101,102\"}\nandroid_feed = {0: \"0,1,2\"}\n\n\ndef citus_row(date):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select device_code, cast(sum(est_free_app_download) as int) as est_free_app_download, cast(sum(est_paid_app_download) as int) as est_paid_app_download, cast(sum(est_revenue) as int) as est_revenue from store.store_est_fact_v1 where date='{}' group by device_code\".format(\n            date)\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n\n    result = get_data_in_citus(date)\n    return [Row(device_code=r[0], sum_est_free_app_download=r[1], sum_est_paid_app_download=r[2],\n                sum_est_revenue=r[3]) for r in result]\n\n\ndef plproxy_row(date, device_feed_dict):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n\n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n    if device_feed_dict.keys()[0] == 1:\n        store_id = 1\n    else:\n        store_id = 1001\n\n    sql = '''select feed_id, Cast(sum(cnt) as int) as metric_sum from plproxy.execute_select_nestloop($$\n    select feed_id, sum(estimate)  as cnt\n    from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_{}\n        where date = '{}'  and  feed_id in ({}) and store_id not in ({}) and isunique='t' ) as prod group by feed_id \n        $$) tbl (feed_id smallint, cnt bigint) group by feed_id '''.format(\n        device_feed_dict.keys()[0], date, ','.join(device_feed_dict.values()), store_id\n    )\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [Row(feed_id=r[0], metric_sum=r[1]) for r in rows]\n\n\ndef generate_plploxy_result(spark, r1, r2):\n    schema = StructType([\n        StructField(\"feed_id\", StringType(), True),\n        StructField(\"metric_sum\", IntegerType(), True)])\n    df1 = spark.createDataFrame(r1, schema)\n    df1.createOrReplaceTempView(\"plploxy_ios\")\n\n    df2 = spark.createDataFrame(r2, schema)\n    df2.createOrReplaceTempView(\"plploxy_android\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='100' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='102' THEN \"sum_est_revenue\"\n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_ios\n    ''').createOrReplaceTempView(\"plploxy_metric\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_android\n    ''').createOrReplaceTempView(\"plploxy_metric_android\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"ios-tablet\"\n    WHEN feed_id='100' THEN \"ios-tablet\"\n    WHEN feed_id='102' THEN \"ios-tablet\"\n    WHEN feed_id='0' THEN \"ios-phone\"\n    WHEN feed_id='1' THEN \"ios-phone\"\n    WHEN feed_id='2' THEN \"ios-phone\"\n    END AS device_code from plploxy_metric\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code\")\n\n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"android-all\"\n    WHEN feed_id='1' THEN \"android-all\"\n    WHEN feed_id='2' THEN \"android-all\"\n    END AS device_code from plploxy_metric_android\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code_android\")\n\n    spark.sql('''\n    SELECT * FROM plploxy_metric_device_code_android\n    UNION ALL\n    SELECT * FROM plploxy_metric_device_code\n    ''').createOrReplaceTempView(\"all_device\")\n\n    spark.sql('''\n    SELECT \n        device_code, sum_est_free_app_download , sum_est_paid_app_download , sum_est_revenue\n    FROM\n          all_device\n     PIVOT (\n        max(metric_sum) \n        FOR metric IN ('sum_est_free_app_download','sum_est_paid_app_download', 'sum_est_revenue')\n      )\n    ''').createOrReplaceTempView(\"after_pivot\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_free_app_download FROM after_pivot\n    WHERE sum_est_free_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_1\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_paid_app_download FROM after_pivot\n    WHERE sum_est_paid_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_2\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_revenue FROM after_pivot\n    WHERE sum_est_revenue is not null\n    ''').createOrReplaceTempView(\"after_pivot_3\")\n\n    return spark.sql('''\n    SELECT c.device_code, c.sum_est_free_app_download, c.sum_est_paid_app_download, d.sum_est_revenue\n    FROM (\n    SELECT a.device_code, a.sum_est_free_app_download, b.sum_est_paid_app_download FROM after_pivot_1 a\n    JOIN after_pivot_2 b\n    ON a.device_code=b.device_code\n    ) AS c\n    JOIN after_pivot_3 d\n    on c.device_code=d.device_code\n    order by device_code desc\n    ''')\n\n\ndef generate_citus_result(spark, citus_data):\n    schema = StructType([\n        StructField(\"device_code\", StringType(), True),\n        StructField(\"sum_est_free_app_download\", IntegerType(), True),\n        StructField(\"sum_est_paid_app_download\", IntegerType(), True),\n        StructField(\"sum_est_revenue\", IntegerType(), True)])\n\n    df_3 = spark.createDataFrame(citus_data, schema)\n    df_3.createOrReplaceTempView(\"citus_data\")\n    return spark.sql(\"select * from citus_data order by device_code desc\")\n\n\ndef test_store_prproxy_data(spark, d):\n    print d\n    r1 = plproxy_row(d, ios_feed)\n    r2 = plproxy_row(d, android_feed)\n    citus_reseult = citus_row(d)\n\n    d1 = generate_plploxy_result(spark, r1, r2)\n    d2 = generate_citus_result(spark, citus_reseult)\n\n    d1.createOrReplaceTempView(\"plploxy_r\")\n    d2.createOrReplaceTempView(\"citus_r\")\n    plproxy_except_result = spark.sql(\"select * from plploxy_r except all select * from citus_r\")\n    citus_except_result = spark.sql(\"select * from citus_r except all select * from plploxy_r\")\n\n    plproxy_except_result = plproxy_except_result.withColumn(\"result_from\", F.lit(\"proproxy\")).withColumn(\n        \"date\", F.lit(str(d)))\n    citus_except_result = citus_except_result.withColumn(\"result_from\", F.lit(\"citus\")).withColumn(\"date\",\n                                                                                                   F.lit(str(\n                                                                                                       d)))\n\n    result = plproxy_except_result.union(citus_except_result)\n    return plproxy_except_result.take(5), citus_except_result.take(5)\n\n\n\nDEFAULT_RECIPIENTS = \"fzhang@appannie.com\"\n\ndef send_db_check_email(title, text_context, key=None):\n    default_recipients = [DEFAULT_RECIPIENTS]\n    email.send(title, text_context, default_recipients, sender='dev-qa-data-quality@appannie.com')\n\n\n\n# start = params.get('start_date')\n# end = params.get('end_date')\nstart = '2020-06-01'\nend = '2020-06-02'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\ndates.sort(reverse=True)\n\nresult = list()\nfor d in dates:\n    result.append(test_store_prproxy_data(spark, d))\n\ntitle = \"store daily check\"\ntext_context = result\nsend_db_check_email(title, text_context)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200707-102910_1565651105","metadata":{},"outputs":[],"source":["\ndf_routine = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.1/fact/granularity=daily/date=2020-07-03/\")\ndf_previously = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v2/fact/granularity=daily/date=2020-07-03/\")\n\ndf_routine.createOrReplaceTempView(\"routine\")\ndf_previously.createOrReplaceTempView(\"previously\")\n\nspark.sql(\"select app_id, country_code, free_app_download_rank as free_app_download, paid_app_download_rank as paid_app_download, revenue_rank as revenue, revenue_iap_rank as revenue_iap, revenue_non_iap_rank as revenue_non_iap, device_code, category_id from routine where data_stage='final' except all select app_id, country_code, free_app_download, paid_app_download, revenue, revenue_iap, revenue_non_iap, device_code, category_id from previously \").show()\n\nspark.sql(\"select app_id, country_code, free_app_download, paid_app_download, revenue, revenue_iap, revenue_non_iap, device_code, category_id from previously except all select app_id, country_code, free_app_download_rank as free_app_download, paid_app_download_rank as paid_app_download, revenue_rank as revenue, revenue_iap_rank as revenue_iap, revenue_non_iap_rank as revenue_non_iap, device_code, category_id from routine where data_stage='final' \").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200707-074025_139326769","metadata":{},"outputs":[],"source":["\ndf_routine = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v4/fact/granularity=daily/date=2020-07-03/\")\ndf_previously = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2020-07-03/\")\n\ndf_routine.createOrReplaceTempView(\"routine\")\ndf_previously.createOrReplaceTempView(\"previously\")\n\nspark.sql(\"select app_id, country_code, free_app_download_rank as free_app_download, paid_app_download_rank as paid_app_download, revenue_rank as revenue, revenue_iap_rank as revenue_iap, revenue_non_iap_rank as revenue_non_iap, device_code, category_id from routine where data_stage='final' except all select app_id, country_code, free_app_download, paid_app_download, revenue, revenue_iap, revenue_non_iap, device_code, category_id from previously \").show()\n\nspark.sql(\"select app_id, country_code, free_app_download, paid_app_download, revenue, revenue_iap, revenue_non_iap, device_code, category_id from previously except all select app_id, country_code, free_app_download_rank as free_app_download, paid_app_download_rank as paid_app_download, revenue_rank as revenue, revenue_iap_rank as revenue_iap, revenue_non_iap_rank as revenue_non_iap, device_code, category_id from routine where data_stage='final' \").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200707-030839_607582611","metadata":{},"outputs":[],"source":["\ndf_routine = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v4/fact/granularity=daily/date=2020-07-03/\")\ndf_previously = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-07-03/\")\n\ndf_routine.createOrReplaceTempView(\"routine\")\ndf_previously.createOrReplaceTempView(\"previously\")\n\nspark.sql(\"select app_id, country_code, free_app_download, paid_app_download, revenue, revenue_iap, revenue_non_iap, device_code from routine where data_stage='final' except all select app_id, country_code, free_app_download, paid_app_download, revenue, revenue_iap, revenue_non_iap, device_code from previously \").show()\n\nspark.sql(\"select app_id, country_code, free_app_download, paid_app_download, revenue, revenue_iap, revenue_non_iap, device_code from previously except all select app_id, country_code, free_app_download, paid_app_download, revenue, revenue_iap, revenue_non_iap, device_code from routine \").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200706-035604_1166697495","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-"]},{"cell_type":"code","execution_count":0,"id":"20200706-035318_396630751","metadata":{},"outputs":[],"source":["CREATE TABLE unified_est (\n    data_stage text NOT NULL,\n    country_code text NOT NULL,\n    device_code text NOT NULL,\n    granularity text NOT NULL,\n    date date NOT NULL,\n    app_id int NOT NULL,\n    revenue_non_iap int,\n    revenue_iap int ,\n    free_app_download int ,\n    paid_app_download int ,\n    revenue int,\n    PRIMARY KEY(data_stage,country_code,device_code,date,granularity,app_id)\n)\nPARTITION BY COLUMNS(granularity,date,data_stage,device_code)\nWITH (\n    NAMESPACE = 'aa.store.app-est.v4',\n    COALESCE = 1,\n    SAVEMODE = 'overwrite',\n    DIMENSIONS = COLUMNS(data_stage,country_code,device_code,date,granularity,app_id),\n    METRICS = COLUMNS(revenue_non_iap,revenue_iap,free_app_download,paid_app_download,revenue)\n);\nCREATE TABLE unified_category (\n  data_stage text NOT NULL,\n  country_code text NOT NULL,\n  device_code text NOT NULL,\n  granularity text NOT NULL,\n  date date NOT NULL,\n  category_id int NOT NULL,\n  app_id bigint NOT NULL,\n  revenue_non_iap int,\n  revenue_non_iap_rank int,\n  revenue_iap int ,\n  revenue_iap_rank int,\n  free_app_download int,\n  free_app_download_rank int,\n  paid_app_download int ,\n  paid_app_download_rank int ,\n  revenue int ,\n  revenue_rank int,\n  PRIMARY KEY(data_stage,country_code,device_code,date,granularity,category_id,app_id)\n)\nPARTITION BY COLUMNS(granularity,date,data_stage,device_code)\nWITH (\n  NAMESPACE = 'aa.store.app-est-category.v4',\n  COALESCE = 2,\n  SAVEMODE = 'overwrite',\n  DIMENSIONS = COLUMNS(data_stage,country_code,device_code,date,granularity,category_id,app_id),\n  METRICS = COLUMNS(revenue_non_iap,revenue_iap,free_app_download,paid_app_download,revenue)\n);\nCREATE TABLE unified_est_preload_dna_log (\n    granularity text NOT NULL,\n    date date NOT NULL,\n    country_code text NOT NULL,\n    device_code text NOT NULL,\n    app_id bigint NOT NULL,\n    publisher_id bigint NOT NULL,\n    company_id bigint NOT NULL,\n    parent_company_id bigint NOT NULL,\n    est_free_app_download int,\n    est_paid_app_download int,\n    est_revenue int,\n    est_paid_download int,\n    est_organic_download int,\n    PRIMARY KEY(granularity,date,country_code,device_code,app_id,company_id, parent_company_id, publisher_id)\n)\nPARTITION BY COLUMNS(granularity,date,device_code)\nWITH (\n    NAMESPACE = 'aa.store.app-est-dna-log.v2',\n    COALESCE = 1,\n    SAVEMODE = 'overwrite',\n    DIMENSIONS = COLUMNS(country_code,device_code,date,granularity,app_id, company_id, parent_company_id, publisher_id),\n    METRICS = COLUMNS(est_free_app_download,est_paid_app_download,est_revenue,est_paid_download,est_organic_download)\n);\nCREATE TABLE unified_category_preload (\n    granularity text NOT NULL,\n    date date NOT NULL,\n    country_code text NOT NULL,\n    device_code text NOT NULL,\n    app_id bigint NOT NULL,\n    category_id int NOT NULL,\n    est_free_app_download int,\n    est_paid_app_download int,\n    est_revenue int,\n    est_paid_download int,\n    est_organic_download int,\n  PRIMARY KEY(country_code,device_code,date,granularity,app_id,category_id)\n)\nPARTITION BY COLUMNS(granularity,date,device_code)\nWITH (\n  NAMESPACE = 'aa.store.app-est-category-load.v4',\n  COALESCE = 1,\n  SAVEMODE = 'overwrite',\n  DIMENSIONS = COLUMNS(country_code,device_code,date,granularity,app_id,category_id),\n  METRICS = COLUMNS(est_free_app_download,est_paid_app_download,est_revenue, est_paid_download, est_organic_download)\n);\n\n\n\n\nCREATE TABLE unified_download_attribution (\n    country_code text NOT NULL,\n    device_code text NOT NULL,\n    granularity text NOT NULL,\n    date date NOT NULL,\n    app_id bigint NOT NULL,\n    organic_download_share decimal(36,20) NOT NULL,\n    PRIMARY KEY(country_code,device_code,date,granularity,app_id)\n)\n    PARTITION BY COLUMNS(granularity,date,device_code)\n    WITH (\n    NAMESPACE = 'aa.store.download-attribution.v4',\n    COALESCE = 1,\n    SAVEMODE = 'overwrite',\n    DIMENSIONS = COLUMNS(country_code,device_code,date,granularity,app_id),\n    METRICS = COLUMNS(organic_download_share)\n);\nCREATE TABLE est_table (\n    country_code text NOT NULL,\n    device_code text NOT NULL,\n    granularity text NOT NULL,\n    date date NOT NULL,\n    app_id bigint NOT NULL,\n    company_id bigint NOT NULL,\n    parent_company_id bigint NOT NULL,\n    publisher_id bigint NOT NULL,\n    est_free_app_download int ,\n    est_paid_app_download int ,\n    est_revenue int,\n    est_organic_download int,\n    est_paid_download int,\n    PRIMARY KEY(country_code, device_code, date, granularity, app_id, company_id, parent_company_id, publisher_id)\n)\nPARTITION BY COLUMNS(granularity, date, device_code)\nWITH (\n    NAMESPACE = 'aa.store.download-attribution-dna-log.v2',\n    coalesce=1,\n    SAVEMODE = 'overwrite',\n    DIMENSIONS = COLUMNS(country_code, device_code, date, granularity, app_id, company_id, parent_company_id, publisher_id),\n    METRICS = COLUMNS(est_free_app_download, est_paid_app_download, est_revenue, est_paid_download, est_organic_download)\n);\nCREATE TABLE cate_table (\n    country_code text NOT NULL,\n    device_code text NOT NULL,\n    granularity text NOT NULL,\n    date date NOT NULL,\n    app_id bigint NOT NULL,\n    category_id int NOT NULL,\n    est_free_app_download int ,\n    est_paid_app_download int ,\n    est_revenue int,\n    est_organic_download int,\n    est_paid_download int,\n    PRIMARY KEY(country_code, device_code, date, granularity, app_id, category_id)\n)\nPARTITION BY COLUMNS(granularity, date, device_code)\nWITH (\n    NAMESPACE = 'aa.store.download-attribution-category-load.v4',\n    coalesce=1,\n    SAVEMODE = 'overwrite',\n    DIMENSIONS = COLUMNS(country_code, device_code, date, granularity, app_id, category_id),\n    METRICS = COLUMNS(est_free_app_download, est_paid_app_download, est_revenue, est_paid_download, est_organic_download)\n);"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}