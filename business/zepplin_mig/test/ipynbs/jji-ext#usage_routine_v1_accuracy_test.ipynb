{"cells":[{"cell_type":"code","execution_count":0,"id":"20200528-083748_82990940","metadata":{},"outputs":[],"source":["\nimport datetime\nimport random\nfrom pyspark.sql.functions import lit\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.types import LongType\nfrom pyspark.sql import functions\nfrom pyspark.sql import Row\n\n\ntest_result = []\ndevice_code_dict = {1: {'1': 'Android-phone', '2': 'Android-tablet'}, 2: {'1': 'ios-phone', '2': 'ios-tablet'}}\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 29)\n    start = datetime.date(2019, 10, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 03, 28)\n    start = datetime.date(2019, 10, 05)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 03, 28)\n    start = datetime.date(2019, 11, 22)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef check_not_empty(df, date):\n    empty_count = df.select('AU').filter(\"AU is null\").count()\n    if empty_count != 0:\n        print \"AU is Not Empty Test Fail!!! empty_count: {}, date: {}\".format(empty_count, date)\n    else:\n        print \"AU is Not Empty Test Pass! date: {}\".format(date)\n\n\ndef check_percentage_accuracy(df, date):\n    illegal_percentage_count = df.select('IP', 'MBWFT', 'OR', 'PAD', 'UP', 'SOI', 'SOU').filter(\n        \"IP>1 or MBWFT>1 or OR>1 or PAD>1 or UP>1 or SOI>1 or SOU>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Percentage<1 Test Fail!!! illegal_percentage_count: {}, date: {}\".format(illegal_percentage_count, date)\n    else:\n        print \"Percentage<1 Test Pass! date: {}\".format(date)\n\n\ndef check_routine_v1_accuracy(date_list, _granularity):\n    v1_path = 's3://b2c-prod-data-pipeline-unified-usage/' \\\n                   'unified/usage.basic-kpi.v1/fact/granularity={unified_granularity}/date={unified_date}/'\n    routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/' \\\n               'version=v3.0.0/range_type={raw_granularity}/date={raw_date}/'\n    for month in date_list:\n        sample_index = random.randint(0, len(month[1]) - 1)\n        date = month[1][sample_index]\n        \n        v1_path_parse = v1_path.format(unified_granularity=_granularity, unified_date=date)\n        routine_path_parse = routine_path.format(raw_granularity=raw_granularity_dict[_granularity], raw_date=date)\n        routine_df = spark.read.parquet(routine_path_parse)\n        v1_df = spark.read.parquet(v1_path_parse).drop('_identifier')\n\n        check_not_empty(routine_df, date)\n        check_percentage_accuracy(routine_df, date)\n\n        routine_df = (\n                routine_df\n                .withColumn('device_code', functions.UserDefinedFunction(\n                lambda x, y: device_code_dict[x][y])(routine_df['platform'], routine_df['device_type']))\n                .withColumnRenamed('country', 'country_code')\n                .withColumn('app_id', routine_df['app_id'].cast(LongType()))\n                .withColumnRenamed('AU', 'est_average_active_users')\n                .withColumnRenamed('AFU', 'est_average_session_per_user')\n                .withColumnRenamed('ADU', 'est_average_session_duration')\n                .withColumnRenamed('IP', 'est_install_penetration')\n                .withColumnRenamed('AAD', 'est_average_active_days')\n                .withColumnRenamed('PAD', 'est_percentage_active_days')\n                .withColumnRenamed('MBPU', 'est_average_bytes_per_user')\n                .withColumnRenamed('ATU', 'est_average_time_per_user')\n                .withColumnRenamed('UP', 'est_usage_penetration')\n                .withColumnRenamed('OR', 'est_open_rate')\n                .withColumnRenamed('MBPS', 'est_average_bytes_per_session')\n                .withColumnRenamed('MBWFT', 'est_percent_of_wifi_total')\n                .withColumnRenamed('MBS', 'est_mb_per_second')\n                .withColumnRenamed('IS', 'est_installs')\n                .withColumnRenamed('SOU', 'est_average_active_users_country_share')\n                .withColumnRenamed('SOI', 'est_installs_country_share')\n                .withColumn('est_share_of_category_time', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_category_session', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_category_bytes', lit(None).cast(DoubleType()))\n                .withColumn('est_panel_size', lit(None).cast(DoubleType()))\n                .drop('device_type')\n                .drop('platform')\n        )\n        subtract_count = routine_df.select(v1_df.columns).subtract(v1_df).count()\n        subtract_count_reverse = v1_df.select(routine_df.columns).subtract(routine_df).count()\n        if subtract_count != 0 or subtract_count_reverse != 0:\n            print 'Accuracy Test Fail!!!! subtract_count: {}, date={}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        else:\n            print 'Accuracy Test Pass! date={}'.format(date)\n\n\ngraularity_list = [\"daily\", \"weekly\", \"monthly\"]\nfor graularity in graularity_list:\n    check_routine_v1_accuracy(get_path_date_list(graularity), graularity)\n"]},{"cell_type":"code","execution_count":0,"id":"20200528-083824_2118979973","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}