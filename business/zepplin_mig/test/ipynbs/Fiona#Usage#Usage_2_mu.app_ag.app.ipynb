{"cells":[{"cell_type":"code","execution_count":0,"id":"20191211-090314_625053228","metadata":{},"outputs":[],"source":["%%sh\nm='201307'\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\nSELECT device_id,store_id,date,app_id,kpi,estimate,age, gender\n  FROM plproxy.execute_select_nestloop(\\$proxy\\$select device_id,store_id,date,app_id,kpi,estimate,age, gender from ag.app_monthly_1001_1_201802\n \\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT, age SMALLINT, gender SMALLINT)  where app_id= '20600006257774' and kpi=22 order by estimate desc limit 50;\n\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191211-090616_952897425","metadata":{},"outputs":[],"source":["\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity=weekly/month=2014-02/device_id=2001/store_id=143441/\"\n# print 'raw data', spark.read.parquet(path).groupBy(\"kpi\").agg(count(\"kpi\")).show()\n# print spark.read.parquet(path).groupBy(\"kpi\").agg(count(\"kpi\")).show()\n# df1=spark.read.parquet(path).filter(\"kpi=22\").select(\"kpi\",\"age\",\"app_id\",\"gender\").distinct().withColumnRenamed(\"app_id\",\"raw_app_id\")\ndf1 = spark.read.parquet(path).filter(\"date='2014-02-22'\").select(\"kpi\",\"age\",\"app_id\",\"gender\").distinct().groupBy(\"kpi\").agg(count(\"kpi\")).collect()\nprint df1\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.segmented-by-age-gender.v1/fact/granularity=weekly/date=2014-02-22\"\ndf2 = spark.read.parquet(path).filter(\"device_code='ios-phone' and country_code='US' and est_panel_size is not null\").count()\nprint df2\n# df2=spark.read.parquet(path).filter(\"device_code='ios-phone' and country_code='US' and est_panel_size is not null\").withColumnRenamed(\"app_id\",\"unified_app_id\")\n\n# union_df=df1.join(df2, df1[\"raw_app_id\"]==df2[\"unified_app_id\"], how='full').filter(\"unified_app_id is  null\")\n# print 'union count', union_df.count()\n# union_df.show()\n# AssertionError:  raw: 31476 ~ unified data: 25788,device:2001,  store_id:143441 , month: (u'2014-02', u'2014-02-22'), KPI 22\n\n\n\n# AssertionError:  raw: 134348 ~ unified data: 134364,device:2001,  store_id:143441 , month: (u'2017-04', u'2017-04-30')\n\n#  AssertionError:  raw: 108809 ~ unified data: 100440,device:2001,  store_id:143441 , month: (u'2015-02', u'2015-02-28')\n\n#                     raw_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={raw_device_id}/store_id={raw_store_id}/\"\n#                     unified_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.segmented-by-age-gender.v1/fact/granularity={unified_granularity}/date={unified_date}/\"\n\n# +---+----------+\n# |kpi|count(kpi)|\n# +---+----------+\n# |  1|    134348|\n# |  9|    134348|\n# |  4|    134268|\n# | 10|    134280|\n# +---+----------+\n"]},{"cell_type":"code","execution_count":0,"id":"20191224-020408_1404781076","metadata":{},"outputs":[],"source":["\n\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity=weekly/month=2014-02/device_id=2001/store_id=143441/\"\n# print 'raw data', spark.read.parquet(path).groupBy(\"kpi\").agg(count(\"kpi\")).show()\n# print spark.read.parquet(path).groupBy(\"kpi\").agg(count(\"kpi\")).show()\ndf1=spark.read.parquet(path).filter(\"kpi=22 and app_id=307312434 and date= \").show()"]},{"cell_type":"code","execution_count":0,"id":"20191211-090654_1493463940","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity=weekly/month=2014-02/device_id=2001/store_id=143441/\n \n\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ad_app.v1/fact/granularity=monthly/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191223-032234_706197458","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\n\nkpi_mapping={1:\"est_average_active_users\", 2: \"est_average_session_per_user\", 3:\"est_average_session_duration\", 4:\"est_install_penetration\", 5:\"est_average_active_days\", 6:\"est_percentage_active_days\", 7:\"est_average_bytes_per_user\" , 8:\"est_average_time_per_user\", 9:\"est_usage_penetration\", 10:\"est_open_rate\",11:\"est_total_time\", 12:\"est_share_of_category_time\",14:\"est_total_sessions\", 15:\"est_share_of_category_session\", 17:\"est_average_bytes_per_session\", 18:\"est_share_of_category_bytes\", 20:\"est_percent_of_wifi_total\", 21:\"est_mb_per_second\", 22:\"est_panel_size\", 23:\"est_installs\", 24:\"est_average_active_users_country_share\", 25:\"est_installs_country_share\", 26:\"est_audience_index\", 27:\"est_audience_percentage\"}\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return {\"2001\":\"ios-phone\",\"2002\":\"ios-tablet\"}\n    else:\n        return {\"1001\":\"android-phone\" ,\"1002\":\"android-tablet\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)  \n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db,current  ))\n        current += relativedelta(months=1)  \n    return result\n\n\ndef get_path_date_list(granularity):\n    # df_date = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity={}/\".format(granularity)).select('date').dropDuplicates()\n    # collect_date= df_date.collect()\n    collect_date=[Row(date=u'2018-12-31'), Row(date=u'2019-08-31'), Row(date=u'2015-02-28'), Row(date=u'2017-04-30'), Row(date=u'2014-08-31'), Row(date=u'2017-10-31'), Row(date=u'2018-10-31'), Row(date=u'2018-05-31'), Row(date=u'2017-08-31'), Row(date=u'2013-09-30'), Row(date=u'2016-09-30'), Row(date=u'2014-06-30'), Row(date=u'2013-07-31'), Row(date=u'2018-06-30'), Row(date=u'2014-07-31'), Row(date=u'2016-12-31'), Row(date=u'2014-09-30'), Row(date=u'2017-05-31'), Row(date=u'2019-06-30'), Row(date=u'2013-05-31'), Row(date=u'2015-03-31'), Row(date=u'2015-11-30'), Row(date=u'2019-10-31'), Row(date=u'2017-06-30'), Row(date=u'2019-02-28'), Row(date=u'2015-06-30'), Row(date=u'2018-04-30'), Row(date=u'2016-05-31'), Row(date=u'2013-03-31'), Row(date=u'2016-03-31'), Row(date=u'2019-09-30'), Row(date=u'2015-12-31'), Row(date=u'2015-01-31'), Row(date=u'2013-01-31'), Row(date=u'2014-02-28'), Row(date=u'2019-03-31'), Row(date=u'2016-08-31'), Row(date=u'2018-02-28'), Row(date=u'2013-06-30'), Row(date=u'2016-07-31'), Row(date=u'2015-10-31'), Row(date=u'2018-03-31'), Row(date=u'2014-01-31'), Row(date=u'2018-09-30'), Row(date=u'2017-07-31'), Row(date=u'2019-04-30'), Row(date=u'2014-05-31'), Row(date=u'2019-01-31'), Row(date=u'2018-08-31'), Row(date=u'2014-04-30'), Row(date=u'2016-01-31'), Row(date=u'2017-12-31'), Row(date=u'2019-05-31'), Row(date=u'2017-09-30'), Row(date=u'2018-11-30'), Row(date=u'2018-01-31'), Row(date=u'2016-06-30'), Row(date=u'2015-04-30'), Row(date=u'2015-05-31'), Row(date=u'2018-07-31'), Row(date=u'2016-02-29'), Row(date=u'2015-09-30'), Row(date=u'2013-12-31'), Row(date=u'2014-12-31'), Row(date=u'2013-08-31'), Row(date=u'2013-04-30'), Row(date=u'2019-07-31'), Row(date=u'2013-02-28'), Row(date=u'2017-01-31'), Row(date=u'2017-11-30'), Row(date=u'2013-11-30'), Row(date=u'2013-10-31'), Row(date=u'2017-02-28'), Row(date=u'2016-11-30'), Row(date=u'2016-04-30'), Row(date=u'2014-03-31'), Row(date=u'2014-11-30'), Row(date=u'2015-07-31'), Row(date=u'2017-03-31'), Row(date=u'2014-10-31'), Row(date=u'2016-10-31'), Row(date=u'2015-08-31')]\n    date_list = [(x[0][:7],x[0]) for x in collect_date]\n    print date_list\n    return date_list\n    \n    \n\n    \nimport traceback\ndef check_ag_data_count(store_id_list, device_id_list, _granularity, date_list):\n    t = unittest.TestCase('run')\n    for id,country_code in store_id_list.items():\n        for device,device_code in device_id_list.items():\n            for m in date_list:\n                raw_count_with_KPI=''\n                # print id, device, m[0] , m[1]\n                try:\n                    raw_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={raw_device_id}/store_id={raw_store_id}/\"\n                    unified_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.segmented-by-age-gender.v1/fact/granularity={unified_granularity}/date={unified_date}/\"\n                    raw_path_parse=raw_path.format(raw_device_id=device,raw_store_id=id, raw_month=m[0], raw_granularity=_granularity)\n                    raw_count_with_KPI=spark.read.parquet(raw_path_parse).filter(\"date='{}'\".format(m[1])).select(\"kpi\",\"age\",\"app_id\",\"gender\").distinct().groupBy(\"kpi\").agg(count(\"kpi\")).collect()\n                    # print raw_count_with_KPI\n                except AnalysisException as e: \n                    break\n                    # traceback.print_exc()\n                # print 'raw count', raw_count\n                for row in raw_count_with_KPI:\n                    # print 'row _ test', row[\"kpi\"], row[\"count(kpi)\"]\n                    unified_path_parse=unified_path.format(unified_date=m[1], unified_granularity=_granularity)\n                    unified_count= spark.read.parquet(unified_path_parse).filter(\"device_code='{}' and country_code='{}'\".format(device_code,country_code)).filter(\"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                    # print 'unified count' , unified_count\n                    t.assertEqual(row[\"count(kpi)\"], unified_count+1, \" raw: {} ~ unified data: {},device:{},  store_id:{} , month: {}, KPI {}\".format(row[\"count(kpi)\"], unified_count, device, id , m, row[\"kpi\"]))\n\ngraularity_list=[\"monthly\",\"weekly\"]\nfor graularity in graularity_list:\n    date_list=get_path_date_list(graularity)\n    print graularity\n    check_ag_data_count(IOS_COUNTRY_ID_CODES, get_device_list('ios'),graularity,date_list )\n    check_ag_data_count(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),graularity, date_list)\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191211-090722_1970897908","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom aaintdatapipeline.application.app_qa.db_check_v1.constants import pg_settings\nfrom aaintdatapipeline.core.urn import Urn\nfrom aaintdatapipeline.application.app_qa.conf import settings\nfrom aaintdatapipeline.core.loader.plproxy import build_db_settings\nfrom aaplproxy.connection import ClusterConnection\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport psycopg2\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\n# IOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES'}\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return ['2001','2002']\n    else:\n        return ['1001','1002']\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db ))\n        current += relativedelta(months=1)    \n    return result\n    \n    \ndef pg_settings(urn, sql, granularity, schema):\n    result = {}\n    db_settings = build_db_settings(urn, schema)\n    connection = ClusterConnection(db_settings)\n    master_runner = connection.master_runner\n    rows, _ = master_runner.select(sql)\n    result[granularity] = rows\n    return result\n\n\nurn = Urn(\n    namespace='app-qa.db-validation.v1',\n    owner='app_qa'\n)\n\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={device}/store_id={sid}/\"\n\n\nimport traceback\ndef check_ag_app_dump_data(store_id_list, device_id_list, _granularity):\n    t = unittest.TestCase('run')\n    for id in store_id_list:\n        for device in device_id_list:\n            for m in get_month_list():\n                # print id, device, m[0] \n                try:\n                    weekly_basic = \"\"\"\n                        SELECT app_id_count FROM plproxy.execute_select_nestloop($$select count(app_id) from ag.app_{granularity}_{device_id}_{store_id}_{month} $$) t (app_id_count BIGINT)\n                    \"\"\".format(device_id=device,store_id=id, month=m[1], granularity=_granularity)\n                    granularity_ = _granularity.capitalize()\n                    result = pg_settings(urn, weekly_basic, granularity_,'usage')\n                    l = [int(x[0]) for x in result[granularity_]]\n                    if sum(l)==0:\n                        pass\n                        # print 'the table is empty  - device:{}  store_id:{} month:{}'.format(device, id, m[0])\n                    else:\n                        dump_data_path=path.format(device=device, raw_month=m[0], sid=id, raw_granularity=_granularity)\n                        dump_count=spark.read.parquet(dump_data_path).count()\n                        t.assertEqual(dump_count, sum(l), \"{}:{} ~ raw: {} ~ dumped data: {} \".format(granularity_, m[0], dump_count, sum(l)))\n                except psycopg2.ProgrammingError : \n                    pass\n                    # traceback.print_exc()\n                    # print(e)\n                    #  print 'the table is not existed - granularity {}, device:{}  store_id:{} month:{}'.format(granularity_, device, id, m[0])\n                \n\ncheck_ag_app_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'monthly')\ncheck_ag_app_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'monthly')\ncheck_ag_app_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'weekly')\ncheck_ag_app_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'weekly')\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191211-101113_697239301","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom aaintdatapipeline.application.app_qa.db_check_v1.constants import pg_settings\nfrom aaintdatapipeline.core.urn import Urn\nfrom aaintdatapipeline.application.app_qa.conf import settings\nfrom aaintdatapipeline.core.loader.plproxy import build_db_settings\nfrom aaplproxy.connection import ClusterConnection\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport psycopg2\n\n\nkpi_mapping={1:\"est_average_active_users\", 2: \"est_average_session_per_user\", 3:\"est_average_session_duration\", 4:\"est_install_penetration\", 5:\"est_average_active_days\", 6:\"est_percentage_active_days\", 7:\"est_average_bytes_per_user\" , 8:\"est_average_time_per_user\", 9:\"est_usage_penetration\", 10:\"est_open_rate\", 12:\"est_share_of_category_time\", 15:\"est_share_of_category_session\", 17:\"est_average_bytes_per_session\", 18:\"est_share_of_category_bytes\", 20:\"est_percent_of_wifi_total\", 21:\"est_mb_per_second\", 22:\"est_panel_size\", 23:\"est_installs\", 24:\"est_average_active_users_country_share\", 25:\"est_installs_country_share\"}\n\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return {\"2001\":\"ios-phone\",\"2002\":\"ios-tablet\"}\n    else:\n        return {\"1001\":\"android-phone\",\"1002\":\"android-tablet\"}\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db,current ))\n        current += relativedelta(months=1)    \n    return result\n    \n    \ndef pg_settings(urn, sql, granularity, schema):\n    result = {}\n    db_settings = build_db_settings(urn, schema)\n    connection = ClusterConnection(db_settings)\n    master_runner = connection.master_runner\n    rows, _ = master_runner.select(sql)\n    result[granularity] = rows\n    return result\n\n\nurn = Urn(\n    namespace='app-qa.db-validation.v1',\n    owner='app_qa'\n)\n\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v2/fact/granularity={raw_granularity}/date={raw_date}/\"\n\n\ndef last_day_of_month(any_day):\n    next_month = any_day.replace(day=28) + datetime.timedelta(days=4)  # this will never fail\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_every_date():\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    delta = today - current\n    return [current + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n\n\nimport traceback\ndef check_au_app_transform_data(store_id_list, device_id_list, _granularity):\n    t = unittest.TestCase('run')\n    for id in store_id_list:\n        for device in device_id_list:\n            for m in get_month_list():\n                # print id, device, m[0] \n                try:\n                    app_count=\"\"\"SELECT app_id\n                                FROM plproxy.execute_select_nestloop($$\n                            \tselect count(distinct app_id) from mu.app_{granularity}_{device_id}_{store_id}_{month}$$) t (app_id BIGINT)\"\"\".format(device_id=device,store_id=id, month=m[1], granularity=_granularity)\n                    # print one_app_sql\n                    granularity_ = _granularity.capitalize()\n                    app_result = pg_settings(urn, app_count, granularity_,'usage')\n                    # print app_result\n                    \n                    \n                    l = [int(x[0]) for x in app_result[granularity_]]\n                    if sum(l)==0:\n                        pass\n                    else:\n                        transform_data_path=path.format(raw_granularity=_granularity, raw_date=last_day_of_month(m[2]))\n                        transform_count=spark.read.parquet(transform_data_path).filter(\"device_code='{}' and country_code='{}'\".format(device_id_list[device],store_id_list[id] )).count()\n                        print id, device, m[0] ,sum(l) , transform_count\n                        t.assertEqual(sum(l),transform_count , \"raw : transform data is not equal: {} ~ {}, store id :{} , device: {}, month: {} \".format(sum(l), transform_count , id, device, m[0] ))\n                except psycopg2.ProgrammingError : \n                    pass\n                # except Exception as e: \n                #     traceback.print_exc()\n                #     print(e)\n                    # traceback.print_exc()\n                    # print(e)\n                    #  print 'the table is not existed - granularity {}, device:{}  store_id:{} month:{}'.format(granularity_, device, id, m[0])\n                \ncheck_au_app_transform_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'daily')\ncheck_au_app_transform_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'daily')\ncheck_au_app_transform_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'weekly')\ncheck_au_app_transform_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'weekly')\n# check_au_app_transform_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'monthly')\n# check_au_app_transform_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'monthly')\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191218-022407_422272964","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF  > /tmp/count_mu.txt\n\nSELECT app_id, date\nFROM plproxy.execute_select_nestloop(\\$proxy\\$\n\tselect date, count(distinct app_id) from mu.app_monthly_1001_1000_201305\\$proxy\\$) t (app_id BIGINT, date DATE) group by date,app_id;\n\n\n\n\n-- SELECT device_id,store_id,date,app_id,kpi,estimate,category_id,rank\n-- FROM plproxy.execute_select_nestloop (\\$proxy\\$\n-- select device_id,store_id,date,app_id,kpi,estimate, category_id,rank from mu.category_daily_2001_143445_201802\n-- \\$proxy\\$) t (device_id SMALLINT,store_id INT,,app_id BIGINT,kpi SMALLINT, estimate FLOAT, category_id INT, rank INT) where app_id=100 and date='2018-02-28' and device_id='2001' limit 20;\n\n\n-- SELECT device_id,store_id,date,app_id,kpi,estimate,seg_app_id\n--  FROM plproxy.execute_select_nestloop(\\$proxy\\$select device_id,store_id,date,app_id,kpi,estimate,seg_app_id from au.app_monthly_1001_1_201802\n-- \\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT, seg_app_id BIGINT) where app_id='20600003808593' and seg_app_id='20600000011090' order by kpi asc limit 30;\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20191218-023311_1276962845","metadata":{},"outputs":[],"source":["%python\nfrom itertools import islice\n\ncount=0\nwith open('/tmp/count_mu.txt') as f:\n    for line in islice(f, 2, 10):\n        count=count+int(line)\nprint count\n"]},{"cell_type":"code","execution_count":0,"id":"20191218-024441_1691432011","metadata":{},"outputs":[],"source":["\n# raw_path_1=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity=monthly/date=2013-02/\"\n# spark.read.parquet(raw_path_1).printSchema()\n\n# print spark.read.parquet(raw_path_1).filter(\"country_code='WW' and device_code='android-phone'\").count()\n# granularity='monthly'\n# GRANULARIY_LIST=['monthly']\n# for granularity in GRANULARIY_LIST:\n#     df_date = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity={}/\".format(granularity)).select('date').dropDuplicates()\n#     collect_date= df_date.collect()\n#     print collect_date\n#     date_list = [(x[0], x[0][:7]) for x in collect_date]\n#     print date_list\n       \n    \n"]},{"cell_type":"code","execution_count":0,"id":"20191218-024453_1764404487","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ad_app.v1/fact/granularity=monthly/month=2013-01/device_id=1001/store_id=1/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.audience-demographics.v1/fact/granularity=monthly/date=2013-01-31/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.segmented-by-age-gender.v1/fact/granularity=weekly/date=2018-12-31\n# s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity=weekly/month=2016-07/device_id=2001/store_id=0\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v1/fact/granularity=monthly/month=2018-12/device_id=2001/\n"]},{"cell_type":"code","execution_count":0,"id":"20191218-024712_1348636900","metadata":{},"outputs":[],"source":["%python\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef last_day_of_month(any_day):\n    next_month = any_day.replace(day=28) + datetime.timedelta(days=4)  # this will never fail\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2018, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db,current ))\n        current += relativedelta(months=1)    \n        \n    return result\n    \n\ndef get_every_date():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2018, 1, 1)    \n    delta = today - current\n    return [current + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n         \n        \nprint get_every_date()\nprint get_month_list()\n"]},{"cell_type":"code","execution_count":0,"id":"20191218-091854_1841375988","metadata":{},"outputs":[],"source":["%python\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}