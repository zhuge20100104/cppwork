{"cells":[{"cell_type":"code","execution_count":0,"id":"20200526-035827_1402013703","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.10.254', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from store.usage_basic_kpi_fact_v1_p_{} where date='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2017, 9, 20)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(m, day))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n\n\ngraularity_list = [\"daily\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200526-035904_537716853","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\nselect device_id,store_id,date,app_id,kpi,estimate from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select device_id,store_id,date,app_id,kpi,estimate\n    from mu.app_daily\n    where \n        date = '2019-11-01'\n\\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT) limit 50;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200526-082742_2095148925","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2019-11-30/')\n# df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-10-31/\")\nprint df.filter('AU is not null').count()"]},{"cell_type":"code","execution_count":0,"id":"20200603-141104_811471423","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\nselect count(app_id) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select app_id\n    from mu.app_daily\n    where \n        date = '2019-11-30' and kpi=1\n\\$proxy\\$) t (app_id BIGINT);\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200604-021830_1250366005","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2019-10/\")\nprint df.filter(\"date='2019-10-31'\").count()\nprint df.filter(\"date='2019-10-31' and kpi=1\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200603-141403_551857747","metadata":{},"outputs":[],"source":["\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2015-05-23/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and (est_install_penetration!=0 or est_installs!=0 or est_open_rate!=0 or est_installs_country_share!=0)\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200604-020803_1321226745","metadata":{},"outputs":[],"source":["\nv1_df_weekly = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2015-06-13/')\nprint 'weekly', v1_df_weekly.filter(\"device_code like 'and%' and est_install_penetration!=0 and est_installs!=0 and est_open_rate!=0 and est_installs_country_share!=0\").select('country_code').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200604-023747_1073310333","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2019-12-01/')\n# df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-10-31/\")\nprint df.filter('AU is not null').count()"]},{"cell_type":"code","execution_count":0,"id":"20200604-033718_327545028","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type=DAY/\n"]},{"cell_type":"code","execution_count":0,"id":"20200604-040002_634942208","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/"]},{"cell_type":"code","execution_count":0,"id":"20200604-040042_17361683","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2019-01-29/')\ndf.select('app_id', 'device_code', 'country_code', 'est_average_active_users', 'est_share_of_users').filter(\"app_id=326251330 and device_code='ios-all' and (country_code='CN' or country_code='WW')\").distinct().show()\ndf2 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-01-29/')\ndf2.select('app_id', 'device_code', 'country_code', 'est_average_active_users', 'est_average_active_users_country_share').filter(\"app_id=326251330 and device_code='ios-phone' and (country_code='CN' or country_code='WW')\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200604-055408_598905826","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2019-01-29/')\ndf.select('app_id', 'device_code', 'country_code', 'est_average_active_users', 'est_share_of_users').filter(\"device_code='android-phone' and app_id=20600001429645 and country_code in ('CN', 'WW')\").distinct().show()"]},{"cell_type":"code","execution_count":0,"id":"20200616-100234_2111943092","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200616-100407_2132058753","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from store.usage_basic_kpi_fact_v6_p_{} where date='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(m, day))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                      'granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n            test_result.append((graularity, day, v3_count, db_count))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'v3_count', 'db_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngraularity_list = [\"daily\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200617-012640_109163625","metadata":{},"outputs":[],"source":["\ndf =spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/weekly/').orderBy('date').collect()\nfor row in df:\n    print row['date'],'\\t',int(row['v3_count']),'\\t',row['db_count']"]},{"cell_type":"code","execution_count":0,"id":"20200618-014231_1277555700","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200619-013747_744304753","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from usage.usage_basic_kpi_fact_v6_p_{} where date='{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(m, day, graularity))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                      'granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n            test_result.append((graularity, day, v3_count, db_count))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'v3_count', 'db_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/daily/\",\n            mode=\"overwrite\",\n            partitionBy=[\"type\"])\n\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngraularity_list = [\"daily\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200619-052130_582812987","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=usage;\nselect * from usage_basic_kpi_fact_v6 where granularity='monthly' and date='2020-05-31' and app_id=100 and country_code='US';\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200619-044745_1902293666","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from usage.usage_basic_kpi_fact_v6_p_{} where date='{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 23)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(m, day, graularity))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                      'granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n            test_result.append((graularity, day, v3_count, db_count))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'v3_count', 'db_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/weekly/\",\n            mode=\"overwrite\",\n            partitionBy=[\"type\"])\n\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngraularity_list = [\"weekly\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200619-065247_1252141173","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from usage.usage_basic_kpi_fact_v6_p_{} where date='{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 23)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(m, day, graularity))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                      'granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n            test_result.append((graularity, day, v3_count, db_count))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'v3_count', 'db_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/monthly/\",\n            mode=\"overwrite\",\n            partitionBy=[\"type\"])\n\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngraularity_list = [\"monthly\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200619-071506_487120199","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from usage.usage_basic_kpi_fact_v6_p_{} where date='{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2016, 06, 30)\n    start = datetime.date(2016, 06, 30)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2016, 06, 25)\n    start = datetime.date(2016, 06, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(m, day, graularity))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                      'granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n            test_result.append((graularity, day, v3_count, db_count))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'v3_count', 'db_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/monthly/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngraularity_list = [\"monthly\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200622-012316_1159464975","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from usage.usage_basic_kpi_fact_v6 where date='{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2016, 06, 30)\n    start = datetime.date(2016, 06, 30)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2016, 06, 25)\n    start = datetime.date(2016, 06, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 14)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(day, graularity))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/' \\\n                      'granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n            test_result.append((graularity, day, v3_count, db_count))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'v3_count', 'db_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngraularity_list = [\"daily\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200629-093407_1340834836","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from usage.usage_basic_kpi_fact_v6 where date='{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2016, 06, 30)\n    start = datetime.date(2016, 06, 30)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 20)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 14)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(day, graularity))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/' \\\n                      'granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n            test_result.append((graularity, day, v3_count, db_count))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'v3_count', 'db_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/weekly/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngraularity_list = [\"weekly\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200629-094014_191774357","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from usage.usage_basic_kpi_fact_v6 where date='{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2020, 05, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 20)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 14)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(day, graularity))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/' \\\n                      'granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print v3_count, db_count\n                print \"Completeness Test Pass! date : {}\".format(day)\n            test_result.append((graularity, day, v3_count, db_count))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'v3_count', 'db_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/monthly/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngraularity_list = [\"monthly\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200629-094118_77612669","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=usage;\nselect count(1) from usage_basic_kpi_fact_v6 where granularity='weekly' and date='2020-06-20';\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200702-122928_234200493","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF\nselect count(uniqlo_id) from plproxy.execute_select_nestloop(\\$proxy\\$ \n                    select max(app_id) as uniqlo_id\n                from mu.app_monthly\n                where \n                date='2020-05-31'\n                group by\n                app_id,\n                store_id,\n                device_id\n            \\$proxy\\$) t (uniqlo_id BIGINT);\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200702-123126_1628214707","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type=WEEK/date=2020-06-20')\nprint df.distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200703-015112_726112058","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom applications.db_check_v1.common.db_check_utils import query_df\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select * from usage.usage_basic_kpi_fact_v6 where date='{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2020, 05, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 20)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 14)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query_df(aa_dsn, sql.format(day, graularity))\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/' \\\n                      'granularity={}/date={}/'\n            unified_v3 = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).drop('_identifier')\n            unified_v3 = unified_v3.drop('date').drop('granularity')\n            unified_v3 = unified_v3.na.fill(0)\n            \n            citus_db_df = spark.createDataFrame(result)\n            citus_db_df = citus_db_df.drop('date').drop('granularity').drop('_disable_idx_4_query')\n\n            diff_count1 = citus_db_df.select(unified_v3.columns).subtract(unified_v3).count()\n            diff_count2 = unified_v3.select(citus_db_df.columns).subtract(citus_db_df).count()\n            print diff_count1, diff_count2\n\n\ngraularity_list = [\"daily\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200703-095430_317500771","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom applications.db_check_v1.common.db_check_utils import query_df\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select * from usage.usage_basic_kpi_fact_v6 where date='{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2020, 05, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 20)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 06, 14)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query_df(aa_dsn, sql.format(day, graularity))\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/' \\\n                      'granularity={}/date={}/'\n            unified_v3 = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).drop('_identifier')\n            unified_v3 = unified_v3.drop('date').drop('granularity')\n            unified_v3 = unified_v3.na.fill(0)\n            \n            citus_db_df = spark.createDataFrame(result)\n            citus_db_df = citus_db_df.drop('date').drop('granularity').drop('_disable_idx_4_query')\n\n            diff_count1 = citus_db_df.select(unified_v3.columns).subtract(unified_v3).count()\n            diff_count2 = unified_v3.select(citus_db_df.columns).subtract(citus_db_df).count()\n            print diff_count1, diff_count2\n\n\ngraularity_list = [\"weekly\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200703-082936_227686832","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200703-083104_1747770879","metadata":{},"outputs":[],"source":["\nimport random\nimport datetime\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config, etl_skip\nfrom applications.db_check_v1.common.constants import query\nfrom applications.db_check_v1.common.db_check_utils import query_df\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.utils import string_to_datetime, datetime_to_string\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql import functions\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import LongType\nCITUS_USAGE_HOSTS = [('10.2.10.254', 5432)]\nCITUS_USAGE_NAME = 'aa_store_db'\nCITUS_USAGE_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nCITUS_USAGE_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\nDEVICE_CODE_MAPPING = {\n    1: {'1': 'android-phone', '2': 'android-tablet'},\n    2: {'1': 'ios-phone', '2': 'ios-tablet'}}\n\nGRANULARITY_IN_RAW_PATH_MAPPING = {\n    \"daily\": \"DAY\",\n    \"weekly\": \"WEEK\",\n    \"monthly\": \"MONTH\"\n}\n\nCITUS_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=CITUS_USAGE_NAME,\n        user=CITUS_USAGE_ACCESS_ID,\n        host=CITUS_USAGE_HOSTS[0][0],\n        password=CITUS_USAGE_SECRET_KEY,\n        port=CITUS_USAGE_HOSTS[0][1]\n    )\n)\n\n\nclass UsageRoutineRawData(object):\n    \"\"\"\n    Get data from Data Foundation\n    \"\"\"\n    _raw_s3_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={granularity}/date={date}'\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, granularity, date):\n        \"\"\"\n        :return: Raw data from DF team\n        :rtype: pyspark.sql.DataFrame\n        \"\"\"\n        raw_data = self.spark.read.parquet(self._raw_s3_path.format(\n            granularity=GRANULARITY_IN_RAW_PATH_MAPPING[granularity], date=date))\n        return raw_data\n\n\nclass TestUsageRoutineRawCompleteness(PipelineTest):\n    table_name = 'usage.usage_basic_kpi_fact_v6'\n    db_name = 'usage'\n\n    def setUp(self):\n        # super(PipelineTest, self).setUp()\n        self.check_date = None\n        self.granularity = None\n\n    def check_routine_raw_completeness(self):\n        date_list = [self.check_date]\n\n        if self.granularity == 'daily':\n            date = string_to_datetime(self.check_date)\n            weekly_day_nums = 7\n            date_list = [datetime_to_string(date - datetime.timedelta(days=x)) for x in range(weekly_day_nums)]\n\n        for date in date_list:\n            routine_df = UsageRoutineRawData(self.spark).get(self.granularity, date)\n            routine_count = routine_df.count()\n\n            citus_db_count = self.get_citus_db_count(date)\n            print routine_count, citus_db_count\n\n            self.assertEqual(routine_count, citus_db_count[0][0],\n                             msg=\"fount count mismatch when compare usage routine raw and citus db. \"\n                                 \"granularity is {}, date is {}, raw count is:{}, citus db count is:{}\".format(\n                                    self.granularity, date, routine_count, citus_db_count[0][0]))\n\n    def get_citus_db_count(self, date):\n        sql = \"\"\"select count(1) as cnt from {table_name} where date='{date}' and granularity='{granularity}';\n        \"\"\".format(table_name=self.table_name, date=date, granularity=self.granularity)\n        result = query(CITUS_DSN, sql)\n        return result\n\n\nclass TestUsageRoutineRawCompletenessDaily(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"0 12 * * 5\", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"daily\"\n\n    def test_routine_raw_completeness_daily(self):\n        self.check_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawCompletenessWeekly(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"0 12 * * 5\", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"weekly\"\n\n    @etl_skip()\n    def test_routine_raw_completeness_weekly(self):\n        self.check_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawCompletenessMonthly(TestUsageRoutineRawCompleteness):\n\n    trigger_date_config = (\"0 12 6 * * \", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"monthly\"\n\n    @etl_skip()\n    def test_routine_raw_completeness_monthly(self):\n        self.check_routine_raw_completeness()\n\n\nclass TestUsageRoutineRawAccuracy(PipelineTest):\n    table_name = 'usage.usage_basic_kpi_fact_v6'\n    raw_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={granularity}/date={date}'\n    db_name = 'usage'\n\n    def setUp(self):\n        super(PipelineTest, self).setUp()\n        self.check_date = None\n        self.granularity = None\n\n    def check_routine_raw_accuracy(self):\n        routine_df = UsageRoutineRawData(self.spark).get(self.granularity, self.check_date)\n        unified_v1 = (\n            routine_df\n            .withColumn('device_code', functions.UserDefinedFunction(\n                lambda x, y: DEVICE_CODE_MAPPING[x][y])(routine_df['platform'], routine_df['device_type']))\n            .withColumnRenamed('country', 'country_code')\n            .withColumn('app_id', routine_df['app_id'].cast(LongType()))\n            .withColumnRenamed('AU', 'est_average_active_users')\n            .withColumnRenamed('AFU', 'est_average_session_per_user')\n            .withColumnRenamed('ADU', 'est_average_session_duration')\n            .withColumnRenamed('IP', 'est_install_penetration')\n            .withColumnRenamed('AAD', 'est_average_active_days')\n            .withColumnRenamed('PAD', 'est_percentage_active_days')\n            .withColumnRenamed('MBPU', 'est_average_bytes_per_user')\n            .withColumnRenamed('ATU', 'est_average_time_per_user')\n            .withColumnRenamed('UP', 'est_usage_penetration')\n            .withColumnRenamed('OR', 'est_open_rate')\n            .withColumnRenamed('MBPS', 'est_average_bytes_per_session')\n            .withColumnRenamed('MBWFT', 'est_percent_of_wifi_total')\n            .withColumnRenamed('MBS', 'est_mb_per_second')\n            .withColumnRenamed('IS', 'est_installs')\n            .withColumnRenamed('SOU', 'est_average_active_users_country_share')\n            .withColumnRenamed('SOI', 'est_installs_country_share')\n            .withColumn('est_share_of_category_time', lit(None).cast(DoubleType()))\n            .withColumn('est_share_of_category_session', lit(None).cast(DoubleType()))\n            .withColumn('est_share_of_category_bytes', lit(None).cast(DoubleType()))\n            .withColumn('est_panel_size', lit(None).cast(DoubleType()))\n            .drop('device_type')\n            .drop('platform')\n        )\n\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] *\n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population',\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v3 = self.spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v3 = unified_v3.na.fill(0)\n\n        result = self.get_citus_db_df()\n        citus_db_df = self.spark.createDataFrame(result)\n        citus_db_df = citus_db_df.drop('date').drop('granularity').drop('_disable_idx_4_query')\n\n        diff_count1 = citus_db_df.select(unified_v3.columns).subtract(unified_v3).count()\n        diff_count2 = unified_v3.select(citus_db_df.columns).subtract(citus_db_df).count()\n        self.assertTrue(diff_count1 == 0 and diff_count2 == 0,\n                        msg=\"fount mismatch when compare usage routine raw and citus db.granularity is {}, \"\n                            \"date is {}, diff_count1 is:{}, diff_count2 is:{}\".format(\n                                self.granularity, self.check_date, diff_count1, diff_count2))\n\n    def get_citus_db_df(self):\n        sql = \"\"\"select * from {table_name} where date='{date}' and granularity='{granularity}';\n        \"\"\".format(table_name=self.table_name, date=self.check_date, granularity=self.granularity)\n        result = query_df(CITUS_DSN, sql)\n        return result\n\n\nclass TestUsageRoutineRawAccuracyDaily(TestUsageRoutineRawAccuracy):\n\n    trigger_date_config = (\"0 12 * * 5\", random.randint(6, 12))\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"daily\"\n\n    @etl_skip()\n    def test_routine_raw_accuracy_daily(self):\n        self.check_routine_raw_accuracy()\n\n\nclass TestUsageRoutineRawAccuracyWeekly(TestUsageRoutineRawAccuracy):\n    trigger_date_config = (\"0 12 * * 5\", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"weekly\"\n\n    @etl_skip()\n    def test_routine_raw_accuracy_weekly(self):\n        self.check_routine_raw_accuracy()\n\n\nclass TestUsageRoutineRawAccuracyMonthly(TestUsageRoutineRawAccuracy):\n    trigger_date_config = (\"0 12 6 * * \", 6)\n    check_date = _get_date_from_refresh_routing_config(trigger_date_config)\n\n    def setUp(self):\n        self.granularity = \"monthly\"\n\n    @etl_skip()\n    def test_routine_raw_accuracy_monthly(self):\n        self.check_routine_raw_accuracy()\n\n\nsuite = unittest.TestSuite()\nsuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessDaily))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessWeekly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawCompletenessMonthly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyDaily))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyWeekly))\n# suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestUsageRoutineRawAccuracyMonthly))\nrunner = unittest.TextTestRunner()\nrunner.run(suite)"]},{"cell_type":"code","execution_count":0,"id":"20200707-054625_1578247898","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}