{"cells":[{"cell_type":"code","execution_count":0,"id":"20200526-053827_690484597","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2016, 12, 01)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\nPG_AA_HOSTS = [('10.2.10.254', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select * from store.usage_basic_kpi_fact_v1_p_201701 where date='2017-01-01' limit 50;\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql)\n            print result\n            break\n    \n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity={}/date={}/'\n            v3_count = spark.read.parquet(v3_path.format(graularity, day)).count()\n    \n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n        break\n\n\ngraularity_list = [\"daily\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200526-054159_379318075","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2016, 12, 01)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\nPG_AA_HOSTS = [('10.2.10.254', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select COLUMN_NAME from information_schema.COLUMNS where table_name = 'usage_basic_kpi_fact_v1_p_{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_v3_db_accuracy(date_list, granularity):\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(m))\n            db_columns = [x[0] for x in result]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity={}/date={}/'\n            v3_df = spark.read.parquet(v3_path.format(granularity, day))\n            v3_df = (\n                v3_df\n                .withColumnRenamed('est_share_of_category_bytes', 'est_share_of_category_mb')\n                .withColumnRenamed('est_average_bytes_per_user', 'est_average_mb_per_user')\n                .withColumnRenamed('est_average_bytes_per_session', 'est_average_mb_per_session')\n                .drop('_identifier')\n            )\n            v3_columns = v3_df.columns\n\n            if len(set(v3_columns).difference(set(db_columns))) != 0:\n                print 'Accuracy Test Fail!!! date: {}'.format(day)\n            else:\n                print 'Accuracy Test Pass! date: {}'.format(day)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_v3_db_accuracy(get_path_date_list(granularity), granularity)"]},{"cell_type":"code","execution_count":0,"id":"20200526-061941_836986971","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200526-064748_1597455762","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 01)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select COLUMN_NAME from information_schema.COLUMNS where table_name = 'usage_basic_kpi_fact_v6_p_{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_v3_db_accuracy(date_list, granularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(m))\n            db_columns = [x[0] for x in result]\n            db_columns.pop(0)\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                      'granularity={}/date={}/'\n            v3_df = spark.read.parquet(v3_path.format(granularity, day))\n            v3_df = (\n                v3_df\n                .drop('_identifier')\n            )\n            v3_columns = v3_df.columns\n            if len(set(v3_columns).difference(set(db_columns))) != 0:\n                print 'Accuracy Test Fail!!! date: {}'.format(day)\n            else:\n                print 'Accuracy Test Pass! date: {}'.format(day)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_v3_db_accuracy(get_path_date_list(granularity), granularity)\n"]},{"cell_type":"code","execution_count":0,"id":"20200630-094610_1970189669","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"aa-int-qa-db-check\")\n\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200630-094332_254683614","metadata":{},"outputs":[],"source":["\nfrom pandas.io import sql as sqlio\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.types import LongType\nfrom pyspark.sql import functions\nfrom pyspark.sql.functions import lit, coalesce\nimport psycopg2\n\n\ndef query_df(dsn, sql):\n    conn = psycopg2.connect(dsn)\n    df = sqlio.read_sql_query(sql, conn)\n    return df\n    \ndevice_code_mapping = {\n            1: {'1': 'android-phone', '2': 'android-tablet'},\n            2: {'1': 'ios-phone', '2': 'ios-tablet'}}\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\nsql = \"\"\"select * from usage.usage_basic_kpi_fact_v6 where date='2020-06-14' and granularity='daily';\"\"\"\nresult = query_df(aa_dsn, sql)\ncitus_db_df = spark.createDataFrame(result)\ncitus_db_df = citus_db_df.drop('date').drop('granularity').drop('_disable_idx_4_query')\n\nroutine_df = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type=DAY/date=2020-06-14')\nunified_v1 = (\n            routine_df\n                .withColumn('device_code', functions.UserDefinedFunction(\n                lambda x, y: device_code_mapping[x][y])(routine_df['platform'], routine_df['device_type']))\n                .withColumnRenamed('country', 'country_code')\n                .withColumn('app_id', routine_df['app_id'].cast(LongType()))\n                .withColumnRenamed('AU', 'est_average_active_users')\n                .withColumnRenamed('AFU', 'est_average_session_per_user')\n                .withColumnRenamed('ADU', 'est_average_session_duration')\n                .withColumnRenamed('IP', 'est_install_penetration')\n                .withColumnRenamed('AAD', 'est_average_active_days')\n                .withColumnRenamed('PAD', 'est_percentage_active_days')\n                .withColumnRenamed('MBPU', 'est_average_bytes_per_user')\n                .withColumnRenamed('ATU', 'est_average_time_per_user')\n                .withColumnRenamed('UP', 'est_usage_penetration')\n                .withColumnRenamed('OR', 'est_open_rate')\n                .withColumnRenamed('MBPS', 'est_average_bytes_per_session')\n                .withColumnRenamed('MBWFT', 'est_percent_of_wifi_total')\n                .withColumnRenamed('MBS', 'est_mb_per_second')\n                .withColumnRenamed('IS', 'est_installs')\n                .withColumnRenamed('SOU', 'est_average_active_users_country_share')\n                .withColumnRenamed('SOI', 'est_installs_country_share')\n                .withColumn('est_share_of_category_time', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_category_session', lit(None).cast(DoubleType()))\n                .withColumn('est_share_of_category_bytes', lit(None).cast(DoubleType()))\n                .withColumn('est_panel_size', lit(None).cast(DoubleType()))\n                .drop('device_type')\n                .drop('platform')\n        )\n\nunified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] *\n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\nunified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population',\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\nunified_v1.createOrReplaceTempView(\"v1_df\")\nunified_v3 = spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\").cache()\nunified_v3 = unified_v3.na.fill(0)\nunified_v3.filter(\"app_id=1340929016 and country_code='US' and device_code='ios-phone'\").show()\n\ndiff_count1 = citus_db_df.select(unified_v3.columns).subtract(unified_v3).count()\nprint diff_count1\ndiff_count2 = unified_v3.select(citus_db_df.columns).subtract(citus_db_df).count()\nprint diff_count2\n# unified_v3.select(citus_db_df.columns).subtract(citus_db_df).show()"]},{"cell_type":"code","execution_count":0,"id":"20200630-094338_1193160464","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=usage;\nselect * from usage_basic_kpi_fact_v6 where granularity='daily' and date='2020-06-14' and app_id=20600000000160 and country_code='Android-phone';\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200630-105718_1151888784","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}