{"cells":[{"cell_type":"code","execution_count":0,"id":"20200420-092842_2126856558","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/date=2010-07-04/device_code=ios-all/\n \n \n"]},{"cell_type":"code","execution_count":0,"id":"20200415-132436_1293480011","metadata":{},"outputs":[],"source":["\nimport datetime\nstart = \"2010-07-04\"\nend = \"2010-08-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\nfor day in dates:\n    next_day = day+datetime.timedelta(1)\n    pre_day = day+datetime.timedelta(-1)\n\n    # unified_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(pre_day)).cache()\n    unified_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(day)).cache()\n    unified_2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(next_day)).cache()\n\n\n\n\n    cdf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}\".format(day)).cache()\n    cdf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}\".format(next_day)).cache()\n\n    cdf1.createOrReplaceTempView(\"cur\")\n    cdf2.createOrReplaceTempView(\"next\")\n    unified_1.createOrReplaceTempView(\"unified_cur\")\n    unified_2.createOrReplaceTempView(\"unified_next\")\n\n    print \"compare app id: \" , day\n    spark.sql(\"select app_id from cur except all select app_id from  next \").show()\n    spark.sql(\"select app_id from (select app_id from cur except all select app_id from next ) as cum1 except all select app_id from ( select app_id from unified_cur except all select app_id from unified_next ) as cum2   \").show()\n\n\n    # print \"compare sum data: \", day\n    # spark.sql(\"select * from unified_cur \").show()\n    # spark.sql(\"select app_id, country_code, sum(free_app_download) as total_free_app_download , sum(revenue) as total_revenue, sum(paid_app_download) as total_paid_app_download, device_code  from (  select country_code,app_id,revenue,free_app_download, paid_app_download, device_code from unified_cur  union select country_code,app_id,revenue, free_app_download, paid_app_download ,device_code from unified_next ) as union_data group by country_code,device_code,app_id   \").show()\n\n\n\n\n# cdf2 = "]},{"cell_type":"code","execution_count":0,"id":"20200415-132514_531513824","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2010-07-04\"\nend = \"2010-08-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\nprint dates\n\nunified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % (\",\".join([d.strftime(\"%Y-%m-%d\") for d in dates]))).select(\"app_id\", \"country_code\", \"free_app_download\" ,\"paid_app_download\",\"revenue\", \"device_code\")\n\ncdf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(dates[-1])).cache()\n\nunified_data.createOrReplaceTempView(\"unified_data\")\ncdf1.createOrReplaceTempView(\"cumu\")\n\nprint \"compare sum data: \", dates[-1]\n\nspark.sql(\"select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download  , sum(revenue) as revenue, device_code  from unified_data  group by country_code,device_code,app_id except all select app_id, country_code, free_app_download ,paid_app_download,revenue, device_code from cumu  \").show()\n\nspark.sql(\"select app_id, country_code, free_app_download ,paid_app_download,revenue, device_code from cumu except all select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download  , sum(revenue) as revenue, device_code  from unified_data  group by country_code,device_code,app_id \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-072130_438409966","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2010-07-04\"\nend = \"2010-07-14\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\nunified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={%s}\" % (\",\".join([d.strftime(\"%Y-%m-%d\") for d in dates]))).select(\"app_id\", \"country_code\", \"free_app_download\" ,\"paid_app_download\",\"revenue\", \"device_code\",\"category_id\")\ntest_est_category.createOrReplaceTempView(\"category_unified_data\")\nspark.sql(\"select * from category_unified_data where app_id=283519081 and category_id=100000 and country_code='AE' and device_code='ios-all'\").show()\n\n\n\n\n \ntest_est_category = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-cum.v1/fact/date=2010-07-10\").cache()\ntest_est_category.createOrReplaceTempView(\"category\")\nspark.sql(\"select * from category where app_id=283519081 and category_id=100000 and country_code='AE' and device_code='ios-all'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200415-155616_710779594","metadata":{},"outputs":[],"source":["\n\n\nspark.sql(\" select app_id, country_code, sum(free_app_download) as total_free_app_download , sum(paid_app_download) as toal_paid_app_download, sum(revenue) as total_revenue, device_code  from (  select country_code,app_id,revenue,free_app_download, paid_app_download, device_code from unified_pre_two  union all select country_code,app_id,revenue, free_app_download, paid_app_download ,device_code from unified_pre  union all select country_code,app_id,revenue, free_app_download, paid_app_download ,device_code from unified_cur ) as union_data group by country_code,device_code,app_id\").where(\"app_id='1441499274' and country_code='MT' and device_code='ios-phone'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-090058_1104905910","metadata":{},"outputs":[],"source":["\nstart = \"2019-07-01\"\nend = \"2019-07-05\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\napp_id=[\"1441499274\",\"960832727\",\"283519081\"]\nunified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % (\",\".join([d.strftime(\"%Y-%m-%d\") for d in dates]))).select(\"app_id\", \"country_code\", \"free_app_download\" ,\"paid_app_download\",\"revenue\", \"device_code\").filter(\"app_id in ( %s )\" % \",\".join(app_id)).cache()\n\nunified_data.createOrReplaceTempView(\"unified\")\nspark.sql(\"select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download  , sum(revenue) as revenue, device_code from unified group by country_code,device_code,app_id \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-061000_1007945002","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nimport datetime\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\nstart = \"2010-07-04\"\nend = \"2010-08-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\nest_list = [ \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(test_date) for test_date in dates ]\nrank_list = [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(test_date) for test_date in dates  ]\n# print est_list\n# print rank_list\n\n\n\n\n\n\ndef test_count(test_date):\n    # test_date = '2017-08-01'\n    namespace = \"aa.store.market-size.v1\"\n    \n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                # \"path\": [\n                #     \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                #         test_date)],\n                \"path\": est_list,\n    \n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                # \"path\": [\n                #     \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                #         test_date)],\n                \"path\": rank_list,\n    \n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"category_mapping_deminsion_service\",\n                \"path\": [\"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\"],\n            },\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"ios_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n    \n                \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n            },\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"android_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n    \n                \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n            }\n        ]\n    }\n    \n    sql_text = \"\"\"\n    \n    \n    -- rank_unified,store_unified\n    WITH unified_data_test AS \n    ( \n                    SELECT          store_unified.country_code, \n                                    store_unified.device_code, \n                                    store_unified.free_app_download  , \n                                    store_unified.paid_app_download , \n                                    store_unified.revenue           , \n                                    store_unified.revenue_iap       AS est_revenue_iap, \n                                    store_unified.revenue_non_iap   AS est_revenue_non_iap, \n                                    rank_unified.category_id, \n                                    rank_unified.app_id, \n                                    rank_unified.free_app_download AS rank_free_app_download, \n                                    rank_unified.paid_app_download AS rank_paid_app_download, \n                                    rank_unified.revenue AS rank_revenue, \n                                    rank_unified.revenue_iap, \n                                    rank_unified.revenue_non_iap, \n                                    rank_unified.granularity, \n                                    rank_unified.date \n                    FROM            rank_unified \n                    FULL OUTER JOIN store_unified \n                    ON              rank_unified.app_id = store_unified.app_id \n                    AND             rank_unified.country_code = store_unified.country_code \n                    AND             rank_unified.device_code = store_unified.device_code \n                    AND             rank_unified.date = store_unified.date );\n    \n    \n    \n    \"\"\"\n    \n\n\n    run(spark, ingest_msg, sql_text)\n \n    category_1 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-cum.v1/fact\").where(\"date='{}'\".format(test_date)).cache()\n\n\n    category_1.createOrReplaceTempView(\"cumu_category\")\n    spark.sql(\"select * from cumu_category limit 5\").show()\n\n    spark.sql(\"select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download  , sum(revenue) as revenue, device_code, category_id  from unified_data_test  group by country_code,device_code,app_id,category_id  except all select app_id, country_code, free_app_download ,paid_app_download,revenue, device_code,category_id from cumu_category   \").show()\n    \n    # spark.sql(\"select count(*) from (select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download  , sum(revenue) as revenue, device_code, category_id  from unified_data_test  group by country_code,device_code,app_id,category_id  except all select app_id, country_code, free_app_download ,paid_app_download,revenue, device_code,category_id from cumu_category) as prod   \").show()\n\n    spark.sql(\"select app_id, country_code, free_app_download ,paid_app_download,revenue, device_code,category_id from cumu_category except all select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download  , sum(revenue) as revenue, device_code, category_id  from unified_data_test  group by country_code,device_code,app_id,category_id \").show()\n\n    # spark.sql(\"select count(*) from ( select app_id, country_code, free_app_download ,paid_app_download,revenue, device_code,category_id from cumu_category except all select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download  , sum(revenue) as revenue, device_code, category_id  from unified_data_test  group by country_code,device_code,app_id,category_id ) as new \").show()\n\ntest_count(dates[-1])\n"]},{"cell_type":"code","execution_count":0,"id":"20200418-022717_1893756580","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql.types import StructType, StructField, StringType\nstart = \"2020-01-01\"\nend = \"2020-04-19\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\n\ntest_ios_app_list = [377194688 , 375875657, 366236510, 354990881,365691077 , 343200656 ,20600000009072,20600000000553]\ndevice_code=['ios-phone','ios-tablet','ios-all','android-all']\ncountry_list=['WW', 'US']\n# test_list = [(app, device, country ) for app in test_ios_app_list  for device in device_code for country in country_list ]\n\n\nsql_where = \"app_id in ({}) and device_code in ('{}') and country_code in ('{}')\".format(\",\".join(map(str,test_ios_app_list)),   \"','\".join(device_code), \"','\".join(country_list) )\nprint sql_where\n\n\n_unified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=20{10,11,12,13,14,15,16,17,18,19}-*\").where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\")\n\nresult=list()\nfor day in dates:\n    _unified_data_single_date = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % (day.strftime(\"%Y-%m-%d\"))).where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\").collect()\n    # print _unified_data\n    result.extend(_unified_data_single_date)\n\nschema = StructType([StructField(\"app_id\", StringType(), True), \n                     StructField(\"device_code\", StringType(), True),\n                     StructField(\"country_code\", StringType(), True),\n                     StructField(\"free_app_download\", IntegerType(), True),\n                     StructField(\"paid_app_download\", IntegerType(), True),\n                     StructField(\"revenue\", IntegerType(), True)])\n\n_unified_data.createOrReplaceTempView(\"unified_data_view\")\nspark.createDataFrame(result, schema=schema).createOrReplaceTempView(\"unified_data_singe_date_view\")\nspark.sql(\"select * from unified_data_view union select * from unified_data_singe_date_view\").createOrReplaceTempView(\"union_unified_data\")\nspark.sql(\"select app_id, sum(free_app_download) from union_unified_data group by app_id, device_code, country_code\").show(2)\n\n\n\ncdf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(dates[-1])).where(sql_where).cache()\ncdf1.createOrReplaceTempView(\"cumu\")\n\nprint \"compare sum data: \", dates[-1]\n\n# df1 = spark.sql(\"select app_id, sum(free_app_download) as free_app_download, device_code, country_code from unified_data_view  group by app_id , device_code, country_code EXCEPT ALL select app_id , device_code, country_code, free_app_download from cumu  \").show()\n# spark.sql(\"select app_id, free_app_download, device_code, country_code from cumu EXCEPT ALL select app_id , device_code, country_code , sum(free_app_download) as free_app_download  from unified_data_view  group by app_id , device_code, country_code   \").show()\n\nspark.sql(\"select app_id, sum(free_app_download) as free_app_download, sum(paid_app_download) as paid_app_download, sum(revenue) as revenue, device_code, country_code from union_unified_data  group by app_id , device_code, country_code order by app_id , device_code, country_code\").createOrReplaceTempView(\"df1\")\nspark.sql(\"select app_id, free_app_download, paid_app_download, revenue, device_code, country_code from cumu order by app_id , device_code, country_code\").createOrReplaceTempView(\"df2\")\nspark.sql(\"select * from df1 except all select * from df2\").show()\nspark.sql(\"select * from df2 except all select * from df1\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200422-054028_1475332988","metadata":{},"outputs":[],"source":["\nqa_df=spark.sql(\"select app_id, sum(free_app_download) as free_app_download, sum(paid_app_download) as paid_app_download, sum(revenue) as revenue, device_code, country_code from union_unified_data  group by app_id , device_code, country_code order by app_id , device_code, country_code\").cache()\nqa_df.createOrReplaceTempView(\"df1\")\ndev_df = spark.sql(\"select app_id, free_app_download, paid_app_download, revenue, device_code, country_code from cumu order by app_id , device_code, country_code\").cache()\ndev_df.createOrReplaceTempView(\"df2\")\n\nspark.sql(\"select * from df1 where app_id=365691077 and device_code='ios-phone' and  country_code='WW'\").show()\nspark.sql(\"select * from df2 where app_id=365691077 and device_code='ios-phone' and  country_code='WW'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200506-023820_1157550231","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/2010\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date=2010-07-31/"]},{"cell_type":"code","execution_count":0,"id":"20200506-023951_1917369068","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date=2010-07-04/device_code=ios-tablet/part-00000-bd81af73-bf68-47b1-a42a-df2df32c68df.c000.gz.parquet\").printSchema()\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2010-07-04/device_code=ios-tablet\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200418-081031_891866665","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2010-07-04\"\nend = \"2013-08-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\n\ntest_ios_app_list = [377194688 , 375875657,366236510, 354990881,365691077 , 343200656 ,20600000009072,20600000000553]\ndevice_code=['ios-phone','ios-tablet','ios-all','android-all']\ncountry_list=['WW', 'US']\ncategory_list=[100000, 100026,400025,400000]\n\n\n# test_list = [(app, device, country ) for app in test_ios_app_list  for device in device_code for country in country_list ]\ndate_string=[d.strftime(\"%Y-%m-%d\") for d in dates]\n# print date_string\n\nsql_where = \"date between '{}' and '{}' and app_id in ({}) and device_code in ('{}') and country_code in ('{}') and category_id in ({}) \".format(start, dates[-1], \",\".join(map(str,test_ios_app_list)),   \"','\".join(device_code), \"','\".join(country_list), \",\".join(map(str,category_list)))\nprint sql_where\n\nresult=list()\n# for day in dates:\n#     _unified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={%s}\" % (day.strftime(\"%Y-%m-%d\"))).where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\", \"category_id\").collect()\n#     # print _unified_data\n#     result.extend(_unified_data)\n\n_unified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/\").where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\",\"category_id\")\n\n\n# .format(\",\".join(date_string))).\n# _unified_data = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/\").where(\"granularity ='daily' and date between '{}' and '{}' and device_code in ('{}')\".format(start,dates[-1],\"','\".join(device_code))).where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\",\"category_id\")\n\n\n# _unified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=20{10,11,12,13}-*\").where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\",\"category_id\")\n\n\n_unified_data.createOrReplaceTempView(\"unified_data_view\")\n\n# spark.createDataFrame(result).createOrReplaceTempView(\"unified_data_view\")\n# spark.sql(\"select app_id, sum(free_app_download) from unified_data_view group by app_id, device_code, country_code,category_id\").show(2)\n\nsql_where_cum = \"app_id in ({}) and device_code in ('{}') and country_code in ('{}') and category_id in ({}) \".format( \",\".join(map(str,test_ios_app_list)),   \"','\".join(device_code), \"','\".join(country_list), \",\".join(map(str,category_list)))\n\ncdf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-cum.v1/fact/date={}/\".format(dates[-1])).where(sql_where_cum).cache()\ncdf1.createOrReplaceTempView(\"cumu\")\n\nprint \"compare sum data: \", dates[-1]\n\n# df1 = spark.sql(\"select app_id, sum(free_app_download) as free_app_download, device_code, country_code,category_id from unified_data_view group by app_id , device_code, country_code,category_id EXCEPT ALL select app_id , device_code, country_code, free_app_download , category_id from cumu  \").show()\n# spark.sql(\"select app_id, free_app_download, device_code, country_code,category_id from cumu EXCEPT ALL select app_id , device_code, country_code , sum(free_app_download) as free_app_download ,category_id from unified_data_view  group by app_id , device_code, country_code,category_id   \").show()\n\nspark.sql(\"select app_id, sum(free_app_download) as free_app_download, sum(paid_app_download) as paid_app_download, sum(revenue) as revenue, device_code, country_code ,category_id from unified_data_view  group by app_id , device_code, country_code,category_id order by app_id , device_code, country_code, category_id\").createOrReplaceTempView(\"df1\")\nspark.sql(\"select app_id, free_app_download, paid_app_download, revenue, device_code, country_code ,category_id from cumu order by app_id , device_code, country_code, category_id\").createOrReplaceTempView(\"df2\")\nspark.sql(\"select * from df1 except all select * from df2\").show()\nspark.sql(\"select * from df2 except all select * from df1\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200420-100318_957875639","metadata":{},"outputs":[],"source":["\n_unified_data.show()"]},{"cell_type":"code","execution_count":0,"id":"20200418-091541_965229446","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nimport datetime\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\nstart = \"2010-07-04\"\nend = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\nest_list = [ \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(test_date) for test_date in dates ]\nrank_list = [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(test_date) for test_date in dates  ]\n# print est_list\n# print rank_list\n\n\n\n\n\n\ndef test_count(test_date):\n    # test_date = '2017-08-01'\n    namespace = \"aa.store.market-size.v1\"\n    \n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": est_list,\n    \n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": rank_list,\n    \n            }\n            \n        ]\n    }\n    sql_where = \"app_id in ({}) and device_code in ('{}') and country_code in ('{}') and category_id in ({})\".format(\",\".join(map(str,test_ios_app_list)),   \"','\".join(device_code), \"','\".join(country_list), \",\".join(map(str,category_list)))\n    print sql_where\n\n    \n    sql_text = \"\"\"\n    \n    \n    -- rank_unified,store_unified\n    WITH unified_data_test AS \n    ( \n                    SELECT          store_unified.country_code, \n                                    store_unified.device_code, \n                                    store_unified.free_app_download  , \n                                    store_unified.paid_app_download , \n                                    store_unified.revenue , \n                                    rank_unified.category_id, \n                                    rank_unified.app_id, \n                                    rank_unified.free_app_download AS rank_free_app_download, \n                                    rank_unified.paid_app_download AS rank_paid_app_download, \n                                    rank_unified.revenue AS rank_revenue, \n                                    rank_unified.granularity, \n                                    rank_unified.date \n                    FROM            rank_unified \n                    FULL OUTER JOIN store_unified \n                    ON              rank_unified.app_id = store_unified.app_id \n                    AND             rank_unified.country_code = store_unified.country_code \n                    AND             rank_unified.device_code = store_unified.device_code \n                    AND             rank_unified.date = store_unified.date );\n                    WHERE %s\n    \n    \n    \n    \"\"\"%(sql_where)\n    \n\n\n    run(spark, ingest_msg, sql_text)\n \n    category_1 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-cum.v1/fact\").where(\"date='{}'\".format(test_date)).cache()\n\n\n    test_ios_app_list = [377194688 , 375875657,366236510, 354990881,365691077 , 343200656 ,20600000009072,20600000000553]\n    device_code=['ios-phone','ios-tablet','ios-all','android-all']\n    country_list=['WW', 'US']\n    category_list=[100000, 100026,400025,400000]\n\n\n    \n\n\n    cdf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-cum.v1/fact/date={}/\".format(dates[-1])).where(sql_where).cache()\n    cdf1.createOrReplaceTempView(\"cumu\")\n\n    print \"compare sum data: \", dates[-1]\n\n\n    spark.sql(\"select app_id, sum(free_app_download) as free_app_download, sum(paid_app_download) as paid_app_download, sum(revenue) as revenue, device_code, country_code ,category_id from unified_data_test where {}  group by app_id , device_code, country_code,category_id order by app_id , device_code, country_code, category_id\".format(sql_where)).createOrReplaceTempView(\"df1\")\n    spark.sql(\"select app_id, free_app_download, paid_app_download, revenue, device_code, country_code ,category_id from cumu order by app_id , device_code, country_code, category_id\").createOrReplaceTempView(\"df2\")\n    spark.sql(\"select * from df1 except all select * from df2\").show()\n    spark.sql(\"select * from df2 except all select * from df1\").show()\n\ntest_count(dates[-1])\n"]},{"cell_type":"code","execution_count":0,"id":"20200418-080954_1786177284","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2010-07-04\"\nend = \"2014-01-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\n\ntest_ios_app_list = [377194688 , 375875657,366236510, 354990881,365691077 , 343200656 ,20600000009072,20600000000553]\ndevice_code=['ios-phone','ios-tablet','ios-all','android-all']\ncountry_list=['WW', 'US']\n# test_list = [(app, device, country ) for app in test_ios_app_list  for device in device_code for country in country_list ]\n\n\nsql_where = \"app_id in ({}) and device_code in ('{}') and country_code in ('{}')\".format(\",\".join(map(str,test_ios_app_list)),   \"','\".join(device_code), \"','\".join(country_list) )\nprint sql_where\n\nresult=list()\nfor day in dates:\n    _unified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % (day.strftime(\"%Y-%m-%d\"))).where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\").collect()\n    # print _unified_data\n    result.extend(_unified_data)\n\n\nspark.createDataFrame(result).createOrReplaceTempView(\"unified_data_view\")\nspark.sql(\"select app_id, sum(free_app_download) from unified_data_view group by app_id, device_code, country_code\").show(2)\n\n\n\ncdf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(dates[-1])).where(sql_where).cache()\ncdf1.createOrReplaceTempView(\"cumu\")\n\nprint \"compare sum data: \", dates[-1]\n\n# df1 = spark.sql(\"select app_id, sum(free_app_download) as free_app_download, device_code, country_code from unified_data_view  group by app_id , device_code, country_code EXCEPT ALL select app_id , device_code, country_code, free_app_download from cumu  \").show()\n# spark.sql(\"select app_id, free_app_download, device_code, country_code from cumu EXCEPT ALL select app_id , device_code, country_code , sum(free_app_download) as free_app_download  from unified_data_view  group by app_id , device_code, country_code   \").show()\n\nspark.sql(\"select app_id, sum(free_app_download) as free_app_download, sum(paid_app_download) as paid_app_download, sum(revenue) as revenue, device_code, country_code from unified_data_view  group by app_id , device_code, country_code order by app_id , device_code, country_code\").createOrReplaceTempView(\"df1\")\nspark.sql(\"select app_id, free_app_download, paid_app_download, revenue, device_code, country_code from cumu order by app_id , device_code, country_code\").createOrReplaceTempView(\"df2\")\nspark.sql(\"select * from df1 except all select * from df2\").show()\nspark.sql(\"select * from df2 except all select * from df1\").show()\n\n\nimport datetime\n\nstart = \"2010-07-04\"\nend = \"2014-01-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\n\ntest_ios_app_list = [377194688 , 375875657,366236510, 354990881,365691077 , 343200656 ,20600000009072,20600000000553]\ndevice_code=['ios-phone','ios-tablet','ios-all','android-all']\ncountry_list=['WW', 'US']\n# test_list = [(app, device, country ) for app in test_ios_app_list  for device in device_code for country in country_list ]\n\n\nsql_where = \"app_id in ({}) and device_code in ('{}') and country_code in ('{}')\".format(\",\".join(map(str,test_ios_app_list)),   \"','\".join(device_code), \"','\".join(country_list) )\nprint sql_where\n\nresult=list()\nfor day in dates:\n    _unified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % (day.strftime(\"%Y-%m-%d\"))).where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\").collect()\n    # print _unified_data\n    result.extend(_unified_data)\n\n\nspark.createDataFrame(result).createOrReplaceTempView(\"unified_data_view\")\nspark.sql(\"select app_id, sum(free_app_download) from unified_data_view group by app_id, device_code, country_code\").show(2)\n\n\n\ncdf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(dates[-1])).where(sql_where).cache()\ncdf1.createOrReplaceTempView(\"cumu\")\n\nprint \"compare sum data: \", dates[-1]\n\n# df1 = spark.sql(\"select app_id, sum(free_app_download) as free_app_download, device_code, country_code from unified_data_view  group by app_id , device_code, country_code EXCEPT ALL select app_id , device_code, country_code, free_app_download from cumu  \").show()\n# spark.sql(\"select app_id, free_app_download, device_code, country_code from cumu EXCEPT ALL select app_id , device_code, country_code , sum(free_app_download) as free_app_download  from unified_data_view  group by app_id , device_code, country_code   \").show()\n\nspark.sql(\"select app_id, sum(free_app_download) as free_app_download, sum(paid_app_download) as paid_app_download, sum(revenue) as revenue, device_code, country_code from unified_data_view  group by app_id , device_code, country_code order by app_id , device_code, country_code\").createOrReplaceTempView(\"df1\")\nspark.sql(\"select app_id, free_app_download, paid_app_download, revenue, device_code, country_code from cumu order by app_id , device_code, country_code\").createOrReplaceTempView(\"df2\")\nspark.sql(\"select * from df1 except all select * from df2\").show()\nspark.sql(\"select * from df2 except all select * from df1\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200418-030652_981157073","metadata":{},"outputs":[],"source":["\n\n# spark.sql(\"select app_id, sum(free_app_download) as free_app_download, device_code, country_code from unified_data_view  group by app_id , device_code, country_code except all select app_id , device_code, country_code, free_app_download from cumu  \").show()\n# spark.sql(\"select app_id, free_app_download, device_code, country_code from cumu except all select app_id , device_code, country_code , sum(free_app_download) as free_app_download  from unified_data_view  group by app_id , device_code, country_code   \").show()\n# |354990881|            12027|             null|   null|  ios-phone|          WW|     100000|\n\na = spark.sql(\"select app_id, sum(free_app_download) as free_app_download, device_code, country_code , category_id from unified_data_view  group by app_id , device_code, country_code,category_id  order by app_id , category_id,device_code, country_code\").createOrReplaceTempView(\"df1\")\nb = spark.sql(\"select app_id, free_app_download, device_code, country_code, category_id from cumu order by app_id , device_code, country_code, category_id \").createOrReplaceTempView(\"df2\")\nspark.sql(\"select * from df1 order by app_id , device_code, country_code, category_id\").show()\nspark.sql(\"select * from df2 order by app_id , device_code, country_code, category_id\").show()\n\n# |377194688|          2431365|  ios-phone|          US|\n# |377194688|          2431365|  ios-phone|          US|\n# |377194688|          2431365|    ios-all|          US|\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200418-085100_1889126703","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}