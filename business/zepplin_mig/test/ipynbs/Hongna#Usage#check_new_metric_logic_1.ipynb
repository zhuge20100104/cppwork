{"cells":[{"cell_type":"code","execution_count":0,"id":"20201016-095017_1262389462","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import StructType, StructField, LongType, StringType\n\ndate_3 = ['2019-03-31', '2019-03-30', '2019-03-29', '2019-03-28', '2019-03-27', '2019-03-26', '2019-03-25', '2019-03-24', '2019-03-23', '2019-03-22', '2019-03-21', '2019-03-20', '2019-03-19', '2019-03-18', '2019-03-17', '2019-03-16', '2019-03-15', '2019-03-14', '2019-03-13', '2019-03-12', '2019-03-11', '2019-03-10', '2019-03-09', '2019-03-08', '2019-03-07', '2019-03-06', '2019-03-05', '2019-03-04', '2019-03-03', '2019-03-02', '2019-03-01']\n\ndate_2 = ['2019-02-28', '2019-02-27', '2019-02-26', '2019-02-25', '2019-02-24', '2019-02-23', '2019-02-22', '2019-02-21', '2019-02-20', '2019-02-19', '2019-02-18', '2019-02-17', '2019-02-16', '2019-02-15', '2019-02-14', '2019-02-13', '2019-02-12', '2019-02-11', '2019-02-10', '2019-02-09', '2019-02-08', '2019-02-07', '2019-02-06', '2019-02-05', '2019-02-04', '2019-02-03', '2019-02-02', '2019-02-01']\n\nrention_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-rt_app.v2/fact/granularity=monthly/\"\nusage_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/\"\n# spark.read.format(\"parquet\").load(rention_path).filter(\"date between '2019-02-01' and '2019-03-31'\").createOrReplaceTempView(\"test_retention\")\nspark.read.format(\"delta\").load(usage_path).filter(\"date between '2019-02-01' and '2019-03-31'\").createOrReplaceTempView(\"test_usage\")\n\n_schema =StructType([StructField(\"daily_date\", StringType(), False),StructField(\"month_date\", StringType(), False)])\ndf_data_2 = [Row(daily_date=r, month_date='2019-02-28') for r in date_2]\ndf_date_2 = spark.createDataFrame(data=df_data_2, schema=_schema)\ndf_date_2.createOrReplaceTempView(\"daily_date_2\")\n\ndf_data_3 = [Row(daily_date=r, month_date='2019-03-31') for r in date_3]\ndf_date_3 = spark.createDataFrame(data=df_data_3, schema=_schema)\ndf_date_3.createOrReplaceTempView(\"daily_date_3\")\n\nspark.sql(\"select * from daily_date_2 union select * from daily_date_3 order by daily_date desc\").createOrReplaceTempView(\"daily_date\")\nspark.sql(\"select * from daily_date dd join test_retention tr on tr.date = dd.month_date and tr.store_id = 10 and tr.gender=0 and tr.app_id=20600004251992 and tr.kpi=301\").createOrReplaceTempView(\"retention_table\")\n\nspark.sql(\"\"\"select rt.*, usage.est_average_active_users  from retention_table rt left join test_usage usage \non rt.app_id = usage.app_id\nand rt.month_date = usage.date\nand usage.country_code = 'US'\"\"\").createOrReplaceTempView(\"retention_with_usage\")\n\nspark.sql(\"\"\"select sum(estimate*est_average_active_users) as retention_users, sum(est_average_active_users) \nfrom retention_with_usage \nwhere daily_date between '2019-02-24' and '2019-03-02' \"\"\").show()\n\n\n\n# spark.sql(\"select * from test_retention limit 1\").show(1, False)\n# spark.sql(\"select date, count(*) from test_usage group by date order by date\").show(100, False)\n\n# spark.sql(\"select * from test_retention\").show(1, False)\n# spark.sql(\"select distinct kpi from test_retention \").show(100, False)\n\n# spark.sql(\"select app_id, date, sum(estimate) as estimate from test_retention where app_id = 20600004251992 and date between '2019-02-01' and '2019-08-31'and kpi=301 and store_id=10 group by app_id, date\").createOrReplaceTempView(\"retention_tmp\")\n# spark.sql(\"select app_id, date, sum(est_average_active_users) as au from test_usage where app_id = 20600004251992 and date between '2019-02-01' and '2019-08-31' and country_code  in ('US') group by app_id, date\").createOrReplaceTempView(\"usage_tmp\")\n\n\n# spark.sql(\"\"\"select rt.daily_date, rt.estimate,usage.est_average_active_users from retention_table rt  \n# left join test_usage usage\n# on rt.app_id = usage.app_id \n# and rt.daily_date = usage.date \n# where rt.app_id = 20600004251992 \n# and rt.daily_date between '2019-02-24' and '2019-03-02' \n# and rt.kpi=301 \n# and usage.country_code='US' and rt.store_id=10 and rt.gender=0 order by rt.daily_date desc\"\"\").createOrReplaceTempView('total_retention_users')\n\n# spark.sql(\"select * from total_retention_users\").show(100, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20201019-115552_138170546","metadata":{},"outputs":[],"source":["\nspark.sql(\"\"\"select rt.*, usage.est_average_active_users, usage.date   from retention_table rt left join test_usage usage \non rt.app_id = usage.app_id\nand rt.month_date = usage.date\nand usage.country_code = 'US' order by rt.daily_date desc\"\"\").show(1000)"]},{"cell_type":"code","execution_count":0,"id":"20201019-114728_1071975274","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-03"]},{"cell_type":"code","execution_count":0,"id":"20201019-103506_456738435","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2019, 2, 1)\nend_date = datetime(2019, 3, 31)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\nDATE_GRANULARITY_MAPPINGLIST[\"daily\"].reverse()\nprint DATE_GRANULARITY_MAPPINGLIST[\"daily\"]"]},{"cell_type":"code","execution_count":0,"id":"20201016-095028_1901907793","metadata":{},"outputs":[],"source":["\nrention_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-rt_app.v2/fact/granularity=monthly/\"\nusage_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/\"\nspark.read.format(\"parquet\").load(rention_path).filter(\"date between '2019-02-01' and '2019-03-31'\").createOrReplaceTempView(\"test_retention\")\nspark.read.format(\"parquet\").load(usage_path).filter(\"date between '2019-02-01' and '2019-03-31'\").createOrReplaceTempView(\"test_usage\")\nspark.sql(\"select * from test_retention limit 1\").show(1, False)\nspark.sql(\"select * from test_usage limit 1\").show(1, False)\n# spark.sql(\"select distinct kpi from test_retention \").show(100, False)\n\n# spark.sql(\"select app_id, date, sum(estimate) as estimate from test_retention where app_id = 20600004251992 and date between '2019-02-01' and '2019-08-31'and kpi=301 and store_id=10 group by app_id, date\").createOrReplaceTempView(\"retention_tmp\")\n# spark.sql(\"select app_id, date, sum(est_average_active_users) as au from test_usage where app_id = 20600004251992 and date between '2019-02-01' and '2019-08-31' and country_code  in ('US') group by app_id, date\").createOrReplaceTempView(\"usage_tmp\")\n\n\nspark.sql(\"\"\"select test_retention.date, test_retention.estimate, test_usage.est_average_active_users from test_retention  \nleft join test_usage \non test_retention.app_id = test_usage.app_id \nand test_retention.date = test_usage.date \nwhere test_retention.app_id = 20600004251992 \nand test_retention.date between '2019-02-01' and '2019-03-31' \nand test_retention.kpi=301 \nand test_usage.country_code in ('US', 'JP') and test_retention.store_id in (10, 9) and test_retention.gender = 0\nand test_retention.age = 0 \nand test_retention.device_id=1001 \nand test_usage.device_code='android-phone'\"\"\").createOrReplaceTempView('total_retention_users')\n\nspark.sql(\"select * from total_retention_users limit 10\").show(10, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20201019-101904_890547803","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-rt_app.v2/fact/"]},{"cell_type":"code","execution_count":0,"id":"20201019-024751_769526145","metadata":{},"outputs":[],"source":["select \n    sum(au) / sum(pop) as up, \n    sum(atd) / sum(au) as aad, \n    sum(tt) / sum(au) as atu, \n    sum(tv) / sum(au) as afu, \n    sum(tsd) / sum(au) as asd, \n    sum(tt) / sum(tv) as asd_1,\n    sum(ib) / sum(pop) as ip,\n    sum(tt) as tt,\n    sum(tb) / sum(tv) / 1024 / 1024 as abps,\n    sum(tb) / sum(au) / 1024 / 1024 as abpu,\n    sum(atd) / sum(au) / 7 as atd_w\n    --sum(tv) / sum(tsmc) as smc,\n    --sum(tt) / sum(ttmc) as tmc\nfrom (\n    select \n        date, \n        country_code,\n        device_code,\n        app_id,\n        est_average_session_duration * est_average_active_users / 60 / 1000 as tsd,\n        est_population as pop,\n        est_average_session_per_user * est_average_active_users as tv,\n        est_total_time as tt, \n        est_average_active_days * est_average_active_users as ATD, \n        est_average_active_users AS au,\n        est_installs as ib,\n        est_average_bytes_per_user * est_average_active_users * est_average_session_per_user as tb\n        --est_average_active_users * est_average_session_per_user / est_share_of_category_session as tsmc,\n        --est_total_time / 60 / 1000 / est_share_of_category_time as ttmc\n        from (\n            select * from usage_basic_kpi_fact_v1 \n            where \n            app_id in (20600005513979, 1033282601) \n            and date between '2019-10-29' and '2020-09-22' \n            and granularity = 'daily' \n            --and country_code in ('RU', 'US', 'JP', 'IT')\n            and country_code in ('US')\n            ) tmp1\n) tmp2;"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}