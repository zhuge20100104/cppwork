{"cells":[{"cell_type":"code","execution_count":0,"id":"20191217-042701_1511942019","metadata":{},"outputs":[],"source":["\n\nfrom pandas import pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\nbegin_date = datetime.datetime.strptime(\"2020-01-20\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2020-02-03\", '%Y-%m-%d')\n\ndate_list = get_date_list(begin_date, end_date, \"D\")\n\nfor date in date_list:\n    df = spark.read.option(\"delimiter\", \"\\t\").csv(\"s3://prod.appannie.aso/ckl/INCREMENTAL/{}/ios/iphone_US.tsv\".format(date))\n    df = df.toPandas()\n    print date\n    print df[df._c2=='6325117']\n    print '*'*100\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200203-104155_667956555","metadata":{},"outputs":[],"source":["\n\nfrom pandas import pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\nbegin_date = datetime.datetime.strptime(\"2020-01-24\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2020-01-30\", '%Y-%m-%d')\n\ndate_list = get_date_list(begin_date, end_date, \"D\")\n\ndate_list = [  \"2019-11-30\", \"2019-12-31\", \"2020-01-31\"]\nfor date in date_list:\n    df = spark.read.option(\"delimiter\", \"\\t\").csv(\"s3://prod.appannie.aso/ckl/MONTH/{}/ios/iphone_US.tsv\".format(date))\n    df = df.toPandas()\n    print date\n    print df[df._c2=='6325117']\n    print '*'*100\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191217-042803_194782803","metadata":{},"outputs":[],"source":["\n\n# df = spark.read.option(\"delimiter\", \"\\t\").csv(\"s3://prod.appannie.aso/ckl/MONTH/2019-09-30/ios/iphone_US.tsv\")\n# df = spark.read.option(\"delimiter\", \"\\t\").csv(\"s3://prod.appannie.aso/ckl/MONTH/2019-10-31/ios/iphone_US.tsv\")\n\n# df_list = []\n# for date in [\"2019-05-31\", \"2019-06-30\", \"2019-07-31\", \"2019-08-31\",\"2019-09-30\", \"2019-10-31\", \"2019-11-30\"]:\n#     df = spark.read.option(\"delimiter\", \"\\t\").csv(\"s3://prod.appannie.aso/ckl/MONTH/{}/ios/iphone_US.tsv\".format(date))\n#     df = df.toPandas()\n#     df_list.append(df)\n#     print date\n#     print df[df._c2=='221666']\n#     print '*'*100\n\n# for df in df_list:\n#     print len(df)\n#     print df[df._c2=='221666']\n \n\nprint(pd.merge(df[0],df[1],on=['_c2']))\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191217-043832_40060985","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='rn*Wh%osCl2C' psql -h internal-aa-citus-mkt-elb-1511434527.us-east-1.elb.amazonaws.com  -U citus_mkt_qa -d aa_mkt_db -p 7432 << EOF \n\nSELECT * FROM aso.aso_sv_kd_fact_v1 where keyword_id=221666 AND country_code='US' AND device_code='ios-phone' AND granularity='daily' AND date='2019-12-01' limit 1;\n\n\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191217-044355_1160297790","metadata":{},"outputs":[],"source":["%%sh\n\n%%sh\n\nPGPASSWORD='rn*Wh%osCl2C' psql -h internal-aa-citus-mkt-elb-1511434527.us-east-1.elb.amazonaws.com  -U citus_mkt_qa -d aa_mkt_db -p 7432 << EOF \n\nSELECT * FROM aso.aso_sv_kd_fact_v1 where keyword_id=221666 AND country_code='US' limit 5 ;\n\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20191217-042832_476095691","metadata":{},"outputs":[],"source":["\n\n# 1. 具体的抓取逻辑? 是否是一个bug? 导致epix没有被抓到的原因? kw_list? 任务失败? proxy? 5-10次没有抓到.\n1. 是否有任务抓取这个keyword? 每次抓取会请求几次? \"221666\"\n2. 有的话, 抓取结果是什么? e.g. 超时, 还是返回空?\n3. 能否用proxy独立debug run一下, 看当前结果.\n\n# 2. 如果不是bug的话, 这种不稳定复现率有多少?\n"]},{"cell_type":"code","execution_count":0,"id":"20191217-064338_1998626437","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://prod.appannie.aso/search_volume/keyword-tool-api/raw/MONTH/2020-01-31/AE/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191227-063218_922413870","metadata":{},"outputs":[],"source":["\nimport pandas as pd\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\ndate_list = get_date_list(\"2020-01-01\", \"2020-02-22\")\nfor date in date_list:\n    count = spark.read.parquet(\"s3://prod.appannie.mktint.data/oss/SEARCH_VOLUME/routine/granularity=daily/date={}/\".format(date)).filter(\"keyword_id in (6297851)\").count()\n    print \"{} {}\".format(date, count)\n"]},{"cell_type":"code","execution_count":0,"id":"20200103-104910_889481818","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://prod.appannie.aso/ckl/WEEK/\n"]},{"cell_type":"code","execution_count":0,"id":"20200103-104953_1066163664","metadata":{},"outputs":[],"source":["\nimport boto3\n\ndef read_s3(key):\n    print '*'*100 \n    print key\n    s3 = boto3.resource('s3')\n    obj = s3.Object('b2c-prod-data-pipeline-qa', key)\n    body = obj.get()['Body'].read()\n    print body\n\nread_s3('tom/all/regression1.txt')\nread_s3('tom/all/regression2.txt')\nread_s3('tom/all/regression3.txt')\nread_s3('tom/all/regression4.txt')\nread_s3('tom/all/regression5.txt')\n"]},{"cell_type":"code","execution_count":0,"id":"20200222-103213_1864941258","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}