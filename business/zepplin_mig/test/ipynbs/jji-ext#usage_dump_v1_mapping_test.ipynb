{"cells":[{"cell_type":"code","execution_count":0,"id":"20200512-014314_309180137","metadata":{},"outputs":[],"source":["\nimport datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\ntest_result = []\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\nDEVICE_ID_CODE_MAPPING = {\n    1001: 'android-phone',\n    1002: 'android-tablet',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR',\n                             7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH',\n                             13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR',\n                             19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY',\n                             25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW',\n                             31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ',\n                             37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO',\n                             43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA',\n                             49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE',\n                             80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW'}\n\nIOS_COUNTRY_ID_CODES = {143460: 'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',\n                      143450: 'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',\n                      143471: 'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',\n                      143469: 'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',\n                      143475: 'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',\n                      143489: 'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',\n                      143453: 'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',\n                      143493: 'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',\n                      143494: 'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\n\ndef sample_date(month, day, date_list):\n    sample_date_list = []\n    month_random_list = np.random.randint(0, len(date_list), month).tolist()\n    for month in month_random_list:\n        sample_date_per_month = []\n        day_random_list = np.random.randint(0, len(date_list[month][1]), day).tolist()\n        for day in day_random_list:\n            sample_date_per_month.append(date_list[month][1][day])\n        sample_date_list.append((date_list[month][0], sample_date_per_month))\n    return sample_date_list\n\n\ndef _merge_dicts(dict1, dict2):\n    dict1.update(dict2)\n    return dict1\n\n\ndef write_test_result(df_write_result):\n    df_write_result.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_mapping_0512/daily/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 01, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_mapping(_granularity, date_list):\n    sample_date_list = sample_date(3, 3, date_list)\n    for month_day_list_tuple in sample_date_list:\n        raw_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\" \\\n                   \"month={raw_month}/\"\n        raw_path_parse = raw_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        raw_df = spark.read.parquet(raw_path_parse)\n        raw_df = (\n            raw_df\n            .withColumn('device_id', functions.UserDefinedFunction(\n                lambda x: DEVICE_ID_CODE_MAPPING[x])(raw_df['device_id']))\n            .withColumn('store_id', functions.UserDefinedFunction(\n                lambda x: _merge_dicts(IOS_COUNTRY_ID_CODES, ANDROID_COUNTRY_ID_CODES)[x])(raw_df['store_id']))\n            .withColumnRenamed('device_id', 'device_code')\n            .withColumnRenamed('store_id', 'country_code')\n        )\n        for day in month_day_list_tuple[1]:\n            raw_df_date_filtered = raw_df.filter(\n                \"date='{}'\".format(day)).select(\"app_id\", \"device_code\", \"country_code\", \"kpi\", \"estimate\")\n            kpi_row_list = raw_df_date_filtered.select('kpi').distinct().collect()\n            for row in kpi_row_list:\n                raw_df_kpi_filtered = raw_df_date_filtered.filter(\"kpi='{}'\".format(row[\"kpi\"]))\\\n                    .withColumnRenamed('estimate', kpi_mapping[row[\"kpi\"]]).drop('kpi')\n                unified_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                               \"granularity={unified_granularity}/date={unified_date}/\"\n                unified_path_parse = unified_path.format(unified_date=day, unified_granularity=_granularity)\n                unified_df = spark.read.parquet(unified_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(\n                    'app_id', 'device_code', 'country_code', kpi_mapping[row[\"kpi\"]])\n                subtract_count = unified_df.subtract(raw_df_kpi_filtered).count()\n                if subtract_count != 0:\n                    print \"Mapping Test Wrong!!! granularity: {} , subtract_count: {}, date: {}, kpi: {} \".format(\n                        _granularity, subtract_count, day, kpi_mapping[row[\"kpi\"]]\n                    )\n                else:\n                    print subtract_count, day, kpi_mapping[row[\"kpi\"]]\n                test_result.append((_granularity, subtract_count, day, kpi_mapping[row[\"kpi\"]]))\n            print \"date={} test complete!\".format(day)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_mapping(granularity, get_path_date_list(granularity))\nprint 'pass'\ndf_write_result = spark.createDataFrame(test_result, schema=['type', 'subtract_count', 'date', 'kpi'])\nwrite_test_result(df_write_result)"]},{"cell_type":"code","execution_count":0,"id":"20200512-032233_1598083304","metadata":{},"outputs":[],"source":["\nimport datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\ntest_result = []\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\nDEVICE_ID_CODE_MAPPING = {\n    1001: 'android-phone',\n    1002: 'android-tablet',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR',\n                             7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH',\n                             13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR',\n                             19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY',\n                             25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW',\n                             31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ',\n                             37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO',\n                             43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA',\n                             49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE',\n                             80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW'}\n\nIOS_COUNTRY_ID_CODES = {143460: 'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',\n                      143450: 'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',\n                      143471: 'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',\n                      143469: 'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',\n                      143475: 'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',\n                      143489: 'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',\n                      143453: 'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',\n                      143493: 'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',\n                      143494: 'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\n\ndef sample_date(month, day, date_list):\n    sample_date_list = []\n    month_random_list = np.random.randint(0, len(date_list), month).tolist()\n    for month in month_random_list:\n        sample_date_per_month = []\n        day_random_list = np.random.randint(0, len(date_list[month][1]), day).tolist()\n        for day in day_random_list:\n            sample_date_per_month.append(date_list[month][1][day])\n        sample_date_list.append((date_list[month][0], sample_date_per_month))\n    return sample_date_list\n\n\ndef _merge_dicts(dict1, dict2):\n    dict1.update(dict2)\n    return dict1\n\n\ndef write_test_result(df_write_result):\n    df_write_result.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_mapping_0512/weekly/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 26)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_mapping(_granularity, date_list):\n    sample_date_list = sample_date(3, 3, date_list)\n    for month_day_list_tuple in sample_date_list:\n        raw_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\" \\\n                   \"month={raw_month}/\"\n        raw_path_parse = raw_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        raw_df = spark.read.parquet(raw_path_parse)\n        raw_df = (\n            raw_df\n            .withColumn('device_id', functions.UserDefinedFunction(\n                lambda x: DEVICE_ID_CODE_MAPPING[x])(raw_df['device_id']))\n            .withColumn('store_id', functions.UserDefinedFunction(\n                lambda x: _merge_dicts(IOS_COUNTRY_ID_CODES, ANDROID_COUNTRY_ID_CODES)[x])(raw_df['store_id']))\n            .withColumnRenamed('device_id', 'device_code')\n            .withColumnRenamed('store_id', 'country_code')\n        )\n        for day in month_day_list_tuple[1]:\n            raw_df_date_filtered = raw_df.filter(\n                \"date='{}'\".format(day)).select(\"app_id\", \"device_code\", \"country_code\", \"kpi\", \"estimate\")\n            kpi_row_list = raw_df_date_filtered.select('kpi').distinct().collect()\n            for row in kpi_row_list:\n                raw_df_kpi_filtered = raw_df_date_filtered.filter(\"kpi='{}'\".format(row[\"kpi\"]))\\\n                    .withColumnRenamed('estimate', kpi_mapping[row[\"kpi\"]]).drop('kpi')\n                unified_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                               \"granularity={unified_granularity}/date={unified_date}/\"\n                unified_path_parse = unified_path.format(unified_date=day, unified_granularity=_granularity)\n                unified_df = spark.read.parquet(unified_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(\n                    'app_id', 'device_code', 'country_code', kpi_mapping[row[\"kpi\"]])\n                subtract_count = unified_df.subtract(raw_df_kpi_filtered).count()\n                if subtract_count != 0:\n                    print \"Mapping Test Wrong!!! granularity: {} , subtract_count: {}, date: {}, kpi: {} \".format(\n                        _granularity, subtract_count, day, kpi_mapping[row[\"kpi\"]]\n                    )\n                else:\n                    print subtract_count, day, kpi_mapping[row[\"kpi\"]]\n                test_result.append((_granularity, subtract_count, day, kpi_mapping[row[\"kpi\"]]))\n            print \"date={} test complete!\".format(day)\n\n\ngranularity_list = [\"weekly\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_mapping(granularity, get_path_date_list(granularity))\nprint 'pass'\ndf_write_result = spark.createDataFrame(test_result, schema=['type', 'subtract_count', 'date', 'kpi'])\nwrite_test_result(df_write_result)"]},{"cell_type":"code","execution_count":0,"id":"20200512-050733_393896633","metadata":{},"outputs":[],"source":["\nimport datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\ntest_result = []\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\nDEVICE_ID_CODE_MAPPING = {\n    1001: 'android-phone',\n    1002: 'android-tablet',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR',\n                             7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH',\n                             13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR',\n                             19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY',\n                             25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW',\n                             31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ',\n                             37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO',\n                             43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA',\n                             49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE',\n                             80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW'}\n\nIOS_COUNTRY_ID_CODES = {143460: 'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',\n                      143450: 'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',\n                      143471: 'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',\n                      143469: 'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',\n                      143475: 'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',\n                      143489: 'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',\n                      143453: 'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',\n                      143493: 'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',\n                      143494: 'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\n\ndef sample_date(month, day, date_list):\n    sample_date_list = []\n    month_random_list = np.random.randint(0, len(date_list), month).tolist()\n    for month in month_random_list:\n        sample_date_per_month = []\n        if len(date_list[month][1]) == 1:\n            day_random_list = [0]\n        else:\n            day_random_list = np.random.randint(0, len(date_list[month][1]), day).tolist()\n        for day in day_random_list:\n            sample_date_per_month.append(date_list[month][1][day])\n        sample_date_list.append((date_list[month][0], sample_date_per_month))\n    return sample_date_list\n\n\ndef _merge_dicts(dict1, dict2):\n    dict1.update(dict2)\n    return dict1\n\n\ndef write_test_result(df_write_result):\n    df_write_result.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_mapping_0512/monthly/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 26)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_mapping(_granularity, date_list):\n    sample_date_list = sample_date(9, 1, date_list)\n    for month_day_list_tuple in sample_date_list:\n        raw_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\" \\\n                   \"month={raw_month}/\"\n        raw_path_parse = raw_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        raw_df = spark.read.parquet(raw_path_parse)\n        raw_df = (\n            raw_df\n            .withColumn('device_id', functions.UserDefinedFunction(\n                lambda x: DEVICE_ID_CODE_MAPPING[x])(raw_df['device_id']))\n            .withColumn('store_id', functions.UserDefinedFunction(\n                lambda x: _merge_dicts(IOS_COUNTRY_ID_CODES, ANDROID_COUNTRY_ID_CODES)[x])(raw_df['store_id']))\n            .withColumnRenamed('device_id', 'device_code')\n            .withColumnRenamed('store_id', 'country_code')\n        )\n        for day in month_day_list_tuple[1]:\n            raw_df_date_filtered = raw_df.filter(\n                \"date='{}'\".format(day)).select(\"app_id\", \"device_code\", \"country_code\", \"kpi\", \"estimate\")\n            kpi_row_list = raw_df_date_filtered.select('kpi').distinct().collect()\n            for row in kpi_row_list:\n                raw_df_kpi_filtered = raw_df_date_filtered.filter(\"kpi='{}'\".format(row[\"kpi\"]))\\\n                    .withColumnRenamed('estimate', kpi_mapping[row[\"kpi\"]]).drop('kpi')\n                unified_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                               \"granularity={unified_granularity}/date={unified_date}/\"\n                unified_path_parse = unified_path.format(unified_date=day, unified_granularity=_granularity)\n                unified_df = spark.read.parquet(unified_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(\n                    'app_id', 'device_code', 'country_code', kpi_mapping[row[\"kpi\"]])\n                subtract_count = unified_df.subtract(raw_df_kpi_filtered).count()\n                if subtract_count != 0:\n                    print \"Mapping Test Wrong!!! granularity: {} , subtract_count: {}, date: {}, kpi: {} \".format(\n                        _granularity, subtract_count, day, kpi_mapping[row[\"kpi\"]]\n                    )\n                else:\n                    print subtract_count, day, kpi_mapping[row[\"kpi\"]]\n                test_result.append((_granularity, subtract_count, day, kpi_mapping[row[\"kpi\"]]))\n            print \"date={} test complete!\".format(day)\n\n\ngranularity_list = [\"monthly\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_mapping(granularity, get_path_date_list(granularity))\nprint 'pass'\ndf_write_result = spark.createDataFrame(test_result, schema=['type', 'subtract_count', 'date', 'kpi'])\nwrite_test_result(df_write_result)"]},{"cell_type":"code","execution_count":0,"id":"20200512-050812_1654759697","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/"]},{"cell_type":"code","execution_count":0,"id":"20200608-100349_1811533709","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/granularity=daily/date=2020-01-01')\ndf.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200608-101036_1856862031","metadata":{},"outputs":[],"source":["\nimport datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\n\n\n\nTEST_RESULT = []\nCOUNTRY_CODE_MAPPING_BY_MARKET_CODE = {\n    'google-play': {1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US',\n                    11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN',\n                    20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL',\n                    29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU',\n                    38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR',\n                    47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 53: 'KZ', 54: 'PK', 55: 'IQ',\n                    56: 'PE', 57: 'MA', 58: 'BY', 59: 'DZ', 60: 'VE', 61: 'AZ', 62: 'EC', 63: 'JO', 64: 'CR',\n                    65: 'LB', 66: 'BD', 67: 'GT', 68: 'RS', 69: 'DO', 70: 'IR', 71: 'OM', 72: 'BO', 73: 'QA',\n                    74: 'NG', 75: 'SV', 76: 'KH', 77: 'PA', 78: 'LT', 79: 'TN', 80: 'HR', 81: 'JM', 82: 'LK',\n                    83: 'HN', 84: 'PR', 85: 'UY', 86: 'LV', 87: 'BA', 88: 'KG', 89: 'PY', 90: 'MD', 91: 'NP',\n                    92: 'TZ', 93: 'BH', 94: 'GH', 95: 'KE', 96: 'SI', 97: 'AM', 98: 'UZ', 99: 'TT', 100: 'MK',\n                    101: 'YE', 102: 'MO', 103: 'LU', 1000: 'WW'},\n    'apple-store': {143441: 'US', 143442: 'FR', 143443: 'DE', 143444: 'GB', 143445: 'AT', 143446: 'BE', 143447: 'FI',\n                    143448: 'GR', 143449: 'IE', 143450: 'IT', 143451: 'LU', 143452: 'NL', 143453: 'PT', 143454: 'ES',\n                    143455: 'CA', 143456: 'SE', 143457: 'NO', 143458: 'DK', 143459: 'CH', 143460: 'AU', 143461: 'NZ',\n                    143462: 'JP', 143463: 'HK', 143464: 'SG', 143465: 'CN', 143466: 'KR', 143467: 'IN', 143468: 'MX',\n                    143469: 'RU', 143470: 'TW', 143471: 'VN', 143472: 'ZA', 143473: 'MY', 143474: 'PH', 143475: 'TH',\n                    143476: 'ID', 143477: 'PK', 143478: 'PL', 143479: 'SA', 143480: 'TR', 143481: 'AE', 143482: 'HU',\n                    143483: 'CL', 143484: 'NP', 143485: 'PA', 143486: 'LK', 143487: 'RO', 143489: 'CZ', 143491: 'IL',\n                    143492: 'UA', 143493: 'KW', 143494: 'HR', 143495: 'CR', 143496: 'SK', 143497: 'LB', 143498: 'QA',\n                    143499: 'SI', 143501: 'CO', 143502: 'VE', 143503: 'BR', 143504: 'GT', 143505: 'AR', 143506: 'SV',\n                    143507: 'PE', 143508: 'DO', 143509: 'EC', 143510: 'HN', 143511: 'JM', 143512: 'NI', 143513: 'PY',\n                    143514: 'UY', 143515: 'MO', 143516: 'EG', 143517: 'KZ', 143518: 'EE', 143519: 'LV', 143520: 'LT',\n                    143521: 'MT', 143523: 'MD', 143524: 'AM', 143525: 'BW', 143526: 'BG', 143528: 'JO', 143529: 'KE',\n                    143530: 'MK', 143531: 'MG', 143532: 'ML', 143533: 'MU', 143534: 'NE', 143535: 'SN', 143536: 'TN',\n                    143537: 'UG', 143538: 'AI', 143539: 'BS', 143540: 'AG', 143541: 'BB', 143542: 'BM', 143543: 'VG',\n                    143544: 'KY', 143545: 'DM', 143546: 'GD', 143547: 'MS', 143548: 'KN', 143549: 'LC', 143550: 'VC',\n                    143551: 'TT', 143552: 'TC', 143553: 'GY', 143554: 'SR', 143555: 'BZ', 143556: 'BO', 143557: 'CY',\n                    143558: 'IS', 143559: 'BH', 143560: 'BN', 143561: 'NG', 143562: 'OM', 143563: 'DZ', 143564: 'AO',\n                    143565: 'BY', 143566: 'UZ', 143568: 'AZ', 143571: 'YE', 143572: 'TZ', 143573: 'GH', 143575: 'AL',\n                    143576: 'BJ', 143577: 'BT', 143578: 'BF', 143579: 'KH', 143580: 'CV', 143581: 'TD', 143582: 'CG',\n                    143583: 'FJ', 143584: 'GM', 143585: 'GW', 143586: 'KG', 143587: 'LA', 143588: 'LR', 143589: 'MW',\n                    143590: 'MR', 143591: 'FM', 143592: 'MN', 143593: 'MZ', 143594: 'NA', 143595: 'PW', 143597: 'PG',\n                    143598: 'ST', 143599: 'SC', 143600: 'SL', 143601: 'SB', 143602: 'SZ', 143603: 'TJ', 143604: 'TM',\n                    143605: 'ZW', 0: 'WW'},\n    'amazon-store': {\n        'android-all': {\n            'UK': 'GB',\n        }\n    }\n}\nKPI_MAPPING = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\nDEVICE_ID_CODE_MAPPING = {\n    1001: 'android-phone',\n    1002: 'android-tablet',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\nANDROID_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['google-play']\nIOS_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['apple-store']\n\n\ndef sample_date(month, day, date_list):\n    sample_date_list = []\n    month_random_list = np.random.randint(0, len(date_list), month).tolist()\n    for m in month_random_list:\n        sample_date_per_month = []\n        if len(date_list[m][1]) == 1:\n            day_random_list = [0]\n        else:\n            day_random_list = np.random.randint(0, len(date_list[m][1]), day).tolist()\n        for d in day_random_list:\n            sample_date_per_month.append(date_list[m][1][d])\n        sample_date_list.append((date_list[m][0], sample_date_per_month))\n    return sample_date_list\n\n\ndef _merge_dicts(dict1, dict2):\n    dict1.update(dict2)\n    return dict1\n\n\ndef write_test_result(result_df):\n    result_df.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_mapping_0512/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 1, 31)\n    start = datetime.date(2013, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 2, 15)\n    start = datetime.date(2013, 1, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_mapping(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    sample_date_list = sample_date(3, 3, date_list)\n    for month_day_list_tuple in sample_date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        dump_df = (\n            dump_df\n            .withColumn('device_id', functions.UserDefinedFunction(\n                lambda x: DEVICE_ID_CODE_MAPPING[x])(dump_df['device_id']))\n            .withColumn('store_id', functions.UserDefinedFunction(\n                lambda x: _merge_dicts(IOS_COUNTRY_ID_CODES, ANDROID_COUNTRY_ID_CODES)[x])(dump_df['store_id']))\n            .withColumnRenamed('device_id', 'device_code')\n            .withColumnRenamed('store_id', 'country_code')\n        )\n        for day in month_day_list_tuple[1]:\n            dump_df_date_filtered = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"app_id\", \"device_code\", \"country_code\", \"kpi\", \"estimate\")\n            kpi_list = dump_df_date_filtered.select('kpi').distinct().collect()\n\n            v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/\" \\\n                      \"granularity={unified_granularity}/date={unified_date}/\"\n            v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n            v1_df = spark.read.parquet(v1_path_parse)\n            for row in kpi_list:\n                dump_kpi_df = dump_df_date_filtered.filter(\"kpi='{}'\".format(row[\"kpi\"]))\\\n                    .withColumnRenamed('estimate', KPI_MAPPING[row[\"kpi\"]]).drop('kpi')\n                v1_kpi_df = v1_df.filter(\n                    \"{} is not null\".format(KPI_MAPPING[row[\"kpi\"]])).select(\n                        'app_id', 'device_code', 'country_code', KPI_MAPPING[row[\"kpi\"]])\n                subtract_count = v1_kpi_df.subtract(dump_kpi_df).count()\n                subtract_count_reverse = dump_kpi_df.subtract(v1_kpi_df).count()\n                if subtract_count != 0 or subtract_count_reverse != 0:\n                    print \"Accuracy Test Wrong!!! granularity: {} , subtract_count: {}, date: {}, kpi: {}\".format(\n                        _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]])\n                else:\n                    print max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]\n                TEST_RESULT.append((\n                    _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]))\n            print \"date={} test complete!\".format(day)\n\n\nif __name__ == '__main__':\n    granularity_list = [\"daily\"]\n    for granularity in granularity_list:\n        check_usage_dump_v1_mapping(granularity, get_path_date_list(granularity))\n    print 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200608-102312_125511311","metadata":{},"outputs":[],"source":["\nimport datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom applications.db_check_v1.common.constants import COUNTRY_CODE_MAPPING_BY_MARKET_CODE\n\n\nTEST_RESULT = []\nKPI_MAPPING = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\nDEVICE_ID_CODE_MAPPING = {\n    1001: 'android-phone',\n    1002: 'android-tablet',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\nANDROID_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['google-play']\nIOS_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['apple-store']\n\n\ndef sample_date(month, day, date_list):\n    sample_date_list = []\n    month_random_list = np.random.randint(0, len(date_list), month).tolist()\n    for m in month_random_list:\n        sample_date_per_month = []\n        if len(date_list[m][1]) == 1:\n            day_random_list = [0]\n        else:\n            day_random_list = np.random.randint(0, len(date_list[m][1]), day).tolist()\n        for d in day_random_list:\n            sample_date_per_month.append(date_list[m][1][d])\n        sample_date_list.append((date_list[m][0], sample_date_per_month))\n    return sample_date_list\n\n\ndef _merge_dicts(dict1, dict2):\n    dict1.update(dict2)\n    return dict1\n\n\ndef write_test_result(result_df):\n    result_df.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_mapping_0512/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_mapping(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    sample_date_list = sample_date(3, 1, date_list)\n    for month_day_list_tuple in sample_date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        dump_df = (\n            dump_df\n            .withColumn('device_id', functions.UserDefinedFunction(\n                lambda x: DEVICE_ID_CODE_MAPPING[x])(dump_df['device_id']))\n            .withColumn('store_id', functions.UserDefinedFunction(\n                lambda x: _merge_dicts(IOS_COUNTRY_ID_CODES, ANDROID_COUNTRY_ID_CODES)[x])(dump_df['store_id']))\n            .withColumnRenamed('device_id', 'device_code')\n            .withColumnRenamed('store_id', 'country_code')\n        )\n        for day in month_day_list_tuple[1]:\n            dump_df_date_filtered = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"app_id\", \"device_code\", \"country_code\", \"kpi\", \"estimate\")\n            kpi_list = dump_df_date_filtered.select('kpi').distinct().collect()\n\n            v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/\" \\\n                      \"granularity={unified_granularity}/date={unified_date}/\"\n            v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n            v1_df = spark.read.parquet(v1_path_parse)\n            for row in kpi_list:\n                dump_kpi_df = dump_df_date_filtered.filter(\"kpi='{}'\".format(row[\"kpi\"]))\\\n                    .withColumnRenamed('estimate', KPI_MAPPING[row[\"kpi\"]]).drop('kpi')\n                v1_kpi_df = v1_df.filter(\n                    \"{} is not null\".format(KPI_MAPPING[row[\"kpi\"]])).select(\n                        'app_id', 'device_code', 'country_code', KPI_MAPPING[row[\"kpi\"]])\n                subtract_count = v1_kpi_df.subtract(dump_kpi_df).count()\n                subtract_count_reverse = dump_kpi_df.subtract(v1_kpi_df).count()\n                if subtract_count != 0 or subtract_count_reverse != 0:\n                    print \"Accuracy Test Wrong!!! granularity: {} , subtract_count: {}, date: {}, kpi: {}\".format(\n                        _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]])\n                else:\n                    print max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]\n                TEST_RESULT.append((\n                    _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]))\n            print \"date={} test complete!\".format(day)\n\n\nif __name__ == '__main__':\n    granularity_list = [\"weekly\"]\n    for granularity in granularity_list:\n        check_usage_dump_v1_mapping(granularity, get_path_date_list(granularity))\n    print 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200609-030031_978711311","metadata":{},"outputs":[],"source":["\nimport datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\n\n\n\nTEST_RESULT = []\nCOUNTRY_CODE_MAPPING_BY_MARKET_CODE = {\n    'google-play': {1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US',\n                    11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN',\n                    20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL',\n                    29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU',\n                    38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR',\n                    47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 53: 'KZ', 54: 'PK', 55: 'IQ',\n                    56: 'PE', 57: 'MA', 58: 'BY', 59: 'DZ', 60: 'VE', 61: 'AZ', 62: 'EC', 63: 'JO', 64: 'CR',\n                    65: 'LB', 66: 'BD', 67: 'GT', 68: 'RS', 69: 'DO', 70: 'IR', 71: 'OM', 72: 'BO', 73: 'QA',\n                    74: 'NG', 75: 'SV', 76: 'KH', 77: 'PA', 78: 'LT', 79: 'TN', 80: 'HR', 81: 'JM', 82: 'LK',\n                    83: 'HN', 84: 'PR', 85: 'UY', 86: 'LV', 87: 'BA', 88: 'KG', 89: 'PY', 90: 'MD', 91: 'NP',\n                    92: 'TZ', 93: 'BH', 94: 'GH', 95: 'KE', 96: 'SI', 97: 'AM', 98: 'UZ', 99: 'TT', 100: 'MK',\n                    101: 'YE', 102: 'MO', 103: 'LU', 1000: 'WW'},\n    'apple-store': {143441: 'US', 143442: 'FR', 143443: 'DE', 143444: 'GB', 143445: 'AT', 143446: 'BE', 143447: 'FI',\n                    143448: 'GR', 143449: 'IE', 143450: 'IT', 143451: 'LU', 143452: 'NL', 143453: 'PT', 143454: 'ES',\n                    143455: 'CA', 143456: 'SE', 143457: 'NO', 143458: 'DK', 143459: 'CH', 143460: 'AU', 143461: 'NZ',\n                    143462: 'JP', 143463: 'HK', 143464: 'SG', 143465: 'CN', 143466: 'KR', 143467: 'IN', 143468: 'MX',\n                    143469: 'RU', 143470: 'TW', 143471: 'VN', 143472: 'ZA', 143473: 'MY', 143474: 'PH', 143475: 'TH',\n                    143476: 'ID', 143477: 'PK', 143478: 'PL', 143479: 'SA', 143480: 'TR', 143481: 'AE', 143482: 'HU',\n                    143483: 'CL', 143484: 'NP', 143485: 'PA', 143486: 'LK', 143487: 'RO', 143489: 'CZ', 143491: 'IL',\n                    143492: 'UA', 143493: 'KW', 143494: 'HR', 143495: 'CR', 143496: 'SK', 143497: 'LB', 143498: 'QA',\n                    143499: 'SI', 143501: 'CO', 143502: 'VE', 143503: 'BR', 143504: 'GT', 143505: 'AR', 143506: 'SV',\n                    143507: 'PE', 143508: 'DO', 143509: 'EC', 143510: 'HN', 143511: 'JM', 143512: 'NI', 143513: 'PY',\n                    143514: 'UY', 143515: 'MO', 143516: 'EG', 143517: 'KZ', 143518: 'EE', 143519: 'LV', 143520: 'LT',\n                    143521: 'MT', 143523: 'MD', 143524: 'AM', 143525: 'BW', 143526: 'BG', 143528: 'JO', 143529: 'KE',\n                    143530: 'MK', 143531: 'MG', 143532: 'ML', 143533: 'MU', 143534: 'NE', 143535: 'SN', 143536: 'TN',\n                    143537: 'UG', 143538: 'AI', 143539: 'BS', 143540: 'AG', 143541: 'BB', 143542: 'BM', 143543: 'VG',\n                    143544: 'KY', 143545: 'DM', 143546: 'GD', 143547: 'MS', 143548: 'KN', 143549: 'LC', 143550: 'VC',\n                    143551: 'TT', 143552: 'TC', 143553: 'GY', 143554: 'SR', 143555: 'BZ', 143556: 'BO', 143557: 'CY',\n                    143558: 'IS', 143559: 'BH', 143560: 'BN', 143561: 'NG', 143562: 'OM', 143563: 'DZ', 143564: 'AO',\n                    143565: 'BY', 143566: 'UZ', 143568: 'AZ', 143571: 'YE', 143572: 'TZ', 143573: 'GH', 143575: 'AL',\n                    143576: 'BJ', 143577: 'BT', 143578: 'BF', 143579: 'KH', 143580: 'CV', 143581: 'TD', 143582: 'CG',\n                    143583: 'FJ', 143584: 'GM', 143585: 'GW', 143586: 'KG', 143587: 'LA', 143588: 'LR', 143589: 'MW',\n                    143590: 'MR', 143591: 'FM', 143592: 'MN', 143593: 'MZ', 143594: 'NA', 143595: 'PW', 143597: 'PG',\n                    143598: 'ST', 143599: 'SC', 143600: 'SL', 143601: 'SB', 143602: 'SZ', 143603: 'TJ', 143604: 'TM',\n                    143605: 'ZW', 0: 'WW'},\n    'amazon-store': {\n        'android-all': {\n            'UK': 'GB',\n        }\n    }\n}\nKPI_MAPPING = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\nDEVICE_ID_CODE_MAPPING = {\n    1001: 'android-phone',\n    1002: 'android-tablet',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\nANDROID_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['google-play']\nIOS_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['apple-store']\n\n\ndef sample_date(month, day, date_list):\n    sample_date_list = []\n    month_random_list = np.random.randint(0, len(date_list), month).tolist()\n    for m in month_random_list:\n        sample_date_per_month = []\n        if len(date_list[m][1]) == 1:\n            day_random_list = [0]\n        else:\n            day_random_list = np.random.randint(0, len(date_list[m][1]), day).tolist()\n        for d in day_random_list:\n            sample_date_per_month.append(date_list[m][1][d])\n        sample_date_list.append((date_list[m][0], sample_date_per_month))\n    return sample_date_list\n\n\ndef _merge_dicts(dict1, dict2):\n    dict1.update(dict2)\n    return dict1\n\n\ndef write_test_result(result_df):\n    result_df.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_mapping_0512/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 1, 31)\n    start = datetime.date(2013, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 2, 15)\n    start = datetime.date(2013, 1, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_mapping(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    sample_date_list = sample_date(3, 3, date_list)\n    for month_day_list_tuple in sample_date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        dump_df = (\n            dump_df\n            .withColumn('device_id', functions.UserDefinedFunction(\n                lambda x: DEVICE_ID_CODE_MAPPING[x])(dump_df['device_id']))\n            .withColumn('store_id', functions.UserDefinedFunction(\n                lambda x: _merge_dicts(IOS_COUNTRY_ID_CODES, ANDROID_COUNTRY_ID_CODES)[x])(dump_df['store_id']))\n            .withColumnRenamed('device_id', 'device_code')\n            .withColumnRenamed('store_id', 'country_code')\n        )\n        for day in month_day_list_tuple[1]:\n            dump_df_date_filtered = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"app_id\", \"device_code\", \"country_code\", \"kpi\", \"estimate\")\n            kpi_list = dump_df_date_filtered.select('kpi').distinct().collect()\n\n            v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                      \"granularity={unified_granularity}/date={unified_date}/\"\n            v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n            v1_df = spark.read.parquet(v1_path_parse)\n            for row in kpi_list:\n                dump_kpi_df = dump_df_date_filtered.filter(\"kpi='{}'\".format(row[\"kpi\"]))\\\n                    .withColumnRenamed('estimate', KPI_MAPPING[row[\"kpi\"]]).drop('kpi')\n                v1_kpi_df = v1_df.filter(\n                    \"{} is not null\".format(KPI_MAPPING[row[\"kpi\"]])).select(\n                        'app_id', 'device_code', 'country_code', KPI_MAPPING[row[\"kpi\"]])\n                subtract_count = v1_kpi_df.subtract(dump_kpi_df).count()\n                subtract_count_reverse = dump_kpi_df.subtract(v1_kpi_df).count()\n                if subtract_count != 0 or subtract_count_reverse != 0:\n                    print \"Accuracy Test Wrong!!! granularity: {} , subtract_count: {}, date: {}, kpi: {}\".format(\n                        _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]])\n                else:\n                    print max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]\n                TEST_RESULT.append((\n                    _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]))\n            print \"date={} test complete!\".format(day)\n\n\nif __name__ == '__main__':\n    granularity_list = [\"daily\"]\n    for granularity in granularity_list:\n        check_usage_dump_v1_mapping(granularity, get_path_date_list(granularity))\n    print 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200617-073614_764298075","metadata":{},"outputs":[],"source":["\nimport datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\n\n\nCOUNTRY_CODE_MAPPING_BY_MARKET_CODE = {\n    'google-play': {1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US',\n                    11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN',\n                    20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL',\n                    29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU',\n                    38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR',\n                    47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 53: 'KZ', 54: 'PK', 55: 'IQ',\n                    56: 'PE', 57: 'MA', 58: 'BY', 59: 'DZ', 60: 'VE', 61: 'AZ', 62: 'EC', 63: 'JO', 64: 'CR',\n                    65: 'LB', 66: 'BD', 67: 'GT', 68: 'RS', 69: 'DO', 70: 'IR', 71: 'OM', 72: 'BO', 73: 'QA',\n                    74: 'NG', 75: 'SV', 76: 'KH', 77: 'PA', 78: 'LT', 79: 'TN', 80: 'HR', 81: 'JM', 82: 'LK',\n                    83: 'HN', 84: 'PR', 85: 'UY', 86: 'LV', 87: 'BA', 88: 'KG', 89: 'PY', 90: 'MD', 91: 'NP',\n                    92: 'TZ', 93: 'BH', 94: 'GH', 95: 'KE', 96: 'SI', 97: 'AM', 98: 'UZ', 99: 'TT', 100: 'MK',\n                    101: 'YE', 102: 'MO', 103: 'LU', 1000: 'WW'},\n    'apple-store': {143441: 'US', 143442: 'FR', 143443: 'DE', 143444: 'GB', 143445: 'AT', 143446: 'BE', 143447: 'FI',\n                    143448: 'GR', 143449: 'IE', 143450: 'IT', 143451: 'LU', 143452: 'NL', 143453: 'PT', 143454: 'ES',\n                    143455: 'CA', 143456: 'SE', 143457: 'NO', 143458: 'DK', 143459: 'CH', 143460: 'AU', 143461: 'NZ',\n                    143462: 'JP', 143463: 'HK', 143464: 'SG', 143465: 'CN', 143466: 'KR', 143467: 'IN', 143468: 'MX',\n                    143469: 'RU', 143470: 'TW', 143471: 'VN', 143472: 'ZA', 143473: 'MY', 143474: 'PH', 143475: 'TH',\n                    143476: 'ID', 143477: 'PK', 143478: 'PL', 143479: 'SA', 143480: 'TR', 143481: 'AE', 143482: 'HU',\n                    143483: 'CL', 143484: 'NP', 143485: 'PA', 143486: 'LK', 143487: 'RO', 143489: 'CZ', 143491: 'IL',\n                    143492: 'UA', 143493: 'KW', 143494: 'HR', 143495: 'CR', 143496: 'SK', 143497: 'LB', 143498: 'QA',\n                    143499: 'SI', 143501: 'CO', 143502: 'VE', 143503: 'BR', 143504: 'GT', 143505: 'AR', 143506: 'SV',\n                    143507: 'PE', 143508: 'DO', 143509: 'EC', 143510: 'HN', 143511: 'JM', 143512: 'NI', 143513: 'PY',\n                    143514: 'UY', 143515: 'MO', 143516: 'EG', 143517: 'KZ', 143518: 'EE', 143519: 'LV', 143520: 'LT',\n                    143521: 'MT', 143523: 'MD', 143524: 'AM', 143525: 'BW', 143526: 'BG', 143528: 'JO', 143529: 'KE',\n                    143530: 'MK', 143531: 'MG', 143532: 'ML', 143533: 'MU', 143534: 'NE', 143535: 'SN', 143536: 'TN',\n                    143537: 'UG', 143538: 'AI', 143539: 'BS', 143540: 'AG', 143541: 'BB', 143542: 'BM', 143543: 'VG',\n                    143544: 'KY', 143545: 'DM', 143546: 'GD', 143547: 'MS', 143548: 'KN', 143549: 'LC', 143550: 'VC',\n                    143551: 'TT', 143552: 'TC', 143553: 'GY', 143554: 'SR', 143555: 'BZ', 143556: 'BO', 143557: 'CY',\n                    143558: 'IS', 143559: 'BH', 143560: 'BN', 143561: 'NG', 143562: 'OM', 143563: 'DZ', 143564: 'AO',\n                    143565: 'BY', 143566: 'UZ', 143568: 'AZ', 143571: 'YE', 143572: 'TZ', 143573: 'GH', 143575: 'AL',\n                    143576: 'BJ', 143577: 'BT', 143578: 'BF', 143579: 'KH', 143580: 'CV', 143581: 'TD', 143582: 'CG',\n                    143583: 'FJ', 143584: 'GM', 143585: 'GW', 143586: 'KG', 143587: 'LA', 143588: 'LR', 143589: 'MW',\n                    143590: 'MR', 143591: 'FM', 143592: 'MN', 143593: 'MZ', 143594: 'NA', 143595: 'PW', 143597: 'PG',\n                    143598: 'ST', 143599: 'SC', 143600: 'SL', 143601: 'SB', 143602: 'SZ', 143603: 'TJ', 143604: 'TM',\n                    143605: 'ZW', 0: 'WW'},\n    'amazon-store': {\n        'android-all': {\n            'UK': 'GB',\n        }\n    }\n}\nTEST_RESULT = []\nKPI_MAPPING = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\nDEVICE_ID_CODE_MAPPING = {\n    1001: 'android-phone',\n    1002: 'android-tablet',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\nANDROID_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['google-play']\nIOS_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['apple-store']\n\n\ndef sample_date(month, day, date_list):\n    sample_date_list = []\n    month_random_list = np.random.randint(0, len(date_list), month).tolist()\n    for m in month_random_list:\n        sample_date_per_month = []\n        if len(date_list[m][1]) == 1:\n            day_random_list = [0]\n        else:\n            day_random_list = np.random.randint(0, len(date_list[m][1]), day).tolist()\n        for d in day_random_list:\n            sample_date_per_month.append(date_list[m][1][d])\n        sample_date_list.append((date_list[m][0], sample_date_per_month))\n    return sample_date_list\n\n\ndef _merge_dicts(dict1, dict2):\n    dict1.update(dict2)\n    return dict1\n\n\ndef write_test_result(result_df):\n    result_df.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_mapping_0512/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2015, 1, 31)\n    start = datetime.date(2015, 1, 3)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_mapping(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    sample_date_list = date_list\n    for month_day_list_tuple in sample_date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        dump_df = (\n            dump_df\n            .withColumn('device_id', functions.UserDefinedFunction(\n                lambda x: DEVICE_ID_CODE_MAPPING[x])(dump_df['device_id']))\n            .withColumn('store_id', functions.UserDefinedFunction(\n                lambda x: _merge_dicts(IOS_COUNTRY_ID_CODES, ANDROID_COUNTRY_ID_CODES)[x])(dump_df['store_id']))\n            .withColumnRenamed('device_id', 'device_code')\n            .withColumnRenamed('store_id', 'country_code')\n        )\n        for day in month_day_list_tuple[1]:\n            dump_df_date_filtered = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"app_id\", \"device_code\", \"country_code\", \"kpi\", \"estimate\")\n            kpi_list = dump_df_date_filtered.select('kpi').distinct().collect()\n\n            v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                      \"granularity={unified_granularity}/date={unified_date}/\"\n            v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n            v1_df = spark.read.parquet(v1_path_parse)\n            for row in kpi_list:\n                dump_kpi_df = dump_df_date_filtered.filter(\"kpi='{}'\".format(row[\"kpi\"]))\\\n                    .withColumnRenamed('estimate', KPI_MAPPING[row[\"kpi\"]]).drop('kpi')\n                v1_kpi_df = v1_df.filter(\n                    \"{} is not null\".format(KPI_MAPPING[row[\"kpi\"]])).select(\n                        'app_id', 'device_code', 'country_code', KPI_MAPPING[row[\"kpi\"]])\n                subtract_count = v1_kpi_df.subtract(dump_kpi_df).count()\n                subtract_count_reverse = dump_kpi_df.subtract(v1_kpi_df).count()\n                if subtract_count != 0 or subtract_count_reverse != 0:\n                    print \"Accuracy Test Wrong!!! granularity: {} , subtract_count: {}, date: {}, kpi: {}\".format(\n                        _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]])\n                else:\n                    print max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]\n                TEST_RESULT.append((\n                    _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]))\n            print \"date={} test complete!\".format(day)\n\n\nif __name__ == '__main__':\n    granularity_list = [\"weekly\"]\n    for granularity in granularity_list:\n        check_usage_dump_v1_mapping(granularity, get_path_date_list(granularity))\n    print 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200618-093518_1206900944","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions\nCOUNTRY_CODE_MAPPING_BY_MARKET_CODE = {\n    'google-play': {1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US',\n                    11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN',\n                    20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL',\n                    29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU',\n                    38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR',\n                    47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 53: 'KZ', 54: 'PK', 55: 'IQ',\n                    56: 'PE', 57: 'MA', 58: 'BY', 59: 'DZ', 60: 'VE', 61: 'AZ', 62: 'EC', 63: 'JO', 64: 'CR',\n                    65: 'LB', 66: 'BD', 67: 'GT', 68: 'RS', 69: 'DO', 70: 'IR', 71: 'OM', 72: 'BO', 73: 'QA',\n                    74: 'NG', 75: 'SV', 76: 'KH', 77: 'PA', 78: 'LT', 79: 'TN', 80: 'HR', 81: 'JM', 82: 'LK',\n                    83: 'HN', 84: 'PR', 85: 'UY', 86: 'LV', 87: 'BA', 88: 'KG', 89: 'PY', 90: 'MD', 91: 'NP',\n                    92: 'TZ', 93: 'BH', 94: 'GH', 95: 'KE', 96: 'SI', 97: 'AM', 98: 'UZ', 99: 'TT', 100: 'MK',\n                    101: 'YE', 102: 'MO', 103: 'LU', 1000: 'WW'},\n    'apple-store': {143441: 'US', 143442: 'FR', 143443: 'DE', 143444: 'GB', 143445: 'AT', 143446: 'BE', 143447: 'FI',\n                    143448: 'GR', 143449: 'IE', 143450: 'IT', 143451: 'LU', 143452: 'NL', 143453: 'PT', 143454: 'ES',\n                    143455: 'CA', 143456: 'SE', 143457: 'NO', 143458: 'DK', 143459: 'CH', 143460: 'AU', 143461: 'NZ',\n                    143462: 'JP', 143463: 'HK', 143464: 'SG', 143465: 'CN', 143466: 'KR', 143467: 'IN', 143468: 'MX',\n                    143469: 'RU', 143470: 'TW', 143471: 'VN', 143472: 'ZA', 143473: 'MY', 143474: 'PH', 143475: 'TH',\n                    143476: 'ID', 143477: 'PK', 143478: 'PL', 143479: 'SA', 143480: 'TR', 143481: 'AE', 143482: 'HU',\n                    143483: 'CL', 143484: 'NP', 143485: 'PA', 143486: 'LK', 143487: 'RO', 143489: 'CZ', 143491: 'IL',\n                    143492: 'UA', 143493: 'KW', 143494: 'HR', 143495: 'CR', 143496: 'SK', 143497: 'LB', 143498: 'QA',\n                    143499: 'SI', 143501: 'CO', 143502: 'VE', 143503: 'BR', 143504: 'GT', 143505: 'AR', 143506: 'SV',\n                    143507: 'PE', 143508: 'DO', 143509: 'EC', 143510: 'HN', 143511: 'JM', 143512: 'NI', 143513: 'PY',\n                    143514: 'UY', 143515: 'MO', 143516: 'EG', 143517: 'KZ', 143518: 'EE', 143519: 'LV', 143520: 'LT',\n                    143521: 'MT', 143523: 'MD', 143524: 'AM', 143525: 'BW', 143526: 'BG', 143528: 'JO', 143529: 'KE',\n                    143530: 'MK', 143531: 'MG', 143532: 'ML', 143533: 'MU', 143534: 'NE', 143535: 'SN', 143536: 'TN',\n                    143537: 'UG', 143538: 'AI', 143539: 'BS', 143540: 'AG', 143541: 'BB', 143542: 'BM', 143543: 'VG',\n                    143544: 'KY', 143545: 'DM', 143546: 'GD', 143547: 'MS', 143548: 'KN', 143549: 'LC', 143550: 'VC',\n                    143551: 'TT', 143552: 'TC', 143553: 'GY', 143554: 'SR', 143555: 'BZ', 143556: 'BO', 143557: 'CY',\n                    143558: 'IS', 143559: 'BH', 143560: 'BN', 143561: 'NG', 143562: 'OM', 143563: 'DZ', 143564: 'AO',\n                    143565: 'BY', 143566: 'UZ', 143568: 'AZ', 143571: 'YE', 143572: 'TZ', 143573: 'GH', 143575: 'AL',\n                    143576: 'BJ', 143577: 'BT', 143578: 'BF', 143579: 'KH', 143580: 'CV', 143581: 'TD', 143582: 'CG',\n                    143583: 'FJ', 143584: 'GM', 143585: 'GW', 143586: 'KG', 143587: 'LA', 143588: 'LR', 143589: 'MW',\n                    143590: 'MR', 143591: 'FM', 143592: 'MN', 143593: 'MZ', 143594: 'NA', 143595: 'PW', 143597: 'PG',\n                    143598: 'ST', 143599: 'SC', 143600: 'SL', 143601: 'SB', 143602: 'SZ', 143603: 'TJ', 143604: 'TM',\n                    143605: 'ZW', 0: 'WW'},\n    'amazon-store': {\n        'android-all': {\n            'UK': 'GB',\n        }\n    }\n}\nTEST_RESULT = []\nKPI_MAPPING = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\nDEVICE_ID_CODE_MAPPING = {\n    1001: 'android-phone',\n    1002: 'android-tablet',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\nANDROID_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['google-play']\nIOS_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['apple-store']\ndump_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=weekly/month=2015-01/\").filter(\"date='2015-01-03'\")\nv1_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=weekly/date=2015-01-03/\")\ndump_df = (\n    dump_df\n    .withColumn('device_id', functions.UserDefinedFunction(\n        lambda x: DEVICE_ID_CODE_MAPPING[x])(dump_df['device_id']))\n    .withColumn('store_id', functions.UserDefinedFunction(\n        lambda x: _merge_dicts(IOS_COUNTRY_ID_CODES, ANDROID_COUNTRY_ID_CODES)[x])(dump_df['store_id']))\n    .withColumnRenamed('device_id', 'device_code')\n    .withColumnRenamed('store_id', 'country_code')\n)\ndump_df = dump_df.select(\"app_id\", \"device_code\", \"country_code\", \"kpi\", \"estimate\")\ndump_kpi_df = dump_df.filter(\"kpi='{}'\".format(3))\\\n                    .withColumnRenamed('estimate', KPI_MAPPING[3]).drop('kpi')\nv1_kpi_df = v1_df.filter(\n    \"{} is not null\".format(KPI_MAPPING[3])).select(\n        'app_id', 'device_code', 'country_code', KPI_MAPPING[3])\nv1_kpi_df.subtract(dump_kpi_df).show()\ndump_kpi_df.subtract(v1_kpi_df).show()"]},{"cell_type":"code","execution_count":0,"id":"20200618-093953_1766268514","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}