{"cells":[{"cell_type":"code","execution_count":0,"id":"20200514-034812_1403692612","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_count(_granularity, date_list):\n    unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                   'usage.basic-kpi.v1/fact/granularity={v1_granularity}/date={v1_date}/'\n    unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/' \\\n                   'usage.basic-kpi.v5/fact/granularity={v3_granularity}/date={v3_date}/'\n    for date in date_list:\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n\n        unified_v1 = unified_v1.withColumn(\n            'device_code', functions.UserDefinedFunction(lambda x: device_code_agg_mapping[x])(unified_v1['device_code']))\n        unified_v1_agg_count = unified_v1.select('app_id', 'device_code', 'country_code').distinct().count()\n\n        unified_v1_total_count = unified_v1_count + unified_v1_agg_count\n\n        unified_v3_count = spark.read.parquet(unified_v3_path_parse).count()\n\n        if unified_v1_total_count != unified_v3_count:\n            print 'Count Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                    unified_v1_total_count, unified_v3_count, date[1])\n        else:\n            print 'date: {} test PASS, unified_v1 data: {}, unified_v3 data: {}'.format(\n            date[1], unified_v1_total_count, unified_v3_count)\n        test_result.append((_granularity, unified_v1_total_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0514/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_count(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200514-034855_1977969751","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0514/daily/"]},{"cell_type":"code","execution_count":0,"id":"20200514-053946_387082451","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0514/daily/')\nresult = df.distinct().orderBy('date').collect()\nfor row in result:\n    print row['date'], '\\t', row['raw_count'], '\\t', row['unified_count']"]},{"cell_type":"code","execution_count":0,"id":"20200514-054354_692668873","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2019, 10, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact' \\\n                          '/granularity={v3_granularity}/date={v3_date}/'\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n        unified_v3_count = spark.read.format(\"delta\").load(unified_v3_path_parse).count()\n\n        if unified_v1_count != unified_v3_count:\n            print 'Completeness Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        else:\n            print 'Completeness Test Pass! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        test_result.append((_granularity, unified_v1_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0609/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200609-034553_1220933966","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact' \\\n                          '/granularity={v3_granularity}/date={v3_date}/'\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n        unified_v3_count = spark.read.format(\"delta\").load(unified_v3_path_parse).count()\n\n        if unified_v1_count != unified_v3_count:\n            print 'Completeness Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        else:\n            print 'Completeness Test Pass! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        test_result.append((_granularity, unified_v1_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0609/weekly/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"weekly\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200609-091003_274864968","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2018, 3, 31)\n    start = datetime.date(2018, 2, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact' \\\n                          '/granularity={v3_granularity}/date={v3_date}/'\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n        unified_v3_count = spark.read.format(\"delta\").load(unified_v3_path_parse).count()\n\n        if unified_v1_count != unified_v3_count:\n            print 'Completeness Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        else:\n            print 'Completeness Test Pass! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        test_result.append((_granularity, unified_v1_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0616/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200616-075136_89118198","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact' \\\n                          '/granularity={v3_granularity}/date={v3_date}/'\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n        unified_v3_count = spark.read.format(\"delta\").load(unified_v3_path_parse).count()\n\n        if unified_v1_count != unified_v3_count:\n            print 'Completeness Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        else:\n            print 'Completeness Test Pass! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        test_result.append((_granularity, unified_v1_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0616/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200618-021934_930383977","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0616/"]},{"cell_type":"code","execution_count":0,"id":"20200618-062850_806773321","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2018, 1, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact' \\\n                          '/granularity={v3_granularity}/date={v3_date}/'\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n        unified_v3_count = spark.read.format(\"delta\").load(unified_v3_path_parse).count()\n\n        if unified_v1_count != unified_v3_count:\n            print 'Completeness Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        else:\n            print 'Completeness Test Pass! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        test_result.append((_granularity, unified_v1_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0616/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200618-070121_633256814","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}