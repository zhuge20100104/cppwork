{"cells":[{"cell_type":"code","execution_count":0,"id":"20200923-101619_1475208431","metadata":{},"outputs":[],"source":["\n\ndate='2020-07-01'\n\ndef poc(date):\n    new_app_est_df = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=\"+date)\n    new_app_est_df.createOrReplaceTempView(\"new_app_est\")\n    app_est_category_rank_df = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=\"+date)\n    date = \"'\"+date+\"'\"\n    app_est_category_rank_df.createOrReplaceTempView(\"app_est_category_rank\")\n    app_est_df = spark.sql(\"\"\"\n        select new_app_est.app_id,\n               new_app_est.country_code,\n               new_app_est.free_app_download,\n               new_app_est.paid_app_download,\n               new_app_est.revenue,\n               new_app_est.revenue_iap,\n               new_app_est.revenue_non_iap,\n               new_app_est.granularity,\n               new_app_est.date,\n               new_app_est.device_code,\n               category_id\n        from app_est_category_rank\n        left join new_app_est\n        on new_app_est.app_id = app_est_category_rank.app_id and \n        new_app_est.country_code = app_est_category_rank.country_code and\n        new_app_est.granularity = app_est_category_rank.granularity and \n        new_app_est.date =  app_est_category_rank.date and \n        new_app_est.device_code = app_est_category_rank.device_code \n               \"\"\")\n    \n    \n    app_est_df.createOrReplaceTempView(\"app_est\")\n    \n   \n    dna_mapping_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.mapping-log.v1/dimension\").select(\"publisher_id\", \"app_id\", \"end_date\",\"start_date\").where(\"update_date ='2020-09-21'\")\n    dna_mapping_df.createOrReplaceTempView(\"dna_mapping\")\n    \n    app_est_publisher_dna = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher-dna-log.v1/fact/\").where(\" date = \"+ date)\n    app_est_publisher_dna.createOrReplaceTempView(\"app_est_publisher\")\n    \n    app_est_category_publisher_df = spark.sql(\"\"\"\n        select free_app_download,\n              paid_app_download,\n              revenue,\n              revenue_iap,\n              revenue_non_iap,\n              country_code,\n              category_id,\n              granularity,\n              date,\n              device_code, \n              publisher_id,\n              app_est.app_id\n        from app_est \n        left join dna_mapping on app_est.app_id= dna_mapping.app_id\n        where date >=  start_date and (end_date is null or date <=  end_date)\n    \"\"\")\n    app_est_category_publisher_df.createOrReplaceTempView(\"app_est_category_publisher\")\n    \n    \n    \n    \n    \n    app_sum_est_category_publisher_df = spark.sql(\"\"\"\n        select sum(free_app_download) as free_app_download,\n              sum(paid_app_download) as paid_app_download,\n              sum(revenue) as revenue,\n              sum(revenue_iap) as revenue_iap,\n              sum(revenue_non_iap) as revenue_non_iap,\n              country_code,\n              category_id,\n              granularity,\n              date,\n              device_code, \n              publisher_id\n        from app_est_category_publisher \n        group by date,granularity,device_code,category_id,country_code,publisher_id\n    \"\"\")\n    app_sum_est_category_publisher_df.createOrReplaceTempView(\"app_sum_est_category_publisher_df\")\n    \n    \n    app_sum_est_category_publisher_df.cache()\n    app_est_publisher_df = spark.sql(\"\"\"\n        select est_free_app_download as free_app_download, \n               est_paid_app_download as paid_app_download, \n               est_revenue as revenue,\n               est_revenue_iap as revenue_iap,\n               est_revenue_non_iap as revenue_non_iap,\n               country_code,\n               category_id,\n               granularity,\n               date,\n               device_code,\n               publisher_id \n        from app_est_publisher\n        where granularity = 'daily'\n    \"\"\")\n    app_est_publisher_df.cache()\n    app_est_publisher_df.createOrReplaceTempView(\"app_est_publisher_df\")\n    \n    \n    \n    app_est_category_publisher_count = app_sum_est_category_publisher_df.count()\n    app_est_category_publisher_diff = app_sum_est_category_publisher_df.exceptAll(app_est_publisher_df)\n    app_est_category_publisher_diff_count = app_est_category_publisher_diff.count()\n    \n    app_est_publisher_count = app_est_publisher_df.count()\n    app_est_publisher_diff = app_est_publisher_df.exceptAll(app_sum_est_category_publisher_df)\n    app_est_publisher_diff_count = app_est_publisher_diff.count()\n    print(date)\n    print(\"app_est_category_publisher_count:\",app_est_category_publisher_count)\n    print(\"app_est_category_publisher_diff_count:\",app_est_category_publisher_diff_count)\n    app_different_rate = 100*app_est_category_publisher_diff_count/app_est_category_publisher_count\n    print(\"app_different_rate:\"+str(app_different_rate)+'%')\n    \n    print(\"app_est_publisher_count:\",app_est_publisher_count)\n    print(\"app_est_publisher_diff_count:\",app_est_publisher_diff_count)\n    publisher_different_rate = 100*app_est_publisher_diff_count/app_est_publisher_count\n    print(\"publisher_different_rate:\"+str(publisher_different_rate)+'%')\n    \n    \n    print(\"------------------------------------------------------\")\n    print(\"Top 1000 pyblisher order by free_app_download\")\n    app_top_free_download_df = spark.sql(\"\"\"\n        select * from app_sum_est_category_publisher_df order by free_app_download desc,publisher_id  limit 1000 \n    \"\"\")\n    publisher_top_free_download_df = spark.sql(\"\"\"\n        select * from app_est_publisher_df order by free_app_download desc,publisher_id limit 1000\n    \"\"\")\n    \n    app_top_free_download_diff_rate = 100*app_top_free_download_df.exceptAll(publisher_top_free_download_df).count()/1000\n    print(\"app_top_free_download_diff_rate:\"+str(app_top_free_download_diff_rate)+\"%\")\n    \n    publisher_top_free_download_rate = 100*publisher_top_free_download_df.exceptAll(app_top_free_download_df).count()/1000\n    print(\"publisher_top_free_download_rate:\"+str(publisher_top_free_download_rate)+\"%\")\n    \n    \n    print(\"------------------------------------------------------\")\n    \n    print(\"Top 1000 pyblisher order by paid_app_download\")\n    app_top_paid_download_df = spark.sql(\"\"\"\n        select * from app_sum_est_category_publisher_df order by paid_app_download desc,publisher_id limit 1000\n    \"\"\")\n    publisher_top_paid_download_df = spark.sql(\"\"\"\n        select * from app_est_publisher_df order by paid_app_download desc,publisher_id limit 1000\n    \"\"\")\n    \n    app_top_paid_download_diff_rate = 100*app_top_paid_download_df.exceptAll(publisher_top_paid_download_df).count()/1000\n    print(\"app_top_paid_download_diff_rate:\"+str(app_top_paid_download_diff_rate)+\"%\")\n    \n    publisher_top_paid_download_rate = 100*publisher_top_paid_download_df.exceptAll(app_top_paid_download_df).count()/1000\n    print(\"publisher_top_paid_download_rate:\"+str(publisher_top_paid_download_rate)+\"%\")\n    print(\"------------------------------------------------------\")\n    \n    \n    print(\"Top 1000 pyblisher order by revenue\")\n    app_top_revenue_df = spark.sql(\"\"\"\n        select * from app_sum_est_category_publisher_df order by revenue desc,publisher_id limit 1000\n    \"\"\")\n    publisher_top_revenue_df = spark.sql(\"\"\"\n        select * from app_est_publisher_df order by revenue desc,publisher_id limit 1000\n    \"\"\")\n    \n    app_top_revenue_diff_rate = 100*app_top_revenue_df.exceptAll(publisher_top_revenue_df).count()/1000\n    print(\"app_top_revenue_diff_rate:\"+str(app_top_revenue_diff_rate)+\"%\")\n    \n    publisher_top_revenue_rate = 100*publisher_top_revenue_df.exceptAll(app_top_revenue_df).count()/1000\n    print(\"publisher_top_revenue_rate:\"+str(publisher_top_revenue_rate)+\"%\")\n    print(\"=========================================================\")\n    app_est_publisher_df.unpersist()\n    app_sum_est_category_publisher_df.unpersist()\n    \ndates = [\"2017-07-01\", \"2017-07-02\", \"2017-07-03\", \"2017-07-04\", \"2017-07-05\", \"2017-07-06\", \"2017-07-07\", \"2017-07-08\", \"2017-07-09\", \"2017-07-10\",\n \"2017-07-11\", \"2017-07-12\", \"2017-07-13\", \"2017-07-14\", \"2017-07-15\", \"2017-07-16\", \"2017-07-17\", \"2017-07-18\", \"2017-07-19\", \"2017-07-20\",\n \"2017-07-21\", \"2017-07-22\", \"2017-07-23\", \"2017-07-24\", \"2017-07-25\", \"2017-07-26\", \"2017-07-27\", \"2017-07-28\", \"2017-07-29\", \"2017-07-30\",\n \"2017-07-31\"\n        ]\nfor d in dates:\n    poc(d)\n"]},{"cell_type":"code","execution_count":0,"id":"20200923-101741_1604052169","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}