{"cells":[{"cell_type":"code","execution_count":0,"id":"20201104-091314_502091228","metadata":{},"outputs":[],"source":["# %sh\nraw_data_path = s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type=DAY/date=2018-01-04/platform=1/\nunified_data_path_legacy = s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/\nraw_domain_path = s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_DOMAIN_BASIC_METRICS/version=v1.0.0/\naws s3 ls  s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/"]},{"cell_type":"code","execution_count":0,"id":"20201105-114045_999223898","metadata":{},"outputs":[],"source":["\n\nrank_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=monthly/date=2020-09-30/\"\nspark.read.format(\"parquet\").load(rank_path).createOrReplaceTempView(\"rank_category\")\nspark.sql(\"select * from rank_category where app_id = 1477487725 limit 10\").show(10, False)\nspark.sql(\"select distinct app_id, category_id from rank_category where app_id in (select app_id from {app_view}) and category_id = 100004\".format(app_view=app_view)).show(100, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20201104-121045_1883843894","metadata":{},"outputs":[],"source":["\npath = \"s3://b2c-prod-data-pipeline-unified-store-free/unified/store.app.v1/dimension/\"\nspark.read.format(\"parquet\").load(path).createOrReplaceTempView(\"test\")\nspark.sql(\"select last_updated, release_date,current_release_date, category_id  from test where id in  (1517006344)\").show(10, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20201106-022935_1365146337","metadata":{},"outputs":[],"source":["\nselect * from ADL_USAGE_PAID.FACT_USAGE_CATEGORY_V1_CLUSTER_BY_DATE_COUNTRY_CODE\nwhere product_type_code='app' \nand granularity_code='monthly' and date = '2020-09-30' \nand product_key = 20600010054776\nand country_code = 'JP';"]},{"cell_type":"code","execution_count":0,"id":"20200927-062841_140991717","metadata":{},"outputs":[],"source":["\nunified_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=app/granularity_code=monthly/date=2020-09-30/\"\ncategory_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.category.v6/dimension/product_type_code=app/granularity_code=monthly/date=2020-09-30/\"\n# spark.read.format(\"delta\").load(unified_path).createOrReplaceTempView(\"test_unified\")\nspark.read.format(\"delta\").load(category_path).createOrReplaceTempView(\"test_category\")\n# spark.sql(\"select count(distinct product_key) as basic_count from test_unified \").show()\nspark.sql(\"select count(1) as category_count from test_category where country_code <> 'ROW'\").show()\n# spark.sql(\"select count(1) as basic_count from test_unified \").show()\n# spark.sql(\"select count(1) as category_null from test_category where category_key is null and unified_category_key is null\").show()\n# spark.sql(\"select * from test_category where product_key = 20600010054776 and country_code = 'JP'\").show(10, False)\n# spark.sql(\"select sum(est_active_users), sum(est_population) from test_unified where product_type_code='app' and granularity_code='monthly' and country_code <> 'ROW'\").show()\n# spark.sql(\"select count(distinct product_key, device_code, country_code, granularity_code, date) as basic_distinct_count from test_unified\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200928-070101_176167197","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom pyspark.sql import Row\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\nfrom pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType, ShortType\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\ndate_count_sql =  \"\"\"select  sum(count) from plproxy.execute_select($proxy$\nSELECT count(1) as count\nFROM mu.category_{granularity}_{device} where \ndate = '{date}' and category_id = {legacy_category_id} $proxy$)\n t (count bigint) ;\n\"\"\"\n\n\n# granularity_list = ['daily', 'weekly', 'monthly']\ngranularity_list = ['weekly']\ndevice_list = [1001, 1002, 2001, 2002]\nios_device_list = [ 2001, 2002]\ngp_device_list = [1001, 1002]\ndevice_code_list = ['android-phone', 'android-tablet', 'ios-phone', 'ios-tablet']\n# device_code_list = ['ios-tablet']\ndevice_mapping = {1001:'android-phone', 1002:'android-tablet', 2001:'ios-phone', 2002:'ios-tablet'}\n\n\nlegacy_ios_category_list = [6016, 6017, 6018, 6020, 6021, 6022, 6023, 6024, 36, 6001, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 100, 360, 361, 362, 6000, 363, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015]\n\n# legacy_ios_category_list = [6016, 6017, 6014]\n\nios_category_mapping = {6016: 800022, 6017: 800021, 6018: 800019, 6020: 800028, 6021: 800030, 6022: 800019, 6023: 800024, 6024: 800033, 36: 800000, 6001: 800038, 7001: 800002, 7002: 800003, 7003: 800002, 7004: 800004, 7005: 800006, 7006: 800006, 7007: 800006, 7008: 800007, 7009: 800008, 7010: 800008, 7011: 800010, 7012: 800005, 7013: 800011, 7014: 800012, 7015: 800013, 7016: 800014, 7017: 800012, 7018: 800015, 7019: 800016, 100: 800018, 360: 800041, 361: 800042, 362: 800043, 6000: 800020, 363: 800044, 6002: 800036, 6003: 800037, 6004: 800035, 6005: 800034, 6006: 800019, 6007: 800032, 6008: 800031, 6009: 800030, 6010: 800037, 6011: 800029, 6012: 800026, 6013: 800025, 6014: 800001, 6015: 800023}\n\nlegacy_gp_category_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75]\n\ngp_category_mapping = {1: 800000, 2: 800001, 3: 800002, 4: 800005, 5: 800006, 6: 800005, 7: 800009, 8: 800011, 9: 800014, 10: 800017, 11: 800018, 12: 800019, 13: 800020, 14: 800019, 15: 800034, 16: 800021, 17: 800022, 18: 800023, 19: 800025, 20: 800019, 21: 800026, 22: 800027, 23: 800031, 24: 800028, 25: 800029, 26: 800030, 27: 800036, 28: 800031, 29: 800032, 30: 800033, 31: 800034, 32: 800035, 33: 800036, 34: 800037, 35: 800037, 36: 800038, 37: 800040, 38: 800002, 39: 800003, 40: 800016, 41: 800002, 42: 800004, 43: 800006, 44: 800006, 46: 800007, 47: 800008, 48: 800010, 49: 800005, 51: 800012, 52: 800013, 54: 800012, 55: 800015, 56: 800041, 57: 800042, 58: 800043, 59: 800044, 60: 800041, 61: 800041, 62: 800041, 63: 800041, 64: 800041, 65: 800041, 66: 800039, 67: 800032, 68: 800026, 69: 800033, 70: 800026, 71: 800026, 72: 800022, 73: 800024, 75: 800025}\n\nlegacy_to_individual_ios = {36: 100000, 6014: 100001, 7001: 100002, 7002: 100003, 7003: 100004, 7004: 100005, 7005: 100006, 7006: 100007, 7007: 100008, 7008: 100009, 7009: 100010, 7010: 100011, 7011: 100012, 7012: 100013, 7013: 100014, 7014: 100015, 7015: 100016, 7016: 100017, 7017: 100018, 7018: 100019, 7019: 100020, 100: 100021, 6018: 100022, 6000: 100023, 6022: 100024, 6017: 100025, 6016: 100026, 6015: 100027, 6023: 100028, 6013: 100029, 360: 100030, 361: 100031, 362: 100032, 363: 100033, 6012: 100034, 6021: 100035, 6020: 100064, 6011: 100065, 6010: 100066, 6009: 100067, 6008: 100068, 6007: 100069, 6006: 100070, 6024: 100071, 6005: 100072, 6004: 100073, 6003: 100075, 6002: 100076, 6001: 100077}\n\nlegacy_to_individual_gp = {1: 400000, 2: 400001, 38: 400002, 39: 400003, 41: 400004, 42: 400005, 43: 400006, 44: 400007, 6: 400008, 46: 400009, 47: 400010, 7: 400011, 48: 400012, 49: 400013, 8: 400014, 51: 400015, 52: 400016, 9: 400017, 54: 400018, 55: 400019, 10: 400020, 40: 400021, 3: 400022, 4: 400023, 5: 400024, 11: 400025, 66: 400026, 67: 400027, 69: 400028, 70: 400029, 12: 400030, 13: 400031, 14: 400032, 15: 400033, 71: 400034, 16: 400035, 17: 400036, 72: 400037, 18: 400038, 73: 400039, 19: 400040, 68: 400041, 20: 400042, 21: 400043, 22: 400044, 34: 400045, 24: 400046, 25: 400047, 26: 400048, 75: 400049, 27: 400050, 28: 400051, 29: 400052, 30: 400053, 31: 400054, 32: 400055, 33: 400056, 35: 400057, 23: 400058, 36: 400059, 37: 400060, 56: 400061, 60: 400062, 57: 400063, 58: 400064, 59: 400065, 61: 400066, 62: 400067, 63: 400068, 64: 400069, 65: 400070}\n\n\ndef get_plproxy_count(sql_str=date_count_sql):\n    for device in gp_device_list:\n        for granularity in granularity_list:\n            for date in DATE_GRANULARITY_MAPPINGLIST[granularity]:\n                for category in legacy_gp_category_list:\n                    result = query(PLPROXY_DSN, sql_str.format(granularity=granularity, date=date, device=device, legacy_category_id=category))\n                    print device_mapping[device], granularity, date, category, result[0][0]\n\ndef get_unified_result():\n    category_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.category.v6/dimension/product_type_code=app/granularity_code={granularity}/date={date}/\"\n    for device in gp_device_list:\n        for granularity in granularity_list:\n            for date in DATE_GRANULARITY_MAPPINGLIST[granularity]:\n                for category in legacy_gp_category_list:\n                    spark.read.format(\"delta\").load(category_path.format(granularity=granularity, date=date)).createOrReplaceTempView(\"test_category\")\n                    df = spark.sql(\"select count(1) as count from test_category where device_code = '{device}' and category_key = {category} and country_code <> 'ROW'\".format(device=device_mapping[device], category=legacy_to_individual_gp[category])).collect()\n                    # print df[0][0]\n                    print device_mapping[device], granularity, date, category, df[0][0]\n                \ndef get_unified_tmp(date, device_code, individual_category_id):\n    category_path_tmp = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.category.v6/dimension/product_type_code=app/granularity_code=monthly/date={date}/\".format(date=date)\n    spark.read.format(\"delta\").load(category_path_tmp).createOrReplaceTempView(\"test_category_tmp\")\n    spark.sql(\"select distinct product_key as app_id from test_category_tmp where device_code = '{device_code}' and category_key = {individual_category_id} and country_code <> 'ROW'\".format(device_code=device_code, individual_category_id=individual_category_id)).createOrReplaceTempView(\"unified_app\")\n    # spark.sql(\"select distinct country_code from test_category_tmp where device_code = 'ios-tablet'   and product_key = 284882215\").show(100, False)\n            \n            \ndef get_plproxy_tmp(date, device_id, legacy_category_id):\n    date_count_sql_tmp =  \"\"\"select  distinct app_id from plproxy.execute_select($proxy$\nSELECT distinct app_id as app_id\nFROM mu.category_monthly_{device_id} where \ndate = '{date}' and category_id in ({legacy_category_id}) $proxy$)\n t (app_id bigint);\n\"\"\".format(device_id=device_id, date=date, legacy_category_id=legacy_category_id)\n    print date_count_sql_tmp\n    result = query(PLPROXY_DSN, date_count_sql_tmp)\n    df_data = [Row(app_id=r[0]) for r in result]\n    _schema =StructType([StructField(\"app_id\", LongType(), False)])\n    df_plproxy = spark.createDataFrame(data=df_data, schema=_schema)\n    return df_plproxy\n    # print result[0][0]\n    \ndef get_main_category(app_view):\n    path = \"s3://b2c-prod-data-pipeline-unified-store-free/unified/store.app.v1/dimension/\"\n    spark.read.format(\"parquet\").load(path).createOrReplaceTempView(\"test\")\n    spark.sql(\"select id, last_updated, release_date,current_release_date, category_id  from test where id in  (select app_id from {app_view})\".format(app_view=app_view)).show(100, False)\n    \ndef get_rank_category(app_view):\n    rank_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=monthly/date=2020-09-30/\"\n    spark.read.format(\"parquet\").load(rank_path).createOrReplaceTempView(\"rank_category\")\n    spark.sql(\"select * from rank_category where app_id = 1477487725 limit 10\").show(10, False)\n    spark.sql(\"select distinct app_id, category_id from rank_category where app_id in (select app_id from {app_view}) and category_id = 100004\".format(app_view=app_view)).show(100, False)\n    \ndef compare():\n    date = '2020-09-30'\n    device_id = 2001\n    device_code = 'ios-phone'\n    legacy_category_id = 6020\n    individual_category_id = 100064\n    df_plproxy = get_plproxy_tmp(date, device_id, legacy_category_id)\n    get_unified_tmp(date, device_code, individual_category_id)\n    df_plproxy.createOrReplaceTempView(\"plproxy_app\")\n    spark.sql(\"select app_id from plproxy_app except select app_id from unified_app\").createOrReplaceTempView(\"plproxy_diff_unified\")\n    spark.sql(\"select app_id from unified_app except select app_id from plproxy_app \").createOrReplaceTempView(\"unifid_diff_plproxy\")\n    \n    spark.sql(\"select app_id as plproxy_diff_unified from plproxy_diff_unified\").show(100, False)\n    spark.sql(\"select app_id as unifid_diff_plproxy from unifid_diff_plproxy\").show(100, False)\n    \n    get_main_category(\"plproxy_diff_unified\")\n    get_main_category(\"unifid_diff_plproxy\")\n    \n    \n    \n# get_rank_category(\"plproxy_diff_unified\")\nget_unified_result()\n\n    "]},{"cell_type":"code","execution_count":0,"id":"20200928-020925_1539775034","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\n# begin_date = datetime(2019, 1, 1)\n# end_date = datetime(2019, 1, 31)\nbegin_date = datetime(2013, 1, 1)\nend_date = datetime(2013, 12, 31)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\nDATE_GRANULARITY_MAPPINGLIST[\"monthly\"].reverse()\nDATE_GRANULARITY_MAPPINGLIST[\"weekly\"].reverse()\nDATE_GRANULARITY_MAPPINGLIST[\"daily\"].reverse()\n\nprint DATE_GRANULARITY_MAPPINGLIST[\"monthly\"]\nprint DATE_GRANULARITY_MAPPINGLIST[\"weekly\"]\nprint DATE_GRANULARITY_MAPPINGLIST[\"daily\"]"]},{"cell_type":"code","execution_count":0,"id":"20201110-054943_1526053806","metadata":{},"outputs":[],"source":["\n# est_average_active_days * est_active_users as est_total_active_days,\n# est_average_bytes_per_session * est_active_users * est_average_session_count_per_user as est_total_bytes,\n# est_active_users * est_average_session_count_per_user / est_share_of_session_product_in_main_category AS est_total_session_count_of_main_category,\n# est_active_users * est_average_session_count_per_user * est_average_bytes_per_session / est_share_of_bytes_product_in_main_category AS est_total_bytes_of_main_category,\n# est_active_users * est_average_time_per_user_milliseconds / est_share_of_time_product_in_main_category AS est_total_time_milliseconds_of_main_category,\n# est_average_bytes_per_session * est_active_users * est_average_session_count_per_user * est_percentage_of_bytes_wifi_in_total * 1024 * 1024 * 1024 * 1024 AS est_wifi_bytes\n\nusage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=app/\"\nusage_basic_before_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\"\ngranularity_list = ['monthly']\ndef test_bytes():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_after = spark.sql(\"\"\"select count(1) from after_trans\n            where (est_total_bytes = 0 and est_average_bytes_per_session <> 0 and est_average_session_count_per_user <> 0)\n            or (est_total_session_count_of_main_category = 0 and est_share_of_session_product_in_main_category <> 0) \n            or (est_total_bytes_of_main_category = 0 and est_share_of_bytes_product_in_main_category <> 0)\n            or (est_total_time_milliseconds_of_main_category = 0 and est_share_of_time_product_in_main_category <> 0)\n            or (est_wifi_bytes = 0 and est_percentage_of_bytes_wifi_in_total <> 0 and est_average_session_count_per_user <> 0 and est_average_bytes_per_session <> 0)\"\"\").collect()\n            if df_after[0][0] != 0:\n                print granularity, date, \"count\", df_after[0][0]\n            else:\n                print granularity, date, \"PASS\"\n                \ndef test_bytes_tmp():\n    # monthly 2019-06-30 count 9646\n    # monthly 2019-05-31 count 9061\n    filter_str_after = \"date between '2019-01-31' and 2019-12-31'' and granularity_code = 'monthly' and product_key = 20600000002550 and device_code = 'android-phone' and country_code = 'US'\"\n    spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n    # spark.sql(\"select count(1) from after_trans where est_total_bytes = 0 and est_average_bytes_per_session <> 0 and est_average_session_count_per_user <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_total_session_count_of_main_category = 0 and est_share_of_session_product_in_main_category <> 0\").show(10, False)\n    spark.sql(\"select est_total_bytes_of_main_category, est_share_of_bytes_product_in_main_category, est_average_bytes_per_session, est_average_session_count_per_user from after_trans\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_total_time_milliseconds_of_main_category = 0 and est_share_of_time_product_in_main_category <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_wifi_bytes = 0 and est_percentage_of_bytes_wifi_in_total <> 0 and est_average_session_count_per_user <> 0 and est_average_bytes_per_session <> 0\").show(10, False)\n    \n    \ndef test_other_metrics():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            s = \"\"\"select count(1) from after_trans\n            where (est_install_penetration = 0  and est_open_rate <> 0)\n            or (est_install_base = 0 and est_open_rate <> 0)\n            or (est_population = 0 and est_usage_penetration <> 0)\n            or (est_total_time_milliseconds = 0  and est_average_session_count_per_user <> 0)\"\"\"\n            df_after = spark.sql(s).collect()\n            if df_after[0][0] != 0:\n                print granularity, date, \"count\", df_after[0][0]\n            else:\n                print granularity, date, \"PASS\"\n\ndef test_other_metrics_tmp():\n    filter_str_after = \"date = '2019-08-31' and granularity_code = 'monthly'\"\n    filter_str = \"date ='2019-08-31' and granularity = 'monthly'\"\n    spark.read.format(\"delta\").load(usage_basic_before_transform).filter(filter_str).createOrReplaceTempView(\"before_trans\")\n    spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n    # spark.sql(\"select count(1) from after_trans where est_install_penetration = 0  and est_open_rate <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_install_base = 0 and est_open_rate <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_population = 0 and est_usage_penetration <> 0\").show(10, False)\n    # spark.sql(\"select count(1) from after_trans where est_total_time_milliseconds = 0  and est_average_session_count_per_user <> 0\").show(10, False)\n    spark.sql(\"select count(1) from before_trans where est_total_time = 0 and est_average_session_per_user <> 0\").show(10, False)\n    spark.sql(\"select app_id, date, granularity, country_code, est_total_time, est_average_session_per_user, est_average_time_per_user, est_average_session_duration from before_trans where est_total_time = 0 and est_average_session_per_user <> 0 order by app_id desc limit 10\").show(10, False)\n    # spark.sql(\"select count(1) from before_trans\").show(1, False)\n\ntest_bytes_tmp()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20201111-034321_65017582","metadata":{},"outputs":[],"source":["\nusage_raw_path = \"s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type=MONTH/date=2020-08-31/\"\nunified_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=app/\"\nspark.read.format(\"delta\").load(unified_path).createOrReplaceTempView(\"test_raw\")\nspark.sql(\"select * from test_raw limit 1\").show(10, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20201109-062143_1191192879","metadata":{},"outputs":[],"source":["\ngranularity_list = [\"monthly\", \"weekly\", \"daily\"]\ngranularity_mapping = {\"weekly\":\"WEEK\", \"daily\":\"DAY\", \"monthly\":\"MONTH\"}\ngranularity_list = [\"monthly\"]\n\nusage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=app/\"\nusage_basic_before_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\"\ndomain_before_transform = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity={granularity}/month={month}/date={date}/\"\napp_basic_raw_path = \"s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={range_type}/date={date}/\"\n\ndef compare_count():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str = \"date ='{date}' and granularity = '{granularity}' and est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0  and device_code <> 'android-all' \".format(date=date, granularity=granularity)\n            # filter_raw = \"AU is not null and AU <> 0 and UP is not null and UP <> 0 \".format(date=date, granularity=granularity[:-2].upper())\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            # spark.read.format(\"parquet\").load(app_basic_raw_path.format(range_type=granularity_mapping[granularity], date=date)).filter(filter_raw).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(usage_basic_before_transform).filter(filter_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select sum(est_average_active_users) as sum_au, count(1) as before_trans from before_trans \").collect()\n            df_after = spark.sql(\"select  sum(est_active_users) as sum_au, count(1) as after_trans from after_trans\").collect()\n            diff =  abs(df_before[0][0] - ( df_after[0][0] if df_after[0][0] else 0 )) / df_before[0][0]\n            if (df_before[0][1] == df_after[0][1]) and  diff < 0.000001:\n                print granularity, date, df_before[0][1], df_after[0][1], df_before[0][0], df_after[0][0], \"PASS\"\n            else:\n                print granularity, date, df_before[0][1], df_after[0][1], df_before[0][0], df_after[0][0], \"Failed\"\n\ndef compare_sum_value():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str = \"date ='{date}' and granularity = '{granularity}' and est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0 \".format(date=date, granularity=granularity)\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(usage_basic_before_transform).filter(filter_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select sum(est_average_active_users) as before_trans from before_trans \").collect()\n            df_after = spark.sql(\"select sum(est_active_users) as after_trans from after_trans \").collect()\n            diff =  abs(df_before[0][0] - df_after[0][0]) / df_before[0][0]\n            if diff > 0.000001:\n                print granularity, date, \"before\", df_before[0][0], \"after\", df_after[0][0]\n            else:\n                print granularity, date, \"PASS\"\n\ndef compare_domain_count():\n    usage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=domain/\"\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str = \"est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0 \"\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(domain_before_transform.format(month=date[:-3].replace(\"-\", \"\"), granularity=granularity[0], date=date)).filter(filter_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select count(1) as before_trans from before_trans \").collect()\n            df_after = spark.sql(\"select count(1) as after_trans from after_trans\").collect()\n            if df_before[0][0] != df_after[0][0]:\n                print granularity, date, \"Failed\", df_before[0][0], df_after[0][0]\n            else:\n                print granularity, date, \"PASS\", df_before[0][0], df_after[0][0]\n\n\ndef compare_domain_sum_value():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str = \"est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0 \"\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(domain_before_transform.format(month=date[:-3].replace(\"-\", \"\"), granularity=granularity[0], date=date)).filter(filter_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select sum(est_average_active_users) as before_trans from before_trans\").collect()\n            df_after = spark.sql(\"select sum(est_active_users) as after_trans from after_trans\").collect()\n            \n            diff =  abs(df_before[0][0] - df_after[0][0]) / df_before[0][0]\n            if diff > 0.000001:\n                print granularity, date, \"Failed\", df_before[0][0], df_after[0][0]\n            else:\n                print granularity, date, \"PASS\", df_before[0][0], df_after[0][0]\n        \ncompare_count()\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20201127-013854_876695886","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/"]},{"cell_type":"code","execution_count":0,"id":"20201126-034322_1985700669","metadata":{},"outputs":[],"source":["\nspark.sql(\"select count(1) as before_trans from before_trans where device_code <> 'android-all'\").show(10, False)\nspark.sql(\"select count(1) as after_trans from after_trans\").show(10, False)"]},{"cell_type":"code","execution_count":0,"id":"20201121-034601_965201847","metadata":{},"outputs":[],"source":["\nusage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=domain/\"\ndomain_before_transform = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity=m/month=201901/date=2019-01-31/\"\nfilter_str = \"est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0 \"\nfilter_str_after = \"date = '2019-01-31' and granularity_code = 'monthly'\"\nspark.read.format(\"parquet\").load(domain_before_transform).filter(filter_str).createOrReplaceTempView(\"before_trans\")\nspark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\nspark.sql(\"select count(distinct domain_id) as before_trans from before_trans \").show(10, False)\nspark.sql(\"select count(distinct product_key) as after_trans from after_trans\").show(10, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20201121-014034_26171485","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/"]},{"cell_type":"code","execution_count":0,"id":"20201109-073329_1818233788","metadata":{},"outputs":[],"source":["\nusage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=app/\"\nusage_basic_before_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\"\nfilter_str = \"date ='2015-12-31' and granularity_code = 'monthly' \"\n# filter_before_str = \"date ='2015-12-31' and granularity = 'monthly' and est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0 and est_open_rate <> 0 and est_open_rate is not null \"\n\nfilter_before_str = \"date ='2015-12-31' and granularity = 'monthly' and (est_open_rate = 0 or est_open_rate is null) and  est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0\"\n\nspark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str).createOrReplaceTempView(\"after_trans\")\nspark.read.format(\"delta\").load(usage_basic_before_transform).filter(filter_before_str).createOrReplaceTempView(\"before_trans\")\nspark.sql(\"select app_id, device_code, est_average_active_users, est_usage_penetration, est_install_base, est_open_rate, est_installs, est_install_penetration from before_trans where country_code = 'US' and device_code = 'ios-phone' and est_installs <> 0 limit 100 \").show(100, False)\n# spark.sql(\"select device_code, count(1) from after_trans group by device_code order by device_code desc\").show(100, False)\n\n# spark.sql(\"select distinct app_id from before_trans where device_code = 'android-phone' and country_code = 'US'\").createOrReplaceTempView(\"before_app\")\n# spark.sql(\"select distinct product_key as app_id from after_trans where device_code = 'android-phone' and country_code = 'US'\").createOrReplaceTempView(\"after_app\")\n\n# spark.sql(\"select app_id as before_diff_after from before_app except select app_id from after_app\").show(1000, False)\n# spark.sql(\"select count(1) from after_trans\").show(100, False)\n# spark.sql(\"select count(1) from after_trans where est_open_rate is null or est_open_rate = 0\").show(10, False)\n"]},{"cell_type":"code","execution_count":0,"id":"20200927-024126_1751715556","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141 -p 5432 -Ucitus_bdp_prod_app_int_qa -d aa_store_db << EOF\nset search_path='usage';\n\nselect  est_share_of_category_bytes from usage_basic_kpi_fact_v1 where granularity ='weekly' \nand date = '2019-01-19' \nand country_code = 'US'\nand device_code in ('android-phone')\nand app_id = 20600000234348;"]},{"cell_type":"code","execution_count":0,"id":"20201109-115456_225081087","metadata":{},"outputs":[],"source":["est_share_of_category_session,est_share_of_category_bytes,est_percent_of_wifi_total\nselect  max(date), min(date) from usage_basic_kpi_fact_v1 where est_share_of_category_session is not null and est_share_of_category_session <> 0 and granularity = 'daily';"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}