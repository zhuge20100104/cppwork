{"cells":[{"cell_type":"code","execution_count":0,"id":"20200504-132205_36055822","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/granularity=daily/date=2020-04-01/device_code=ios-phone/\").printSchema()\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-015854_1253084991","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/review.v4/fact/\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=daily/process_date=2020-09-08/process_hour=23/device_code=ios-all/market_code=apple-store/ | sort -n | head -10\n\n\n\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\n\ndate\n\n# aws s3 ls s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2020-09-08/23/gp/  | head -10\naws s3 ls s3://prod_appannie_uniform_public_app_store_data/review-uniform/HOURLY/2020-09-09/ --recursive | grep 284882215\n\n# aws s3 ls s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2020-09-08/23/ios/544007664.gz \n\n\n\n "]},{"cell_type":"code","execution_count":0,"id":"20200909-063304_364626009","metadata":{},"outputs":[],"source":["\n# spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2020-09-07/23/ios/284882215.gz\", sep = \"\\t\").orderBy(\"_c11\", ascending=False).show(20)\nspark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/HOURLY/2020-08-01/16/ios/284882215.gz\", sep = \"\\t\").orderBy(\"_c11\", ascending=False).show(20)\n"]},{"cell_type":"code","execution_count":0,"id":"20200909-032211_858170393","metadata":{},"outputs":[],"source":["\n\n\n\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact\").where(\"process_granularity='daily' and process_date='2020-09-10' and process_hour=23 and device_code='ios-all' and market_code='apple-store'\").orderBy(\"time\", ascending=False).show(300)\n"]},{"cell_type":"code","execution_count":0,"id":"20200316-033904_870243021","metadata":{},"outputs":[],"source":["\ndf = spark.read.option('basePath','s3://b2c-prod-data-pipeline-qa/aa.review/').parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/{event_time_repartition_20200116_20200224_count,event_time_repartition_streaming_20200220_20200310_count,event_time_union_data_200806_20200116_count}\")\nresult =  df.groupBy(\"event_month\").agg({\"agg_count\":\"sum\"}).cache().collect()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200730-070900_385439751","metadata":{},"outputs":[],"source":["\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\").where(\"process_date = '2020-07-28'\").createOrReplaceTempView(\"adv_data\")\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(\"process_date = '2020-07-28'\").createOrReplaceTempView(\"review_data\")\nspark.sql(\"\"\"\nselect * from adv_data a left join review_data rv\non a.review_id = rv.review_id \nwhere rv.review_id is null\n\"\"\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200316-033854_1105926777","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.utils import AnalysisException\n\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\n\ndef es_doc(month):\n    month = month.replace(\"-\",\"\")\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp\", \"C38vEJEuraCw\")\n    ios_url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1-*-{}/_count\".format(\n        month)\n    ios_resp = plain_get(ios_url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    ios_doc = json.loads(ios_resp.text)\n    return ios_doc[\"count\"]\n\nfor x in result:\n    db_count = es_doc(x[0])\n    if db_count != x[1]:\n        print \"not equal , month is {},  unified is {},  db is {}, diff is {} \".format(x[0], x[1],db_count,db_count- x[1])\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-020137_137556342","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport datetime\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom pyspark.sql.functions import date_format\n\n\ndf = spark.read.option('basePath','s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/').parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={hourly,daily,weekly}/process_date={2020-01-16,2020-01-17,2020-01-18,2020-01-19,2020-01-20,2020-01-21,2020-01-22,2020-01-23,2020-01-24,2020-01-25,2020-01-26,2020-01-27,2020-01-28,2020-01-29,2020-01-30,2020-01-31,2020-02-01,2020-02-02,2020-02-03,2020-02-04,2020-02-05,2020-02-06,2020-02-07,2020-02-08,2020-02-09,2020-02-10,2020-02-11,2020-02-12,2020-02-13,2020-02-14,2020-02-15,2020-02-16,2020-02-17,2020-02-18,2020-02-19,2020-02-20,2020-02-21,2020-02-22,2020-02-23,2020-02-24}\").where(\"market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\").cache()\n# df =spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(\"(process_date between '2020-02-20' and '2020-03-09') and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\").cache()\ndf = df.withColumn(\"time\",df[\"time\"].cast('date'))\nnew_df = df.withColumn(\"event_month\",date_format('time', 'yyyy-MM'))\nresult =  new_df.groupBy(\"event_month\").agg({\"*\":\"count\"}).cache().collect()\nfor x in result:\n    c_temp_load=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_temp/review_load_temp_streaming_20200220_20200310/event_month={}\".format(x[0])).filter(\"process_date != '2020-03-10' and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all') \").count()\n    if x[1] != c_temp_load:\n    # if x[1] != spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_temp/review_load_temp_20200116_20200224/event_month={}\".format(x[0])).count():\n\n        print 'falied  month is {},  v4 unified is : {}, temp load data is {}, diff is {}  '.format( x[0], x[1], c_temp_load, c_temp_load-x[1] )\n    else:\n        print 'pass', x[0]"]},{"cell_type":"code","execution_count":0,"id":"20200313-031204_1807351404","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.utils import AnalysisException\n\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\nprint result[0][0]\nfor x in result:\n    c1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_temp/review_load_temp_streaming_20200220_20200310/event_month={}\".format(x[0])).select(\"review_id\").distinct().count()\n    c2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_aggr/review_load_aggr_streaming_20200220_20200310/event_month={}\".format(x[0])).select(\"review_id\").distinct().count()\n    # c3 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_aggr/review_load_aggr_20200116_20200224/event_month={}\".format(x[0])).select(\"review_id\").distinct().count()\n    # spark.createDataFrame([(x[0], c3)] , schema=[\"event_month\",\"agg_count\"]).coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/event_time_repartition_20200116_20200224_count/\")\n    # spark.createDataFrame([(month, unified_count)], schema=[\"event_month\",\"agg_count\"]).coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/event_time_union_data_200806_20200116_count/\")\n    if c1 != c2:\n        spark.createDataFrame(result).withColumnRenamed(\"count(1)\",\"agg_count\").coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/event_time_repartition_streaming_20200220_20200310_count/\")\n\n        print 'failed', x[0]\n    else:\n        print 'pass',x[0]\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-032852_1675290925","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nsource_path='s3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_aggr/review_load_aggr_streaming_20200220_20200310/'\ndest_path='s3://b2c-prod-data-pipeline-unified-review/_obsolete/bulk_file/bulk_file_streaming_20200220_20200310/date={}-01/'\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"_obsolete/review_load_aggr/review_load_aggr_streaming_20200220_20200310/\", depth_is_1=True)\nprint path_list[0]\n# print path_list[129]\nfor path in path_list:\n    # print path.split(\"/\")[3].split(\"=\")[1]\n    count_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).select(\"review_id\").distinct().count()\n    df = spark.read.json(dest_path.format(path.split(\"/\")[3].split(\"=\")[1])).cache()\n    count_2 = df.filter(\"index is not null\").count()\n    count_3 = df.filter(\"_identifier is not null\").count()\n    # print count_1\n    if count_1 == count_2 == count_3 :\n        print 'pass', path, count_1\n    else:\n        print \"not equal!!!!!!\", count_1 , count_2, count_3,  path\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200313-020049_997302000","metadata":{},"outputs":[],"source":["\n\nfor x in dup_list:\n    result1 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2019-09').filter(\"review_id='{}'\".format(x[\"review_id\"])).groupBy(\"process_date\",\"process_hour\",\"time\").agg({\"*\":\"count\"}).orderBy(desc(\"time\"),desc(\"process_date\"),desc(\"process_hour\")).take(1)\n    result2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v1/event_month=2019-09\").filter(\"review_id='{}'\".format(x[\"review_id\"])).take(1)\n    print result1,result2\n    if result1[0][\"process_date\"]==result2[0][\"process_date\"] and result1[0][\"process_hour\"]==result2[0][\"process_hour\"] and result1[0][\"time\"]==result2[0][\"time\"]  :\n        print 'pass'\n    else:\n        print 'failed!!!',result1, result2,x\n\nfor x in process_data_set:\n    path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={weekly,daily,hourly}/process_date=%s/\" % x[\"process_date\"].strftime(\"%Y-%m-%d\")\n    total_count = spark.read.option(\"basePath\",\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").parquet(path).where(\"market_code in ('apple-store','google-play')\").count()\n    if x[\"total_review\"] != total_count:\n       print \"Not Equal!!!!!!!!\", x[\"process_date\"].strftime(\"%Y-%m-%d\") ,x[\"total_review\"], total_count\n    else:\n        print x[\"process_date\"].strftime(\"%Y-%m-%d\") ,x[\"total_review\"],total_count"]},{"cell_type":"code","execution_count":0,"id":"20200312-135828_202169847","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.utils import AnalysisException\n\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_load_temp/review_agg_v3/\", depth_is_1=True)\nprint len(path_list)\n\ndef get_month_count(path):\n    df1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).select(\"review_id\")\n    \n\n    month=path.split(\"=\")[1].replace(\"/\",\"\")\n    print month\n    review_path_list=[\"\"]\n    try:\n        multiple_path = \"s3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_aggr/[review_load_aggr_20200116_20200224,review_load_aggr_streaming_20200220_20200310,review_load_aggr_streaming_20200310_20200312]/event_month=%s\" % month\n        print multiple_path\n        df2 = spark.read.option('basePath','s3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_aggr/').parquet(multiple_path)\n        df3 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date={}*/\".format(month)).filter(\"market_code in ('apple-store','google-play')\").select(\"review_id\")\n        final_df = df1.union(df2).union(df3)\n        return month, final_df.select(\"review_id\").distinct().count()\n    except AnalysisException:\n        print 'no monthly data'.format(month)\n        return month, df1.select(\"review_id\").distinct().count()\n        \n\n# df.coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/historical_data\")\n\n\ndef es_doc(month):\n    month = month.replace(\"-\",\"\")\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp\", \"C38vEJEuraCw\")\n    ios_url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1-*-{}/_count\".format(\n        month)\n    ios_resp = plain_get(ios_url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    ios_doc = json.loads(ios_resp.text)\n    return ios_doc[\"count\"]\n\n\nfor path in path_list[::-1]:\n    month, unified_count = get_month_count(path)\n    print month, unified_count\n    # spark.createDataFrame([(month, unified_count)], schema=[\"event_month\",\"agg_count\"]).coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/event_time_union_data_200806_20200116_count/\")\n    # # spark.createDataFrame([month, unified_count]).coalesce(1).write.mode(\"append\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/event_time_repartition_streaming_20200220_20200310_count/\")\n    db_count = es_doc(month)\n    if unified_count!= db_count:\n        print 'not equal!!!! month {}, unified count is {}, db count is {}, db-unified diff is {} '.format(month, unified_count, db_count, db_count-unified_count)\n    else:\n        print 'month {}, unified count is {}, db count is {}'.format(month, unified_count, db_count)"]},{"cell_type":"code","execution_count":0,"id":"20200313-115142_738956779","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.review/event_time_repartition_20200116_20200224_count/\").printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200312-063540_534271846","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/_obsolete/bulk_file/bulk_file_streaming_20200220_20200310/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=daily/process_date=2020-01-31/process_hour=23/device_code=ios-all/market_code=apple-store/\n# aws s3 ls s3://b2c-prod-data-pipeline-qa/aa.review/test_03_12/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/event_month=2011-12/\naws s3 ls s3://b2c-prod-data-pipeline-qa/aa.review/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200312-085027_352446053","metadata":{},"outputs":[],"source":["\n# spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").show()\nreview_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact\").where(\"process_granularity in ('daily','weekly','hourly') and process_date between '2020-02-20' and '2020-03-06' and process_hour between 0 and 23 and device_code='ios-all' and market_code='apple-store'\")\n# unified_df1= unified_df1.withColumn(\"event_date\",unified_df1[\"time\"].cast('date'))\n\nreview_df = review_df.withColumn(\"time\",review_df[\"time\"].cast('date')).repartition(\"time\")\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200312-090419_727911132","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import StringType\n\nspark.createDataFrame([('2020-01', \"10\")], schema=[\"event_month\",\"agg_count\"]).show()"]},{"cell_type":"code","execution_count":0,"id":"20200312-090549_599976597","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"_obsolete/bulk_file/bulk_file_streaming_20200220_20200310/\", depth_is_1=True)\n\nresult_set=set()\n\nprint path_list[0]\nprint len(path_list)\nfor path in path_list:\n    \n    index_list = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).filter(\"index is not null\").select(\"index\").collect()\n    # print len(index_list)\n    for x in range(0, len(index_list)):\n        result_set.add(index_list[x][\"index\"][\"_id\"])\n        \nprint len(result_set)\n# index_list = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format()).filter(\"index is not null\").select(\"index\")\n# index_list.show()\n# data_set = index_list.collect()\n# result_set=set()\n# review_id_list_month =[ x[\"index\"] for x in result_month]\n"]},{"cell_type":"code","execution_count":0,"id":"20200312-030124_317679171","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\nsource_path = 's3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/'\ndest_path = 's3://b2c-prod-data-pipeline-unified-review/_obsolete/review_load_aggr/review_load_aggr_20200116_20200224/date={}'\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review.v1/fact/process_granularity=monthly/event_month=\", depth_is_1=True)\nprint path_list[0]\nfor path in path_list[155:]:\n    print path.split(\"/\")[4].split(\"=\")[1]\n\n    count_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).where(\n        \"market_code in ('apple-store','google-play')\").select(\"review_id\").distinct().count()\n    new_path = path.split(\"/\")[4].split(\"=\")[1]\n    df = spark.read.json(dest_path.format(new_path)).cache()\n    count_2 = df.filter(\"index is not null\").count()\n    count_3 = df.filter(\"_identifier is not null\").count()\n\n    # print count_1\n    if count_1 == count_2 == count_3:\n        pass\n        # print 'pass', path\n    else:\n        print \"not equal!!!!!!\", count_1, count_2, count_3, path\n"]},{"cell_type":"code","execution_count":0,"id":"20200310-113753_1610347139","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\n# source_path = 's3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json.v3/'\n#\n# source_path = 's3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/'\n# dest_path = 's3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_month.v3/date={}'\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_bulk_json_month.v3/\", depth_is_1=True)\npath_list = path_list[8:40]\nprint path_list\n\n\ndef get_total_distinct_month_data( path):\n    df = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).filter(\"index is not null\")\n    result_month = df.collect()\n    review_id_list_month =[ x[\"index\"] for x in result_month]\n\n    bulk_date = path.split(\"=\")[-1][:7]\n    df2 = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json.v3/date={}-01/\".format(bulk_date)).filter(\"index is not null\")\n    review_id_bulk = df2.collect()\n    review_id_list_bulk=[ x[\"index\"] for x in review_id_bulk]\n\n    review_month = spark.createDataFrame(review_id_list_month)\n    review_bulk = spark.createDataFrame(review_id_list_bulk)\n\n    new_df = review_month.join(review_bulk, \"_id\", \"outer\")\n    count = new_df.select(\"_id\").distinct().count()\n    return count\n\ndef get_path():\n    index_list = []\n    for path in path_list:\n        # total = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).filter(\n        #     \"_identifier is not null\").count()\n        total = get_total_distinct_month_data(path)\n\n        new_path = path.split(\"=\")[1][:7].replace(\"-\", \"\")\n\n        index_list.append((\"int-ss-review_v1-apple-store-ios-all-{}\".format(new_path),\n                           \"int-ss-review_v1-google-play-android-all-{}\".format(new_path), total))\n    return index_list\n\n\ndef get_es_result(index):\n    # print index\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp\", \"C38vEJEuraCw\")\n    url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/{}/_count\".format(\n        index)\n    try:\n        resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n        doc = json.loads(resp.text)\n        return doc[\"count\"]\n    except KeyError:\n        print 'this index is not exist ', index\n        return 0\n\n\ndef es_doc(index):\n    print 'es doc'\n    total_count = get_es_result(index[0]) + get_es_result(index[1])\n    print total_count, index[2]\n    if total_count != index[2]:\n        print 'not equal! path : {} unified count: {} , db count: {}'.format(index[0], index[2], total_count)\n    else:\n        print index[0], index[1], total_count\n\n\ndef compare(index_list):\n    for row in index_list:\n        es_doc( row)\n        # compare(doc, row)\n\n\npath_list_1 = get_path()\nprint path_list_1\ncompare( path_list_1)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200307-023757_937562057","metadata":{},"outputs":[],"source":["\n\n# granularity=\"hourly\"\n# date='2020-03-06'\n# path_ = '''\n#         process_granularity='{}' and process_date='{}' and market_code in ('apple-store','google-play')\n#         and device_code in ('android-all', 'ios-all')'''.format(granularity, date)\n\n# unified_df1 = spark.read.format(\"delta\").load(\n#             \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").cache().where(\"process_granularity='daily' and process_date='2020-03-06' and process_hour='23' and device_code='android-all' and market_code='google-play' \")\nunified_df1=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/process_granularity=daily/process_date=2020-03-06/process_hour=23/device_code=android-all/market_code=google-play/part-00000-c374f6b1-ca8b-4f71-9ba9-2b4431c52ce1.c000.gz.parquet\")\n# unified_df1= unified_df1.withColumn(\"event_date\",unified_df1[\"time\"].cast('date'))\nunified_df1.createOrReplaceTempView(\"temp_unified\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200307-024141_654441697","metadata":{},"outputs":[],"source":["\n# unified_df1.show()\nspark.sql(\"select DISTINCT review_id from (select *, cast(time as date) as converted_date from temp_unified ) as converted  where converted_date='{}' \".format('2020-03-06')).show()"]},{"cell_type":"code","execution_count":0,"id":"20200229-023912_1400595685","metadata":{},"outputs":[],"source":["%%sh\n# curl -XGET -u bdp:C38vEJEuraCw  -H 'Content-Type: application/json'   http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1-apple-store-ios-all-202002/_mapping?pretty\ncurl -XGET -u bdp:C38vEJEuraCw  -H 'Content-Type: application/json'   http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1-apple-store-ios-all-2019*/_alias?pretty\ncurl -X GET -u bdp:\"C38vEJEuraCw\" \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1*202009*/_mapping/field/content?pretty\"\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200224-090555_1440836362","metadata":{},"outputs":[],"source":["%%sh\ncurl -XGET -u  bdp_read:E9vjhU2qG1bAcM83  -H 'Content-Type: application/json'  http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com:19200/int-reviews-ios-202002/ios/5549941283?routing=1439363292\n\ncurl -XGET -u bdp_read:E9vjhU2qG1bAcM83  -H 'Content-Type: application/json'   http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com:19200/int-reviews-ios-202009*/ios/_search?pretty -d '{\n  \"query\": {\n    \"match\": {\n    \"date\":\"2020-09-08T00:00:00.000000Z\"\n    }\n  }\n}'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200224-085202_600655393","metadata":{},"outputs":[],"source":["\n# review_id = 4945989288\n\n# {\"query\":{\"match\":{\"_id\":\"4945989288\"}}}\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\ndef es_doc():\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp_read\",\"E9vjhU2qG1bAcM83\")\n    \n    url = '''http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com:19200/int-reviews-ios-201910/_search?q=_id:5474203681'''\n    resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    doc = json.loads(resp.text)\n    print doc\nes_doc()\n"]},{"cell_type":"code","execution_count":0,"id":"20200222-024756_686368520","metadata":{},"outputs":[],"source":["\nfrom elasticsearch import Elasticsearch\n\ndef es_doc(identifier):\n    # print 'review_id ', review_id\n    es_connection = Elasticsearch([\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200\"],http_auth=(\"bdp_qa\",\"6trmJcZBJrKWdtN2\"),port=19200)\n    doc = es_connection.count(index=\"int-ss-review_v1-*\", body={\"query\":{\"match\":{\"_identifier\":\"{}\".format(identifier)}}})\n    return doc[\"count\"]\nprint es_doc(\"220200211082455980\")"]},{"cell_type":"code","execution_count":0,"id":"20201223-104504_376104529","metadata":{},"outputs":[],"source":["%%sh\n\ncurl -XGET -u bdp:C38vEJEuraCw  -H 'Content-Type: application/json'   http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1-google-play-android-all*/_search?pretty -d '{\n  \"query\": {\n    \"match\": {\n      \"_id\": \"gp:AOqpTOFuW2_xRLOefHTGXc8Q8XuPS9GoYj3_Ybq6Zh3ci3EnmN9ug8ixmPMEnH8jh1KSlekTAu4cnF6R6KNLTg\"\n    }\n  }\n}'\n\n# curl -XGET -u bdp_read:E9vjhU2qG1bAcM83  -H 'Content-Type: application/json'   http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com:19200/int-reviews-ios-202002/_search?pretty -d '{\n# \"match\":{\"_id\":\"gp:AOqpTOFuW2_xRLOefHTGXc8Q8XuPS9GoYj3_Ybq6Zh3ci3EnmN9ug8ixmPMEnH8jh1KSlekTAu4cnF6R6KNLTg\"}\n# }'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200226-061451_435420938","metadata":{},"outputs":[],"source":["%%sh\ncurl -XGET -u bdp_read:E9vjhU2qG1bAcM83  -H 'Content-Type: application/json'   http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com:19200/int-reviews-ios-202002/_search?pretty -d '{\n\"query\":{\"range\":{\"date\":{\"gte\":\"2020-02-26\",\"lte\":\"2020-02-26\"}}}}'\n\necho 'hello hello'\n\ncurl -XGET -u bdp:C38vEJEuraCw  -H 'Content-Type: application/json'   http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1-google-play-android-all-202002/_count?pretty -d '{\n\"query\":{\"range\":{\"date\":{\"gte\":\"2020-02-26\",\"lte\":\"2020-02-26\"}}}}'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200214-082348_705640682","metadata":{},"outputs":[],"source":["\n# review_id = 4945989288\n\n# {\"query\":{\"match\":{\"_id\":\"4945989288\"}}}\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\ndef es_doc():\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp_read\",\"E9vjhU2qG1bAcM83\")\n    \n    url = '''http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com:19200/int-reviews-ios-201910/_search?q=_id:5474203681'''\n    resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    doc = json.loads(resp.text)\n    print doc\nes_doc()\n"]},{"cell_type":"code","execution_count":0,"id":"20200214-054535_1816965619","metadata":{},"outputs":[],"source":["%python\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\n\nindex = [(\"int-ss-review_v1-google-play-android-all\",\"bdp\",\"C38vEJEuraCw\" ,\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200\"), ( \"int-reviews-gp\" , \"bdp_read\",\"E9vjhU2qG1bAcM83\",\"http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com:19200\" ) ,(\"int-ss-review_v1-apple-store-ios-all\" , \"bdp\",\"C38vEJEuraCw\" ,\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200\"), ( \"int-reviews-ios\" , \"bdp_read\",\"E9vjhU2qG1bAcM83\",\"http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com:19200\" )]\n\nresult=[]\ndef es_doc(index):\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(x[1],x[2])\n    \n    url = \"{}/{}-202003*/_count\".format(index[3], index[0])\n    resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    doc = json.loads(resp.text)\n    return doc[\"count\"]\n\nfor x in index:\n    print x[0] , es_doc(x)\n    result.append((x[0], es_doc(x)))\n\nprint int(result[0][1]) - int(result[1][1])\nprint int(result[2][1]) - int(result[3][1])"]},{"cell_type":"code","execution_count":0,"id":"20200214-074352_1103334758","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\nsource_path = 's3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_month.v3/'\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_bulk_json_month.v3/\", depth_is_1=True)\nprint path_list[2].split(\"=\")[1].replace(\"-\",\"\").strip(\"/\")[:-2]\n\ndef es_doc(month):\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp\",\"C38vEJEuraCw\")\n    url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-review_v1-*-all-{}*/_count\".format(month.split(\"=\")[1].replace(\"-\",\"\").strip(\"/\")[:-2])\n    resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    doc = json.loads(resp.text)\n    print doc\n    \nfor x in path_list:\n    es_doc(x)\n"]},{"cell_type":"code","execution_count":0,"id":"20200213-034802_861450615","metadata":{},"outputs":[],"source":["\ndf = spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2019-10-15/23/ios/1147074917.gz\",sep=\"\\t\").filter(\"_c0='4945989288'\").show(20,False)\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2019-12-01/\").filter(\"app_id='1147074917' and review_id=5218497980\").show(20,False)\n\n\n# path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={weekly,daily,hourly}/process_date=%s/\" % x[\"process_date\"].strftime(\"%Y-%m-%d\")\n# total_count = spark.read.option(\"basePath\",\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").parquet(path).where(\"market_code in ('apple-store','google-play')\").count()\n\n# print spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2019-12-01\").count()\n# print spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2019-12-01/\").count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200212-085855_605106689","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_bulk_json_month.v3/\", depth_is_1=True)\npath_list = path_list[:102]\nprint path_list[0]\n\nfrom pyspark.sql.functions import *\ndf = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_month.v3/date=2008-07-31/\").filter(\"index is not null\") \nresult_month = df.collect()\nreview_id_list_month =[ x[\"index\"] for x in result_month]\n\n\ndf2 = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json.v3/date=2008-07-01/\").filter(\"index is not null\")\nreview_id_bulk = df2.collect()\nreview_id_list_bulk=[ x[\"index\"] for x in review_id_bulk]\n\nreview_month = spark.createDataFrame(review_id_list_month)\nreview_bulk = spark.createDataFrame(review_id_list_bulk)\n\nnew_df = review_month.join(review_bulk, \"_id\", \"outer\")\n\nprint new_df.select(\"_id\").distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200213-021730_146672107","metadata":{},"outputs":[],"source":["\ncolumns = {'review_id', 'process_granularity', 'process_date', 'process_hour', 'market_code', 'device_code', 'app_id', 'product_version', 'user_id', 'user_name', 'user_language', 'user_device', 'user_purchased', 'user_review_url', 'rating', 'country_code', 'date', 'language', 'title', 'title_language', 'content', 'content_language', 'reply_id', 'reply', 'reply_date', 'reply_language', 'chinese_content', 'chinese_reply', 'chinese_title', 'english_content', 'english_reply', 'english_title', 'japanese_content', 'japanese_reply', 'japanese_title', 'korean_content', 'korean_reply', 'korean_title', 'other_language_content', 'other_language_reply', 'other_language_title', '_identifier'}\nignored_columns = {'process_granularity', 'process_date', 'process_hour','user_review_url','reply_id'}\ncolumns -= ignored_columns\nfrom pyspark.sql.functions import desc\nfrom pyspark.sql import functions as F, types as T\nunified_df = spark.read.option('basePath','s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/').parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={daily,hourly,weekly}/process_date=2020-01-25/\").where(\"market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\")\ndup_list = unified_df.select('review_id','process_date','process_hour').distinct().groupBy(\"review_id\").agg({\"*\":\"count\"}).where(\"count(1)>1\").orderBy(\"count(1)\",ascending = False).take(10)\ndf = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_by_process_date.v3/date=2020-01-25/\").withColumn(\"id_1\",(F.monotonically_increasing_id()/2).cast(\"long\"))\nbulk_df = df.groupBy(\"id_1\").agg(*(F.max(column).alias(column) for column in df.columns if column != \"id_1\"))\nprint bulk_df.show()\ntry:\n    bulk_df = bulk_df.withColumn(\"review_id\",bulk_df[\"index\"][\"_id\"]).drop(\"id_1\").drop(\"index\").persist()\n    for x in dup_list:\n        try:\n            print x[\"review_id\"]\n            result1_df = unified_df.filter(\"review_id='{}'\".format(x[\"review_id\"])).orderBy(desc(\"process_date\"), desc(\"time\"), desc(\"process_hour\")).withColumn(\"date\", F.from_unixtime(F.unix_timestamp(\"time\", \"yyyy-MM-dd\"), \"yyyy-MM-dd\")).persist()\n            result1 = result1_df.limit(1).take(1)\n            result2 = bulk_df.filter(\"review_id='{}'\".format(x[\"review_id\"])).take(1)\n            validate_df = result1_df.where((result1_df[\"process_date\"] == result1[0][\"process_date\"]) & (result1_df[\"time\"] == result1[0][\"time\"]) & (result1_df[\"process_hour\"] == result1[0][\"process_hour\"]))\n            for item in columns:\n                words = item.split(\"_\")\n                if \"chinese_\" in item:\n                    if result2[0][item] is not None:\n                        validate_df = validate_df.where(\"%s_language in ('zh','zh_Hant')\" % words[-1])\n                elif \"korean_\" in item:\n                    if result2[0][item] is not None:\n                        validate_df = validate_df.where(\"%s_language in ('ko')\" % words[-1])\n                elif \"english_\" in item:\n                    if result2[0][item] is not None:\n                        validate_df = validate_df.where(\"%s_language in ('en')\" % words[-1])\n                elif \"japanese_\" in item:\n                    if result2[0][item] is not None:\n                        validate_df = validate_df.where(\"%s_language in ('ja')\" % words[-1])\n                elif \"other_language_\" in item:\n                    if result2[0][item] is not None:\n                        validate_df = validate_df.where(\"%s_language not in ('zh','zh_Hant','ko','en','ja')\" % words[-1])\n                else:\n                    if result2[0][item] is not None:\n                        validate_df = validate_df.where(result1_df[item] == result2[0][item])\n                    else:\n                        validate_df = validate_df.where(result1_df[item].isNull())\n            c = validate_df.count()\n            print c,result1,result2\n            if c>0:\n                print 'pass'\n            else:\n                print 'failed!!!',result1, result2, x\n        finally:\n            result1_df.unpersist()\nfinally:\n    bulk_df.unpersist()\n"]},{"cell_type":"code","execution_count":0,"id":"20200212-085912_1152965508","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nfrom aadatapipelinecore.core.log import logger\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\nimport json\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_bulk_json_month.v3/\", depth_is_1=True)\npath_list = path_list[:102]\n# print path_list\n\n\ndef get_path():\n    index_list = []\n    for path in path_list:\n        # print path\n        # total = spark.read.json(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).filter(\n        #     \"_identifier is not null\").count()\n\n        new_path = path.split(\"=\")[1][:7].replace(\"-\", \"\")\n\n        index_list.append((\"int-ss-review_v1-apple-store-ios-all-{}\".format(new_path),\n                           \"int-ss-review_v1-google-play-android-all-{}\".format(new_path)))\n        # print new_path, total\n    return index_list\n\n\ndef get_es_result(index):\n    # print index\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp\", \"C38vEJEuraCw\")\n    url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/{}/_count\".format(\n        index)\n    try:\n        resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n        doc = json.loads(resp.text)\n        doc[\"count\"]\n    except KeyError:\n        print index\n        return 0\n\n    return doc[\"count\"]\n\n\ndef es_doc(index):\n    # print  get_es_result(index[0])\n    # print get_es_result(index[1])\n    total_count = get_es_result(index[0]) + get_es_result(index[1])\n    print 'total', total_count\n    # if total_count != index[2]:\n    print ' path : {}, {},  db count: {}'.format(index[0],index[1], total_count)\n    # else:\n    #     print index[0], total_count\n\n\ndef compare(index_list):\n    for row in index_list[:30]:\n        es_doc(row)\n        \ncompare(get_path())"]},{"cell_type":"code","execution_count":0,"id":"20200209-030511_1333489186","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\nsource_path = 's3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/'\ndest_path = 's3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_month.v3/date={}'\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review.v1/fact/process_granularity=monthly/\", depth_is_1=True)\nprint path_list[155]\nfor path in path_list[155:]:\n    print path.split(\"/\")[4].split(\"=\")[1]\n\n    count_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).where(\n        \"market_code in ('apple-store','google-play')\").select(\"review_id\").distinct().count()\n    new_path = path.split(\"/\")[4].split(\"=\")[1]\n    df = spark.read.json(dest_path.format(new_path)).cache()\n    count_2 = df.filter(\"index is not null\").count()\n    count_3 = df.filter(\"_identifier is not null\").count()\n\n    # print count_1\n    if count_1 == count_2 == count_3:\n        pass\n        # print 'pass', path\n    else:\n        print \"not equal!!!!!!\", count_1, count_2, count_3, path\n"]},{"cell_type":"code","execution_count":0,"id":"20200209-083930_823293488","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nimport pyspark.sql.functions as F\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\n\nsource_path='s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/'\ndest_path='s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json.v3/date={}-01/'\n\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_load_temp/review_agg_v3/\", depth_is_1=True)\n\n# review_bulk_json_by_process_date.v3/date=2020-01-20\ndef compare_data(test_month):\n    print test_month\n    df1 = spark.read.parquet(\n    \"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/event_month={}\".format(test_month))\n    df2 = spark.read.json(\n    \"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_by_process_date.v3/date={}-01\".format(test_month)).withColumn(\"id_1\",\n                                                                                                         F.monotonically_increasing_id())\n\n    sample_df = df2.sample(False, 0.001, seed=0).limit(5).take(5)\n    # print sample_df\n\n\n    review_id = ''\n    remove_column = ['english_content', 'english_reply', 'english_title', 'chinese_content', 'chinese_reply',\n                 'chinese_title', 'japanese_content', 'japanese_reply', 'japanese_title', 'korean_content',\n                 'korean_reply', 'korean_title', 'other_language_content', 'other_language_reply',\n                 'other_language_title', 'index', 'id_1'] #,'_corrupt_record', '_id','_index', 'routing']\n    column = df2.columns\n\n    for x in remove_column:\n        column.remove(x)\n\n\n    for sd in sample_df:\n        id_1 = sd[\"id_1\"]\n        id_2 = ''\n        # print id_1\n        if id_1 % 2 == 0:\n            id_2 = id_1 + 1\n        else:\n            id_2 = id_1\n            id_1 = id_1 - 1\n\n        df2.registerTempTable(\"result_detail_tmp\")\n        test_result_load = spark.sql(\"select * from result_detail_tmp where id_1 in ('{}', '{}' )\".format(id_1, id_2)).take(\n            2)\n        review_id = test_result_load[0][\"index\"][\"_id\"]\n\n        # print review_id\n\n\n        df1.registerTempTable(\"result_tmp\")\n\n        test_result_transform = spark.sql(\n            \"select * from result_tmp WHERE review_id = '{}'\".format(str(review_id))).take(1)\n\n        for value in column:\n            if value == 'date':\n                if test_result_load[1][value] != test_result_transform[0][\"time\"].split(\"T\")[0]:\n                    print 'failed', test_result_load[1][value], test_result_transform[0][value]\n            elif value == '_identifier':\n                if str(test_result_load[1][value]) != str(test_result_transform[0][value]):\n                    print 'failed', test_result_load[1][value], test_result_transform[0][value]\n\n            else:\n                if test_result_load[1][value] != test_result_transform[0][value]:\n                    print 'failed', test_result_load[1][value], test_result_transform[0][value]\n\n\nfor x in path_list:\n    compare_data(x.split(\"/\")[3].split(\"=\")[1])"]},{"cell_type":"code","execution_count":0,"id":"20200209-104847_533360197","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_load_temp/review_load_temp_v4/\", depth_is_1=True)\n# print path_list\nfor path in path_list[:107]:\n    count_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/{}\".format(path)).select(\"review_id\").distinct().count()\n    count_2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/{}\".format(path.split(\"/\")[3])).select(\"review_id\").count()\n    if count_1 != count_2:\n        print \"not equal!!!!!!\", count_1 , count_2, path\n    else:\n        print 'pass', path"]},{"cell_type":"code","execution_count":0,"id":"20200207-142925_487408725","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import desc\n\n#dup_list = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2019-09/').select('review_id','process_date','process_hour').distinct().groupBy(\"review_id\").agg({\"*\":\"count\"}).where(\"count(1)>1\").orderBy(\"count(1)\",ascending = False).take(10)\n\nfor x in dup_list:\n    result1 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2019-09').filter(\"review_id='{}'\".format(x[\"review_id\"])).groupBy(\"process_date\",\"process_hour\",\"time\").agg({\"*\":\"count\"}).orderBy(desc(\"time\"),desc(\"process_date\"),desc(\"process_hour\")).take(1)\n    result2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v1/event_month=2019-09\").filter(\"review_id='{}'\".format(x[\"review_id\"])).take(1)\n    print result1,result2\n    if result1[0][\"process_date\"]==result2[0][\"process_date\"] and result1[0][\"process_hour\"]==result2[0][\"process_hour\"] and result1[0][\"time\"]==result2[0][\"time\"]  :\n        print 'pass'\n    else:\n        print 'failed!!!',result1, result2,x\n"]},{"cell_type":"code","execution_count":0,"id":"20200211-221524_131070280","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nimport pyspark.sql.functions as F\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\n\nsource_path='s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/'\ndest_path='s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json.v3/date={}-01/'\n\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = s3_bucket_list.all(prefix=\"unified/review_load_temp/review_agg_v3/\", depth_is_1=True)\n\n    \n    \n    \ndef get_monthly_sample_data(monthly):\n    \n    review_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v3/event_month={}/\".format(monthly))\n    review_df.registerTempTable(\"reviews\")\n    ios_samples_df = spark.sql(\"SELECT * FROM reviews WHERE market_code = 'apple-store' LIMIT 50\").cache()\n    ios_samples_df.count()\n    android_samples_df = spark.sql(\"SELECT * FROM reviews WHERE market_code = 'google-play' LIMIT 50\").cache()\n    android_samples_df.count()\n    samples = []\n    month_index = monthly.replace(\"-\",\"\")\n    for idx, df in [\n        (\"int-ss-review_v1-apple-store-ios-all-{}\".format(month_index), ios_samples_df),\n        (\"int-ss-review_v1-google-play-android-all-{}\".format(month_index), android_samples_df)]:\n        print df.columns\n        rows = (\n            df\n            .withColumn(\"date\", F.date_format(\"time\", \"yyyy-MM-dd\"))\n            .drop(\"time\")\n            .withColumn(\"chinese_content\", F.when((df.content_language == \"zh\") | (df.content_language == \"zh_Hant\"), df.content))\n            .withColumn(\"chinese_reply\", F.when((df.reply_language == \"zh\") | (df.reply_language == \"zh_Hant\"), df.reply))\n            .withColumn(\"chinese_title\", F.when((df.title_language == \"zh\") | (df.title_language == \"zh_Hant\"), df.title))\n            .withColumn(\"english_content\", F.when(df.content_language == \"en\", df.content))\n            .withColumn(\"english_reply\", F.when(df.reply_language == \"en\", df.reply))\n            .withColumn(\"english_title\", F.when(df.title_language == \"en\", df.title))\n            .withColumn(\"japanese_content\", F.when(df.content_language == \"ja\", df.content))\n            .withColumn(\"japanese_reply\", F.when(df.reply_language == \"ja\", df.reply))\n            .withColumn(\"japanese_title\", F.when(df.title_language == \"ja\", df.title))\n            .withColumn(\"korean_content\", F.when(df.content_language == \"ko\", df.content))\n            .withColumn(\"korean_reply\", F.when(df.reply_language == \"ko\", df.reply))\n            .withColumn(\"korean_title\", F.when(df.title_language == \"ko\", df.title))\n            .withColumn(\"other_language_content\", F.when(\n                (df.content_language != \"zh\") & (df.content_language != \"zh_Hant\") & (df.content_language != \"en\") &\n                (df.content_language != \"ja\") & (df.content_language != \"ko\"), df.content))\n            .withColumn(\"other_language_reply\", F.when(\n                (df.reply_language != \"zh\") & (df.reply_language != \"zh_Hant\") & (df.reply_language != \"en\") &\n                (df.reply_language != \"ja\") & (df.reply_language != \"ko\"), df.reply))\n            .withColumn(\"other_language_title\", F.when(\n                (df.title_language != \"zh\") & (df.title_language != \"zh_Hant\") & (df.title_language != \"en\") &\n                (df.title_language != \"ja\") & (df.title_language != \"ko\"), df.title))\n            .collect()\n        )\n        samples.append((idx, rows))\n    return samples\n    \n    \nsample_data_set=[]\nfor x in path_list[100:]:\n    sample_data_set.append(get_monthly_sample_data(x.split(\"/\")[3].split(\"=\")[1]))\n    \ndef compare(doc, row):\n    keys = [u\"review_id\", u'_identifier', u'app_id', u'chinese_content', u'chinese_reply', u'chinese_title', u'content', u'content_language', u'country_code', u'date', u'device_code', u'english_content', u'english_reply', u'english_title', u'japanese_content', u'japanese_reply', u'japanese_title', u'korean_content', u'korean_reply', u'korean_title', u'language', u'market_code', u'other_language_content', u'other_language_reply', u'other_language_title', u'product_version', u'rating', u'reply', u'reply_date', u'reply_language', u'title', u'title_language', u'user_device', u'user_id', u'user_language', u'user_name', u'user_purchased']\n    row_dict =  row.asDict()\n    for key in keys:\n        if key in (\"app_id\", \"_identifier\"):\n            assert str(doc[key]) == str(row_dict[key]), \"{} is diffrent\".format(key)\n        else:\n            assert doc[key] == row_dict[key], \"{} is diffrent\".format(key)\ndef es_doc(index, doc_id, routing_id):\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp\",\"C38vEJEuraCw\")\n    \n    url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/{}/_doc/{}?routing={}\".format(index, doc_id, routing_id)\n    resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    doc = json.loads(resp.text)\n    source = doc['_source']\n    source['review_id'] = doc[\"_id\"]\n    return source\n    \n    \nfor idx, rows in samples:\n    for row in rows:\n        doc = es_doc(idx, row.review_id, row.app_id)\n        compare(doc, row)\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-144037_1423666262","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\n\n# path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={weekly,daily,hourly}/process_date=%s/\" % x[\"process_date\"].strftime(\"%Y-%m-%d\")\n# total_count = spark.read.option(\"basePath\",\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").parquet(path).where(\"market_code in ('apple-store','google-play')\").count()\n\nreview_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json.v1/date=2019-01-01/\")\nreview_df.registerTempTable(\"reviews\")\nios_samples_df = spark.sql(\"SELECT * FROM reviews WHERE market_code = 'apple-store' LIMIT 50\").cache()\nios_samples_df.count()\nandroid_samples_df = spark.sql(\"SELECT * FROM reviews WHERE market_code = 'google-play' LIMIT 50\").cache()\nandroid_samples_df.count()\nsamples = []\nfor idx, df in [\n    (\"int-ss-review_v1-apple-store-ios-all-200807\", ios_samples_df), \n    (\"int-ss-review_v1-google-play-android-all-200807\", android_samples_df)]:\n    print df.columns\n    rows = (\n        df\n        .withColumn(\"date\", F.date_format(\"time\", \"yyyy-MM-dd\"))\n        .drop(\"time\")\n        .withColumn(\"chinese_content\", F.when((df.content_language == \"zh\") | (df.content_language == \"zh_Hant\"), df.content))\n        .withColumn(\"chinese_reply\", F.when((df.reply_language == \"zh\") | (df.reply_language == \"zh_Hant\"), df.reply))\n        .withColumn(\"chinese_title\", F.when((df.title_language == \"zh\") | (df.title_language == \"zh_Hant\"), df.title))\n        .withColumn(\"english_content\", F.when(df.content_language == \"en\", df.content))\n        .withColumn(\"english_reply\", F.when(df.reply_language == \"en\", df.reply))\n        .withColumn(\"english_title\", F.when(df.title_language == \"en\", df.title))\n        .withColumn(\"japanese_content\", F.when(df.content_language == \"ja\", df.content))\n        .withColumn(\"japanese_reply\", F.when(df.reply_language == \"ja\", df.reply))\n        .withColumn(\"japanese_title\", F.when(df.title_language == \"ja\", df.title))\n        .withColumn(\"korean_content\", F.when(df.content_language == \"ko\", df.content))\n        .withColumn(\"korean_reply\", F.when(df.reply_language == \"ko\", df.reply))\n        .withColumn(\"korean_title\", F.when(df.title_language == \"ko\", df.title))\n        .withColumn(\"other_language_content\", F.when(\n            (df.content_language != \"zh\") & (df.content_language != \"zh_Hant\") & (df.content_language != \"en\") &\n            (df.content_language != \"ja\") & (df.content_language != \"ko\"), df.content))\n        .withColumn(\"other_language_reply\", F.when(\n            (df.reply_language != \"zh\") & (df.reply_language != \"zh_Hant\") & (df.reply_language != \"en\") &\n            (df.reply_language != \"ja\") & (df.reply_language != \"ko\"), df.reply))\n        .withColumn(\"other_language_title\", F.when(\n            (df.title_language != \"zh\") & (df.title_language != \"zh_Hant\") & (df.title_language != \"en\") &\n            (df.title_language != \"ja\") & (df.title_language != \"ko\"), df.title))\n        .collect()\n    )\n    samples.append((idx, rows))\nprint samples\n"]},{"cell_type":"code","execution_count":0,"id":"20200209-071046_1503209247","metadata":{},"outputs":[],"source":["\nimport json\nimport time\nfrom requests.auth import HTTPBasicAuth\nfrom aadatapipelinecore.core.utils.http import plain_get\ndef compare(doc, row):\n    keys = [u\"review_id\", u'_identifier', u'app_id', u'chinese_content', u'chinese_reply', u'chinese_title', u'content', u'content_language', u'country_code', u'date', u'device_code', u'english_content', u'english_reply', u'english_title', u'japanese_content', u'japanese_reply', u'japanese_title', u'korean_content', u'korean_reply', u'korean_title', u'language', u'market_code', u'other_language_content', u'other_language_reply', u'other_language_title', u'product_version', u'rating', u'reply', u'reply_date', u'reply_language', u'title', u'title_language', u'user_device', u'user_id', u'user_language', u'user_name', u'user_purchased']\n    row_dict =  row.asDict()\n    for key in keys:\n        if key in (\"app_id\", \"_identifier\"):\n            assert str(doc[key]) == str(row_dict[key]), \"{} is diffrent\".format(key)\n        else:\n            assert doc[key] == row_dict[key], \"{} is diffrent\".format(key)\ndef es_doc(index, doc_id, routing_id):\n    headers = {\n        'content-type': 'application/json'\n    }\n    auth = HTTPBasicAuth(\"bdp\",\"C38vEJEuraCw\")\n    \n    url = \"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/{}/_doc/{}?routing={}\".format(index, doc_id, routing_id)\n    resp = plain_get(url, timeout=30, auth=auth, headers=headers, is_resource_of_appannie=False)\n    doc = json.loads(resp.text)\n    source = doc['_source']\n    source['review_id'] = doc[\"_id\"]\n    return source\nfor idx, rows in samples:\n    for row in rows:\n        doc = es_doc(idx, row.review_id, row.app_id)\n        compare(doc, row)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200115-031837_1324783590","metadata":{},"outputs":[],"source":["\nfrom dateutil.relativedelta import relativedelta\nimport datetime\nfrom pyspark.sql.utils import AnalysisException\ndef generate_data():\n    result = []\n    start_date = datetime.date(2018, 8, 14) \n    today = datetime.date(2019, 12, 31)    \n    while start_date < today:\n        delta_date=relativedelta(days=1)\n        start_date = start_date + delta_date\n        result.append((start_date))\n    return result\n\nprint generate_data()\nfor x in generate_data():\n    cal_date=x.strftime(\"%Y-%m-%d\")\n    # print(\"time:\", cal_date)\n    spark_result=''\n    print \"s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/{process_date}/23/\".format(process_date=cal_date)\n    try:\n        spark_result=spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/{process_date}/23/\".format(process_date=cal_date),sep=\"\\t\").filter(\"_c6 is not null\").take(2)\n    except AnalysisException:\n        pass\n    if spark_result:\n        print spark_result\n    # if result:\n    #     print result\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200115-025401_1860493475","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count, avg\ndf = spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/HOURLY/2020-01-15/05/gp/\",sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\", \"_c2 as product_version\", \"_c3 as user_id\", \"_c4 as user_name\" , \"_c5 as user_language\", \"_c6 as user_device\", \"_c7 as user_purchased\", \"_c8 as rating\", \"_c9 as country\", \"_c10 as date\",\"_c11 as language\", \"_c12 as title\", \"_c13 as title_t\", \"_c14 as content\", \"_c15 as content_t\", \"_c16 as reply\", \"_c17 as reply_t\", \"_c18 as reply_date\")\nresult= df.groupBy(\"id\").agg(count(\"id\")).orderBy(\"count(id)\", ascending=False).take(20)\n\nfor x in result:\n    print x"]},{"cell_type":"code","execution_count":0,"id":"20200110-021638_1355307943","metadata":{},"outputs":[],"source":["\n\nfrom aadatapipelinecore.core.log import logger\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\ndest_path='s3://b2c-prod-data-pipeline-unified-review/unified/review_bulk_json_by_process_date.v3/date={}/'\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_list = ['process_granularity={hourly,daily,weekly}/process_date=%s/' % item for item in [\"2020-02-24\"]]\nprint path_list[0]\nfor path in path_list:\n    print path.split(\"/\")[1].split(\"=\")[1]\n    count_1 = spark.read.option('basePath','s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/').parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/{}\".format(path)).where(\"market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\").select(\"review_id\").distinct().count()\n    df = spark.read.json(dest_path.format(path.split(\"/\")[1].split(\"=\")[1])).cache()\n    count_2 = df.filter(\"index is not null\").count()\n    count_3 = df.filter(\"_identifier is not null\").count()\n    print count_1, count_2, count_3\n    if count_1 == count_2 == count_3 :\n        print 'pass', path\n    else:\n        print \"not equal!!!!!!\", count_1 , count_2, count_3,  path\n"]},{"cell_type":"code","execution_count":0,"id":"20200113-075805_1798205316","metadata":{},"outputs":[],"source":["\nimport unittest\nimport boto3\nfrom pyspark.sql.utils import AnalysisException\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\ns3 = boto3.resource('s3')\nmy_bucket = s3.Bucket('prod_appannie_uniform_public_app_store_data')\n\n\n# get current path list\npath=[]\nfor object_summary in my_bucket.objects.filter(Prefix=\"review-uniform/MONTHLY/\"):\n    path.append(object_summary.key[:object_summary.key.rfind(\"/\")])\n\npath = list(dict.fromkeys(path))\n# print path\n\n# compared the data from uniform(raw data) to unified data\nraw_path=\"s3://prod_appannie_uniform_public_app_store_data/\"\nunified_path=\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date={unified_process_date}/process_hour=23/device_code={unified_device_code}/market_code={unified_market_code}\"\n\ndef count_raw_unified_data():\n    t = unittest.TestCase('run')\n    for p in path:\n        raw_count = spark.read.csv(raw_path+p,sep=\"\\t\").count()\n        # print raw_path+p\n        # print 'raw_count', raw_count\n        if raw_count == 0:\n            # print 'raw is empty, path is : ' , raw_path+p\n            continue\n        \n        upath=unified_path.format(unified_process_date=p.split(\"/\")[2],unified_device_code=platforms[p.split(\"/\")[3]][\"device_code\"],unified_market_code=platforms[p.split(\"/\")[3]][\"market_code\"] )\n        try:\n            unified_count = spark.read.parquet(upath).count()\n        except AnalysisException as e:\n            print 'the path is not exist for unified level!!! the path is', raw_path+p, upath\n        \n        # print 'unified_count', unified_count\n\n        if raw_count != unified_count:\n            print 'count is not equal!!! the path is : ', raw_path+p, upath, \"raw is: {} , unified is : {} \".format(raw_count, unified_count)\n\n\n\ncount_raw_unified_data()\n"]},{"cell_type":"code","execution_count":0,"id":"20200119-101458_843308510","metadata":{},"outputs":[],"source":["\nimport unittest\nimport boto3\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3 \nfrom aadatapipelinecore.core.fs import Conf\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n\n# get up level data set\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_w= s3_bucket_list.all(prefix=\"review-uniform/WEEKLY/\", depth_is_1=True )\npath_w = path_w[10:]\n\n\n# compared the data from uniform(raw data) to unified data\nraw_path_weekly=\"s3://prod_appannie_uniform_public_app_store_data/\"\nunified_path_weekly=\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=weekly/process_date={unified_process_date}/process_hour=23/device_code={unified_device_code}/market_code={unified_market_code}\"\n\ndef count_raw_unified_data_weekly():\n    t = unittest.TestCase('run')\n    for p in path_w:\n        deep_raw_path=s3_bucket_list.all(p+'23', depth_is_1=True)\n        for dp in deep_raw_path:\n            # print 'raw_path is: ', raw_path_weekly+dp\n            raw_count = spark.read.csv(raw_path_weekly+dp,sep=\"\\t\").count()\n       \n            if raw_count == 0:\n                continue\n        \n            upath=unified_path_weekly.format(unified_process_date=dp.split(\"/\")[2],unified_device_code=platforms[dp.split(\"/\")[4]][\"device_code\"],unified_market_code=platforms[dp.split(\"/\")[4]][\"market_code\"] )\n            try:\n                unified_count = spark.read.parquet(upath).count()\n            except AnalysisException as e:\n                print 'the path is not exist for unified level!!! the path is', raw_path+dp, upath\n        \n    \n            if raw_count != unified_count:\n                print 'count is not equal!!! the path is : ', raw_path+dp, upath, \"raw is: {} , unified is : {} \".format(raw_count, unified_count)\n\n\n\ncount_raw_unified_data_weekly()\n"]},{"cell_type":"code","execution_count":0,"id":"20200121-074319_603099084","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_d = s3_bucket_list.all(prefix=\"review-uniform/HOURLY/\", depth_is_1=True)\n# print path_d\nprint path_d[274]\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n# compared the data from uniform(raw data) to unified data\nraw_path_d = \"s3://prod_appannie_uniform_public_app_store_data/\"\nunified_path_d = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=hourly/process_date={unified_process_date}/process_hour={unified_process_hour}/device_code={unified_device_code}/market_code={unified_market_code}\"\n\n\ndef count_raw_unified_data_hourly():\n    unified_count = 0\n    for p in path_d[985:]:\n        process_hour_raw_path = s3_bucket_list.all(p, depth_is_1=True)\n        for x in process_hour_raw_path:\n            logger.info('raw path with process hour : '.format(x))\n            platforms_process_hour_raw_path_list = s3_bucket_list.all(x, depth_is_1=True)\n\n            # raw_count_sum, root_raw_data_path = calculate_sum_data_hourly(platforms_process_hour_raw_path_list)\n            # logger.info('raw_count_sum : '.format(raw_count_sum))\n            for dp in platforms_process_hour_raw_path_list:\n                print dp\n                root_raw_data_path = dp\n                raw_count_hourly = spark.read.csv(raw_path_d + dp, sep=\"\\t\").count()\n                logger.info('raw count : {}'.format(raw_count_hourly))\n\n                if raw_count_hourly == 0:\n                    continue\n                upath = unified_path_d.format(unified_process_date=root_raw_data_path.split(\"/\")[2],\n                                          unified_device_code=platforms[root_raw_data_path.split(\"/\")[4]][\n                                              \"device_code\"],\n                                          unified_market_code=platforms[root_raw_data_path.split(\"/\")[4]][\n                                              \"market_code\"],\n                                          unified_process_hour=str(int(root_raw_data_path.split(\"/\")[3])))\n                try:\n                    unified_count = spark.read.parquet(upath).count()\n                    logger.info('unified_count : {}'.format(unified_count))\n\n                except AnalysisException as e:\n                    logger.info('the path is not exist for unified level!!! the raw path is {}, unified path is {}'.format(\n                    raw_path_d + root_raw_data_path, upath))\n\n            # logger.info( 'unified_count', unified_count\n\n                if raw_count_hourly != unified_count:\n                    logger.info(\n                    'count is not equal!!! the raw path is {}, unified path is {} , raw count is : {}, unified count is : {}'.format(\n                        raw_path_d + root_raw_data_path, upath,\n                        raw_count_hourly, unified_count))\n\n\ndef calculate_sum_data_hourly( deep_raw_path):\n    logger.info(deep_raw_path)\n    count_one_day = 0\n    for dp in deep_raw_path:\n        raw_count = spark.read.csv(raw_path_d + dp, sep=\"\\t\").count()\n        count_one_day = count_one_day + raw_count\n\n    return count_one_day, deep_raw_path[0]\n\n# count_raw_unified_data_hourly()\n"]},{"cell_type":"code","execution_count":0,"id":"20200122-020609_273980995","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nimport time\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nraw_path_d = \"s3://prod_appannie_uniform_public_app_store_data/\"\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_daily_count = s3_bucket_list.list(prefix=\"review-uniform/DAILY/2017-06-11/23/\", limit=100, depth_is_1=False)\n\npath_daily_count=list(map(\"s3://prod_appannie_uniform_public_app_store_data/\".__add__,path_daily_count))\n\nprint path_daily_count\n\ntotal_count=0\nstart=time.time()\ntotal_count = spark.read.csv(path_daily_count).count()\nend=time.time()\nprint end-start\nprint total_count\n# print spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/DAILY/2017-06-11/23/gp\").count()\n# print spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date=2017-06-10/process_hour=23/device_code=android-all/market_code=google-play\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200122-024901_383906073","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\n\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n# compared the data from uniform(raw data) to unified data\nraw_path = \"s3://prod_appannie_uniform_public_app_store_data/\"\nunified_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=daily/process_date={unified_process_date}/process_hour=23/device_code={unified_device_code}/market_code={unified_market_code}\"\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_h = s3_bucket_list.all(prefix=\"review-uniform/MONTHLY/\", depth_is_1=True)\n\n\ndef get_data_frame(column_length, data_path):\n    if len(column_length) == 19:\n        # print 'this is 19'\n        raw_result = spark.read.csv(data_path, sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\",\n                                                                           \"_c2 as product_version\", \"_c3 as user_id\",\n                                                                           \"_c4 as user_name\", \"_c5 as user_language\",\n                                                                           \"_c6 as user_device\", \"_c7 as user_purchased\",\n                                                                           \"_c8 as rating\", \"_c9 as country\",\n                                                                           \"_c10 as date\", \"_c11 as language\",\n                                                                           \"_c12 as title\", \"_c13 as title_t\",\n                                                                           \"_c14 as content\", \"_c15 as content_t\",\n                                                                           \"_c16 as reply\", \"_c17 as reply_t\",\n                                                                           \"_c18 as reply_date\").take(1)\n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\"\n                      }\n        return raw_result,column_dic\n    elif len(column_length) == 21:\n        raw_result = spark.read.csv(data_path, sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\", \"_c2 as product_version\", \"_c3 as user_id\", \"_c4 as user_name\" , \"_c5 as user_language\", \"_c6 as user_device\", \"_c7 as user_purchased\", \"_c8 as userreview_url\", \"_c9 as rating\", \"_c10 as country\",\"_c11 as date\", \"_c12 as language\", \"_c13 as title\", \"_c14 as title_t\", \"_c15 as content\", \"_c16 as content_t\", \"_c17 as reply_id\", \"_c18 as reply\", \"_c19 as reply_t\", \"_c20 as reply_date\").take(1)\n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\", \"reply_id\":\"reply_id\", \"userreview_url\":\"userreview_url\"\n                      }\n        return raw_result,column_dic\n    elif len(column_length) == 0:\n        logger.info(\"data is empty\")\n        return [],{}\n        \n    \n    \n    \ndef check_raw_unified_data_equals():\n    for p in path_h:\n        sub_raw_path = s3_bucket_list.all(p, depth_is_1=True)\n        for dp in sub_raw_path:\n            print raw_path + dp\n            global raw_result \n            raw_result = spark.read.csv(raw_path + dp, sep=\"\\t\").cache().take(1)\n            \n            if len(raw_result) != 0:\n                # print raw_result[0]\n                sample_raw_result, column_dic = get_data_frame(raw_result[0], raw_path + dp)\n                # print sample_raw_result, column_dic\n                if sample_raw_result:\n                    check_id = sample_raw_result[0][\"id\"]\n                    # print dp\n                    upath = unified_path.format(unified_process_date=dp.split(\"/\")[2],\n                                        unified_device_code=platforms[dp.split(\"/\")[4]][\"device_code\"],\n                                        unified_market_code=platforms[dp.split(\"/\")[4]][\"market_code\"])\n\n                    unified_result = spark.read.parquet(upath).filter(\"review_id='{test_id}'\".format(test_id=check_id)).take(2)\n\n                   \n\n                    for key, value in column_dic.items():\n                        check_value(key, sample_raw_result[0][key], unified_result[0][value], p)\n\n            elif len(raw_result) == 0:\n                print 'No data'\n                continue\n\n\ndef check_value(index, exp, act, p):\n    # print index, exp, act, p\n    try:\n        if exp:\n            if index == 'rating' or index == \"product_id\":\n                exp = int(exp)\n                act = int(act)\n                if exp == act:\n                    pass\n                elif exp != act:\n                    logger.info( 'number NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp,\n                                                                                                        act))\n            elif index == 'user_purchased':\n                exp = str(exp).lower()\n                act = str(act).lower()\n                if exp == act:\n                    pass\n                if exp != act:\n                    logger.info( 'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act))\n            else:\n                if exp.lower().strip('\"') == act.lower().strip('\"'):\n                    pass\n                if exp != act:\n                    logger.info( 'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act))\n\n    except UnicodeEncodeError as e:\n        logger.info(e)\n        \n        \n\n\n    \n    \n    \ncheck_raw_unified_data_equals()\n"]},{"cell_type":"code","execution_count":0,"id":"20200122-034516_991349822","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\n\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n# compared the data from uniform(raw data) to unified data\nraw_path = \"s3://prod_appannie_uniform_public_app_store_data/\"\nunified_path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=weekly/process_date={unified_process_date}/process_hour={unified_process_hour}/device_code={unified_device_code}/market_code={unified_market_code}\"\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_w = s3_bucket_list.all(prefix=\"review-uniform/WEEKLY/\", depth_is_1=True)\npath_w = path_w[14:]\n\ndef get_data_frame(column_length, data_path):\n    if len(column_length) == 19:\n        # print 'this is 19'\n        raw_result = spark.read.csv(data_path, sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\",\n                                                                           \"_c2 as product_version\", \"_c3 as user_id\",\n                                                                           \"_c4 as user_name\", \"_c5 as user_language\",\n                                                                           \"_c6 as user_device\", \"_c7 as user_purchased\",\n                                                                           \"_c8 as rating\", \"_c9 as country\",\n                                                                           \"_c10 as date\", \"_c11 as language\",\n                                                                           \"_c12 as title\", \"_c13 as title_t\",\n                                                                           \"_c14 as content\", \"_c15 as content_t\",\n                                                                           \"_c16 as reply\", \"_c17 as reply_t\",\n                                                                           \"_c18 as reply_date\").take(1)\n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\"\n                      }\n        return raw_result,column_dic\n    elif len(column_length) == 21:\n        raw_result = spark.read.csv(data_path, sep=\"\\t\").selectExpr(\"_c0 as id\", \"_c1 as product_id\", \"_c2 as product_version\", \"_c3 as user_id\", \"_c4 as user_name\" , \"_c5 as user_language\", \"_c6 as user_device\", \"_c7 as user_purchased\", \"_c8 as userreview_url\", \"_c9 as rating\", \"_c10 as country\",\"_c11 as date\", \"_c12 as language\", \"_c13 as title\", \"_c14 as title_t\", \"_c15 as content\", \"_c16 as content_t\", \"_c17 as reply_id\", \"_c18 as reply\", \"_c19 as reply_t\", \"_c20 as reply_date\").take(1)\n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\", \"reply_id\":\"reply_id\", \"userreview_url\":\"userreview_url\"\n                      }\n        return raw_result,column_dic\n    elif len(column_length) == 0:\n        logger.info(\"data is empty\")\n        return [],{}\n        \n    \n    \n    \ndef check_raw_unified_data_equals():\n            # for dp in platforms_process_hour_raw_path_list:\n            #     print dp\n            #     root_raw_data_path = dp\n            #     raw_count_hourly = spark.read.csv(raw_path_d + dp, sep=\"\\t\").count()\n            #     logger.info('raw count : {}'.format(raw_count_hourly))\n    for p in path_w:\n        sub_raw_path = s3_bucket_list.all(p, depth_is_1=True)\n        for sub_path in sub_raw_path:\n            sub_path_list = s3_bucket_list.all(sub_path, depth_is_1=True)\n            for dp in sub_path_list:\n                print 'dp_path is :' , raw_path + dp\n\n                global raw_result \n                raw_result = spark.read.csv(raw_path + dp, sep=\"\\t\").take(1)\n            \n                if len(raw_result) != 0:\n                    # print raw_result[0]\n                    sample_raw_result, column_dic = get_data_frame(raw_result[0], raw_path + dp)\n                    # print sample_raw_result, column_dic\n                    if sample_raw_result:\n                        check_id = sample_raw_result[0][\"id\"]\n                        # print dp\n                        upath = unified_path.format(unified_process_date=dp.split(\"/\")[2],\n                                        unified_device_code=platforms[dp.split(\"/\")[4]][\"device_code\"],\n                                        unified_market_code=platforms[dp.split(\"/\")[4]][\"market_code\"],\n                                        unified_process_hour=dp.split(\"/\")[3])\n\n                        unified_result = spark.read.parquet(upath).filter(\"review_id='{test_id}'\".format(test_id=check_id)).take(2)\n\n                   \n\n                        for key, value in column_dic.items():\n                            check_value(key, sample_raw_result[0][key], unified_result[0][value], p)\n\n                elif len(raw_result) == 0:\n                    print 'No data'\n                    continue\n\n\ndef check_value(index, exp, act, p):\n    print index, exp, act, p\n    try:\n        if exp:\n            if index == 'rating' or index == \"product_id\":\n                exp = int(exp)\n                act = int(act)\n                if exp == act:\n                    pass\n                elif exp != act:\n                    logger.info( 'number NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp,\n                                                                                                        act))\n            elif index == 'user_purchased':\n                exp = str(exp).lower()\n                act = str(act).lower()\n                if exp == act:\n                    pass\n                if exp != act:\n                    logger.info( 'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act))\n            else:\n                if exp.lower().strip('\"') == act.lower().strip('\"'):\n                    pass\n                if exp != act:\n                    logger.info( 'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} ~ {}'.format(index, p, exp, act))\n\n    except UnicodeEncodeError as e:\n        logger.info(e)\n        \n        \n\n\n    \n    \n    \ncheck_raw_unified_data_equals()\n"]},{"cell_type":"code","execution_count":0,"id":"20200123-104139_1991770985","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nfrom pyspark.sql.utils import AnalysisException\nimport random\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n\ndef get_data_frame(spark, column_length, data_path):\n    if len(column_length) == 19:\n        raw_result = spark.read.csv(data_path, sep=\"\\t\").sample(False, 0.1, seed=0).limit(1).selectExpr(\"_c0 as id\",\n                                                                                                        \"_c1 as product_id\",\n                                                                                                        \"_c2 as product_version\",\n                                                                                                        \"_c3 as user_id\",\n                                                                                                        \"_c4 as user_name\",\n                                                                                                        \"_c5 as user_language\",\n                                                                                                        \"_c6 as user_device\",\n                                                                                                        \"_c7 as user_purchased\",\n                                                                                                        \"_c8 as rating\",\n                                                                                                        \"_c9 as country\",\n                                                                                                        \"_c10 as date\",\n                                                                                                        \"_c11 as language\",\n                                                                                                        \"_c12 as title\",\n                                                                                                        \"_c13 as title_t\",\n                                                                                                        \"_c14 as content\",\n                                                                                                        \"_c15 as content_t\",\n                                                                                                        \"_c16 as reply\",\n                                                                                                        \"_c17 as reply_t\",\n                                                                                                        \"_c18 as reply_date\").take(\n            1)\n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\"\n                      }\n        return raw_result, column_dic\n    elif len(column_length) == 21:\n        raw_result = spark.read.csv(data_path, sep=\"\\t\").sample(False, 0.1, seed=0).limit(1).selectExpr(\"_c0 as id\",\n                                                                                                        \"_c1 as product_id\",\n                                                                                                        \"_c2 as product_version\",\n                                                                                                        \"_c3 as user_id\",\n                                                                                                        \"_c4 as user_name\",\n                                                                                                        \"_c5 as user_language\",\n                                                                                                        \"_c6 as user_device\",\n                                                                                                        \"_c7 as user_purchased\",\n                                                                                                        \"_c8 as userreview_url\",\n                                                                                                        \"_c9 as rating\",\n                                                                                                        \"_c10 as country\",\n                                                                                                        \"_c11 as date\",\n                                                                                                        \"_c12 as language\",\n                                                                                                        \"_c13 as title\",\n                                                                                                        \"_c14 as title_t\",\n                                                                                                        \"_c15 as content\",\n                                                                                                        \"_c16 as content_t\",\n                                                                                                        \"_c17 as reply_id\",\n                                                                                                        \"_c18 as reply\",\n                                                                                                        \"_c19 as reply_t\",\n                                                                                                        \"_c20 as reply_date\").take(\n            1)\n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\",\n                      \"reply_id\": \"reply_id\", \"userreview_url\": \"userreview_url\"\n                      }\n        return raw_result, column_dic\n    elif len(column_length) == 0:\n        logger.info(\"data is empty\")\n        return [], {}\n\n\ndef check_raw_unified_data_equals_with_granularity_for_one_day(granularity):\n    raw_path_granularity = \"s3://prod_appannie_uniform_public_app_store_data/\"\n    unified_path_granularity = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={unified_granularity}/process_date={unified_process_date}/process_hour={unified_process_hour}/device_code={unified_device_code}/market_code={unified_market_code}\"\n\n    s3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\n    path_w = s3_bucket_list.all(prefix=\"review-uniform/{}/\".format(granularity.upper()), depth_is_1=True)\n    path_w = path_w[8:]\n    random_path = random.sample(path_w, k=10)\n    logger.info(\"random_path is {}\".format(random_path))\n    for p in random_path:\n        sub_raw_path = s3_bucket_list.all(p, depth_is_1=True)\n        for sub_path in sub_raw_path:\n            sub_path_list = s3_bucket_list.all(sub_path, depth_is_1=True)\n            for dp in sub_path_list:\n                logger.info('dp_path is : {}'.format(raw_path_granularity + dp))\n                global raw_result\n                raw_result_list = spark.read.csv(raw_path_granularity + dp, sep=\"\\t\").collect()\n                for raw_result in raw_result_list:\n                    if len(raw_result) != 0:\n                        # print raw_result[0]\n                        sample_raw_result, column_dic = get_data_frame(raw_result[0], raw_path_granularity + dp)\n                        # print sample_raw_result, column_dic\n                        if sample_raw_result:\n                            check_time = sample_raw_result[0][\"time\"]\n                            # print dp\n                            upath = unified_path_granularity.format(unified_process_date=dp.split(\"/\")[2],\n                                                                    unified_device_code=platforms[dp.split(\"/\")[4]][\n                                                                        \"device_code\"],\n                                                                    unified_market_code=platforms[dp.split(\"/\")[4]][\n                                                                        \"market_code\"],\n                                                                    unified_process_hour=dp.split(\"/\")[3],\n                                                                    unified_granularity=granularity)\n                            try:\n        \n                                unified_result = spark.read.parquet(upath).filter(\"\").take(2)\n                            except AnalysisException as e:\n                                logger.info(\n                                    'the path is not exist for unified level!!! expected unified path is {}'.format(\n                                        upath))\n        \n                            for key, value in column_dic.items():\n                                check_value(key, sample_raw_result[0][key], unified_result[0][value], p)\n        \n                    elif len(raw_result) == 0:\n                        print 'No data'\n                        continue\n\n\ndef check_value(index, exp, act, p):\n    logger.info(index, exp, act, p)\n    try:\n        if exp:\n            if index == 'rating' or index == \"product_id\":\n                exp = int(exp)\n                act = int(act)\n                if exp == act:\n                    pass\n                elif exp != act:\n                    logger.info(\n                        'number NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} #~~~~# {}'.format(index, p,\n                                                                                                               exp,\n                                                                                                               act))\n            elif index == 'user_purchased':\n                exp = str(exp).lower()\n                act = str(act).lower()\n                if exp == act:\n                    pass\n                if exp != act:\n                    logger.info(\n                        'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} #~~~~# {}'.format(index, p, exp,\n                                                                                                        act))\n            else:\n                if exp.lower().strip('\"') == act.lower().strip('\"'):\n                    pass\n                if exp != act:\n                    logger.info(\n                        'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} #~~~~# {}'.format(index, p, exp,\n                                                                                                        act))\n\n    except UnicodeEncodeError as e:\n        logger.info(e)\n\n\n# def main(spark, params):\n#     # assert params.get('cluster_name'), _usage(\"cluster_name is required\")\n#     graularity = [\"weekly\", \"daily\", \"hourly\"]\n#     for x in graularity:\n#         logger.info(\"start to test granulairty as {}\".format(x))\ngraularity = [\"weekly\", \"daily\", \"hourly\"]\nfor x in graularity:\n    check_raw_unified_data_equals_with_granularity_for_one_day(x)\n    logger.info(\"start to test granulairty as {}\".format(x))\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200123-030020_1380083974","metadata":{},"outputs":[],"source":["%python\nimport csv\nfrom gzip import GzipFile\nfrom StringIO import StringIO\nTSV_TITLE_ORDER = [\n    \"id\",\n    \"product_id\",\n    \"product_version\",\n    \"user_id\",\n    \"user_name\",\n    \"user_language\",\n    \"user_device\",\n    \"user_purchased\",\n    \"userreview_url\",\n    \"rating\",\n    \"country\",\n    \"date\",\n    \"language\",\n    \"title\",\n    \"title_t\",\n    \"content\",\n    \"content_t\",\n    \"reply_id\",\n    \"reply\",\n    \"reply_t\",\n    \"reply_date\"\n]\nf = open(\"/tmp/30600000000027.gz\", \"rb\")\ngz = GzipFile(None, 'rb', 9, StringIO(f.read()))\n#print gz.read()\n#gz = [gz.next()]\nreader = csv.reader(gz, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_ALL)\ntsv = [row for row in reader]\nprint \"\\n\".join((\"\\t\".join(item) for item in tsv))\n"]},{"cell_type":"code","execution_count":0,"id":"20200122-075531_399648041","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\npath_d = s3_bucket_list.all(prefix=\"review-uniform/DAILY/\", depth_is_1=True)\n\ndate_list_raw = [ x.split(\"/\")[2] for x in path_d ]\n\n\ns3_bucket_list_unified = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-review'))\npath_d_unified = s3_bucket_list_unified.all(prefix=\"unified/review.v1/fact/process_granularity=daily/\", depth_is_1=True)\ndate_list_unified = [ x.split(\"/\")[4].split(\"=\")[1] for x in path_d_unified ]\n\nnot_exist_data_set = sorted(set(date_list_raw)-set(date_list_unified))\n\nprint not_exist_data_set\n# for x in not_exist_data_set:\n#     el = \"review-uniform//{}/\".format(x)\n#     path_d.remove(el)\n# print path_d"]},{"cell_type":"code","execution_count":0,"id":"20200126-092920_1583091041","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\nfrom pyspark.sql.utils import AnalysisException\nimport random\n\nplatforms = {\n    'amazon': {\"device_code\": \"android-all\", \"market_code\": \"amazon-store\"},\n    'gp': {\"device_code\": \"android-all\", \"market_code\": \"google-play\"},\n    'ios': {\"device_code\": \"ios-all\", \"market_code\": \"apple-store\"},\n    'win': {\"device_code\": \"all\", \"market_code\": \"windows-store\"},\n    'wp': {\"device_code\": \"windows-phone\", \"market_code\": \"windows-store\"}\n}\n\n\ndef get_data_frame(column_length, data_path):\n    if len(column_length) == 19:\n        raw_result = spark.read.csv(data_path, sep=\"\\t\").sample(False, 0.1, seed=0).limit(1).selectExpr(\"_c0 as id\",\n                                                                                                        \"_c1 as product_id\",\n                                                                                                        \"_c2 as product_version\",\n                                                                                                        \"_c3 as user_id\",\n                                                                                                        \"_c4 as user_name\",\n                                                                                                        \"_c5 as user_language\",\n                                                                                                        \"_c6 as user_device\",\n                                                                                                        \"_c7 as user_purchased\",\n                                                                                                        \"_c8 as rating\",\n                                                                                                        \"_c9 as country\",\n                                                                                                        \"_c10 as date\",\n                                                                                                        \"_c11 as language\",\n                                                                                                        \"_c12 as title\",\n                                                                                                        \"_c13 as title_t\",\n                                                                                                        \"_c14 as content\",\n                                                                                                        \"_c15 as content_t\",\n                                                                                                        \"_c16 as reply\",\n                                                                                                        \"_c17 as reply_t\",\n                                                                                                        \"_c18 as reply_date\").take(\n            1)\n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\"\n                      }\n        return raw_result, column_dic\n    elif len(column_length) == 21:\n        raw_result = spark.read.csv(data_path, sep=\"\\t\").sample(False, 0.1, seed=0).limit(1).selectExpr(\"_c0 as id\",\n                                                                                                        \"_c1 as product_id\",\n                                                                                                        \"_c2 as product_version\",\n                                                                                                        \"_c3 as user_id\",\n                                                                                                        \"_c4 as user_name\",\n                                                                                                        \"_c5 as user_language\",\n                                                                                                        \"_c6 as user_device\",\n                                                                                                        \"_c7 as user_purchased\",\n                                                                                                        \"_c8 as userreview_url\",\n                                                                                                        \"_c9 as rating\",\n                                                                                                        \"_c10 as country\",\n                                                                                                        \"_c11 as date\",\n                                                                                                        \"_c12 as language\",\n                                                                                                        \"_c13 as title\",\n                                                                                                        \"_c14 as title_t\",\n                                                                                                        \"_c15 as content\",\n                                                                                                        \"_c16 as content_t\",\n                                                                                                        \"_c17 as reply_id\",\n                                                                                                        \"_c18 as reply\",\n                                                                                                        \"_c19 as reply_t\",\n                                                                                                        \"_c20 as reply_date\").take(\n            1)\n        column_dic = {\"product_id\": \"app_id\", \"product_version\": \"product_version\", \"user_id\": \"user_id\",\n                      \"user_name\": \"user_name\", \"user_language\": \"user_language\", \"user_device\": \"user_device\",\n                      \"user_purchased\": \"user_purchased\", \"rating\": \"rating\", \"country\": \"country_code\", \"date\": \"time\",\n                      \"title\": \"title\", \"content\": \"content\", \"reply\": \"reply\", \"reply_date\": \"reply_date\",\n                      \"reply_id\": \"reply_id\", \"userreview_url\": \"userreview_url\"\n                      }\n        return raw_result[0], column_dic\n    elif len(column_length) == 0:\n        logger.info(\"data is empty\")\n        return [], {}\n\n\ndef check_raw_unified_data_equals_with_granularity_for_one_day(granularity):\n    raw_path_granularity = \"s3://prod_appannie_uniform_public_app_store_data/\"\n    unified_path_granularity = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={unified_granularity}/process_date={unified_process_date}/process_hour={unified_process_hour}/device_code={unified_device_code}/market_code={unified_market_code}\"\n\n    s3_bucket_list = s3.S3Bucket(Conf(bucket_name='prod_appannie_uniform_public_app_store_data'))\n    path_w = s3_bucket_list.all(prefix=\"review-uniform/{}/\".format(granularity.upper()), depth_is_1=True)\n    path_w = path_w[8:]\n    random_path = random.sample(path_w, k=10)\n    logger.info(\"random_path is {}\".format(random_path))\n    for p in random_path:\n        sub_raw_path = s3_bucket_list.all(p, depth_is_1=True)\n        for sub_path in sub_raw_path:\n            sub_path_list = s3_bucket_list.all(sub_path, depth_is_1=True)\n            for dp in sub_path_list:\n                logger.info('dp_path is : {}'.format(raw_path_granularity + dp))\n                global raw_result\n                raw_result_list = spark.read.csv(raw_path_granularity + dp, sep=\"\\t\").collect()\n                for raw_result in raw_result_list:\n                    print raw_result\n                    if len(raw_result) != 0:\n                        # print raw_result[0]\n                        sample_raw_result, column_dic = get_data_frame(raw_result, raw_path_granularity + dp)\n                        print 'sample_raw_result: ',sample_raw_result\n                        # print sample_raw_result, column_dic\n                        if sample_raw_result:\n                            check_time = sample_raw_result[0][\"date\"]\n                            # print dp\n                            upath = unified_path_granularity.format(unified_process_date=dp.split(\"/\")[2],\n                                                                    unified_device_code=platforms[dp.split(\"/\")[4]][\n                                                                        \"device_code\"],\n                                                                    unified_market_code=platforms[dp.split(\"/\")[4]][\n                                                                        \"market_code\"],\n                                                                    unified_process_hour=dp.split(\"/\")[3],\n                                                                    unified_granularity=granularity)\n                            try:\n        \n                                unified_result = spark.read.parquet(upath).filter(\"review_id='{}'\".format(sample_raw_result[0][\"id\"])).take(2)\n                            except AnalysisException as e:\n                                logger.info(\n                                    'the path is not exist for unified level!!! expected unified path is {}'.format(\n                                        upath))\n        \n                            for key, value in column_dic.items():\n                                check_value(key, sample_raw_result[0][key], unified_result[0][value], p)\n        \n                    elif len(raw_result) == 0:\n                        print 'No data'\n                        continue\n\n\ndef check_value(index, exp, act, p):\n    print index, exp, act, p\n    try:\n        if exp:\n            if index == 'rating' or index == \"product_id\":\n                exp = int(exp)\n                act = int(act)\n                if exp == act:\n                    pass\n                elif exp != act:\n                    logger.info(\n                        'number NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} #~~~~# {}'.format(index, p,\n                                                                                                               exp,\n                                                                                                               act))\n            elif index == 'user_purchased':\n                exp = str(exp).lower()\n                act = str(act).lower()\n                if exp == act:\n                    pass\n                if exp != act:\n                    logger.info(\n                        'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} #~~~~# {}'.format(index, p, exp,\n                                                                                                        act))\n            else:\n                if exp.lower().strip('\"') == act.lower().strip('\"'):\n                    pass\n                if exp != act:\n                    logger.info(\n                        'NOT Equal!!! !!!!!!!! index is {} , path is: {}  value is {} #~~~~# {}'.format(index, p, exp,\n                                                                                                        act))\n\n    except UnicodeEncodeError as e:\n        logger.info(e)\n\n\n# def main(spark, params):\n#     # assert params.get('cluster_name'), _usage(\"cluster_name is required\")\n#     graularity = [\"weekly\", \"daily\", \"hourly\"]\n#     for x in graularity:\n#         logger.info(\"start to test granulairty as {}\".format(x))\ngraularity = [\"weekly\", \"daily\", \"hourly\"]\nfor x in graularity:\n    check_raw_unified_data_equals_with_granularity_for_one_day(x)\n    logger.info(\"start to test granulairty as {}\".format(x))\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200127-025144_287646823","metadata":{},"outputs":[],"source":["\nspark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/HOURLY/2019-02-14/00/gp/\",sep=\"\\t\").filter(\"_c0='gp:AOqpTOF9guumkPxptpbPNMxsxuyxpSwgKy0QozyfIgqHczswqPDq4ThKXvFlf1-6Pom-jTJSsfH4sHAOoyrCAQU'\").show(20,False)\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=hourly/process_date=2019-02-14/process_hour=0/device_code=android-all/market_code=google-play/\").filter(\"review_id='gp:AOqpTOF9guumkPxptpbPNMxsxuyxpSwgKy0QozyfIgqHczswqPDq4ThKXvFlf1-6Pom-jTJSsfH4sHAOoyrCAQU'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200203-072911_470463425","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import countDistinct\n# spark.read.csv(\"s3://prod_appannie_uniform_public_app_store_data/review-uniform/MONTHLY/2017-01-31/gp/\",sep=\"\\t\").filter(\"_c0='gp:AOqpTOHUKdAGUV_C94UVZHgqaXl9iqnCCdE-8Uqp3vdNW89Qt6UV9ADIOCo9P73sEEXBRFnDmALiiklCpHj9Gr4'\").show(20,False,vertical=False)\n\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=monthly/process_date=2017-03-31/\").filter(\"(time<='2017-03-31' and time >='2017-03-01') and market_code='apple-store' and _identifier='220200118161416198'\").cache().agg(countDistinct(\"review_id\")).collect()\n"]},{"cell_type":"code","execution_count":0,"id":"20200206-100403_2057904642","metadata":{},"outputs":[],"source":["\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.storagelevel import StorageLevel\nfrom pyspark.sql import functions as F\nstart_month = datetime.strptime(\"2017-06\",\"%Y-%m\")\nend_month = datetime.strptime(\"2020-01\",\"%Y-%m\")\na_month = relativedelta(months=1)\nnow = start_month\nwhile now <= end_month:\n    path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={weekly,daily,hourly}/process_date=%s-*/\" % now.strftime(\"%Y-%m\")\n    print \"%s started\" % path \n    review_df = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").parquet(path).where(\"process_granularity in ('daily', 'weekly', 'hourly') and market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\")\n    review_df = review_df.withColumn(\"event_month\",F.from_unixtime(F.unix_timestamp(\"time\", \"yyyy-MM-dd\"), \"yyyy-MM\")).withColumn(\"event_date\",F.from_unixtime(F.unix_timestamp(\"time\", \"yyyy-MM-dd\"), \"yyyy-MM-dd\")).repartition(1000, \"event_date\").drop(\"event_date\")\n    review_df.write.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v2/\", mode=\"append\", partitionBy=[\"event_month\"])\n    now += a_month\n    print \"%s ended\" % path\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-073820_788585069","metadata":{},"outputs":[],"source":["\nfor x in dup_list:\n    result1 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_load_temp_v4/event_month=2019-09').filter(\"review_id='{}'\".format(x[\"review_id\"])).groupBy(\"process_date\",\"process_hour\",\"time\").agg({\"*\":\"count\"}).orderBy(desc(\"time\"),desc(\"process_date\"),desc(\"process_hour\")).take(1)\n    result2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_load_temp/review_agg_v1/event_month=2019-09\").filter(\"review_id='{}'\".format(x[\"review_id\"])).take(1)\n    print result1,result2\n    if result1[0][\"process_date\"]==result2[0][\"process_date\"] and result1[0][\"process_hour\"]==result2[0][\"process_hour\"] and result1[0][\"time\"]==result2[0][\"time\"]  :\n        print 'pass'\n    else:\n        print 'failed!!!',result1, result2,x\n\nfor x in process_data_set:\n    path = \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity={weekly,daily,hourly}/process_date=%s/\" % x[\"process_date\"].strftime(\"%Y-%m-%d\")\n    total_count = spark.read.option(\"basePath\",\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").parquet(path).where(\"market_code in ('apple-store','google-play')\").count()\n    if x[\"total_review\"] != total_count:\n       print \"Not Equal!!!!!!!!\", x[\"process_date\"].strftime(\"%Y-%m-%d\") ,x[\"total_review\"], total_count\n    else:\n        print x[\"process_date\"].strftime(\"%Y-%m-%d\") ,x[\"total_review\"],total_count"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}