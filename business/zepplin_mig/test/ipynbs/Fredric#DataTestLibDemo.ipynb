{"cells":[{"cell_type":"code","execution_count":0,"id":"20211220-031640_233556143","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D7_routine/version=v1.0.0/range_type=MONTH/date=2021-10-31/\n\n# aws s3 ls s3://b2c-prod-dca-bdp-data/BDP-PROD-APP-INT-QA/bdp/user_data/application_name=aa_data_test_app/application_code/version=latest/code/bdp_resources/\n\naws s3 ls s3://b2c-prod-dca-bdp-data/BDP-PROD-APP-INT-QA/bdp/user_data/application_name=aa_data_test_app_debug/application_code/version=latest/code/bdp_resources/"]},{"cell_type":"code","execution_count":0,"id":"20211220-033345_1347336023","metadata":{},"outputs":[],"source":["\n## Spark Parquet Loader Test Example\n## Load the data test lib to python runtime\nspark.sparkContext.addPyFile(\"s3://b2c-prod-dca-bdp-data/BDP-PROD-APP-INT-QA/bdp/user_data/application_name=aa_data_test_app/application_code/version=latest/code/bdp_resources/usr/lib/spark/python/dependencies.zip\")\n\n# import spark loader\nfrom data_lib.api import spark_loader\n\nloader = spark_loader(spark)\n\np_path = \"s3://b2c-prod-data-pipeline-qa/fredric/retention_data_2021_10_31/\"\n\n## Build a temp view in memory called retention\ndf = loader.load_parquet_as_table(p_path, \"retention\")\n\nprint(df.count())\ndf_metrics_names = spark.sql(\"select distinct(metric_name) from retention\")\ndf_metrics_names.show(df_metrics_names.count())\n"]},{"cell_type":"code","execution_count":0,"id":"20211220-032955_953664368","metadata":{},"outputs":[],"source":["  \n## Convert narrow table to wide example\nfrom pyspark.sql.functions import *\nfrom data_lib.api import NarrowToWideParams, convert_narrow_table_to_wide\nfrom data_lib.api import spark_loader\n\nloader = spark_loader(spark)\np_path = \"s3://b2c-prod-data-pipeline-qa/fredric/retention_data_2021_10_31/\"\n\n## Load the parquet to memory and give it a name retention_narrow\ndf = loader.load_parquet_as_table(p_path, \"retention_narrow\")\n\ndf.show(1, False)\n\nnarrow_to_wide_p = NarrowToWideParams(\"retention_narrow\", [\"app_id\", \"device_type\", \"country\", \"age\", \"gender\", \"platform\"], \"metric_name\", [\"RRD0\", \"RRD1\", \"RRD2\", \"RRD3\", \"RRD4\", \"RRD5\", \"RRD6\", \"RRD7\"], \"value\", None)\nwide_df = convert_narrow_table_to_wide(spark, narrow_to_wide_p)\n\n## Save wide df to a new path of QA s3\n# wide_df.write.parquet(\"s3://b2c-prod-data-pipeline-qa/fredric/retention_data_2021_10_31_wide/\")\n\nwide_df.show(5, False)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20211220-034752_1590476511","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql.functions import *\nfrom data_lib.api import spark_loader\nfrom data_lib.api.data_op import compare_columns_and_get_result_df\n\n## Load wide df & do some comparison\n\nloader = spark_loader(spark)\np_path = \"s3://b2c-prod-data-pipeline-qa/fredric/retention_data_2021_10_31_wide/\"\n\n## this one is raw data in S3\nwide_df = loader.load_parquet_as_table(p_path, \"retention_narrow\")\n\nprint(wide_df.count())\n\nsf_df = loader.load_sf_as_table(\"select * from AA_INTELLIGENCE_PRODUCTION.ADL_USAGE_PAID.FACT_USAGE_RETENTION_DAY_V1_CLUSTER_BY_DATE where date_key=20211031 \", \"retention_sf\")\n## Snowfake data here\nprint(sf_df.count())\n\njoined_df = wide_df.join(sf_df, [wide_df.app_id==sf_df.PRODUCT_KEY, wide_df.country==sf_df.COUNTRY_CODE,  wide_df.device_type==sf_df.DEVICE_KEY], \"inner\")\n\njoined_df = joined_df.withColumn(\"RRD1_SRC\", (col(\"EST_TOTAL_RETENTION_USERS_D0\") * col(\"RRD1\")).cast(\"int\")) \\\n    .withColumn(\"RRD2_SRC\",  (col(\"EST_TOTAL_RETENTION_USERS_D0\") * col(\"RRD2\")).cast(\"int\")) \\\n    .withColumn(\"RRD1_DST\", col(\"EST_TOTAL_RETENTION_USERS_D1\").cast(\"int\")) \\\n    .withColumn(\"RRD2_DST\", col(\"EST_TOTAL_RETENTION_USERS_D2\").cast(\"int\"))\n\n## \"RRD1_SRC\", \"RRD2_SRC\"  Raw Data metrics\n## \"RRD1_DST\", \"RRD2_DST\"  Snowfake data metrics\nres_df = compare_columns_and_get_result_df(joined_df, [\"RRD1_SRC\", \"RRD2_SRC\"], [\"RRD1_DST\", \"RRD2_DST\"])\n\nequals_df = res_df.filter(col(\"cal_result\") == True)\nnot_equals_df = res_df.filter(col(\"cal_result\") == False)\nprint(equals_df.count())\nprint(not_equals_df.count())\n\nnot_equals_df.show(1)\n"]},{"cell_type":"code","execution_count":0,"id":"20211220-035403_640032694","metadata":{},"outputs":[],"source":["\n\n## Extended\n## Apache Arrow Loader, a C++ based column-storaged library.\nfrom data_lib.api import arrow_loader\n\nloader = arrow_loader() \nquery = \"select * from AA_INTELLIGENCE_PRODUCTION.ADL_USAGE_PAID.FACT_USAGE_RETENTION_DAY_V1_CLUSTER_BY_DATE where date_key=20211031\"\ndf_table = loader.load_sf(query)\n\nis_greater_than_zero = True\nfor i in range(0, len(df_table)):\n    r_d1 = df_table[\"EST_TOTAL_RETENTION_USERS_D1\"][i].as_py()\n    if r_d1 is not None and  r_d1 < 0:\n        print(r_d1)\n        is_greater_than_zero = False\n        break\n\nassert is_greater_than_zero is True, \"Test retention_d1 greater than zero failed\"\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20211220-105154_449290418","metadata":{},"outputs":[],"source":["\n\n## Extended\n## Pandas loader \n\n## Highly Recommend you not to use pandas loader & Operations, it's too slower\n\nfrom data_lib.api import pd_loader\n\nloader = pd_loader() \nquery = \"select * from AA_INTELLIGENCE_PRODUCTION.ADL_USAGE_PAID.FACT_USAGE_RETENTION_DAY_V1_CLUSTER_BY_DATE where date_key=20211031\"\ndf_table = loader.load_sf(query)\n\ni = 0\nis_greater_than_zero = True\nfor _, row in df_table.iterrows():\n    r_d1 = row[\"EST_TOTAL_RETENTION_USERS_D1\"]\n    if r_d1 is not None and  r_d1 < 0:\n        is_greater_than_zero = False\n        break\n\nassert is_greater_than_zero is True, \"Test retention_d1 greater than zero failed\"\n\n    \n"]},{"cell_type":"code","execution_count":0,"id":"20211220-115956_1269680208","metadata":{},"outputs":[],"source":[" \n\nfrom pyspark.sql.functions import *\nfrom data_lib.api import spark_loader\nfrom data_lib.api.data_op import compare_columns_and_get_result_df\nfrom data_lib.api import Conv\n\n## Load wide df & Then convert to Apache arrow df and do some comparison\n\nloader = spark_loader(spark)\np_path = \"s3://b2c-prod-data-pipeline-qa/fredric/retention_data_2021_10_31_wide/\"\n\n## this one is raw data in S3\nwide_df = loader.load_parquet_as_table(p_path, \"retention_wide\")\n\n## Snowfake data here\nsf_df = loader.load_sf_as_table(\"select * from AA_INTELLIGENCE_PRODUCTION.ADL_USAGE_PAID.FACT_USAGE_RETENTION_DAY_V1_CLUSTER_BY_DATE where date_key=20211031 \", \"retention_sf\")\n\n\n## Join snowflake and raw data, then select the specific field\njoined_df = wide_df.join(sf_df, [wide_df.app_id==sf_df.PRODUCT_KEY, wide_df.country==sf_df.COUNTRY_CODE,  wide_df.device_type==sf_df.DEVICE_KEY], \"inner\") \\\n    .select(\"RRD1\", \"RRD2\", \"EST_TOTAL_RETENTION_USERS_D0\", \"EST_TOTAL_RETENTION_USERS_D1\", \"EST_TOTAL_RETENTION_USERS_D2\")\n\nprint(joined_df.count())\n\n## Convert pyspark table to Apache Arrow table\narrow_df = Conv.spark_df_to_arrow(joined_df)\n\n\n## Do some comparison using raw Python API\nis_equal = True\n\nfor i in range(0, len(arrow_df)):\n    total_rd = arrow_df[\"EST_TOTAL_RETENTION_USERS_D0\"][i].as_py()\n    \n    src_rd_percent1 = arrow_df[\"RRD1\"][i].as_py()\n    src_rd_percent2 = arrow_df[\"RRD2\"][i].as_py()\n    \n    act_r_d1 = arrow_df[\"EST_TOTAL_RETENTION_USERS_D1\"][i].as_py()\n    act_r_d2 = arrow_df[\"EST_TOTAL_RETENTION_USERS_D2\"][i].as_py()\n    if act_r_d1 is not None and act_r_d2 is not None and  act_r_d1 >= 0:\n        src_rd1 = int(total_rd * src_rd_percent1)\n        src_rd2 = int(total_rd * src_rd_percent2)\n        dst_rd1 = int(act_r_d1)\n        dst_rd2 = int(act_r_d2)\n        if i < 10:\n            print(\"src_rd1: \" + str(src_rd1))\n            print(\"src_rd2: \" + str(src_rd2))\n            print(\"dst_rd1: \" + str(dst_rd1))\n            print(\"dst_rd2: \" + str(dst_rd2)) \n        if src_rd1 != dst_rd1 or src_rd2 != dst_rd2:\n            is_equal = False\n            break\n\nassert is_equal is True, \"Test retention_d1 and  retention_d2 equals failed\""]},{"cell_type":"code","execution_count":0,"id":"20211227-074047_1026083621","metadata":{},"outputs":[],"source":["\n\nfrom data_lib.api import get_date_granularity_mapping_list\nimport datetime\n\n## Test get date granularity mapping list\nbegin_date = datetime.datetime(2020, 9, 20, 16, 59)\nend_date = datetime.datetime(2020, 9, 26, 16, 59)\ndate_granularity_mapping_list = get_date_granularity_mapping_list(begin_date, end_date)\n\nprint(date_granularity_mapping_list)\n"]},{"cell_type":"code","execution_count":0,"id":"20211227-092209_688710270","metadata":{},"outputs":[],"source":["\n\nfrom data_lib.api import arrow_loader\nfrom data_lib.utils.const import AURORA_OPTIONS\n\nloader =  arrow_loader()\ndf = loader.load_aurora(\"select * from adl_master.dim_event_service_v1 limit 10\", AURORA_OPTIONS)\n\n\nfor i in range(0, len(df)):\n    print(str(df[\"product_key\"][i].as_py()) + \"\\t\" + df[\"event_type_name\"][i].as_py() + \"\\t\" + df[\"old_value\"][i].as_py() + \"\\t\" + df[\"new_value\"][i].as_py())\n    print(\"\\n\")\n    \n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20211230-065549_1981438359","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}