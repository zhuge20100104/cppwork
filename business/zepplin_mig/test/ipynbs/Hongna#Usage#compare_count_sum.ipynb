{"cells":[{"cell_type":"code","execution_count":0,"id":"20201121-044431_1334843136","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY, PG_AA_HOSTS, PG_AA_NAME,PG_AA_ACCESS_ID,PG_AA_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\nPG_AA_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2019, 1, 1)\nend_date = datetime(2020, 9, 30)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n    }\n\n\nDATE_GRANULARITY_MAPPINGLIST[\"monthly\"].reverse()\nDATE_GRANULARITY_MAPPINGLIST[\"weekly\"].reverse()\nDATE_GRANULARITY_MAPPINGLIST[\"daily\"].reverse()"]},{"cell_type":"code","execution_count":0,"id":"20201124-023231_1580578293","metadata":{},"outputs":[],"source":["\ngranularity_list = [\"monthly\", \"weekly\", \"daily\"]\ngranularity_mapping = {\"weekly\":\"WEEK\", \"daily\":\"DAY\", \"monthly\":\"MONTH\"}\n# granularity_list = [\"monthly\"]\n\nusage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=app/\"\nusage_basic_before_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\"\ndomain_before_transform = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity={granularity}/month={month}/date={date}/\"\napp_basic_raw_path = \"s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={range_type}/date={date}/\"\n\ndef compare_count():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            # filter_str = \"date ='{date}' and granularity = '{granularity}' and est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0 \".format(date=date, granularity=granularity)\n            filter_raw = \"AU is not null and AU <> 0 and UP is not null and UP <> 0 \".format(date=date, granularity=granularity[:-2].upper())\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"parquet\").load(app_basic_raw_path.format(range_type=granularity_mapping[granularity], date=date)).filter(filter_raw).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select count(1) as before_trans from before_trans \").collect()\n            df_after = spark.sql(\"select count(1) as after_trans from after_trans\").collect()\n            if df_before[0][0] != df_after[0][0]:\n                print granularity, date, \"before\", df_before[0][0], \"after\", df_after[0][0]\n            else:\n                print granularity, date, \"PASS\"\n\ndef compare_sum_value():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str = \"date ='{date}' and granularity = '{granularity}' and est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0 \".format(date=date, granularity=granularity)\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(usage_basic_before_transform).filter(filter_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select sum(est_average_active_users) as before_trans from before_trans \").collect()\n            df_after = spark.sql(\"select sum(est_active_users) as after_trans from after_trans \").collect()\n            diff =  abs(df_before[0][0] - df_after[0][0]) / df_before[0][0]\n            if diff > 0.000001:\n                print granularity, date, \"before\", df_before[0][0], \"after\", df_after[0][0]\n            else:\n                print granularity, date, \"PASS\"\n\ndef compare_domain_count():\n    usage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=domain/\"\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str = \"est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0 \"\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(domain_before_transform.format(month=date[:-3].replace(\"-\", \"\"), granularity=granularity[0], date=date)).filter(filter_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select count(1) as before_trans from before_trans \").collect()\n            df_after = spark.sql(\"select count(1) as after_trans from after_trans\").collect()\n            if df_before[0][0] != df_after[0][0]:\n                print granularity, date, \"Failed\", df_before[0][0], df_after[0][0]\n            else:\n                print granularity, date, \"PASS\", df_before[0][0], df_after[0][0]\n\n\ndef compare_domain_sum_value():\n    usage_basic_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v6/fact/product_type_code=domain/\"\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_str = \"est_average_active_users is not null and est_average_active_users <> 0 and est_usage_penetration is not null and est_usage_penetration <> 0 \"\n            filter_str_after = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"delta\").load(domain_before_transform.format(month=date[:-3].replace(\"-\", \"\"), granularity=granularity[0], date=date)).filter(filter_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(usage_basic_after_transform).filter(filter_str_after).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select sum(est_average_active_users) as before_trans from before_trans\").collect()\n            df_after = spark.sql(\"select sum(est_active_users) as after_trans from after_trans\").collect()\n            diff =  abs(df_before[0][0] - df_after[0][0]) / df_before[0][0]\n            if diff > 0.000001:\n                print granularity, date, \"Failed\", df_before[0][0], df_after[0][0]\n            else:\n                print granularity, date, \"PASS\", df_before[0][0], df_after[0][0]\n        \ncompare_domain_sum_value()\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20201121-041903_2008890679","metadata":{},"outputs":[],"source":["\nage_gender_before_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity={granularity}/month={date}/\"\nage_gender_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.seg-by-age-gender.v6/fact/granularity_code={granularity}/date={date}/\"\n\ngranularity_list = ['monthly','weekly']\n\ndef compare_count():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_before_str = \"kpi=1 and estimate is not null and estimate <> 0 and date='{date}'\".format(date=date)\n            # filter_after_str = \"date = '{date}' and granularity_code = '{granularity}'\".format(date=date, granularity=granularity)\n            spark.read.format(\"parquet\").load(age_gender_before_transform.format(granularity=granularity, date=date[:-3])).filter(filter_before_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(age_gender_after_transform.format(granularity=granularity, date=date)).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select count(1) as before_trans from before_trans \").collect()\n            df_after = spark.sql(\"select count(1) as after_trans from after_trans\").collect()\n            if df_before[0][0] != df_after[0][0]:\n                print granularity, date, \"before\", df_before[0][0], \"after\", df_after[0][0]\n            else:\n                print granularity, date, \"PASS\"\n                \ndef compare_sum_value():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_before_str = \"kpi=1 and estimate is not null and estimate <> 0 and date='{date}'\".format(date=date)\n            spark.read.format(\"parquet\").load(age_gender_before_transform.format(granularity=granularity, date=date[:-3])).filter(filter_before_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(age_gender_after_transform.format(granularity=granularity, date=date)).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select sum(estimate) as before_trans from before_trans \").collect()\n            df_after = spark.sql(\"select sum(est_active_users) as after_trans from after_trans \").collect()\n            diff =  abs(df_before[0][0] - df_after[0][0]) / df_before[0][0]\n            if diff > 0.000001:\n                print granularity, date, \"before\", df_before[0][0], \"after\", df_after[0][0]\n            else:\n                print granularity, date, \"PASS\"\n                \ncompare_sum_value()"]},{"cell_type":"code","execution_count":0,"id":"20201121-050928_321026543","metadata":{},"outputs":[],"source":["\nseg_by_product_before_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity={granularity}/month={date}/\"\nseg_by_product_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.seg-by-product.v6/fact/granularity_code={granularity}/date={date}/\"\n\ngranularity_list = ['monthly','weekly']\n\ndef compare():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_before_str = \"kpi=9 and estimate is not null and estimate <> 0 and date='{date}'\".format(date=date)\n            # filter_after_str = \"est_active_users is not null and est_active_users <> 0\"\n            spark.read.format(\"parquet\").load(seg_by_product_before_transform.format(granularity=granularity, date=date[:-3])).filter(filter_before_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(seg_by_product_after_transform.format(granularity=granularity, date=date)).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select count(1) as before_trans_count, sum(estimate) as sum from before_trans \").collect()\n            df_after = spark.sql(\"select count(1) as after_trans_count, sum(est_usage_penetration) as sum from after_trans\").collect()\n            diff =  abs(df_before[0][1] - df_after[0][1]) / df_before[0][1]\n            \n            if  diff < 0.000001 and (df_before[0][0] == df_after[0][0]):\n                print granularity, date, \"PASS\"\n            else:\n                 print granularity, date, \"before_count\", df_before[0][0], \"after_count\", df_after[0][0], \"before_sum\", df_before[0][1], \"after_sum\", df_after[0][1]\n                \ncompare()"]},{"cell_type":"code","execution_count":0,"id":"20201122-032253_395628644","metadata":{},"outputs":[],"source":["\nseg_by_product_before_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=monthly/month=2019-01/\"\nfilter_before_str = \"kpi=9 and estimate is not null and estimate <> 0 and date='2019-01-31'\"\nspark.read.format(\"parquet\").load(seg_by_product_before_transform).filter(filter_before_str).createOrReplaceTempView(\"before_trans\")\nspark.sql(\"select count(1) from  before_trans\").show(1, False)"]},{"cell_type":"code","execution_count":0,"id":"20201121-061938_620330684","metadata":{},"outputs":[],"source":["\napp_x_app_before_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity={granularity}/month={date}/\"\ncross_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.cross-product.v6/fact/\"\n\ngranularity_list = ['monthly']\n\ndef compare():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_before_str = \"kpi=9 and estimate is not null and estimate <> 0 and date='{date}'\".format(date=date)\n            spark.read.format(\"parquet\").load(app_x_app_before_transform.format(granularity=granularity, date=date[:-3])).filter(filter_before_str).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(cross_after_transform).filter(\"cross_type = 'app_cross_app' and granularity_code='{granularity}' and date='{date}'\".format(granularity=granularity, date=date)).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select count(1) as before_trans_count, sum(estimate) as sum from before_trans \").collect()\n            df_after = spark.sql(\"select count(1) as after_trans_count, sum(est_usage_penetration) as sum from after_trans\").collect()\n            diff =  abs(df_before[0][1] - df_after[0][1]) / df_before[0][1]\n            \n            if  diff < 0.000001 and (df_before[0][0] == df_after[0][0]):\n                print granularity, date, \"PASS\"\n            else:\n                print granularity, date, \"before_count\", df_before[0][0], \"after_count\", df_after[0][0], \"before_sum\", df_before[0][1], \"after_sum\", df_after[0][1]\n                \ncompare()"]},{"cell_type":"code","execution_count":0,"id":"20201122-030023_443865176","metadata":{},"outputs":[],"source":["\nspark.sql(\"select count(1) from after_trans group by device_code order by device_code\").show(10, False)\nspark.sql(\"select count(distinct store_id) from before_trans group by device_id order by device_id\").show(10, False)"]},{"cell_type":"code","execution_count":0,"id":"20201121-061201_1592859327","metadata":{},"outputs":[],"source":["\napp_retention_before_transfrom = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-rt_app.v2/fact/granularity={granularity}/month={date}/\"\nretention_after_transform = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.cross-product.v6/fact/\"\n\ngranularity_list = ['monthly']\n\ndef compare():\n    for granularity in granularity_list:\n        for date in  DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            filter_before_str = \"kpi=9 and estimate is not null and estimate <> 0 and date='{date}'\".format(date=date)\n            spark.read.format(\"parquet\").load(app_x_app_before_transform.format(granularity=granularity, date=date[:-3])).createOrReplaceTempView(\"before_trans\")\n            spark.read.format(\"delta\").load(cross_after_transform).filter(\"cross_type = 'app_cross_app' and granularity_code='monthly' and date='2019-01-31'\").createOrReplaceTempView(\"after_trans\")\n            spark.read.format(\"delta\").load(cross_after_transform).createOrReplaceTempView(\"after_trans\")\n            df_before = spark.sql(\"select count(1) as before_trans_count, sum(estimate) as sum from before_trans \").collect()\n            df_after = spark.sql(\"select count(1) as after_trans_count, sum(est_usage_penetration) as sum from after_trans\").collect()\n            diff =  abs(df_before[0][1] - df_after[0][1]) / df_before[0][1]\n            \n            if  diff < 0.000001 and (df_before[0][0] == df_after[0][0]):\n                print granularity, date, \"PASS\"\n            else:\n                 print granularity, date, \"before_count\", df_before[0][0], \"after_count\", df_after[0][0], \"before_sum\", df_before[0][1], \"after_sum\", df_after[0][1]\n                \ncompare()"]},{"cell_type":"code","execution_count":0,"id":"20201121-043746_1103312258","metadata":{},"outputs":[],"source":["%%sh\nretention = s3://b2c-prod-data-pipeline-unified-usage/unified/usage.retention.v6/fact/product_type_code=domain/\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/"]},{"cell_type":"code","execution_count":0,"id":"20201121-043804_1354198392","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}