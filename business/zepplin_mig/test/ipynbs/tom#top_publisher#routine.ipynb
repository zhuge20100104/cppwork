{"cells":[{"cell_type":"code","execution_count":0,"id":"20200721-093530_1587586817","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport pandas as pd\npd.set_option('expand_frame_repr', False)\n\n\nimport datetime\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\ndef debug(case_list):\n    \"\"\"\n    method of debug cases in zepplin, result will be print in standard output\n\n    e.g. test case list example\n    TestMarketSizeWeekly(trigger_datetime=t_date, methodName='test_market_size_etl_accuracy_and_completeness')\n    \"\"\"\n    std_out_origin = sys.stdout\n    std_err_origin = sys.stderr\n    try:\n        suite = unittest.TestSuite()\n        for case in case_list:\n            suite.addTest(case)\n        runner = unittest.TextTestRunner(verbosity=2, buffer=True)\n        runner.run(suite)\n    except Exception as ex:\n        print dir(ex)\n        print ex.message\n        traceback.print_exception(type(ex), ex, ex.__traceback__)\n    finally:\n        sys.stdout = std_out_origin\n        sys.stderr = std_err_origin\n\n\ndef load_context_zepplin(spark):\n    \"\"\"\n    load context in zepplin\n    \"\"\"\n    spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n    spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/code.zip\")\n\ndef _compare_df(self, df1, df2, log=''):\n    for diff_type in [\"left\", \"right\"]:\n        diff_df = df1.merge(df2, indicator=True, how=diff_type)  # .loc[lambda x : x['_merge']!='both']\n        diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n        if len(diff_df) != 0:\n            print diff_type\n            print \"dataframe overview of df1 and df2\"\n            print df1\n            print df2\n            print \"dimension overview of diff df\"\n            print diff_df.country_code.unique()\n            print diff_df.category_id.unique()\n            print diff_df.device_code.unique()\n        self.assertEqual(len(diff_df), 0,\n                         msg=\"found mismatch when compare the raw, unified, db.\"\n                             \" diff count is \\n {}, logs:{}\".format(len(diff_df), log))\n        print \"PASS - {}\".format(log)\n        \nimport unittest\nimport datetime\nfrom applications.db_check_v1.common.utils import string_to_datetime, datetime_to_string\nfrom applications.db_check_v1.common.db_check_utils import _get_date_from_refresh_routing_config, etl_skip\nfrom applications.db_check_v1.common.constants import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\nCITUS_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=CITUS_USAGE_NAME,\n        user=CITUS_USAGE_ACCESS_ID,\n        host=CITUS_USAGE_HOSTS[0][0],\n        password=CITUS_USAGE_SECRET_KEY,\n        port=CITUS_USAGE_HOSTS[0][1]\n    )\n)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200721-093400_80964622","metadata":{},"outputs":[],"source":["\n\n# Copyright (c) 2018 App Annie Inc. All rights reserved.\n# pylint: disable=E1101,C0412,C1801\n\n\"\"\"\nDB Check modules\n\"\"\"\n\nimport datetime\n\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df, etl_skip\nfrom applications.db_check_v1.common.constants import COUNTRY_CODE_MAPPING_BY_MARKET_CODE as COUNTRY_CODE_MAPPING, \\\n    CATEGORY_ID_MAPPING_BY_MARLKET_AND_DEVICE_CODE as CATEGORY_ID_MAPPING\nfrom applications.db_check_v1.common.utils import get_week_start_end_date, get_date_list\nfrom applications.db_check_v1.cases.store.app_est_publisher_v1.constants import PUBLISHER_EST_DSN\n\n# todo need moved to constant\nDB_PUBLISHER_EST_METRICS = [\"est_free_app_download\", \"est_paid_app_download\", \"est_revenue\"]\n\nclass PublisherEstRawData(object):\n    raw_s3_path = \"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY\" \\\n                  \"/date={}/\"\n    device_code_mapping = {\n        \"00\": \"android-all\",\n        \"01\": \"android-all\",\n        \"02\": \"android-all\",\n        \"10\": \"ios-phone\",\n        \"11\": \"ios-phone\",\n        \"12\": \"ios-phone\",\n        \"1100\": \"ios-tablet\",\n        \"1101\": \"ios-tablet\",\n        \"1102\": \"ios-tablet\",\n        # \"21000\": \"ios-all\",\n        # \"21001\": \"ios-all\",\n        # \"21002\": \"ios-all\",\n    }\n\n    metric_mapping = {\n        0: \"free_app_download\",\n        1: \"paid_app_download\",\n        2: \"revenue\",\n        101: \"free_app_download\",\n        100: \"paid_app_download\",\n        102: \"revenue\",\n        # 1000: \"free_app_download\",\n        # 1001: \"paid_app_download\",\n        # 1002: \"revenue\"\n    }\n\n    dimension_mapping = {\n        \"id\": \"publisher_id\",\n    }\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code):\n        df = self._get_raw_data_by_date_country(date, country_code)\n        df = self._parse_mapping(df)\n        df = self._parse_unified_format(df)\n        df = self._data_clean_up(df)\n        return df\n\n    def _data_clean_up(self, df):\n        # clean unknown mapping\n        category_id_list = list(set(CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].values() +\n                                    CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].values()))\n\n        country_code_list = list(set(COUNTRY_CODE_MAPPING[\"apple-store\"].values() +\n                                     COUNTRY_CODE_MAPPING[\"google-play\"].values()))\n\n        df = df[(df['category_id'].isin(category_id_list)) & (df['country_code'].isin(country_code_list))]\n        return df\n\n    def _parse_mapping(self, df):\n        # country_code mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"google-play\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"apple-store\"]})\n        df = df.rename(columns={'store_id': 'country_code'})\n\n        # category_id mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"]})\n\n        # device_code mapping\n        df[\"device_code\"] = df[\"platform_id\"].astype(str) + df[\"feed\"].astype(str)\n        df = df.replace({\"device_code\": self.device_code_mapping})\n\n        # granularity\n        df[\"granularity\"] = \"daily\"\n\n        # metrics mapping (from feed)\n        df = df.replace({\"feed\": self.metric_mapping})\n        return df\n\n    def _parse_unified_format(self, df):\n        df = df.rename(columns=self.dimension_mapping)\n        df = df.pivot_table(index=[\"publisher_id\", \"category_id\", \"device_code\", \"country_code\", \"granularity\"],\n                            columns='feed', values='est')\n        df.reset_index(inplace=True)\n        df.columns.name = None\n        return df\n\n    def _get_raw_data_by_date_country(self, date, country_code):\n        \"\"\"\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        |        id|store_id|category_id|platform_id|vertical|rank|feed|  est|platform|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        | 284417353|       0|       6006|          1|       1|   1|1002|45235|     ios|\n        | 349554266|       0|       6006|          1|       1|   2|1002|20732|     ios|\n        |1316153435|       0|       6006|          1|       1|   3|1002|15136|     ios|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        \"\"\"\n        ios_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"apple-store\"].items() if v == country_code]\n        gp_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"google-play\"].items() if v == country_code]\n        raw_df = self.spark.read.parquet(self.raw_s3_path.format(date)).\\\n            filter('store_id in ({})'.format(\",\".join(ios_store_ids + gp_store_ids))).toPandas()\n        return raw_df\n\n    def get_metrics_count(self, date):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n        # ios_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].keys()]\n        # gp_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].keys()]\n\n        fillter_sql = \"platform_id = {} and store_id in ({}) and feed in ({})\"\n        df = self.spark.read.parquet(self.raw_s3_path.format(date))\n        feed_ids_sql = \",\".join([str(x) for x in self.metric_mapping.keys()])\n\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id), feed_ids_sql)).count() + \\\n                    df.filter(fillter_sql.format(0, \",\".join(gp_store_id), feed_ids_sql)).count()\n        return count_all\n\n    def get_v1_raw_metrics_count(self, date):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n\n        df = self.spark.read.option(\"delimiter\", \"\\t\").csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/*/sbe_est_publisher/*/*.csv.gz\".format(date))\n        fillter_sql = \"_c2 = {} and _c0 in ({})\"  # _c2 > platform_id, _c0 > store_id\n\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id))).count() + \\\n                    df.filter(fillter_sql.format(0, \",\".join(gp_store_id))).count()\n        return count_all\n\n\nclass PublisherEstUnifiedData(object):\n    unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher-dna-log.v1/\" \\\n                      \"fact/granularity=daily/date={}/\"\n    available_device_codes = ['ios-phone', 'ios-tablet', 'android-all']\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code):\n        unified_df = self.spark.read.format(\"delta\").\\\n            load(self.unified_s3_path.format(date)).filter(\"country_code = '{}'\".format(country_code)).toPandas()\n        unified_df = unified_df.drop([\"_identifier\", \"revenue_iap\", \"revenue_non_iap\", \"date\"], axis=1)\n        return unified_df\n\n    def get_metrics_count(self, date):\n        df = self.spark.read.format(\"delta\").load(self.unified_s3_path.format(date))\n        metrics_count = 0\n        device_code_list_sql = \"','\".join(self.available_device_codes)\n        for metric in DB_PUBLISHER_EST_METRICS:\n            metrics_count += df.filter(\"device_code in ('{}') and {} is not null\".format(device_code_list_sql,\n                                                                                       metric)).count()\n        return metrics_count\n\n\nclass PublisherEstDBData(object):\n    def get(self, date):\n        sql = \"SELECT * FROM store.store_est_publisher_fact_v2 WHERE date='{}'\".format(date)\n        return query_df(PUBLISHER_EST_DSN, sql)\n\n    def get_metrics_count(self, date):\n        metrics_count = 0\n        for metric in DB_PUBLISHER_EST_METRICS:\n            sql = \"SELECT count(*) AS metrics_count FROM store.store_est_publisher_fact_v2 WHERE date='{}' AND {} IS NOT NULL\".format(date, metric)\n            data = query_df(PUBLISHER_EST_DSN, sql)\n            metrics_count += data.loc[0].metrics_count\n        return metrics_count\n\n\nclass TestIOSPublisherEstWeekly(PipelineTest):\n    # Every Monday 16:00 UTC time will refresh the data of last Full Week.\n    trigger_date_config = ('* 10 * * 1', 2)\n\n    def _compare_df(self, df1, df2, log=''):\n        for diff_type in [\"left\", \"right\"]:\n            diff_df = df1.merge(df2, indicator=True, how=diff_type)  # .loc[lambda x : x['_merge']!='both']\n            diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n            if len(diff_df) != 0:\n                print diff_type\n                print \"dataframe overview of df1 and df2\"\n                print df1\n                print df2\n                print \"dimension overview of diff df\"\n                print diff_df.country_code.unique()\n                print diff_df.category_id.unique()\n                print diff_df.device_code.unique()\n            self.assertEqual(len(diff_df), 0,\n                             msg=\"found mismatch when compare the raw, unified, db.\"\n                                 \" diff count is \\n {}, logs:{}\".format(len(diff_df), log))\n\n    # @etl_skip()\n    # def test_publisher_est_etl_accuracy(self):\n    #     # Every Tuesday 16:00 UTC time will refresh the data of last Full Week.\n    #     country_code = 'US'\n    #     start_date, end_date = get_week_start_end_date(self.check_date_str)\n    #     date_list = get_date_list(start_date, end_date)\n    #     for date in date_list:\n    #         raw_df = PublisherEstRawData(self.spark).get(date, country_code)\n    #         unified_df = PublisherEstUnifiedData(self.spark).get(date, country_code)\n    #         db_df = PublisherEstDBData().get(date)\n    #\n    #         self._compare_df(raw_df, unified_df, log=\"raw / unified - {}\".format(date))\n    #         self._compare_df(unified_df, db_df, log=\"unified / db - {}\".format(date))\n\n    @etl_skip()\n    def test_publisher_est_etl_completeness(self):\n        start_date, end_date = get_week_start_end_date(self.check_date_str)\n        date_list = get_date_list(start_date, end_date)\n        for date in date_list:\n            raw_count = PublisherEstRawData(self.spark).get_metrics_count(date)\n            unified_count = PublisherEstUnifiedData(self.spark).get_metrics_count(date)\n            db_count = PublisherEstDBData().get_metrics_count(date)\n            self.assertEqual(raw_count, unified_count)\n            self.assertEqual(raw_count, db_count)\n            self.assertTrue(db_count>0)\n\n    def test_publisher_est_etl_timelines(self):\n        # Every Monday 10:00 UTC(18:00 BJ) time will refresh the data of last Full Week.\n        # E.g. 2020-02-10 10:00 the data of 2020-02-02 ~ 2020-02-08 will be ready\n        trigger_datetime = datetime.datetime.strptime(\"2020-02-10 10:00:00\", '%Y-%m-%d %H:%M:%S')\n        check_date_str_actual = self._get_check_date_from_routing_config(trigger_datetime).strftime(\"%Y-%m-%d\")\n        self.assertEqual(\"2020-02-08\", check_date_str_actual)\n\n\nclass TestGPPublisherEstWeekly(TestIOSPublisherEstWeekly):\n    # Every Tuesday 10:00 UTC(18:00 BJ) time will refresh the data of last Full Week.\n    trigger_date_config = ('* 10 * * 2', 2)\n\n    def test_publisher_est_etl_timelines(self):\n        # Every Tuesday 10:00 UTC(18:00 BJ) time will refresh the data of last Full Week.\n        # E.g. 2020-02-11 10:00 the data of 2020-02-02 ~ 2020-02-08 will be ready\n        trigger_datetime = datetime.datetime.strptime(\"2020-02-11 10:00:00\", '%Y-%m-%d %H:%M:%S')\n        check_date_str_actual = self._get_check_date_from_routing_config(trigger_datetime).strftime(\"%Y-%m-%d\")\n        self.assertEqual(\"2020-02-08\", check_date_str_actual)\n"]},{"cell_type":"code","execution_count":0,"id":"20200721-093454_1955364068","metadata":{},"outputs":[],"source":["\n\ndate = \"2020-03-02\"\n\nbegin_date = datetime.datetime(2020, 6, 1)\nend_date = datetime.datetime(2020, 7, 22)\n\ndate_list = get_date_list(begin_date, end_date, \"D\")\n\nfor date in date_list:\n    \n    count1 = PublisherEstRawData(spark).get_metrics_count(date)\n    # count1 = PublisherEstRawData(spark).get_v1_raw_metrics_count(date)\n    \n    count2 = PublisherEstUnifiedData(spark).get_metrics_count(date)\n\n    count3 = PublisherEstDBData().get_metrics_count(date)\n    print \"{}, {}, {}, {}, {}\".format(date, count1+count2-count3*2, count1, count2, count3 )\n"]},{"cell_type":"code","execution_count":0,"id":"20200722-114629_1728550516","metadata":{},"outputs":[],"source":["\n# spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher.v1/fact/granularity=daily/date=2020-02-05/device_code=ios-phone/\").show(10)\nspark.read.format(\"parquet\").load(\"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-05-01/platform=ios/\").filter(\"platform_id=1\").show(10)"]},{"cell_type":"code","execution_count":0,"id":"20200721-093652_1947618283","metadata":{},"outputs":[],"source":["%%sh\n\n#aws  s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/ | grep app-est-publisher\naws  s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher-dna-log.v1/fact/granularity=daily/\n\n# aws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-05-01/platform=ios/"]},{"cell_type":"code","execution_count":0,"id":"20200722-114436_259092976","metadata":{},"outputs":[],"source":["\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom applications.db_check_v1.common.utils import get_date_list\nimport datetime\nfrom applications.db_check_v1.cases.usage.basic_kpi_v3.test_basic_kpi_v3_routine_plproxy import CITUS_DSN\n\nPUBLISHER_EST_DSN = CITUS_DSN\n\ncheckdate = datetime.datetime.now()\n\ngranularity_db_mapping = {\n    \"daily\": \"store_est_publisher_fact_v2\",\n    \"weekly\": \"store_est_publisher_t_w_fact_v2\",\n    \"monthly\": \"store_est_publisher_t_m_fact_v2\",\n    \"quarterly\": \"store_est_publisher_t_q_fact_v2\",\n    \"yearly\": \"store_est_publisher_t_y_fact_v2\"\n}\n\ngranularity_check_rules = {\n    \"weekly\": \"daily\",\n    \"monthly\": \"daily\",\n    \"quarterly\": \"monthly\",\n    \"yearly\": \"quarterly\"\n}\n\ngranularity_freq_mapping = {\n    \"daily\": \"D\",\n    \"weekly\": \"W-SAT\",\n    \"monthly\": \"M\",\n    \"quarterly\": \"Q\",\n    \"yearly\": \"Y\"\n}\n\n\ndef check_aggr_by_sum_period(check_date, granularity):\n    # start date of publisher data\n    start_date = datetime.datetime(2010, 7, 4)\n\n    date_list = get_date_list(start_date, check_date, freq=granularity_freq_mapping[granularity])\n    start_date = (datetime.datetime.strptime(date_list[-2], '%Y-%m-%d') + datetime.timedelta(days=1))\\\n        .strftime('%Y-%m-%d')\n    end_date = date_list[-1]\n    sql = \"SELECT sum(est_revenue) FROM store.{table}  where date between '{start}' and '{end}';\"\n    compare_table = granularity_db_mapping[granularity_check_rules[granularity]]\n    compare_sql = sql.format(table=compare_table, start=start_date, end=end_date)\n\n    actual_table = granularity_db_mapping[granularity]\n    actual_sql = sql.format(table=actual_table, start=start_date, end=end_date)\n\n\n    print \"{} - {} - PASS - {}\".format(check_date, granularity, query(PUBLISHER_EST_DSN,actual_sql))\n    # c_count = query(PUBLISHER_EST_DSN, compare_sql)\n    # a_count = query(PUBLISHER_EST_DSN,actual_sql)\n    # if c_count == a_count:\n    #     print \"{} - {} - PASS - {}\".format(check_date, granularity, a_count)\n    # else:\n    #     print \"{} - {} - FAIL\".format(check_date, granularity)\n    #     print compare_sql\n    #     print actual_sql\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200804-080726_22892394","metadata":{},"outputs":[],"source":["\nmy_start_date = datetime.datetime(2010, 7, 4)\nmy_end_date =  datetime.datetime(2020, 8, 4)\n\nfor granularity in [\"monthly\"]:\n    print \"*\"*200\n    print granularity * 10\n    for my_check_date in get_date_list(my_start_date, my_end_date, freq=granularity_freq_mapping[granularity])[1:]:\n        check_aggr_by_sum_period(my_check_date, granularity)\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200804-083548_610039046","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}