{"cells":[{"cell_type":"code","execution_count":0,"id":"20191216-052706_1623782949","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2018 App Annie Inc. All rights reserved.\n# pylint: disable=E1101,C0412\n\n\"\"\"\nDB Check modules\n\"\"\"\n\nimport zlib\n\nimport pandas as pd\nimport psycopg2\nfrom pyspark.sql import functions as F\nfrom pandas.io import sql as sqlio\n\nfrom applications.db_check_v1.cases.store.app_rank_v1.constants import APP_STORE_RANK_METRICS, rank_bucket, \\\n    CATEGORY_ID_MAPPING, METRIC_MAPPING, COUNTRY_CODE_MAPPING, aa_dsn, aa_amazon_dsn, citus_dsn\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df\n\n\nclass AppStoreRankRawData(object):\n    bucket_name = \"\"\n    bucket_path = \"\"\n    data_split_str = \"\"\n    rank_list_split_str = \"\"\n    rank_split_str = \"\"\n    accept_feeds = []\n    country_code_mapping = {}\n    category_id_mapping = {}\n    metric_mapping = {}\n    market_code = \"\"\n    filename_available = []\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code, category_id):\n        df = self._get_raw_data_by_date(date)\n        df = df.loc[(df.country_code == country_code) & (df.category_id == category_id)]\n        return self._parse_unified_format(df)\n\n    def _parse_mapping(self, df):\n        if self.country_code_mapping:\n            if isinstance(self.country_code_mapping.keys()[0], int):\n                df[\"country_code\"] = pd.to_numeric(df[\"country_code\"])\n            df = df.replace({\"country_code\": self.country_code_mapping})\n\n        if self.category_id_mapping:\n            df[\"category_id\"] = pd.to_numeric(df[\"category_id\"])\n            df = df.replace({\"category_id\": self.category_id_mapping})\n\n        if self.metric_mapping:\n            df[\"metric\"] = pd.to_numeric(df[\"metric\"])\n            df = df.replace({\"metric\": self.metric_mapping})\n        return df\n\n    def _parse_unified_format(self, df):\n        columns = [\"code\", \"category_id\", \"country_code\",\n                   \"free_download\", \"paid_download\", \"revenue\", \"new_free_download\", \"new_paid_download\"]\n        data_list = []\n        for index, row in df.iterrows():\n            if not row.app_rank_list:\n                continue\n            for index, app_id in enumerate(row.app_rank_list, start=1):\n                free_download = index if row.metric == 'free_download' else None\n                paid_download = index if row.metric == 'paid_download' else None\n                revenue = index if row.metric == 'revenue' else None\n                new_free_download = index if row.metric == 'new_free_download' else None\n                new_paid_download = index if row.metric == 'new_paid_download' else None\n                data_list.append([app_id, row.category_id, row.country_code,\n                                  free_download, paid_download, revenue, new_free_download, new_paid_download])\n        new_df = pd.DataFrame(data_list, columns=columns)\n        aggregate_dict = {metric: 'min' for metric in APP_STORE_RANK_METRICS}\n        new_df = new_df.groupby(new_df['code'], as_index=False).aggregate(aggregate_dict). \\\n            reindex(columns=new_df.columns)\n        # code to app_ids\n        code_app_id_mapping = self._get_code_app_id_mapping(new_df.code.tolist())\n        if code_app_id_mapping is not None:\n            new_df = new_df.merge(code_app_id_mapping, on='code', how=\"left\").rename(columns={'id': 'app_id'})\n        else:\n            new_df = new_df.rename(columns={'code': 'app_id'})\n        return new_df\n\n    def _get_raw_data_by_date(self, date):\n        \"\"\"\n        :return: raw_data_frame\n        :rtype: list_of_dic\n        raw_data:\n        _________________________________________________________________________________\n        |    date    |   country_id   |  category_id  |   feed_id   |   rank (app_id)   |\n        |------------|----------------|---------------|-------------|-------------------|\n        | 2019-04-27 | 143441(bigint) |   6016 (int)  |   0 (int)   | 376510438(bigint) |\n        ---------------------------------------------------------------------------------\n        unified_data:\n        _____________________________________________________________________________________\n        |  country_code  |   category_id   |       app_id       | feed_name (free_download) |\n        |----------------|-----------------|--------------------|---------------------------|\n        |      'US'      | 100026 (bigint) | 376510438 (bigint) |    25 (int) (app_rank)    |\n        -------------------------------------------------------------------------------------\n        \"\"\"\n        path = \"{_bucket_path}/{_date}/23/\".format(_date=date, _bucket_path=self.bucket_path)\n        bucket = rank_bucket(self.bucket_name)\n        columns = ['date', 'country_code', 'category_id', 'metric', 'app_rank_list']\n\n        _raw_data_list = []\n        for filepath in bucket.list(path):\n            filename = filepath.replace(path, '')\n            if self.filename_available and filename not in self.filename_available:\n                continue\n            _raw_data = zlib.decompress(bucket.get(filepath))\n            for _line in _raw_data.splitlines():\n                line_data = _line.split(self.data_split_str)\n                app_rank_list = [rank_app for rank_app in line_data[4].split(self.rank_list_split_str) if\n                                 rank_app.strip() != '']\n                if self.rank_split_str:\n                    app_rank_list = [app_rank.split(self.rank_split_str)[1] for app_rank in app_rank_list]\n                line_data[4] = app_rank_list\n                _raw_data_list.append(line_data)\n        return self._parse_mapping(pd.DataFrame(_raw_data_list, columns=columns))\n\n    def _get_code_app_id_mapping(self, code_list):\n        mapping_df = pd.DataFrame([[int(code), code] for code in code_list], columns=[\"id\", \"code\"])\n        return mapping_df\n\n    def get_rank_count_and_sum_by_date(self, date):\n        raw_df = self._get_raw_data_by_date(date)\n        raw_agg_df_list = []\n        for _, row in raw_df.iterrows():\n            if not row.app_rank_list:\n                continue\n            _df = pd.DataFrame([[app_id, app_index] for app_index, app_id in enumerate(row.app_rank_list, start=1)],\n                               columns=[\"code\", \"rank\"])\n            aggregation_functions = {'rank': 'min'}\n            raw_agg_df = _df.groupby(_df['code'], as_index=False) \\\n                .aggregate(aggregation_functions) \\\n                .reindex(columns=_df.columns)\n            raw_agg_df_list.append(raw_agg_df)\n\n        raw_agg_df_all = pd.concat(raw_agg_df_list, ignore_index=True, sort=False)\n        app_id_mapping_df = self._get_code_app_id_mapping(list(set(raw_agg_df_all.code.tolist())))\n        if app_id_mapping_df is not None:\n            raw_agg_df_all = raw_agg_df_all.merge(app_id_mapping_df, on='code', how=\"left\")\n        raw_count = len(raw_agg_df_all[raw_agg_df_all.id.notnull()])\n        raw_sum = raw_agg_df_all[raw_agg_df_all.id.notnull()][\"rank\"].sum()\n        return raw_count, raw_sum\n\n\nclass MacRaw(AppStoreRankRawData):\n    bucket_name = \"prod_appannie_ios\"\n    bucket_path = \"mac/country-ranks\"\n    data_split_str = \"\\t\"\n    rank_list_split_str = \" \"\n\n    device_code = \"mac-os-mac\"\n    market_code = 'apple-store'\n\n    category_id_mapping = CATEGORY_ID_MAPPING['mac-os-mac']\n    metric_mapping = METRIC_MAPPING['mac-os-mac']\n    country_code_mapping = COUNTRY_CODE_MAPPING['ios']\n\n    filename_available = [str(id) for id in COUNTRY_CODE_MAPPING['ios']]\n\n# class IPhoneRaw(AppStoreRankRawData):\n#     bucket_name = \"prod_appannie_ios\"\n#     bucket_path = \"country-ranks\"\n#     data_split_str = \"\\t\"\n#     rank_list_split_str = \" \"\n#     accept_feeds = [0, 1, 2]\n#\n#\n# class IPadRaw(AppStoreRankRawData):\n#     bucket_name = \"prod_appannie_ios\"\n#     bucket_path = \"country-ranks\"\n#     data_split_str = \"\\t\"\n#     rank_list_split_str = \" \"\n\n\nclass AppleTvRaw(AppStoreRankRawData):\n    bucket_name = \"prod_appannie_appletv\"\n    bucket_path = \"country-ranks\"\n    data_split_str = \",\"\n    rank_list_split_str = \" \"\n    rank_split_str = \"-\"\n    country_code_mapping = None\n    category_id_mapping = CATEGORY_ID_MAPPING['tv-os-tv']\n    metric_mapping = METRIC_MAPPING['tv-os-tv']\n\n    device_code = \"tv-os-tv\"\n    market_code = 'apple-store'\n    filename_available = [COUNTRY_CODE_MAPPING['ios'][id] for id in COUNTRY_CODE_MAPPING['ios']]\n\n    # this should be a product_service\n    def _get_code_app_id_mapping(self, code_list):\n        sql_code_list = \"','\".join(code_list)\n        sql = \"select id,code from product where code in ('{}')\".format(sql_code_list)\n        conn = psycopg2.connect(aa_dsn)\n        df = sqlio.read_sql_query(sql, conn)\n        return df\n\n\nclass AmazonRaw(AppStoreRankRawData):\n    bucket_name = \"prod_appannie_amazon\"\n    bucket_path = \"country-ranks\"\n    data_split_str = \",\"\n    rank_list_split_str = \" \"\n    rank_split_str = \"-\"\n    device_code = \"android-all\"\n    market_code = 'amazon-store'\n\n    filename_available = ['FR', 'CN', 'CA', 'DE', 'JP', 'IT', 'US', 'UK', 'ES']\n\n    country_code_mapping = COUNTRY_CODE_MAPPING[market_code][device_code]\n    category_id_mapping = CATEGORY_ID_MAPPING[market_code][device_code]\n    metric_mapping = METRIC_MAPPING[market_code][device_code]\n\n    # this should be a product_service\n    def _get_code_app_id_mapping(self, code_list):\n        sql_code_list = \"','\".join(code_list)\n        sql = \"select id,code from app where code in ('{}')\".format(sql_code_list)\n        conn = psycopg2.connect(aa_amazon_dsn)\n        df = sqlio.read_sql_query(sql, conn)\n        return df\n\n\nclass AppStoreRankUnifiedData(object):\n    device_code = ''\n    s3_path = \"\"\n    category_id_mapping = {}\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code, category_id):\n        filter_str = \"country_code='{country_code}' and category_id='{category_id}'\".format(\n            country_code=country_code, category_id=category_id)\n        df = self._get_unified_df_by_date(date, filter_str)\n        return df.toPandas()\n\n    def _get_unified_df_by_date(self, date, filter_str):\n        df = self.spark.read.parquet(\"{}date={}/device_code={}/\".format(self.s3_path, date, self.device_code))\n        if filter_str:\n            df = df.filter(filter_str)\n        return df\n\n    def get_rank_count_and_sum_by_date(self, date, filter_str=None):\n        df = self._get_unified_df_by_date(date, filter_str)\n        df_agg = df.filter('app_id is not null').agg(\n            F.count(\"free_download\").alias(\"free_download_count\"),\n            F.count(\"paid_download\").alias(\"paid_download_count\"),\n            F.count(\"revenue\").alias(\"revenue_count\"),\n            F.count(\"new_free_download\").alias(\"new_free_download_count\"),\n            F.count(\"new_paid_download\").alias(\"new_paid_download_count\"),\n            F.sum(\"free_download\").alias(\"free_download_sum\"),\n            F.sum(\"paid_download\").alias(\"paid_download_sum\"),\n            F.sum(\"revenue\").alias(\"revenue_sum\"),\n            F.sum(\"new_free_download\").alias(\"new_free_download_sum\"),\n            F.sum(\"new_paid_download\").alias(\"new_paid_download_sum\")\n        ).collect()\n\n        data = df_agg[0]\n        unified_count = sum([data.free_download_count, data.paid_download_count, data.revenue_count,\n                             data.new_free_download_count, data.new_paid_download_count])\n        unified_sum = sum([data.free_download_sum or 0, data.paid_download_sum or 0, data.revenue_sum or 0,\n                           data.new_free_download_sum or 0, data.new_paid_download_sum or 0])\n        return unified_count, unified_sum\n\n\nclass AppleTvUnified(AppStoreRankUnifiedData):\n    device_code = 'tv-os-tv'\n    s3_path = \"s3://b2c-prod-data-pipeline-unified-store-free/unified/app-tech.store.app-rank.v1/fact/\"\n\n\nclass MacUnified(AppStoreRankUnifiedData):\n    device_code = 'mac-os-mac'\n    s3_path = \"s3://b2c-prod-data-pipeline-unified-store-free/unified/app-tech.store.app-rank.v1/fact/\"\n\n\nclass AmazonUnified(AppStoreRankUnifiedData):\n    device_code = 'android-all'\n    s3_path = \"s3://b2c-prod-data-pipeline-unified-store-free/unified/app-tech.store.app-rank.v2/fact/\"\n    market_code = 'amazon-store'\n    category_id_mapping = CATEGORY_ID_MAPPING[market_code][device_code]\n\n\nclass AppStoreRankDBData(object):\n    schema = \"store\"\n    table = \"store_app_rank_fact_v1\"\n    device_code = ''\n    category_id_mapping = {}\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code, category_id):\n        sql = \"SELECT * FROM {schema}.{table} WHERE date ='{date}' AND device_code='{device_code}' AND \" \\\n              \"country_code='{country_code}' AND category_id='{category_id}'\" \\\n            .format(schema=self.schema, table=self.table, date=date, device_code=self.device_code,\n                    country_code=country_code, category_id=category_id)\n        result = query_df(citus_dsn, sql)\n        return result\n\n    def get_rank_count_and_sum_by_date(self, date):\n        sql_select = [\"sum({metric}) AS {metric}_sum ,count({metric}) AS {metric}_count\".format(\n            metric=metric) for metric in APP_STORE_RANK_METRICS]\n        sql = \"SELECT {sql_select} FROM {schema}.{table} WHERE date ='{date}' AND device_code='{device_code}' AND \" \\\n              \"category_id BETWEEN {category_id_min} AND {category_id_max}\" \\\n            .format(sql_select=\",\".join(sql_select), schema=self.schema, table=self.table,\n                    date=date, device_code=self.device_code,\n                    category_id_min=min(self.category_id_mapping.values()),\n                    category_id_max=max(self.category_id_mapping.values()))\n        data = query_df(citus_dsn, sql).iloc[0]\n        db_count = sum([data.free_download_count, data.paid_download_count, data.revenue_count,\n                        data.new_free_download_count, data.new_paid_download_count])\n        db_sum = sum([data.free_download_sum or 0, data.paid_download_sum or 0, data.revenue_sum or 0,\n                      data.new_free_download_sum or 0, data.new_paid_download_sum or 0])\n        return db_count, db_sum\n\n\nclass AppleTvDB(AppStoreRankDBData):\n    device_code = 'tv-os-tv'\n    category_id_mapping = CATEGORY_ID_MAPPING[device_code]\n\n\nclass AmazonDB(AppStoreRankDBData):\n    device_code = 'android-all'\n    market_code = 'amazon-store'\n    category_id_mapping = CATEGORY_ID_MAPPING[market_code][device_code]\n\n\nclass MacDB(AppStoreRankDBData):\n    device_code = 'mac-os-mac'\n    category_id_mapping = CATEGORY_ID_MAPPING[device_code]\n\n\nclass TestAppStoreRankDaily(PipelineTest):\n    routing_config = ('* 9 * * *', 1)\n\n    def _check_app_list_equal_between_data_source(self, raw_df, unified_df, db_df):\n        for metric in APP_STORE_RANK_METRICS:\n            _raw_df = raw_df[pd.notnull(raw_df[metric])].sort_values(metric)\n            _unified_df = unified_df[pd.notnull(unified_df[metric])].sort_values(metric)\n            _db_df = db_df[pd.notnull(db_df[metric])].sort_values(metric)\n\n            data_app = {\n                'raw': _raw_df.app_id.tolist(),\n                'unified': _unified_df.app_id.tolist(),\n                'db': _db_df.app_id.tolist()\n            }\n\n            data_rank = {\n                'raw': _raw_df[metric].tolist(),\n                'unified': _unified_df[metric].tolist(),\n                'db': _db_df[metric].tolist()\n            }\n\n            self.assertTrue(data_app['raw'] == data_app['unified'] == data_app['db'],\n                            msg=\"App list not equal: {}\".format(data_app))\n            self.assertTrue(data_rank['raw'] == data_rank['unified'] == data_rank['db'],\n                            msg=\"Rank list not equal: {}\".format(data_rank))\n\n    def _check_rank_count_equal_between_data_source(self, raw_count, unified_count, db_count):\n        rank_count = {\n            'raw': raw_count,\n            'unified': unified_count,\n            'db': db_count,\n        }\n        self.assertTrue(rank_count['raw'] == rank_count['unified'] == rank_count['db'],\n                        msg=\"Rank count not equal: {}\".format(rank_count))\n\n    def _check_rank_sum_equal_between_data_source(self, raw_sum, unified_sum, db_sum):\n        rank_sum = {\n            'raw': raw_sum,\n            'unified': unified_sum,\n            'db': db_sum\n        }\n        self.assertTrue(rank_sum['raw'] == rank_sum['unified'] == rank_sum['db'],\n                        msg=\"Rank sum not equal: {}\".format(rank_sum))\n\n    def test_apple_tv_etl_accuracy(self):\n        country_code = 'US'\n        category_id = 300000\n\n        raw_df = AppleTvRaw(self.spark).get(self.check_date_str, country_code, category_id)\n        unified_df = AppleTvUnified(self.spark).get(self.check_date_str, country_code, category_id)\n        db_df = AppleTvDB(self.spark).get(self.check_date_str, country_code, category_id)\n\n        self._check_app_list_equal_between_data_source(raw_df, unified_df, db_df)\n\n    def test_amazon_etl_accuracy(self):\n        country_code = 'US'\n        category_id = 720000\n\n        raw_df = AmazonRaw(self.spark).get(self.check_date_str, country_code, category_id)\n        unified_df = AmazonUnified(self.spark).get(self.check_date_str, country_code, category_id)\n        db_df = AmazonDB(self.spark).get(self.check_date_str, country_code, category_id)\n\n        self._check_app_list_equal_between_data_source(raw_df, unified_df, db_df)\n\n    def test_mac_etl_accuracy(self):\n        country_code = 'US'\n        category_id = 200000\n\n        raw_df = MacRaw(self.spark).get(self.check_date_str, country_code, category_id)\n        unified_df = MacUnified(self.spark).get(self.check_date_str, country_code, category_id)\n        db_df = MacDB(self.spark).get(self.check_date_str, country_code, category_id)\n\n        self._check_app_list_equal_between_data_source(raw_df, unified_df, db_df)\n\n    def test_amazon_etl_completeness(self):\n        raw_count, raw_sum = AmazonRaw(self.spark).get_rank_count_and_sum_by_date(self.check_date_str)\n        unified_count, unified_sum = AmazonUnified(self.spark).get_rank_count_and_sum_by_date(self.check_date_str)\n        db_count, db_sum = AmazonDB(self.spark).get_rank_count_and_sum_by_date(self.check_date_str)\n\n        # filtered_unified_count/sum is the count and sum which category_id not in 700000 ~ 799999\n        filter_str = \"category_id NOT BETWEEN 700000 AND 799999\"\n        filtered_unified_count, filterd_unified_sum = AmazonUnified(self.spark).get_rank_count_and_sum_by_date(\n            self.check_date_str, filter_str)\n\n        # filtered data(category not in 700000 ~ 799999) will not be load to db during ETL,\n        # so here plus the filtered count/sum into db count/sum\n        self._check_rank_count_equal_between_data_source(raw_count, unified_count, db_count + filtered_unified_count)\n        self._check_rank_sum_equal_between_data_source(raw_sum, unified_sum, db_sum + filterd_unified_sum)\n\n    def test_apple_tv_etl_completeness(self):\n        raw_count, raw_sum = AppleTvRaw(self.spark).get_rank_count_and_sum_by_date(self.check_date_str)\n        unified_count, unified_sum = AppleTvUnified(self.spark).get_rank_count_and_sum_by_date(self.check_date_str)\n        db_count, db_sum = AppleTvDB(self.spark).get_rank_count_and_sum_by_date(self.check_date_str)\n\n        self._check_rank_count_equal_between_data_source(raw_count, unified_count, db_count)\n        self._check_rank_sum_equal_between_data_source(raw_sum, unified_sum, db_sum)\n\n    def test_mac_etl_completeness(self):\n        raw_count, raw_sum = MacRaw(self.spark).get_rank_count_and_sum_by_date(self.check_date_str)\n        unified_count, unified_sum = MacUnified(self.spark).get_rank_count_and_sum_by_date(self.check_date_str)\n        db_count, db_sum = MacDB(self.spark).get_rank_count_and_sum_by_date(self.check_date_str)\n\n        self._check_rank_count_equal_between_data_source(raw_count, unified_count, db_count)\n        self._check_rank_sum_equal_between_data_source(raw_sum, unified_sum, db_sum)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191217-004506_1936105774","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20191216-052728_720038731","metadata":{},"outputs":[],"source":["\n\nimport sys\nimport datetime\nimport traceback\nimport unittest\n\n# from applications.db_check_v1.cases.store.app_rank_v1.test_app_rank_v1 import TestAppStoreRankDaily\n\n\ndef debug(case_list):\n    std_out_origin= sys.stdout\n    std_err_origin= sys.stderr\n    try:\n        suite =  unittest.TestSuite()\n        for case in case_list:\n            suite.addTest(case)\n        runner = unittest.TextTestRunner(verbosity=2, buffer=True)\n        runner.run(suite)\n    except Exception as ex:\n        print dir(ex)\n        print ex.message\n        traceback.print_exception(type(ex), ex, ex.__traceback__)\n    finally:\n        sys.stdout = std_out_origin\n        sys.stderr = std_err_origin\n    \ncase_name_list = [\n    \"test_amazon_etl_accuracy\",\n    \"test_amazon_etl_completeness\",\n    \n    \"test_apple_tv_etl_accuracy\",\n    \"test_apple_tv_etl_completeness\",\n    \n    \"test_mac_etl_accuracy\",\n    \"test_mac_etl_completeness\",\n    ]\n\ntest_case_list = [ TestAppStoreRankDaily(name, datetime.datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\") ) for name in case_name_list ] # pass \n\ndebug(test_case_list)\n"]},{"cell_type":"code","execution_count":0,"id":"20191217-075146_579437628","metadata":{},"outputs":[],"source":[" \n\n\nfrom bdce.common.utils import update_application_code\nupdate_application_code(\n    spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"aa-int-qa-db-check-debug\"\n)\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200206-120427_770861291","metadata":{},"outputs":[],"source":["\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200206-121941_26840467","metadata":{},"outputs":[],"source":["%%sh\n\nls -al /tmp/zeppelin_application_code\necho 123123123\nls -al /home/hadoop/bdp/application/\n"]},{"cell_type":"code","execution_count":0,"id":"20200206-122354_892187406","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}