{"cells":[{"cell_type":"code","execution_count":0,"id":"20201015-014040_1124705994","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\nprint(PLPROXY_DSN)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2020, 7, 1)\nend_date = datetime(2020, 8, 31)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\nDATE_GRANULARITY_MAPPINGLIST[\"monthly\"]\nDATE_GRANULARITY_MAPPINGLIST[\"weekly\"]\nDATE_GRANULARITY_MAPPINGLIST[\"daily\"]\n\nprint DATE_GRANULARITY_MAPPINGLIST[\"monthly\"]\nprint DATE_GRANULARITY_MAPPINGLIST[\"weekly\"]\nprint DATE_GRANULARITY_MAPPINGLIST[\"daily\"]"]},{"cell_type":"code","execution_count":0,"id":"20201015-014149_1237181699","metadata":{},"outputs":[],"source":["\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\napp_basic_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from mu.app_{granularity}\n    where date='{date}'\n        and kpi=1\n        and estimate is not null\n        and estimate > 0\n$proxy$) tpl (count bigint);\"\"\"\n\n\nsegment_by_age_gender_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from ag.app_{granularity}\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint) ;\n\"\"\"\n\nsegment_by_audience_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from ad.app_{granularity}\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint) ;\n\"\"\"\n# check the country\n# segment_by_product_sql = \"\"\"\n# select distinct store_id AS store_id from plproxy.execute_select_nestloop($proxy$ \n#     select distinct store_id AS store_id\n#     from au.app_monthly\n#     WHERE date='2015-01-31'\n#         and kpi=1\n#         and estimate is not null\n#         and estimate <> 0\n\n# $proxy$) tpl (store_id int) order by store_id;\n# \"\"\"\n\nsegment_by_product_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from au.app_{granularity}\n    WHERE date='{date}'\n    and kpi=1\n    and estimate is not null\n    and estimate <> 0\n\n$proxy$) tpl (count bigint);\n\"\"\"\n\napp_cross_app = \"\"\"\nselect distinct store_id AS store_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct store_id AS store_id\n    from ca.app_monthly\n    where \n        date ='2015-01-31'\n        and kpi = 9\n$proxy$) tpl (store_id int) order by store_id;\n\"\"\"\n\n# app_retention = \"\"\"\n# select sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n#     select count(1) as count\n#     from rt.app_{granularity}\n#     where \n#         date ='{date}'\n# $proxy$) tpl (count bigint) ;\n# \"\"\"\n\napp_retention = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from rt.app_{granularity}\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint) ;\n\"\"\"\n\ngranularity_list = [\"monthly\"]\n\ndef get_plproxy_data(sql_str):\n    for granularity in granularity_list:\n        for date_str in DATE_GRANULARITY_MAPPINGLIST[granularity]:\n            result = query(PLPROXY_DSN, sql_str.format(granularity=granularity,date=date_str))\n            count = result[0][0]\n            print str(granularity)+str(\" \")+str(date_str)+str(\" \")+str(count)\n\n# def get_plproxy_data(sql_str):\n#     result = query(PLPROXY_DSN, sql_str)\n#     count = result[0][0]\n#     for r in result:\n#         print(r[0])\n#     # print str(\"monthly\")+str(\" \")+str(\"2017-05-31\")+str(\" \")+str(result)\n            \ndef get_unified_data():\n    print 1\n\nget_plproxy_data(segment_by_audience_sql)  "]},{"cell_type":"code","execution_count":0,"id":"20201015-014340_1946218542","metadata":{},"outputs":[],"source":["\nbasic_dump=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact/granularity=monthly/\"\nbasic=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/\"\nsegment_by_age_gender_sql=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity=/\"\nsegment_by_audience=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ad_app.v2/fact/granularity=monthly/\"\nsegment_by_product_sql=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=weekly/month=2015-01\"\napp_cross_app=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity=monthly/\"\napp_retention=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-rt_app.v2/fact/granularity=monthly/\"\nstart_str=\"2015-01\"\nend_str=\"2017-05\"\n\n# segment_by_product_sql=spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact\").parquet(segment_by_product_sql)\napp_cross_app=spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact\").parquet(app_cross_app).where(\"month>='{}'\".format(start_str)).where(\"month<='{}'\".format(end_str))\n# basic = spark.read.format('delta').load(basic).where(\"date>='{}'\".format(start_str)).where(\"date<='{}'\".format(end_str))\n# segment_by_product_sql = spark.read.format('delta').load(segment_by_product_sql)\n# print(\"count:\"+str(basic_dump.count().show()))\napp_cross_app.createOrReplaceTempView(\"app_cross_app\")\n# print(\"segment_by_product_sql.count(): \"+str(segment_by_product_sql.filter(\"kpi=1\").groupBy(\"date\").count().orderBy(\"date\").show(1500,truncate=False)))\n\napp_cross_app= spark.sql(\"\"\"\n                SELECT date,count(1)\n                FROM app_cross_app\n                WHERE kpi=9\n                and estimate is not null\n                and estimate <> 0\n                group by date\n                order by date\n                \"\"\")\nbasic= spark.sql(\"\"\"\n                SELECT date,count(1)\n                FROM basic\n                WHERE est_average_active_users is not null\n                and est_average_active_users <> 0\n                group by date\n                order by date\n                \"\"\")\nsegment_by_product_sql= spark.sql(\"\"\"\n                SELECT distinct store_id,date\n                FROM segment_by_product_sql\n                WHERE kpi=1\n                and estimate is not null\n                and estimate <> 0\n                group by date,store_id\n                order by date,store_id\n                \"\"\")\napp_cross_app= spark.sql(\"\"\"\n                SELECT date,count(1)\n                FROM app_cross_app\n                WHERE kpi=9\n                and estimate is not null\n                and estimate <> 0\n                group by date\n                order by date\n                \"\"\")\napp_cross_app.show(1000,False)"]},{"cell_type":"code","execution_count":0,"id":"20201016-022446_1621719854","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ad_app.v2/fact/granularity=monthly/\n"]},{"cell_type":"code","execution_count":0,"id":"20201016-022816_72713087","metadata":{},"outputs":[],"source":["\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import StructType, StructField, IntegerType\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\napp_basic_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from mu.app_{granularity}\n    where date='{date}'\n        and kpi=1\n        and estimate is not null\n        and estimate > 0\n$proxy$) tpl (count bigint);\"\"\"\n\n\nsegment_by_age_gender_sql = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from ag.app_{granularity}\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint) ;\n\"\"\"\n# check the country\nsegment_by_product_sql = \"\"\"\nselect distinct store_id AS store_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct store_id AS store_id\n    from au.app_weekly\n    WHERE date='2015-01-03'\n        and kpi=1\n        and estimate is not null\n        and estimate <> 0\n\n$proxy$) tpl (store_id int) order by store_id;\n\"\"\"\n\n# segment_by_product_sql = \"\"\"\n# select sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n#     select count(1) as count\n#     from au.app_{granularity}\n#     WHERE date='{date}'\n#     and kpi=1\n#     and estimate is not null\n#     and estimate <> 0\n\n# $proxy$) tpl (count bigint);\n# \"\"\"\n\napp_cross_app = \"\"\"\nselect distinct store_id AS store_id from plproxy.execute_select_nestloop($proxy$ \n    select distinct store_id AS store_id\n    from ca.app_monthly\n    where \n        date ='2015-07-31'\n        and kpi = 9\n$proxy$) tpl (store_id int) order by store_id;\n\"\"\"\n\n# app_retention = \"\"\"\n# select sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n#     select count(1) as count\n#     from rt.app_{granularity}\n#     where \n#         date ='{date}'\n# $proxy$) tpl (count bigint) ;\n# \"\"\"\n\napp_retention = \"\"\"\nselect sum(count) as count from plproxy.execute_select_nestloop($proxy$ \n    select count(1) as count\n    from rt.app_{granularity}\n    where \n        date ='{date}'\n$proxy$) tpl (count bigint) ;\n\"\"\"\n\ngranularity_list = [\"monthly\",\"weekly\"]\n\n# def get_plproxy_data(sql_str):\n#     for granularity in granularity_list:\n#         for date_str in DATE_GRANULARITY_MAPPINGLIST[granularity]:\n#             result = query(PLPROXY_DSN, sql_str.format(granularity=granularity,date=date_str))\n#             count = result[0][0]\n#             print str(granularity)+str(\" \")+str(date_str)+str(\" \")+str(count)\n\ndef get_plproxy_data(sql_str):\n    plproxy_result = []\n    result = query(PLPROXY_DSN, segment_by_product_sql)\n    df_data = [Row(store_id=r[0]) for r in result]\n    _schema =StructType([StructField(\"store_id\", IntegerType(), False)])\n    df_plproxy = spark.createDataFrame(data=df_data, schema=_schema)\n    return df_plproxy\n            \ndef get_unified_data():\n    segment_by_product_sql1=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=weekly/month=2015-01\"\n    segment_by_product_sql1=spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact\").parquet(segment_by_product_sql1)\n    # spark.read.format(\"delta\").load(unified_source_path).createOrReplaceTempView(\"test_unified\")\n    segment_by_product_sql1.createOrReplaceTempView(\"segment_by_product_sql1\")\n    segment_by_product_sql1= spark.sql(\"\"\"\n                SELECT store_id\n                FROM segment_by_product_sql1\n                WHERE kpi=1\n                and estimate is not null\n                and estimate <> 0\n                group by store_id\n                order by store_id\n                \"\"\")\n    segment_by_product_sql1.createOrReplaceTempView(\"segment_by_product_sql1\")\n\ndef compare():\n    df_plproxy=get_plproxy_data(segment_by_product_sql)\n    get_unified_data()\n    df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n    spark.sql(\"\"\"\n                select distinct store_id \n                from segment_by_product_sql1\n                 except \n                    select distinct store_id \n                    from plproxy_df_new \n                    order by store_id\n                    \"\"\").show(100,False)\n\n\n# df = get_plproxy_data(segment_by_product_sql)\ncompare()\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20201021-083231_629845829","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20201118-030803_743015749","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}