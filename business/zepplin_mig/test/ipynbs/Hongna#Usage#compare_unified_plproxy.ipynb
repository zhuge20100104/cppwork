{"cells":[{"cell_type":"code","execution_count":0,"id":"20201014-064210_540820186","metadata":{},"outputs":[],"source":["\n\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\nbegin_date = datetime(2019, 1, 1)\nend_date = datetime(2020, 8, 31)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\nDATE_GRANULARITY_MAPPINGLIST[\"monthly\"].reverse()"]},{"cell_type":"code","execution_count":0,"id":"20201014-064335_1630921411","metadata":{},"outputs":[],"source":["\nbasic_dump=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact/granularity=monthly/\"\nbasic=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/date=2017-01-31\"\nsegment_by_age_gender_sql=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ag_app.v2/fact/granularity=/\"\nsegment_by_product_sql=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-au_app.v2/fact/granularity=weekly/\"\napp_cross_app=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-ca_app.v2/fact/granularity=monthly/\"\napp_retention=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-rt_app.v2/fact/granularity=monthly/\"\nstart_str=\"2015-01\"\nend_str=\"2015-05\"\nbasic_dump=spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact\").parquet(basic_dump).where(\"month>='{}'\".format(start_str)).where(\"month<='{}'\".format(end_str))\n# basic_dump=spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact\").parquet(basic_dump).where(\"month>='{}'\".format(start_str)).where(\"month<='{}'\".format(end_str))\n# df_ag = spark.read.format('delta').load(basic).where(\"date>='{}'\".format(start_str)).where(\"date<='{}'\".format(end_str))\n# segment_by_product_sql = spark.read.format('delta').load(segment_by_product_sql)\n# print(\"count:\"+str(basic_dump.count().show()))\nbasic_dump.createOrReplaceTempView(\"basic_dump\")\n# print(\"segment_by_product_sql.count(): \"+str(segment_by_product_sql.filter(\"kpi=1\").groupBy(\"date\").count().orderBy(\"date\").show(1500,truncate=False)))\n\nbasic_dump= spark.sql(\"\"\"\n                SELECT date,count(1) AS count\n                FROM basic_dump\n                WHERE kpi=1\n                and estimate is not null\n                and estimate <> 0\n                GROUP BY date\n                ORDER BY date\n                \"\"\")\nbasic_dump.show(1000,False)"]},{"cell_type":"code","execution_count":0,"id":"20201020-103900_1151297026","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}