{"cells":[{"cell_type":"code","execution_count":0,"id":"20210618-022903_562773749","metadata":{},"outputs":[],"source":["\n# \"aa.usage.basic-kpi.v6\", s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=ios_diamond_1.0.0/ day week month\n# \"aa.usage.audience.v6\", s3://aardvark-prod-pdx-mdm-to-int/to_tech/audience/version=ios_diamond_1.0.0/   month\n# \"aa.usage.seg-by-age-gender.v6\", s3://aardvark-prod-pdx-mdm-to-int/to_tech/demographics/version=ios_diamond_1.0.0/ month week\n# \"aa.usage.app-cross-app.v6\",  s3://aardvark-prod-pdx-mdm-to-int/to_tech/crossapps/version=ios_diamond_1.0.0/ month\n# \"aa.usage.seg-by-product.v6\", s3://aardvark-prod-pdx-mdm-to-int/to_tech/appused/version=ios_diamond_1.0.0/ MONTH WEEK\n# \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_DOMAIN_BASIC_METRICS/version=ios_diamond_1.0.0\": \"aa.usage.domain.basic.v6\",\n# \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_DOMAIN_REFERRAL_METRICS/version=ios_diamond_1.0.0\": \"aa.usage.domain-referral.v6\",\n# \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_DOMAIN_OUTBOUND_METRICS/version=ios_diamond_1.0.0\": \"aa.usage.domain-outbound.v6\",\n# \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_CROSS_DOMAIN_METRICS/version=ios_diamond_1.0.0\": \"aa.usage.domain-cross-domain.v6\",\n# \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_CROSS_APP_METRICS/version=ios_diamond_1.0.0\": \"aa.usage.domain-cross-app.v6\",\n# \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_APP_CROSS_DOMAIN_METRICS/version=ios_diamond_1.0.0\": \"aa.usage.app-cross-domain.v6\",\n# \"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_UNIFIED_PRODUCT_METRICS/version=ios_diamond_1.0.0\": \"aa.usage.unified-attribution.v6\",\n# spark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/to_tech/demographics/version=ios_diamond_1.0.0/range_type=WEEK/date=2018-06-09/\").createOrReplaceTempView(\"test\")\n# spark.sql(\"select * from test\").show(20)\n# df = spark.sql(\"select gender,age,count(*),sum(AU) from test group by gender,age\".format(date='2021-01-01', granularity='MONTH')).show(20)\n\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"daily\": get_date_list(begin_date, end_date, \"D\"),\n        \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n        \"monthly\": get_date_list(begin_date, end_date, \"M\")\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-08-31\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-08-31\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\nrange_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n        try:\n            s3path = \"s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=fix_crow_1.0.0/range_type={range_type}/date={date}/platform=2/\".format(range_type=range_type_mapping[granularity], date=date)\n            spark.read.parquet(s3path).createOrReplaceTempView(\"test_basic_kpi\")\n            df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type, sum(AU) as sum from test_basic_kpi\".format(date=date, granularity=granularity))\n            df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2021-08-13/basic_kpi/\", mode=\"append\") #append\n            print(\"PASS on {} {}\".format(granularity, date))\n        except Exception as e:\n            print(\"ERROR on {} {}\".format(granularity, date))"]},{"cell_type":"code","execution_count":0,"id":"20210618-022920_1512113039","metadata":{},"outputs":[],"source":["\n\n# \"aa.usage.basic-kpi.v6\", s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=ios_diamond_1.0.0/ day week month\n# \"aa.usage.audience.v6\", s3://aardvark-prod-pdx-mdm-to-int/to_tech/audience/version=ios_diamond_1.0.0/   month\n# \"aa.usage.seg-by-age-gender.v6\", s3://aardvark-prod-pdx-mdm-to-int/to_tech/demographics/version=ios_diamond_1.0.0/ month week\n# \"aa.usage.app-cross-app.v6\",  s3://aardvark-prod-pdx-mdm-to-int/to_tech/crossapps/version=ios_diamond_1.0.0/ month\n# \"aa.usage.seg-by-product.v6\", s3://aardvark-prod-pdx-mdm-to-int/to_tech/appused/version=ios_diamond_1.0.0/ MONTH WEEK\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/basic_kpi/\").show(3)\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/audience/\").show(3)\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/demographics/\").show(3)\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/crossapps/\").show(3)\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/appused/\").show(3)\n\ndate='2021-08-13'\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/basic_kpi/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/basic_kpi.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/audience/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/audience.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/demographics/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/demographics.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/crossapps/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/crossapps.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/appused/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/appused.csv\".format(date),header = 'true')\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/domain_outbound/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/domain_outbound.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/domain_referral/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/domain_referral.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/test_app_cross_domain/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/test_app_cross_domain.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/test_domain_cross_app/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/test_domain_cross_app.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/test_domain_cross_domain/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/test_domain_cross_domain.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/test_domain_unified_attribution/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/test_domain_unified_attribution.csv\".format(date),header = 'true')\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/mw_basic_kpi/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/mw_basic_kpi.csv\".format(date),header = 'true')\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20210811-215030_81276159","metadata":{},"outputs":[],"source":["%%sh\n\n# rm -rf  /tmp/basic_kpi.csv/  /tmp/audience.csv/  /tmp/demographics.csv/  /tmp/crossapps.csv/ /tmp/appused.csv/\n\nDATE='2021-08-13'\n\nrm -rf /tmp/$DATE/\nmkdir -p /tmp/$DATE/\n\naws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/basic_kpi.csv/ /tmp/$DATE/basic_kpi.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/audience.csv/ /tmp/$DATE/audience.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/demographics.csv/ /tmp/$DATE/demographics.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/crossapps.csv/ /tmp/$DATE/crossapps.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/appused.csv/ /tmp/$DATE/appused.csv/\n\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/adhoc/$DATE/domain_outbound.csv/ /tmp/$DATE/domain_outbound.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/adhoc/$DATE/domain_referral.csv/ /tmp/$DATE/domain_referral.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/adhoc/$DATE/test_app_cross_domain.csv/ /tmp/$DATE/test_app_cross_domain.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/adhoc/$DATE/test_domain_cross_app.csv/ /tmp/$DATE/test_domain_cross_app.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/adhoc/$DATE/test_domain_cross_domain.csv/ /tmp/$DATE/test_domain_cross_domain.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/adhoc/$DATE/test_domain_unified_attribution.csv/ /tmp/$DATE/test_domain_unified_attribution.csv/\n# aws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/adhoc/$DATE/mw_basic_kpi.csv/ /tmp/$DATE/mw_basic_kpi.csv/\n\n\n\necho \"=======basic_kpi=========\"\ncat /tmp/$DATE/basic_kpi.csv/*.csv\n# echo \"========audience========\"\n# cat /tmp/$DATE/audience.csv/*.csv\n# echo \"======demographics==========\"\n# cat /tmp/$DATE/demographics.csv/*.csv\n# echo \"=====crossapps===========\"\n# cat /tmp/$DATE/crossapps.csv/*.csv\n# echo \"======appused==========\"\n# cat /tmp/$DATE/appused.csv/*.csv\n\n\n# echo \"======domain_outbound==========\"\n# cat /tmp/$DATE/domain_outbound.csv/*.csv\n# echo \"======domain_referral==========\"\n# cat /tmp/$DATE/domain_referral.csv/*.csv\n# echo \"======test_app_cross_domain==========\"\n# cat /tmp/$DATE/test_app_cross_domain.csv/*.csv\n# echo \"======test_domain_cross_app==========\"\n# cat /tmp/$DATE/test_domain_cross_app.csv/*.csv\n# echo \"======test_domain_cross_domain==========\"\n# cat /tmp/$DATE/test_domain_cross_domain.csv/*.csv\n# echo \"======test_domain_unified_attribution==========\"\n# cat /tmp/$DATE/test_domain_unified_attribution.csv/*.csv\n# echo \"======mw_basic_kpi==========\"\n# cat /tmp/$DATE/mw_basic_kpi.csv/*.csv"]},{"cell_type":"code","execution_count":0,"id":"20210811-215353_1512076939","metadata":{},"outputs":[],"source":["\n\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\")\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-06-03\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-08-31\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\nrange_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n        try:\n            s3path = \"s3://aardvark-prod-pdx-mdm-to-int/to_tech/audience/version=fix_crow_1.0.0/range_type={range_type}/date={date}/\".format(range_type=range_type_mapping[granularity], date=date)\n            spark.read.parquet(s3path).createOrReplaceTempView(\"test_audience\")\n            df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type,count(distinct(age)) as count_age, count(distinct(gender)) as count_gender, sum(IDX) as sum from test_audience\".format(date=date, granularity=granularity))\n            df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2021-08-13/audience/\", mode=\"append\") #append\n            print(\"PASS on {} {}\".format(granularity, date))\n        except Exception as e:\n            print(\"ERROR on {} {}\".format(granularity, date))"]},{"cell_type":"code","execution_count":0,"id":"20210811-221036_128267383","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\")\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-06-03\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-08-31\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\nrange_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n        try:\n            s3path = \"s3://aardvark-prod-pdx-mdm-to-int/to_tech/crossapps/version=fix_crow_1.0.0/range_type={range_type}/date={date}/\".format(range_type=range_type_mapping[granularity], date=date)\n            spark.read.parquet(s3path).createOrReplaceTempView(\"test_crossapps\")\n            df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type,count(distinct(selected_app)) as count_selected_app, sum(CROSSAPP_UP) as sum from test_crossapps\".format(date=date, granularity=granularity))\n            df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2021-08-13/crossapps/\", mode=\"append\") #append\n            print(\"PASS on {} {}\".format(granularity, date))\n        except Exception as e:\n            print(\"ERROR on {} {}\".format(granularity, date))"]},{"cell_type":"code","execution_count":0,"id":"20210811-222104_1828175112","metadata":{},"outputs":[],"source":["\n\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n        \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\")\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-06-03\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-08-31\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\nrange_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n        try:\n            s3path = \"s3://aardvark-prod-pdx-mdm-to-int/to_tech/appused/version=fix_crow_1.0.0/range_type={range_type}/date={date}/\".format(range_type=range_type_mapping[granularity], date=date)\n            spark.read.parquet(s3path).createOrReplaceTempView(\"test_appused\")\n            df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type,count(distinct(selected_app)) as count_selected_app, sum(AU) as sum from test_appused\".format(date=date, granularity=granularity))\n            df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2021-08-13/appused/\", mode=\"append\") #append\n            print(\"PASS on {} {}\".format(granularity, date))\n        except Exception as e:\n            print(\"ERROR on {} {}\".format(granularity, date))\n"]},{"cell_type":"code","execution_count":0,"id":"20210811-224230_933524101","metadata":{},"outputs":[],"source":["\n\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n        \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\")\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-06-03\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-08-31\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\nrange_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n        try:\n            s3path = \"s3://aardvark-prod-pdx-mdm-to-int/to_tech/demographics/version=fix_crow_1.0.0/range_type={range_type}/date={date}/\".format(range_type=range_type_mapping[granularity], date=date)\n            spark.read.parquet(s3path).createOrReplaceTempView(\"test_demographics\")\n            df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type,count(distinct(age)) as count_age, count(distinct(gender)) as count_gender, sum(AU) as sum from test_demographics\".format(date=date, granularity=granularity))\n            df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2021-08-13/demographics/\", mode=\"append\") #append\n            print(\"PASS on {} {}\".format(granularity, date))\n        except Exception as e:\n            print(\"ERROR on {} {}\".format(granularity, date))\n"]},{"cell_type":"code","execution_count":0,"id":"20210811-232719_1267439717","metadata":{},"outputs":[],"source":["\n# e2e basic\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=fix_crow_1.0.0/range_type=WEEK/date=2021-05-08/\").createOrReplaceTempView(\"test\")\nspark.sql(\"select * from test where country='ID' and app_id = 284882215\").show(20)"]},{"cell_type":"code","execution_count":0,"id":"20210820-183723_1270781889","metadata":{},"outputs":[],"source":["\n# e2e app used\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/to_tech/appused/version=fix_crow_1.0.0/range_type=MONTH/date=2021-05-31/\").createOrReplaceTempView(\"test\")\nspark.sql(\"select * from test where country='HK' and app_id = 284882215\").show(20)"]},{"cell_type":"code","execution_count":0,"id":"20210820-184100_1861574529","metadata":{},"outputs":[],"source":["\n# e2e cross app\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/to_tech/crossapps/version=fix_crow_1.0.0/range_type=MONTH/date=2021-05-31/\").createOrReplaceTempView(\"test\")\nspark.sql(\"select * from test where country='ID' and app_id = 284882215\").show(20)"]},{"cell_type":"code","execution_count":0,"id":"20210820-184855_1892015297","metadata":{},"outputs":[],"source":["\n# e2e audience\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/to_tech/audience/version=fix_crow_1.0.0/range_type=MONTH/date=2021-05-31/\").createOrReplaceTempView(\"test\")\nspark.sql(\"select * from test where country='ID' and app_id = 284882215\").show(20)"]},{"cell_type":"code","execution_count":0,"id":"20210820-184922_1815444224","metadata":{},"outputs":[],"source":["\n# e2e demographics\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/to_tech/demographics/version=fix_crow_1.0.0/range_type=MONTH/date=2021-05-31/\").createOrReplaceTempView(\"test\")\nspark.sql(\"select * from test where country='HK' and app_id = 284882215\").show(20)"]},{"cell_type":"code","execution_count":0,"id":"20210914-175339_1936171713","metadata":{},"outputs":[],"source":["\ndate='2021-10-02'\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/retention_month/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/retention_month.csv\".format(date),header = 'true')"]},{"cell_type":"code","execution_count":0,"id":"20210914-205042_871473981","metadata":{},"outputs":[],"source":["%%sh\n\n\nDATE='2021-10-02'\n\nrm -rf /tmp/$DATE/\nmkdir -p /tmp/$DATE/\n\naws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/retention_month.csv/ /tmp/$DATE/retention_month.csv/\n\n\n\necho \"=======retention_week=========\"\ncat /tmp/$DATE/retention_month.csv/*.csv\n"]},{"cell_type":"code","execution_count":0,"id":"20210917-010857_1348193794","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2020-01-01\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-08-31\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\nrange_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\nmetric_name_week = [\"RRW0\", \"RRW1\", \"RRW2\", \"RRW3\", \"RRW4\",\"RRW5\",\"RRW6\",\"RRW7\",\"RRW8\",\"RRW9\",\"RRW10\",\"RRW11\",\"RRW12\",\"RRW13\",\"RRW14\",\"RRW15\",\"RRW16\",\"RRW17\",\"RRW18\",\"RRW19\",\"RRW20\"]\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n            try:\n                s3path = \"s3://aardvark-prod-pdx-mdm-to-int/usage_long_term_retention/version=v1.0.0/range_type=MONTH/date={date}/\".format(date=date)\n                spark.read.parquet(s3path).createOrReplaceTempView(\"test_retention_week\")\n                df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type, sum(value) as sum from test_retention_week where value is not null\".format(date=date, granularity=granularity))\n                # df.show(10)\n                # df = spark.sql(\"select distinct(device_type) from test_retention_week\")\n                # df.show(100)\n                # df = spark.sql(\"select * from test_retention_week limit 20\")\n                # print(df)\n                df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2021-10-02/retention_month/\", mode=\"append\") #append\n                print(\"PASS on {} {}\".format(granularity, date))\n            except Exception as e:\n                print(\"ERROR on {} {}\".format(granularity, date))"]},{"cell_type":"code","execution_count":0,"id":"20210917-011301_1779174640","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2020-01-01\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-08-31\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n            try:\n                s3path = \"s3://aardvark-prod-pdx-mdm-to-int/usage_long_term_retention/version=v1.0.0/range_type=MONTH/date={date}/\".format(date=date)\n                spark.read.parquet(s3path).createOrReplaceTempView(\"test_retention_day\")\n                df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type, sum(value) as sum from test_retention_week where value is not null\".format(date=date, granularity=granularity))\n                df.show(100)\n                # df = spark.sql(\"select distinct(device_type) from test_retention_day\")\n                # df.show(100)\n                # df = spark.sql(\"select * from test_retention_week limit 20\")\n                # print(df)\n                df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2021-09-29/retention_day/\", mode=\"append\") #append\n                print(\"PASS on {} {}\".format(granularity, date))\n            except Exception as e:\n                print(\"ERROR on {} {}\".format(granularity, date))"]},{"cell_type":"code","execution_count":0,"id":"20211001-234214_554083519","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\")\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2020-01-01\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-08-27\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\nrange_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\nmetric_name_week = [\"RRW0\", \"RRW1\", \"RRW2\", \"RRW3\", \"RRW4\",\"RRW5\",\"RRW6\",\"RRW7\",\"RRW8\",\"RRW9\",\"RRW10\",\"RRW11\",\"RRW12\",\"RRW13\",\"RRW14\",\"RRW15\",\"RRW16\",\"RRW17\",\"RRW18\",\"RRW19\",\"RRW20\"]\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n            try:\n                s3path = \"s3://aardvark-prod-pdx-mdm-to-int/usage_long_term_retention/version=v1.0.0/range_type=MONTH/date={date}/\".format(date=date)\n                spark.read.parquet(s3path).createOrReplaceTempView(\"test_retention_week\")\n                df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type, sum(value) as sum from test_retention_week where value is not null\".format(date=date, granularity=granularity))\n                df.show(10)\n                # df = spark.sql(\"select distinct(device_type) from test_retention_week\")\n                # df.show(100)\n                # df = spark.sql(\"select * from test_retention_week limit 20\")\n                # print(df)\n                df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2021-10-01/retention_week/\", mode=\"append\") #append\n                print(\"PASS on {} {}\".format(granularity, date))\n            except Exception as e:\n                print(\"ERROR on {} {}\".format(granularity, date))"]},{"cell_type":"code","execution_count":0,"id":"20211005-180658_366408551","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://aardvark-prod-pdx-mdm-to-int/usage_long_term_retention/version=v1.0.0/range_type=MONTH/date=2021-03-31/metric_name=RRW"]},{"cell_type":"code","execution_count":0,"id":"20211005-181344_1072469001","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/usage_long_term_retention/version=v1.0.0/range_type=MONTH/date=2020-08-31/\").createOrReplaceTempView(\"test\")\nspark.sql(\"select * from test\").show(20)"]},{"cell_type":"code","execution_count":0,"id":"20211005-192029_262916212","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://aardvark-prod-pdx-mdm-to-int/usage_week_cohort_retention/version=v1.0.0/range_type=WEEK/date=2021-06-05/metric_name=RRD0/platform=1/\n"]},{"cell_type":"code","execution_count":0,"id":"20211005-203523_123890426","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n        \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-12-01\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2022-02-01\", '%Y-%m-%d')\n\nrange_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\n#s3://aardvark-prod-pdx-mdm-to-int/usage_long_term_retention/version=v1.0.0/range_type=MONTH/date={date}\n#s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D7_routine/version=v1.0.0/range_type=MONTH/date={date}\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n            try:\n                s3path = \"s3://aardvark-prod-pdx-mdm-to-int/to_tech/demographics/version=1.0.0/range_type={range_type}/date={date}/\".format(range_type=range_type_mapping[granularity], date=date)\n                spark.read.parquet(s3path).createOrReplaceTempView(\"test_demographic\")\n                df = spark.sql(\"select '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device, count(distinct(age)) as age, count(distinct(gender)) as gender, sum(AU) as sum from test_demographic\".format(date=date, granularity=granularity))\n                # df.show(100)\n                #df = spark.sql(\"select * from test_demographic_sample\")\n                # df.show(100)\n                # df = spark.sql(\"select distinct(count_device) from test_demographic_week where age=30\")\n                #df = spark.sql(\"select app_id, device_type, country, age, gender, AU, platform from test_demographic_week WHERE app_id is null or device_type is null or country is null or age is null or gender is null or AU is null or platform is null\")\n                #df.show(20)\n                df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2022-02-01/demographics/\", mode=\"append\") #append\n                print(\"PASS on {} {}\".format(granularity, date))\n            except Exception as e:\n                print(\"ERROR on {} {}\".format(granularity, date))\n"]},{"cell_type":"code","execution_count":0,"id":"20211005-204708_2138863299","metadata":{},"outputs":[],"source":["\ndate='2022-02-01'\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/demographics/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/demographics.csv\".format(date),header = 'true')\n"]},{"cell_type":"code","execution_count":0,"id":"20220110-202033_2054769448","metadata":{},"outputs":[],"source":["%%sh\n\n\nDATE='2022-02-01'\n\nrm -rf /tmp/$DATE/\nmkdir -p /tmp/$DATE/\n\naws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/demographics.csv/ /tmp/$DATE/demographics.csv/\n\n\n\necho \"=======Demographics=========\"\ncat /tmp/$DATE/demographics.csv/*.csv\n"]},{"cell_type":"code","execution_count":0,"id":"20211005-205201_1507230831","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-12-01\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2022-02-01\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\n#s3://aardvark-prod-dca-data/oss/USAGE_BASIC_KPI/\n#s3://aardvark-prod-pdx-mdm-to-int/usage_long_term_retention/version=v1.0.0/range_type=MONTH/date={date}\n#s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D7_routine/version=v1.0.0/range_type=MONTH/date={date}\n#s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D14_routine/version=v1.0.0/range_type=MONTH/date={date}\n#s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D30_routine/version=v1.0.0/range_type=MONTH/date={date}\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n            try:\n                s3path = \"s3://aardvark-prod-dca-data/oss/GAMEIQ_GENRE_USAGE_AGE_GENDER_RELEASE_KPI/version=v2.0.0/range_type=MONTH/date={date}/\".format(date=date)\n                spark.read.parquet(s3path).createOrReplaceTempView(\"full_gameiq\")\n                df = spark.sql(\"select '{granularity}' as granularity, '{date}' as date, count(distinct(genre_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device, count(distinct(age)) as count_age, count(distinct(gender)) as count_gender, sum(value) as pct_sum from full_gameiq where metric_name='PCT' and genre_id between 2000000 and 2999999 or genre_id  between 2000 and 2999\".format(date=date, granularity=granularity))\n                # df.show(100)\n                # df = spark.sql(\"select count(distinct(country)) as count_country from test_genre_level\")\n                # df.show(100)\n                # df = spark.sql(\"select genre_id, device_type, country, age, gender, value, metric_name, platform from test_genre_level WHERE genre_id is null or device_type is null or country is null or age is null or gender is null or value is null or metric_name is null or value is null\")\n\n                df.show(100)\n                df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2022-02-04/full_game_iq/\", mode=\"append\") #append\n                print(\"PASS on {} {}\".format(granularity, date))\n            except Exception as e:\n                print(\"ERROR on {} {}\".format(granularity, date))\n\n"]},{"cell_type":"code","execution_count":0,"id":"20220223-204900_855452129","metadata":{},"outputs":[],"source":["\ndate='2022-02-04'\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/full_game_iq/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/full_game_iq.csv\".format(date),header = 'true')\n"]},{"cell_type":"code","execution_count":0,"id":"20211116-215959_45753696","metadata":{},"outputs":[],"source":["%%sh\n\n\nDATE='2022-02-04'\n\nrm -rf /tmp/$DATE/\nmkdir -p /tmp/$DATE/\n\naws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/full_game_iq.csv/ /tmp/$DATE/full_game_iq.csv/\n\n\n\necho \"=======retention_week=========\"\ncat /tmp/$DATE/full_game_iq.csv/*.csv\n"]},{"cell_type":"code","execution_count":0,"id":"20220110-235240_1624041336","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/usage_week_cohort_retention/version=v1.0.0/range_type=WEEK/date=2021-12-18/metric_name=RRM0/\").filter(\"app_id=20600000009072 and country='US'\")\ndf.show(20)"]},{"cell_type":"code","execution_count":0,"id":"20211222-205154_306219878","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        # \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n        \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-06-01\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2022-01-31\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\n#s3://aardvark-prod-pdx-mdm-to-int/usage_long_term_retention/version=v1.0.0/range_type=MONTH/date={date}\n#s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D7_routine/version=v1.0.0/range_type=MONTH/date={date}\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n            try:\n                s3path = \"s3://aardvark-prod-pdx-mdm-to-int/usage_week_cohort_retention/version=v1.0.0/range_type=WEEK/date={date}/\".format(date=date)\n                spark.read.parquet(s3path).createOrReplaceTempView(\"test_weekly_cohorts_full\")\n                df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type, count(distinct(platform)) as platform, sum(value) as sum from test_weekly_cohorts_full where value is not null\".format(date=date, granularity=granularity))\n                df.show(100)\n                df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2022-01-20/weekly_cohorts_full/\", mode=\"append\") #append\n                print(\"PASS on {} {}\".format(granularity, date))\n            except Exception as e:\n                print(\"ERROR on {} {}\".format(granularity, date))\n"]},{"cell_type":"code","execution_count":0,"id":"20211223-195759_2053627552","metadata":{},"outputs":[],"source":["\ndate='2022-01-20'\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/weekly_cohorts_full/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/weekly_cohorts_full.csv\".format(date),header = 'true')\n"]},{"cell_type":"code","execution_count":0,"id":"20211223-195941_1205728921","metadata":{},"outputs":[],"source":["%%sh\n\n\nDATE='2022-01-20'\n\nrm -rf /tmp/$DATE/\nmkdir -p /tmp/$DATE/\n\naws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/weekly_cohorts_full.csv/ /tmp/$DATE/weekly_cohorts_full.csv/\n\n\n\necho \"=======retention_week=========\"\ncat /tmp/$DATE/weekly_cohorts_full.csv/*.csv"]},{"cell_type":"code","execution_count":0,"id":"20211222-205619_470819485","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://aardvark-prod-dca-data/oss/GAMEIQ_GENRE_USAGE_AGE_GENDER_RELEASE_KPI/version=v2.0.0/range_type=MONTH/date=2022-01-31/platform=1/metric_name=IDX/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20211222-205638_100537813","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://aardvark-prod-dca-data/oss/GAMEIQ_GENRE_USAGE_AGE_GENDER_RELEASE_KPI/version=v2.0.0/range_type=MONTH/date=2022-01-31/\").createOrReplaceTempView(\"test\")\n#spark.sql(\"select * from test WHERE app_id is null or device_type is null or country is null or age is null or gender is null or value is null or metric_name is null or platform is null\").show(100)\nspark.sql(\"select * from test\").show(20)\n#spark.sql(\"select * from test WHERE app_id is null or device_type is null or country is null or age is null or gender is null or IDX is null or PCT is null or platform is null\").show(100)\n"]},{"cell_type":"code","execution_count":0,"id":"20211222-205656_451805787","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2021-12-01\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2022-02-01\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\nrange_type_mapping = {\"daily\":\"DAY\",\"monthly\":\"MONTH\",\"weekly\":\"WEEK\"}\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n        \n            try:\n                s3path = \"s3://aardvark-prod-pdx-mdm-to-int/to_tech/audience/version=1.0.0/range_type=MONTH/date={date}/\".format(date=date)\n                spark.read.parquet(s3path).createOrReplaceTempView(\"test_audience\")\n                df = spark.sql(\"select '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device, count(distinct(age)) as count_age, count(distinct(gender)) as count_gender, sum(IDX) as idx_sum, sum(PCT) as pct_sum from test_audience\".format(date=date, granularity=granularity))\n                # df.show(100)\n                # df = spark.sql(\"select count(distinct(country)) as count_country from test_genre_level\")\n                # df.show(100)\n                # df = spark.sql(\"select app_id, device_type, country, age, gender, value, metric_name, platform from test_genre_level WHERE app_id is null or device_type is null or country is null or age is null or gender is null or value is null or metric_name is null or platform is null\")\n\n                # df.show(100)\n                df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/2022-02-01/age_bins_audience/\", mode=\"append\") #append\n                print(\"PASS on {} {}\".format(granularity, date))\n            except Exception as e:\n                print(\"ERROR on {} {}\".format(granularity, date))"]},{"cell_type":"code","execution_count":0,"id":"20220111-184636_1767177698","metadata":{},"outputs":[],"source":["\ndate='2022-02-01'\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/{}/age_bins_audience/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/{}/age_bins_audience.csv\".format(date),header = 'true')"]},{"cell_type":"code","execution_count":0,"id":"20220111-184816_623321751","metadata":{},"outputs":[],"source":["%%sh\n\n\nDATE='2022-02-01'\n\nrm -rf /tmp/$DATE/\nmkdir -p /tmp/$DATE/\n\naws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.gwang.s6usage/adhoc/$DATE/age_bins_audience.csv/ /tmp/$DATE/age_bins_audience.csv/\n\n\n\necho \"=======retention_week=========\"\ncat /tmp/$DATE/age_bins_audience.csv/*.csv\n"]},{"cell_type":"code","execution_count":0,"id":"20220111-184858_1798765532","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-ugc/unified\n"]},{"cell_type":"code","execution_count":0,"id":"20220324-215143_1302101809","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}