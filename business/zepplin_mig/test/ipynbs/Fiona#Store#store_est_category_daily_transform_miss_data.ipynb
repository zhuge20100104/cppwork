{"cells":[{"cell_type":"code","execution_count":0,"id":"20200416-024215_316146387","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-05-30/\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-05-30/\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-023110_823779499","metadata":{},"outputs":[],"source":["\nprint spark.read.option(\"basePath\",\"s3://b2c-prod-data-pipeline-qa/aa.store\").parquet( \"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_data_v1_20*-[2,3,4,5,6,7,8,9,11,12]*/\").filter(\"raw_store_id  not in (2, 1002)\").select(\"date\").orderBy(\"date\", ascending=False).distinct().show()\n\nprint spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-qa/aa.store/\").parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_data_v1_2015-01-02\").filter(\"raw_store_id not in (2, 1002) \").select(\"date\").distinct().orderBy(\"date\", ascending=False).show(500)\n"]},{"cell_type":"code","execution_count":0,"id":"20200420-125910_1089142271","metadata":{},"outputs":[],"source":["\nstart = \"2016-09-03\"\nend = \"2016-09-30\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\nprint sar_list"]},{"cell_type":"code","execution_count":0,"id":"20200416-024410_452686179","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nimport datetime\n\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n\n\nsql_text = \"\"\"\n-- compare raw vs unified data\nWITH compared_data_rank AS (\n    SELECT * from store_unified where \n);\n\n\n\n\"\"\"\n\n\nstart = '2020-03-01'\nend = '2020-03-01'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\n# dates=['2020-03-01','2020-01-24',\"2019-12-06\",\"2020-01-02\",\"2020-01-05\",\"2020-01-10\",\"2020-01-11\",\"2020-01-13\",\"2020-01-20\"]\n\ncountry_code_df = d1.union(d2).where(\"country_code is not null\").cache()\ncountry_code_df = country_code_df.withColumnRenamed(\"store_id\", \"country_code_store_id\")\nprint 'country mapping table'\ncountry_code_df.show(2)\ncountry_code_df.createOrReplaceTempView(\"country_code_mapping\")\n\n\nnamespace = \"aa.store.market-size.v1\"\nfor test_date in dates:\n\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n             {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }\n        ]\n    }\n    run(spark, ingest_msg, sql_text)\n    result = spark.sql(\"select * from miss_data_rank\").show()\n    # result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_rank_data_v2_{}/\".format(start),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n\n\n    result_store = spark.sql(\"select * from miss_data_store\").show()\n    # result_store.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_data_v2_{}/\".format(start),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n\n    eject_all_caches(spark)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200602-093113_339166071","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nselect  app_id,feed_id,store_id,estimate,category_id ,device_id, rank, date from plproxy.execute_select_nestloop(\\$proxy\\$ \nselect  distinct app_id,feed_id,store_id,estimate,category_id ,device_id, rank, date\n    from aa.app_store_daily_estimate_0\n    where \n      app_id = 20600011872874  and date between '2020-05-10' and '2020-05-10' and store_id=11 order by category_id asc, date desc limit 500 \\$proxy\\$) tbl \n      (app_id BIGINT,feed_id SMALLINT,store_id INT, estimate INT, category_id INT, device_id SMALLINT, rank INT, date Date ) order by category_id  asc, date desc limit 500 ;\n\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200602-090915_2068826331","metadata":{},"outputs":[],"source":["\ndf_raw_1 = spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-05-10\").cache()\ndf_unified_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-05-10\").cache()\n\ndf_raw_1.filter(\"store_id=11 and platform='android' and feed=1\").show(5)\ndf_unified_1.filter(\"country_code='BE' and device_code='android-all' \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200416-022909_1496712369","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nimport datetime\n\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\nsql_text = \"\"\"\n\n-- mapping feed as metrc in raw\nWITH feed_metric AS (\nselect *, 'free_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='0' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='1' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric , \"ios-phone\" as device_code from rank_raw where  feed='2' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-tablet\" as device_code from rank_raw where  feed='101' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='100' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='102' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1000' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1001' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1002' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric , \"android-all\" as device_code from rank_raw   where  feed='0' and platform='android'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"android-all\" as device_code from rank_raw  where  feed='1' and platform='android'\nUNION ALL\nselect *, 'revenue' as metric,  \"android-all\" as device_code from rank_raw  where  feed='2' and platform='android'\n);\n\n\n-- select tested column from raw data\nWITH metric_raw_data AS (\nSELECT id, category_id as raw_category_id,rank,store_id as raw_store_id , metric,device_code,date , platform from feed_metric where store_id not in (3,4,5,6, 1002,1003, 1005,1004, 1006,1007)\n);\n\n\n-- group by and count data in raw data\nWITH group_by_raw AS (\nSELECT count(id) AS total_count , raw_category_id, raw_store_id, metric,device_code,date,platform from metric_raw_data where raw_store_id not in (3,4,5,6, 1002,1003, 1004, 1005, 1006,1007) group by raw_category_id, raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_rank_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download, raw_category_id,raw_store_id,device_code, platform,date\nFROM\n      group_by_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n\n-- select tested column from raw data\nWITH metric_raw_store_data AS (\nSELECT distinct id, est, store_id as raw_store_id , metric,device_code, date , platform from feed_metric where store_id not in (3,4,5,6, 1002,1003, 1005,1004, 1006,1007)\n\n);\n\n\n-- group by and count data in raw data\nWITH group_by_store_raw AS (\nSELECT count(est) AS total_count ,raw_store_id, metric,device_code,date,platform from metric_raw_store_data where raw_store_id not in (3,4,5,6,1002,1003, 1004, 1005, 1006,1007) group by raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_store_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download,raw_store_id,device_code, platform, date\nFROM\n      group_by_store_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n-- map raw with category\nWITH category_mapping_raw AS (\n\nSELECT * from ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \nUNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n ) as mapping right join pivot_metric_rank_raw on mapping.legacy_category_id=pivot_metric_rank_raw.raw_category_id and \nmapping.mapping_platform=pivot_metric_rank_raw.platform\n);\n\n\n-- map raw with rank country_code\nWITH country_category_mapping_rank_raw AS (\nselect date, raw_store_id, country_code,device_code,category_id,free_app_download,paid_app_download,revenue from country_code_mapping right join category_mapping_raw on country_code_mapping.country_code_store_id=category_mapping_raw.raw_store_id and country_code_mapping.market_code=category_mapping_raw.platform\n);\n\n\n\n-- map raw with store country_code\nWITH country_mapping_store_raw AS (\nselect date, raw_store_id, country_code,device_code,free_app_download,paid_app_download,revenue from country_code_mapping right join pivot_metric_store_raw on country_code_mapping.country_code_store_id=pivot_metric_store_raw.raw_store_id and country_code_mapping.market_code=pivot_metric_store_raw.platform\n);\n\n\n-- group by unified data\nWITH unified_group_data AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code, category_id as unified_category_id from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code, category_id from  rank_unified ) as unified\ngroup by  category_id,  country_code,  device_code );\n\n\n-- group by unified data\nWITH unified_group_data_store AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code from  store_unified ) as unified\ngroup by   country_code,  device_code );\n\n\n\n\n-- compare raw vs unified data\nWITH compared_data_rank AS (\n    SELECT * from country_category_mapping_rank_raw left join unified_group_data on unified_group_data.unified_country_code==country_category_mapping_rank_raw.country_code and unified_group_data.unified_category_id==country_category_mapping_rank_raw.category_id and unified_group_data.unified_device_code==country_category_mapping_rank_raw.device_code\n);\n\nWITH miss_data_rank AS (\nselect * from compared_data_rank where unified_count_paid_app_download!=paid_app_download or unified_count_free_app_download != free_app_download  or unified_count_revenue != revenue or unified_count_app_id is null\n)\n\n\n\n-- compare raw vs unified data store\nWITH compared_store_data AS (\n    SELECT * from country_mapping_store_raw left join unified_group_data_store on unified_group_data_store.unified_country_code==country_mapping_store_raw.country_code and unified_group_data_store.unified_device_code==country_mapping_store_raw.device_code\n);\n\n\nWITH miss_data_store AS (\nselect * from compared_store_data where free_app_download!=unified_count_free_app_download or paid_app_download!=unified_count_paid_app_download or revenue!=unified_count_revenue or unified_count_app_id is null\n)\n\n\n\"\"\"\n\n\nstart = '2020-05-10'\nend = '2020-05-11'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\n# dates=['2020-03-01','2020-01-24',\"2019-12-06\",\"2020-01-02\",\"2020-01-05\",\"2020-01-10\",\"2020-01-11\",\"2020-01-13\",\"2020-01-20\"]\n\nd1 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"ios\"))\nd1 = spark.createDataFrame([(0, 'WW', 'Worldwide', 'ios')],\n                           schema=[\"store_id\", \"country_code\", \"_c2\", \"market_code\"]).union(d1)\n\nd2 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"android\"))\ncountry_code_df = d1.union(d2).where(\"country_code is not null\").cache()\ncountry_code_df = country_code_df.withColumnRenamed(\"store_id\", \"country_code_store_id\")\nprint 'country mapping table'\ncountry_code_df.show(2)\ncountry_code_df.createOrReplaceTempView(\"country_code_mapping\")\n\ncategory_mapping_table = spark.read.parquet(\n    \"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\")\ncategory_mapping_table.createOrReplaceTempView(\"category_mapping_deminsion_service\")\n\nnamespace = \"aa.store.market-size.v1\"\nfor test_date in dates:\n    print test_date\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"name\": \"rank_raw\",\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"path\": [\n                    \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={}/\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }\n        ]\n    }\n    run(spark, ingest_msg, sql_text)\n    result = spark.sql(\"select * from miss_data_rank\")\n    result.show()\n    # result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_rank_data_v2_{}/\".format(start),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n\n\n    result_store = spark.sql(\"select * from miss_data_store\")\n    result_store.show()\n    # result_store.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_data_v2_{}/\".format(start),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n\n    # eject_all_caches(spark)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-023121_1990742668","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nimport datetime\n\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\nsql_text = \"\"\"\n\n-- mapping feed as metrc in raw\nWITH feed_metric AS (\nselect *, 'free_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='0' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='1' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric , \"ios-phone\" as device_code from rank_raw where  feed='2' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-tablet\" as device_code from rank_raw where  feed='101' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='100' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='102' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1000' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1001' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1002' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric , \"android-all\" as device_code from rank_raw   where  feed='0' and platform='android'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"android-all\" as device_code from rank_raw  where  feed='1' and platform='android'\nUNION ALL\nselect *, 'revenue' as metric,  \"android-all\" as device_code from rank_raw  where  feed='2' and platform='android'\n);\n\n\n-- select tested column from raw data\nWITH metric_raw_data AS (\nSELECT id, category_id as raw_category_id,rank,store_id as raw_store_id , metric,device_code,date , platform from feed_metric where store_id not in (3,4,5,6, 1002,1003, 1005,1004, 1006,1007)\n);\n\n\n-- group by and count data in raw data\nWITH group_by_raw AS (\nSELECT count(id) AS total_count , raw_category_id, raw_store_id, metric,device_code,date,platform from metric_raw_data where raw_store_id not in (3,4,5,6, 1002,1003, 1004, 1005, 1006,1007) group by raw_category_id, raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_rank_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download, raw_category_id,raw_store_id,device_code, platform,date\nFROM\n      group_by_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n\n-- select tested column from raw data\nWITH metric_raw_store_data AS (\nSELECT distinct id, est, store_id as raw_store_id , metric,device_code, date , platform from feed_metric where store_id not in (3,4,5,6, 1002,1003, 1005,1004, 1006,1007)\n\n);\n\n\n-- group by and count data in raw data\nWITH group_by_store_raw AS (\nSELECT count(est) AS total_count ,raw_store_id, metric,device_code,date,platform from metric_raw_store_data where raw_store_id not in (3,4,5,6,1002,1003, 1004, 1005, 1006,1007) group by raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_store_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download,raw_store_id,device_code, platform, date\nFROM\n      group_by_store_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n-- map raw with category\nWITH category_mapping_raw AS (\n\nSELECT * from ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \nUNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n ) as mapping right join pivot_metric_rank_raw on mapping.legacy_category_id=pivot_metric_rank_raw.raw_category_id and \nmapping.mapping_platform=pivot_metric_rank_raw.platform\n);\n\n\n-- map raw with rank country_code\nWITH country_category_mapping_rank_raw AS (\nselect date, raw_store_id, country_code,device_code,category_id,free_app_download,paid_app_download,revenue from country_code_mapping right join category_mapping_raw on country_code_mapping.country_code_store_id=category_mapping_raw.raw_store_id and country_code_mapping.market_code=category_mapping_raw.platform\n);\n\n\n\n-- map raw with store country_code\nWITH country_mapping_store_raw AS (\nselect date, raw_store_id, country_code,device_code,free_app_download,paid_app_download,revenue from country_code_mapping right join pivot_metric_store_raw on country_code_mapping.country_code_store_id=pivot_metric_store_raw.raw_store_id and country_code_mapping.market_code=pivot_metric_store_raw.platform\n);\n\n\n-- group by unified data\nWITH unified_group_data AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code, category_id as unified_category_id from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code, category_id from  rank_unified ) as unified\ngroup by  category_id,  country_code,  device_code );\n\n\n-- group by unified data\nWITH unified_group_data_store AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code from  store_unified ) as unified\ngroup by   country_code,  device_code );\n\n\n\n\n-- compare raw vs unified data\nWITH compared_data_rank AS (\n    SELECT * from country_category_mapping_rank_raw left join unified_group_data on unified_group_data.unified_country_code==country_category_mapping_rank_raw.country_code and unified_group_data.unified_category_id==country_category_mapping_rank_raw.category_id and unified_group_data.unified_device_code==country_category_mapping_rank_raw.device_code\n);\n\nWITH miss_data_rank AS (\nselect * from compared_data_rank where unified_count_paid_app_download!=paid_app_download or unified_count_free_app_download != free_app_download  or unified_count_revenue != revenue or unified_count_app_id is null\n)\n\n\n\n-- compare raw vs unified data store\nWITH compared_store_data AS (\n    SELECT * from country_mapping_store_raw left join unified_group_data_store on unified_group_data_store.unified_country_code==country_mapping_store_raw.country_code and unified_group_data_store.unified_device_code==country_mapping_store_raw.device_code\n);\n\n\nWITH miss_data_store AS (\nselect * from compared_store_data where free_app_download!=unified_count_free_app_download or paid_app_download!=unified_count_paid_app_download or revenue!=unified_count_revenue or unified_count_app_id is null\n)\n\n\n\"\"\"\n\n\nstart = '2020-04-07'\nend = '2020-05-28'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\n# dates=['2020-03-01','2020-01-24',\"2019-12-06\",\"2020-01-02\",\"2020-01-05\",\"2020-01-10\",\"2020-01-11\",\"2020-01-13\",\"2020-01-20\"]\n\nd1 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"ios\"))\nd1 = spark.createDataFrame([(0, 'WW', 'Worldwide', 'ios')],\n                           schema=[\"store_id\", \"country_code\", \"_c2\", \"market_code\"]).union(d1)\n\nd2 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"android\"))\ncountry_code_df = d1.union(d2).where(\"country_code is not null\").cache()\ncountry_code_df = country_code_df.withColumnRenamed(\"store_id\", \"country_code_store_id\")\nprint 'country mapping table'\ncountry_code_df.show(2)\ncountry_code_df.createOrReplaceTempView(\"country_code_mapping\")\n\ncategory_mapping_table = spark.read.parquet(\n    \"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\")\ncategory_mapping_table.createOrReplaceTempView(\"category_mapping_deminsion_service\")\n\nnamespace = \"aa.store.market-size.v1\"\nfor test_date in dates:\n    print test_date\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"name\": \"rank_raw\",\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"path\": [\n                    \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={}/\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }\n        ]\n    }\n    run(spark, ingest_msg, sql_text)\n    result = spark.sql(\"select * from miss_data_rank\")\n    result.show()\n    result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_rank_data_v2_{}/\".format(start),\n                                      mode=\"append\",\n                                      partitionBy=[\"date\"])\n\n\n    result_store = spark.sql(\"select * from miss_data_store\")\n    result_store.show()\n    result_store.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_data_v2_{}/\".format(start),\n                                      mode=\"append\",\n                                      partitionBy=[\"date\"])\n\n    eject_all_caches(spark)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-023756_1641719103","metadata":{},"outputs":[],"source":["\n\n\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nimport datetime\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\n\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\nsql_text = \"\"\"\n\n-- mapping feed as metrc in raw\nWITH feed_metric AS (\nselect *, 'free_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='0' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='1' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric , \"ios-phone\" as device_code from rank_raw where  feed='2' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-tablet\" as device_code from rank_raw where  feed='101' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='100' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='102' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1000' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1001' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1002' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric , \"android-all\" as device_code from rank_raw   where  feed='0' and platform='android'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"android-all\" as device_code from rank_raw  where  feed='1' and platform='android'\nUNION ALL\nselect *, 'revenue' as metric,  \"android-all\" as device_code from rank_raw  where  feed='2' and platform='android'\n);\n\n\n-- select tested column from raw data\nWITH metric_raw_data AS (\nSELECT id, category_id as raw_category_id,rank,store_id as raw_store_id , metric,device_code,date , platform from feed_metric where store_id not in (2,3,4,5,6,1002, 1003, 1005,1004, 1006,1007)\n);\n\n\n-- group by and count data in raw data\nWITH group_by_raw AS (\nSELECT count(id) AS total_count , raw_category_id, raw_store_id, metric,device_code,date,platform from metric_raw_data where raw_store_id not in (2,3,4,5,6, 1002, 1003, 1004, 1005, 1006,1007) group by raw_category_id, raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_rank_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download, raw_category_id,raw_store_id,device_code, platform,date\nFROM\n      group_by_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n\n-- select tested column from raw data\nWITH metric_raw_store_data AS (\nSELECT distinct id, est, store_id as raw_store_id , metric,device_code,date , platform from feed_metric where store_id not in (2, 3,4,5,6, 1002,1003, 1005,1004, 1006,1007)\n);\n\n\n-- group by and count data in raw data\nWITH group_by_store_raw AS (\nSELECT count(est) AS total_count ,raw_store_id, metric,device_code,date,platform from metric_raw_store_data where raw_store_id not in (2, 3,4,5,6, 1002, 1003, 1005, 1004, 1006,1007) group by raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_store_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download,raw_store_id,device_code, platform, date\nFROM\n      group_by_store_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n-- map raw with category\nWITH category_mapping_raw AS (\n\nSELECT * from ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \nUNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n ) as mapping right join pivot_metric_rank_raw on mapping.legacy_category_id=pivot_metric_rank_raw.raw_category_id and \nmapping.mapping_platform=pivot_metric_rank_raw.platform\n);\n\n\n-- map raw with rank country_code\nWITH country_category_mapping_rank_raw AS (\nselect date, raw_store_id, country_code,device_code,category_id,free_app_download,paid_app_download,revenue from country_code_mapping right join category_mapping_raw on country_code_mapping.country_code_store_id=category_mapping_raw.raw_store_id and country_code_mapping.market_code=category_mapping_raw.platform\n);\n\n\n\n-- map raw with store country_code\nWITH country_mapping_store_raw AS (\nselect date, raw_store_id, country_code,device_code,free_app_download,paid_app_download,revenue from country_code_mapping right join pivot_metric_store_raw on country_code_mapping.country_code_store_id=pivot_metric_store_raw.raw_store_id and country_code_mapping.market_code=pivot_metric_store_raw.platform\n);\n\n\n-- group by unified data\nWITH unified_group_data AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code, category_id as unified_category_id from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code, category_id from  rank_unified ) as unified\ngroup by  category_id,  country_code,  device_code );\n\n\n-- group by unified data\nWITH unified_group_data_store AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code from  store_unified ) as unified\ngroup by   country_code,  device_code );\n\n\n\n\n-- compare raw vs unified data\nWITH compared_data_rank AS (\n    SELECT * from country_category_mapping_rank_raw left join unified_group_data on unified_group_data.unified_country_code==country_category_mapping_rank_raw.country_code and unified_group_data.unified_category_id==country_category_mapping_rank_raw.category_id and unified_group_data.unified_device_code==country_category_mapping_rank_raw.device_code\n);\n\nWITH miss_data_rank AS (\nselect * from compared_data_rank where unified_count_paid_app_download!=paid_app_download or unified_count_free_app_download != free_app_download  or unified_count_revenue != revenue or unified_count_app_id is null\n)\n\n\n\n-- compare raw vs unified data store\nWITH compared_store_data AS (\n    SELECT * from country_mapping_store_raw left join unified_group_data_store on unified_group_data_store.unified_country_code==country_mapping_store_raw.country_code and unified_group_data_store.unified_device_code==country_mapping_store_raw.device_code\n);\n\n\nWITH miss_data_store AS (\nselect * from compared_store_data where free_app_download!=unified_count_free_app_download or paid_app_download!=unified_count_paid_app_download or revenue!=unified_count_revenue or unified_count_app_id is null\n)\n\n\n\"\"\"\n\n\n\nstart = \"2015-01-01\"\nend = \"2015-01-11\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\n# dates=[\"2013-06-03\"]\n\nd1 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"ios\"))\nd1 = spark.createDataFrame([(0, 'WW', 'Worldwide', 'ios')],\n                           schema=[\"store_id\", \"country_code\", \"_c2\", \"market_code\"]).union(d1)\n\nd2 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"android\"))\n\ncountry_code_df = d1.union(d2).where(\"country_code is not null\").cache()\ncountry_code_df = country_code_df.withColumnRenamed(\"store_id\", \"country_code_store_id\")\nprint 'country mapping table'\ncountry_code_df.show(2)\ncountry_code_df.createOrReplaceTempView(\"country_code_mapping\")\n\ncategory_mapping_table = spark.read.parquet(\n    \"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\")\ncategory_mapping_table.createOrReplaceTempView(\"category_mapping_deminsion_service\")\n\nnamespace = \"aa.store.market-size.v1\"\nfor test_date in dates:\n    print test_date\n    csv_schema = T.StructType(\n        [\n            T.StructField(\"store_id\", T.IntegerType(), True),\n            T.StructField(\"date\", T.StringType(), True),\n            T.StructField(\"platform_id\", T.IntegerType(), True),\n            T.StructField(\"vertical\", T.IntegerType(), True),\n            T.StructField(\"feed\", T.IntegerType(), True),\n            T.StructField(\"id\", T.LongType(), True),\n            T.StructField(\"est\", T.IntegerType(), True),\n            T.StructField(\"category_id\", T.IntegerType(), True),\n            T.StructField(\"rank\", T.IntegerType(), True)\n        ]\n    )\n\n    raw1 = spark.read.option(\"basePath\",\n                             \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n        csv_schema).csv(\n        \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/android/sbe_est_app/*/\".format(\n            test_date), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).cache()\n    raw2 = spark.read.option(\"basePath\",\n                             \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n        csv_schema).csv(\n        \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/ios/sbe_est_app/*/\".format(test_date),\n        sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).cache()\n\n    df_raw = raw1.union(raw2).cache()\n    df_raw.createOrReplaceTempView(\"rank_raw\")\n\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }\n        ]\n    }\n    run(spark, ingest_msg, sql_text)\n    result = spark.sql(\"select * from miss_data_rank\")\n    result.show()\n    # result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_rank_data_v1_{}/\".format(test_date),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n\n\n    result_store = spark.sql(\"select * from miss_data_store\")\n    result.show()\n\n    # result_store.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_data_v1_{}/\".format(test_date),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n    eject_all_caches(spark)\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-034948_1308766619","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nimport datetime\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\n\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\nsql_text = \"\"\"\n\n-- mapping feed as metrc in raw\nWITH feed_metric AS (\nselect *, 'free_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='0' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='1' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric , \"ios-phone\" as device_code from rank_raw where  feed='2' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-tablet\" as device_code from rank_raw where  feed='101' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='100' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='102' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1000' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1001' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1002' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric , \"android-all\" as device_code from rank_raw   where  feed='0' and platform='android'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"android-all\" as device_code from rank_raw  where  feed='1' and platform='android'\nUNION ALL\nselect *, 'revenue' as metric,  \"android-all\" as device_code from rank_raw  where  feed='2' and platform='android'\n);\n\n\n-- select tested column from raw data\nWITH metric_raw_data AS (\nSELECT id, category_id as raw_category_id,rank,store_id as raw_store_id , metric,device_code,date , platform from feed_metric where store_id not in (2, 3,4,5,6,1003, 1002,1005,1004, 1006,1007)\n);\n\n\n-- group by and count data in raw data\nWITH group_by_raw AS (\nSELECT count(id) AS total_count , raw_category_id, raw_store_id, metric,device_code,date,platform from metric_raw_data where raw_store_id not in (2, 3,4,5,6, 1002, 1003, 1004, 1005, 1006,1007) group by raw_category_id, raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_rank_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download, raw_category_id,raw_store_id,device_code, platform,date\nFROM\n      group_by_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n\n-- select tested column from raw data\nWITH metric_raw_store_data AS (\nSELECT distinct id, est, store_id as raw_store_id , metric,device_code,date , platform from feed_metric where store_id not in (2,3,4,5,6, 1002, 1003, 1005,1004, 1006,1007)\n);\n\n\n-- group by and count data in raw data\nWITH group_by_store_raw AS (\nSELECT count(est) AS total_count ,raw_store_id, metric,device_code,date,platform from metric_raw_store_data where raw_store_id not in (2, 3,4,5,6, 1002, 1003, 1005, 1004, 1006,1007) group by raw_store_id, metric,device_code,date, platform\n);\n\n\n-- pivot metric column\nWITH pivot_metric_store_raw AS (\n\nSELECT \nfree_app_download,revenue, paid_app_download,raw_store_id,device_code, platform, date\nFROM\n      group_by_store_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n\n\n\n-- map raw with category\nWITH category_mapping_raw AS (\n\nSELECT * from ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \nUNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n ) as mapping right join pivot_metric_rank_raw on mapping.legacy_category_id=pivot_metric_rank_raw.raw_category_id and \nmapping.mapping_platform=pivot_metric_rank_raw.platform\n);\n\n\n-- map raw with rank country_code\nWITH country_category_mapping_rank_raw AS (\nselect date, raw_store_id, country_code,device_code,category_id,free_app_download,paid_app_download,revenue from country_code_mapping right join category_mapping_raw on country_code_mapping.country_code_store_id=category_mapping_raw.raw_store_id and country_code_mapping.market_code=category_mapping_raw.platform\n);\n\n\n\n-- map raw with store country_code\nWITH country_mapping_store_raw AS (\nselect date, raw_store_id, country_code,device_code,free_app_download,paid_app_download,revenue from country_code_mapping right join pivot_metric_store_raw on country_code_mapping.country_code_store_id=pivot_metric_store_raw.raw_store_id and country_code_mapping.market_code=pivot_metric_store_raw.platform\n);\n\n\n-- group by unified data\nWITH unified_group_data AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code, category_id as unified_category_id from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code, category_id from  rank_unified ) as unified\ngroup by  category_id,  country_code,  device_code );\n\n\n-- group by unified data\nWITH unified_group_data_store AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code from  store_unified ) as unified\ngroup by   country_code,  device_code );\n\n\n\n\n-- compare raw vs unified data\nWITH compared_data_rank AS (\n    SELECT * from country_category_mapping_rank_raw left join unified_group_data on unified_group_data.unified_country_code==country_category_mapping_rank_raw.country_code and unified_group_data.unified_category_id==country_category_mapping_rank_raw.category_id and unified_group_data.unified_device_code==country_category_mapping_rank_raw.device_code\n);\n\nWITH miss_data_rank AS (\nselect * from compared_data_rank where unified_count_paid_app_download!=paid_app_download or unified_count_free_app_download != free_app_download  or unified_count_revenue != revenue or unified_count_app_id is null\n)\n\n\n\n-- compare raw vs unified data store\nWITH compared_store_data AS (\n    SELECT * from country_mapping_store_raw left join unified_group_data_store on unified_group_data_store.unified_country_code==country_mapping_store_raw.country_code and unified_group_data_store.unified_device_code==country_mapping_store_raw.device_code\n);\n\n\nWITH miss_data_store AS (\nselect * from compared_store_data where free_app_download!=unified_count_free_app_download or paid_app_download!=unified_count_paid_app_download or revenue!=unified_count_revenue or unified_count_app_id is null\n)\n\n\n\"\"\"\n\ndates = [\"2010-07-04\",\"2010-07-05\",\"2010-07-06\",\"2010-07-07\",\"2010-07-08\",\"2010-07-09\",\"2010-07-10\"]\n\nd1 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"ios\"))\nd1 = spark.createDataFrame([(0, 'WW', 'Worldwide', 'ios')],\n                           schema=[\"store_id\", \"country_code\", \"_c2\", \"market_code\"]).union(d1)\n\nd2 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"android\"))\n\ncountry_code_df = d1.union(d2).where(\"country_code is not null\").cache()\ncountry_code_df = country_code_df.withColumnRenamed(\"store_id\", \"country_code_store_id\")\nprint 'country mapping table'\ncountry_code_df.show(2)\ncountry_code_df.createOrReplaceTempView(\"country_code_mapping\")\n\ncategory_mapping_table = spark.read.parquet(\n    \"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\")\ncategory_mapping_table.createOrReplaceTempView(\"category_mapping_deminsion_service\")\n\nnamespace = \"aa.store.market-size.v1\"\nfor test_date in dates:\n    print test_date\n    csv_schema = T.StructType(\n        [\n            T.StructField(\"store_id\", T.IntegerType(), True),\n            T.StructField(\"date\", T.StringType(), True),\n            T.StructField(\"platform_id\", T.IntegerType(), True),\n            T.StructField(\"vertical\", T.IntegerType(), True),\n            T.StructField(\"feed\", T.IntegerType(), True),\n            T.StructField(\"id\", T.LongType(), True),\n            T.StructField(\"est\", T.IntegerType(), True),\n            T.StructField(\"category_id\", T.IntegerType(), True),\n            T.StructField(\"rank\", T.IntegerType(), True)\n        ]\n    )\n\n    raw2 = spark.read.option(\"basePath\",\n                             \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n        csv_schema).csv(\n        \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/ios/sbe_est_app/*/\".format(test_date),\n        sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).cache()\n    raw2.createOrReplaceTempView(\"rank_raw\")\n\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }\n        ]\n    }\n    run(spark, ingest_msg, sql_text)\n    result = spark.sql(\"select * from miss_data_rank\").show()\n    # result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_rank_data_v1_{}/\".format(start),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n\n\n    result_store = spark.sql(\"select * from miss_data_store\").show()\n    # result_store.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_data_v1_{}/\".format(start),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n    eject_all_caches(spark)\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-041512_1651148771","metadata":{},"outputs":[],"source":["\n# |2013-06-03|      143473|          MY| ios-tablet|     100000|             7059|             6358|   4140|               14530|                           6977|                           6357|                 4137|                  MY|         ios-tablet|             100000|\n\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\n\ntest_date='2013-06-03'\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.StringType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\nraw1 = spark.read.option(\"basePath\",\n                         \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n    csv_schema).csv(\n    \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/android/sbe_est_app/*/\".format(\n        test_date), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).cache()\nraw2 = spark.read.option(\"basePath\",\n                         \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n    csv_schema).csv(\n    \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/ios/sbe_est_app/*/\".format(test_date),\n    sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).cache()\n\ndf_raw = raw1.union(raw2).cache()\ndf_raw.createOrReplaceTempView(\"rank_raw\")\n\n\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2013-06-03\").filter(\"country_code='MY' and device_code='ios-tablet' and category_id=100000 and free_app_download is not null\").count()\nspark.sql(\"select count(*) from rank_raw where store_id='143473' and category_id=36 and feed=101\").show()\n\n\n\n\ndf_u = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2013-06-03\").filter(\"country_code='MY' and device_code='ios-tablet' and category_id=100000 and free_app_download is not null\").select(\"app_id\")\ndf_r = spark.sql(\"select id as app_id from rank_raw where store_id='143473' and category_id=36 and feed=101\")\n\ndf_u.createOrReplaceTempView(\"d1\")\ndf_r.createOrReplaceTempView(\"d2\")\n\na1 = spark.sql(\"select * from d2 except all select * from d1\").collect()\nspark.sql(\"select * from d1 except all select * from d2\").show()\n\n\n\n# |      date|raw_store_id|country_code|device_code|category_id|free_app_download|paid_app_download|revenue|unified_count_app_id|unified_count_free_app_download|unified_count_paid_app_download|unified_count_revenue|unified_country_code|unified_device_code|unified_category_id|\n# +----------+------------+------------+-----------+-----------+-----------------+-----------------+-------+--------------------+-------------------------------+-------------------------------+---------------------+--------------------+-------------------+-------------------+\n# |2013-06-03|      143473|          MY| ios-tablet|     100000|             7059|             6358|   4140|               14530|                           6977|                           6357|                 4137|                  MY|         ios-tablet|             100000|\n# +----------+------------+------------+-----------+-----------+-----------------+-----------------+-------+--------------------+-------------------------------+-------------------------------+---------------------+--------------------+-------------------+-------------------+\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-105318_1343496537","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2013-06-03/ios/sbe_est_app/143473/\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-121243_1354966657","metadata":{},"outputs":[],"source":["\nprint 'raw'\nspark.sql(\"select * from rank_raw where store_id='143473' and category_id=36 and feed=101 and id=638447236\").show()\n\nprint 'unified'\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2013-06-03\").filter(\"country_code='MY' and device_code='ios-tablet' and category_id=100000 and free_app_download is not null and app_id=638447236\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-130030_749214510","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nselect  app_id,feed_id,store_id,estimate,category_id ,device_id from plproxy.execute_select_nestloop(\\$proxy\\$ \nselect  app_id,feed_id,store_id,estimate,category_id ,device_id\n    from aa.app_store_daily_estimate_1\n    where \n      date = '2013-06-03' and store_id=143473 and feed_id = 101 and category_id=36 and app_id=638447236 order by estimate  asc limit 5 \\$proxy\\$) tbl \n      (app_id BIGINT,feed_id SMALLINT,store_id INT, estimate INT, category_id INT, device_id SMALLINT ) order by estimate  asc limit 5 ;\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200416-121430_403621957","metadata":{},"outputs":[],"source":["\nraw2 = spark.read.option(\"basePath\",\n                         \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n    csv_schema).csv(\n    \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/ios/sbe_est_app/143473\".format(\"2013-06-03\"),\n    sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).cache()\n\nraw2.createOrReplaceTempView(\"raw2\")\nspark.sql(\"select * from raw2 where store_id='143473' and category_id=36 and feed=101 and id in (503834631, 424162537, 458568213, 368358948, 501709193, 629752226, 538679511, 368360223, 304006512, 542557212, 450407440, 599158061, 324715238, 359737307, 638447236, 397425651, 458302072, 583436811, 335485983, 359354972, 284815942, 592784743, 640972086, 349166314, 600476372, 502998318, 397730127, 327761807, 303692704, 609434838, 469623568, 392583941, 600907460, 364740856, 397467588, 379450383, 291877741, 378478530, 513765092, 291179703, 438477986, 309627313, 546249827, 341922306, 384297569, 398833336, 631885241, 452999495, 401424333, 364195592, 437203244, 367278030, 282935706, 573814137, 404750273, 410525831, 364879902, 584986228, 367163867, 428231977, 429515063, 568032967, 288499125, 291070079, 591633237, 414706506, 519401597, 349554263, 424494669, 529856977, 494875526, 546175939, 519050630, 433349450, 493794484, 530262913, 472182350, 326079517, 349039721, 490225166, 388358640, 454812087) order by id , rank \").show(500)\n"]},{"cell_type":"code","execution_count":0,"id":"20200416-124905_1677106991","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}