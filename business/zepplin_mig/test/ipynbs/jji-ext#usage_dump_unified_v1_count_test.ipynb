{"cells":[{"cell_type":"code","execution_count":0,"id":"20200511-055312_1909304387","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef write_test_result(df_write_result):\n    df_write_result.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_daily_count_0511/daily/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 01, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef check_au_app_data_count(_granularity, date_list):\n    for month_day_list_tuple in date_list:\n        test_result = []\n        raw_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\" \\\n                   \"month={raw_month}/\"\n        raw_path_parse = raw_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        raw_df = spark.read.parquet(raw_path_parse)\n        for day in month_day_list_tuple[1]:\n            try:\n                raw_count_with_KPI = raw_df.filter(\n                    \"date='{}'\".format(day)).select(\"kpi\", \"app_id\").groupBy(\n                    \"kpi\").agg(count(\"kpi\")).collect()\n                # print raw_count_with_KPI\n            except AnalysisException as e:\n                break\n            for row in raw_count_with_KPI:\n                unified_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                               \"granularity={unified_granularity}/date={unified_date}/\"\n                unified_path_parse = unified_path.format(unified_date=day, unified_granularity=_granularity)\n                unified_count = spark.read.parquet(unified_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                if row[\"count(kpi)\"] != unified_count:\n                    print 'Count Test Wrong !!!! raw data: {}, unified data: {}, date: {}, KPI {}'.format(\n                                      row[\"count(kpi)\"], unified_count, day, kpi_mapping[row[\"kpi\"]])\n                test_result.append((_granularity, day, row[\"count(kpi)\"], unified_count, kpi_mapping[row[\"kpi\"]]))\n            print \"date={} test complete!\".format(day)\n        df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'dump', 'unified_v1', 'kpi'])\n        write_test_result(df_write_result)\n        print \"month={} Test complete!\".format(month_day_list_tuple[0])\n\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_au_app_data_count(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200511-055807_1586253840","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/'"]},{"cell_type":"code","execution_count":0,"id":"20200511-145823_1777631384","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 01, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_completeness(_granularity, date_list):\n    for month_day_list_tuple in date_list:\n        test_result = []\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\" \\\n                   \"month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        for day in month_day_list_tuple[1]:\n            v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                      \"granularity={unified_granularity}/date={unified_date}/\"\n            v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n            v1_df = spark.read.parquet(v1_path_parse)\n\n            dump_kpi_count = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"kpi\", \"app_id\").groupBy(\n                \"kpi\").agg(count(\"kpi\")).collect()\n            # print raw_count_with_KPI\n\n            for row in dump_kpi_count:\n                v1_count = v1_df.filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                if row[\"count(kpi)\"] != v1_count:\n                    print 'Completeness Test Fail!!!! dump data: {}, v1 data: {}, date: {}, KPI {}'.format(\n                                      row[\"count(kpi)\"], v1_count, day, kpi_mapping[row[\"kpi\"]])\n                test_result.append((_granularity, day, row[\"count(kpi)\"], v1_count, kpi_mapping[row[\"kpi\"]]))\n            print \"Completeness Test Pass! date: {}\".format(day)\n\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200528-100010_1716830884","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 01, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_completeness(_granularity, date_list):\n    for month_day_list_tuple in date_list:\n        test_result = []\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\" \\\n                   \"month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        for day in month_day_list_tuple[1]:\n            dump_kpi_count = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"kpi\", \"app_id\").groupBy(\n                \"kpi\").agg(count(\"kpi\")).collect()\n            # print raw_count_with_KPI\n\n            for row in dump_kpi_count:\n                v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                      \"granularity={unified_granularity}/date={unified_date}/\"\n                v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n                v1_count = spark.read.parquet(v1_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                if row[\"count(kpi)\"] != v1_count:\n                    print 'Completeness Test Fail!!!! dump data: {}, v1 data: {}, date: {}, KPI {}'.format(\n                                      row[\"count(kpi)\"], v1_count, day, kpi_mapping[row[\"kpi\"]])\n                test_result.append((_granularity, day, row[\"count(kpi)\"], v1_count, kpi_mapping[row[\"kpi\"]]))\n            print \"Completeness Test Pass! date: {}\".format(day)\n\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200528-104551_1387087760","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/granularity=monthly/"]},{"cell_type":"code","execution_count":0,"id":"20200609-023507_1512353727","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 2, 15)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        test_result = []\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        for day in month_day_list_tuple[1]:\n            dump_kpi_count = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"kpi\", \"app_id\").groupBy(\n                \"kpi\").agg(count(\"kpi\")).collect()\n            # print raw_count_with_KPI\n\n            for row in dump_kpi_count:\n                v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/\" \\\n                          \"granularity={unified_granularity}/date={unified_date}/\"\n                v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n                v1_count = spark.read.parquet(v1_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                if row[\"count(kpi)\"] != v1_count:\n                    print 'Completeness Test Fail!!!! dump data: {}, v1 data: {}, date: {}, KPI {}'.format(\n                        row[\"count(kpi)\"], v1_count, day, kpi_mapping[row[\"kpi\"]])\n                test_result.append((_granularity, day, kpi_mapping[row[\"kpi\"]], row[\"count(kpi)\"], v1_count))\n            print \"Completeness Test Pass! date: {}\".format(day)\n        df_write_result = spark.createDataFrame(test_result,\n                                                schema=['type', 'date', 'kpi', 'dump_count', 'unified_v1_count'])\n\n        from aadatapipelinecore.core.utils.retry import retry\n\n        def write_test_result(result_df):\n            result_df.write.format(\"delta\").save(\n                \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0608/monthly/\",\n                mode=\"append\",\n                partitionBy=[\"type\"])\n        retry(write_test_result, (df_write_result,), {}, interval=10)\n        print \"month={} Test complete!\".format(month_day_list_tuple[0])\n\n\ngranularity_list = [\"monthly\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200609-024313_249783578","metadata":{},"outputs":[],"source":["\nimport datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\n\n\nTEST_RESULT = []\nCOUNTRY_CODE_MAPPING_BY_MARKET_CODE = {\n    'google-play': {1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US',\n                    11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN',\n                    20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL',\n                    29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU',\n                    38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR',\n                    47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 53: 'KZ', 54: 'PK', 55: 'IQ',\n                    56: 'PE', 57: 'MA', 58: 'BY', 59: 'DZ', 60: 'VE', 61: 'AZ', 62: 'EC', 63: 'JO', 64: 'CR',\n                    65: 'LB', 66: 'BD', 67: 'GT', 68: 'RS', 69: 'DO', 70: 'IR', 71: 'OM', 72: 'BO', 73: 'QA',\n                    74: 'NG', 75: 'SV', 76: 'KH', 77: 'PA', 78: 'LT', 79: 'TN', 80: 'HR', 81: 'JM', 82: 'LK',\n                    83: 'HN', 84: 'PR', 85: 'UY', 86: 'LV', 87: 'BA', 88: 'KG', 89: 'PY', 90: 'MD', 91: 'NP',\n                    92: 'TZ', 93: 'BH', 94: 'GH', 95: 'KE', 96: 'SI', 97: 'AM', 98: 'UZ', 99: 'TT', 100: 'MK',\n                    101: 'YE', 102: 'MO', 103: 'LU', 1000: 'WW'},\n    'apple-store': {143441: 'US', 143442: 'FR', 143443: 'DE', 143444: 'GB', 143445: 'AT', 143446: 'BE', 143447: 'FI',\n                    143448: 'GR', 143449: 'IE', 143450: 'IT', 143451: 'LU', 143452: 'NL', 143453: 'PT', 143454: 'ES',\n                    143455: 'CA', 143456: 'SE', 143457: 'NO', 143458: 'DK', 143459: 'CH', 143460: 'AU', 143461: 'NZ',\n                    143462: 'JP', 143463: 'HK', 143464: 'SG', 143465: 'CN', 143466: 'KR', 143467: 'IN', 143468: 'MX',\n                    143469: 'RU', 143470: 'TW', 143471: 'VN', 143472: 'ZA', 143473: 'MY', 143474: 'PH', 143475: 'TH',\n                    143476: 'ID', 143477: 'PK', 143478: 'PL', 143479: 'SA', 143480: 'TR', 143481: 'AE', 143482: 'HU',\n                    143483: 'CL', 143484: 'NP', 143485: 'PA', 143486: 'LK', 143487: 'RO', 143489: 'CZ', 143491: 'IL',\n                    143492: 'UA', 143493: 'KW', 143494: 'HR', 143495: 'CR', 143496: 'SK', 143497: 'LB', 143498: 'QA',\n                    143499: 'SI', 143501: 'CO', 143502: 'VE', 143503: 'BR', 143504: 'GT', 143505: 'AR', 143506: 'SV',\n                    143507: 'PE', 143508: 'DO', 143509: 'EC', 143510: 'HN', 143511: 'JM', 143512: 'NI', 143513: 'PY',\n                    143514: 'UY', 143515: 'MO', 143516: 'EG', 143517: 'KZ', 143518: 'EE', 143519: 'LV', 143520: 'LT',\n                    143521: 'MT', 143523: 'MD', 143524: 'AM', 143525: 'BW', 143526: 'BG', 143528: 'JO', 143529: 'KE',\n                    143530: 'MK', 143531: 'MG', 143532: 'ML', 143533: 'MU', 143534: 'NE', 143535: 'SN', 143536: 'TN',\n                    143537: 'UG', 143538: 'AI', 143539: 'BS', 143540: 'AG', 143541: 'BB', 143542: 'BM', 143543: 'VG',\n                    143544: 'KY', 143545: 'DM', 143546: 'GD', 143547: 'MS', 143548: 'KN', 143549: 'LC', 143550: 'VC',\n                    143551: 'TT', 143552: 'TC', 143553: 'GY', 143554: 'SR', 143555: 'BZ', 143556: 'BO', 143557: 'CY',\n                    143558: 'IS', 143559: 'BH', 143560: 'BN', 143561: 'NG', 143562: 'OM', 143563: 'DZ', 143564: 'AO',\n                    143565: 'BY', 143566: 'UZ', 143568: 'AZ', 143571: 'YE', 143572: 'TZ', 143573: 'GH', 143575: 'AL',\n                    143576: 'BJ', 143577: 'BT', 143578: 'BF', 143579: 'KH', 143580: 'CV', 143581: 'TD', 143582: 'CG',\n                    143583: 'FJ', 143584: 'GM', 143585: 'GW', 143586: 'KG', 143587: 'LA', 143588: 'LR', 143589: 'MW',\n                    143590: 'MR', 143591: 'FM', 143592: 'MN', 143593: 'MZ', 143594: 'NA', 143595: 'PW', 143597: 'PG',\n                    143598: 'ST', 143599: 'SC', 143600: 'SL', 143601: 'SB', 143602: 'SZ', 143603: 'TJ', 143604: 'TM',\n                    143605: 'ZW', 0: 'WW'},\n    'amazon-store': {\n        'android-all': {\n            'UK': 'GB',\n        }\n    }\n}\nKPI_MAPPING = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\nDEVICE_ID_CODE_MAPPING = {\n    1001: 'android-phone',\n    1002: 'android-tablet',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\nANDROID_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['google-play']\nIOS_COUNTRY_ID_CODES = COUNTRY_CODE_MAPPING_BY_MARKET_CODE['apple-store']\n\n\ndef sample_date(month, day, date_list):\n    sample_date_list = []\n    month_random_list = np.random.randint(0, len(date_list), month).tolist()\n    for m in month_random_list:\n        sample_date_per_month = []\n        if len(date_list[m][1]) == 1:\n            day_random_list = [0]\n        else:\n            day_random_list = np.random.randint(0, len(date_list[m][1]), day).tolist()\n        for d in day_random_list:\n            sample_date_per_month.append(date_list[m][1][d])\n        sample_date_list.append((date_list[m][0], sample_date_per_month))\n    return sample_date_list\n\n\ndef _merge_dicts(dict1, dict2):\n    dict1.update(dict2)\n    return dict1\n\n\ndef write_test_result(result_df):\n    result_df.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_mapping_0512/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_mapping(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    sample_date_list = sample_date(3, 1, date_list)\n    for month_day_list_tuple in sample_date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        dump_df = (\n            dump_df\n            .withColumn('device_id', functions.UserDefinedFunction(\n                lambda x: DEVICE_ID_CODE_MAPPING[x])(dump_df['device_id']))\n            .withColumn('store_id', functions.UserDefinedFunction(\n                lambda x: _merge_dicts(IOS_COUNTRY_ID_CODES, ANDROID_COUNTRY_ID_CODES)[x])(dump_df['store_id']))\n            .withColumnRenamed('device_id', 'device_code')\n            .withColumnRenamed('store_id', 'country_code')\n        )\n        for day in month_day_list_tuple[1]:\n            dump_df_date_filtered = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"app_id\", \"device_code\", \"country_code\", \"kpi\", \"estimate\")\n            kpi_list = dump_df_date_filtered.select('kpi').distinct().collect()\n\n            v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.basic-kpi.v3/fact/\" \\\n                      \"granularity={unified_granularity}/date={unified_date}/\"\n            v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n            v1_df = spark.read.parquet(v1_path_parse)\n            for row in kpi_list:\n                dump_kpi_df = dump_df_date_filtered.filter(\"kpi='{}'\".format(row[\"kpi\"]))\\\n                    .withColumnRenamed('estimate', KPI_MAPPING[row[\"kpi\"]]).drop('kpi')\n                v1_kpi_df = v1_df.filter(\n                    \"{} is not null\".format(KPI_MAPPING[row[\"kpi\"]])).select(\n                        'app_id', 'device_code', 'country_code', KPI_MAPPING[row[\"kpi\"]])\n                subtract_count = v1_kpi_df.subtract(dump_kpi_df).count()\n                subtract_count_reverse = dump_kpi_df.subtract(v1_kpi_df).count()\n                if subtract_count != 0 or subtract_count_reverse != 0:\n                    print \"Accuracy Test Wrong!!! granularity: {} , subtract_count: {}, date: {}, kpi: {}\".format(\n                        _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]])\n                else:\n                    print max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]\n                TEST_RESULT.append((\n                    _granularity, max(subtract_count, subtract_count_reverse), day, KPI_MAPPING[row[\"kpi\"]]))\n            print \"date={} test complete!\".format(day)\n\n\nif __name__ == '__main__':\n    granularity_list = [\"monthly\"]\n    for granularity in granularity_list:\n        check_usage_dump_v1_mapping(granularity, get_path_date_list(granularity))\n    print 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200609-031013_878166300","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0608/daily/').groupBy('date').agg(sum('dump_count').alias('dump_count'), sum('unified_v1_count').alias('unified_v1_count')).orderBy('date').collect()\nfor row in df:\n    print row['date'],'\\t',row['dump_count'],'\\t',row['unified_v1_count']"]},{"cell_type":"code","execution_count":0,"id":"20200609-034012_1262167751","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0608/monthly/').groupBy('date').agg(sum('dump_count').alias('dump_count'), sum('unified_v1_count').alias('unified_v1_count')).orderBy('date').collect()\nfor row in df:\n    print row['date'],'\\t',row['dump_count'],'\\t',row['unified_v1_count']"]},{"cell_type":"code","execution_count":0,"id":"20200609-064138_753037569","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        test_result = []\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        for day in month_day_list_tuple[1]:\n            dump_kpi_count = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"kpi\", \"app_id\").groupBy(\n                \"kpi\").agg(count(\"kpi\")).collect()\n            # print raw_count_with_KPI\n\n            for row in dump_kpi_count:\n                v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                          \"granularity={unified_granularity}/date={unified_date}/\"\n                v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n                v1_count = spark.read.parquet(v1_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                if row[\"count(kpi)\"] != v1_count:\n                    print 'Completeness Test Fail!!!! dump data: {}, v1 data: {}, date: {}, KPI {}'.format(\n                        row[\"count(kpi)\"], v1_count, day, kpi_mapping[row[\"kpi\"]])\n                test_result.append((_granularity, day, kpi_mapping[row[\"kpi\"]], row[\"count(kpi)\"], v1_count))\n            print \"Completeness Test Pass! date: {}\".format(day)\n        df_write_result = spark.createDataFrame(test_result,\n                                                schema=['type', 'date', 'kpi', 'dump_count', 'unified_v1_count'])\n\n        from aadatapipelinecore.core.utils.retry import retry\n\n        def write_test_result(result_df):\n            result_df.write.format(\"delta\").save(\n                \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0616/daily/\",\n                mode=\"overwrite\",\n                partitionBy=[\"type\"])\n        retry(write_test_result, (df_write_result,), {}, interval=10)\n        print \"month={} Test complete!\".format(month_day_list_tuple[0])\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200616-040657_1098003546","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact' \\\n                          '/granularity={v3_granularity}/date={v3_date}/'\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n        unified_v3_count = spark.read.format(\"delta\").load(unified_v3_path_parse).count()\n\n        if unified_v1_count != unified_v3_count:\n            print 'Completeness Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        else:\n            print 'Completeness Test Pass! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        test_result.append((_granularity, unified_v1_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0616/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200616-075724_1746494118","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0616/daily/')\nresult = df.distinct().groupBy('date').agg(sum('dump_count').alias('dump_count'),sum('unified_v1_count').alias('unified_v1_count')).orderBy('date').collect()\nfor row in result:\n    print row['date'],'\\t',row['dump_count'],'\\t',row['unified_v1_count']"]},{"cell_type":"code","execution_count":0,"id":"20200616-083716_170493057","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0616/daily/').orderBy('date').collect()\nfor row in df:\n    print row['date'],'\\t',int(row['raw_count']),'\\t',row['unified_count']"]},{"cell_type":"code","execution_count":0,"id":"20200616-084749_1062746259","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2018, 3, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_usage_unified_v1_v3_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact' \\\n                          '/granularity={v3_granularity}/date={v3_date}/'\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date[1])\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date[1])\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1_count = unified_v1.count()\n        unified_v3_count = spark.read.format(\"delta\").load(unified_v3_path_parse).count()\n\n        if unified_v1_count != unified_v3_count:\n            print 'Completeness Test FAIL!!!! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        else:\n            print 'Completeness Test Pass! unified_v1 data: {}, unified_v3 data: {}, date: {}'.format(\n                unified_v1_count, unified_v3_count, date[1])\n        test_result.append((_granularity, unified_v1_count, unified_v3_count, date[1]))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'raw_count', 'unified_count', 'date'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_unified_v1_v3_count_0616/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200617-043713_184621745","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200617-035955_645497165","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2018, 3, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        test_result = []\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        for day in month_day_list_tuple[1]:\n            dump_kpi_count = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"kpi\", \"app_id\").groupBy(\n                \"kpi\").agg(count(\"kpi\")).collect()\n            # print raw_count_with_KPI\n\n            for row in dump_kpi_count:\n                v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                          \"granularity={unified_granularity}/date={unified_date}/\"\n                v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n                v1_count = spark.read.parquet(v1_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                if row[\"count(kpi)\"] != v1_count:\n                    print 'Completeness Test Fail!!!! dump data: {}, v1 data: {}, date: {}, KPI {}'.format(\n                        row[\"count(kpi)\"], v1_count, day, kpi_mapping[row[\"kpi\"]])\n                test_result.append((_granularity, day, kpi_mapping[row[\"kpi\"]], row[\"count(kpi)\"], v1_count))\n            print \"Completeness Test Pass! date: {}\".format(day)\n        df_write_result = spark.createDataFrame(test_result,\n                                                schema=['type', 'date', 'kpi', 'dump_count', 'unified_v1_count'])\n\n        from aadatapipelinecore.core.utils.retry import retry\n\n        def write_test_result(result_df):\n            result_df.write.format(\"delta\").save(\n                \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0616/daily/\",\n                mode=\"append\",\n                partitionBy=[\"type\"])\n        retry(write_test_result, (df_write_result,), {}, interval=10)\n        print \"month={} Test complete!\".format(month_day_list_tuple[0])\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200617-043441_1911399911","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 17)\n    start = datetime.date(2020, 5, 17)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndate_list = get_daily_date_list()\nfor date in date_list:\n    print date[0]\n    plproxy_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date={}/'.format(date[0])\n    routine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date={}/'.format(date[0])\n    plproxy_df = spark.read.format(\"delta\").load(plproxy_path)\n    routine_df = spark.read.format(\"delta\").load(routine_path)\n    kpi_mapping_dict = {1: 'est_average_active_users', 2: 'est_average_session_per_user', 3: 'est_average_session_duration',\n                        4: 'est_install_penetration', 5: 'est_average_active_days', 6: 'est_percentage_active_days',\n                        7: 'est_average_bytes_per_user', 8: 'est_average_time_per_user', 9: 'est_usage_penetration',\n                        10: 'est_open_rate', 17: 'est_average_bytes_per_session', 23: 'est_installs', 24: 'est_share_of_users',\n                        25: 'est_share_of_installs'}\n    for i,kpi in kpi_mapping_dict.items():\n        plproxy_count = plproxy_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).count()\n        # routine_count =  routine_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).count()\n        plproxy_df = plproxy_df.withColumn(kpi, functions.UserDefinedFunction(lambda x: format(x, '.0f'))(plproxy_df[kpi]))\n        routine_df = routine_df.withColumn(kpi, functions.UserDefinedFunction(lambda x: format(x, '.0f'))(routine_df[kpi]))\n        plproxy_df.createOrReplaceTempView(\"plproxy_df\")\n        routine_df.createOrReplaceTempView(\"routine_df\")\n        subtract_count = plproxy_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).select('app_id', 'country_code', 'device_code', kpi).subtract(routine_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).select('app_id', 'country_code', 'device_code', kpi)).count()\n        print kpi, '\\t', float(subtract_count) / plproxy_count\n        if subtract_count!=0:\n            print float(subtract_count) / plproxy_count\n            sql = \"\"\"\n                select\n                    plproxy_df.app_id, plproxy_df.device_code, plproxy_df.country_code,\n                    plproxy_df.{kpi} as v3_{kpi},\n                    v5.{kpi} as v5_{kpi}\n                from plproxy_df inner join\n                    (select app_id, device_code, country_code, {kpi}\n                     from routine_df\n                     ) AS v5\n                on (plproxy_df.app_id=v5.app_id) and (plproxy_df.device_code=v5.device_code) and (plproxy_df.country_code=v5.country_code) and (plproxy_df.{kpi}!=v5.{kpi})\n                \"\"\".format(kpi=kpi)\n            unified_v1 = spark.sql(sql)\n            unified_v1 = unified_v1= unified_v1.withColumn('difference', unified_v1['v3_{}'.format(kpi)] - unified_v1['v5_{}'.format(kpi)])\n            unified_v1.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200617-095237_1013401855","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        test_result = []\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        for day in month_day_list_tuple[1]:\n            dump_kpi_count = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"kpi\", \"app_id\").groupBy(\n                \"kpi\").agg(count(\"kpi\")).collect()\n            # print raw_count_with_KPI\n\n            for row in dump_kpi_count:\n                v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                          \"granularity={unified_granularity}/date={unified_date}/\"\n                v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n                v1_count = spark.read.parquet(v1_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                if row[\"count(kpi)\"] != v1_count:\n                    print 'Completeness Test Fail!!!! dump data: {}, v1 data: {}, date: {}, KPI {}'.format(\n                        row[\"count(kpi)\"], v1_count, day, kpi_mapping[row[\"kpi\"]])\n                test_result.append((_granularity, day, kpi_mapping[row[\"kpi\"]], row[\"count(kpi)\"], v1_count))\n            print \"Completeness Test Pass! date: {}\".format(day)\n        df_write_result = spark.createDataFrame(test_result,\n                                                schema=['type', 'date', 'kpi', 'dump_count', 'unified_v1_count'])\n\n        from aadatapipelinecore.core.utils.retry import retry\n\n        def write_test_result(result_df):\n            result_df.write.format(\"delta\").save(\n                \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0616/daily/\",\n                mode=\"append\",\n                partitionBy=[\"type\"])\n        retry(write_test_result, (df_write_result,), {}, interval=10)\n        print \"month={} Test complete!\".format(month_day_list_tuple[0])\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200617-100011_1397867496","metadata":{},"outputs":[],"source":["\nroutine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date={}/'.format('2020-05-17')\nroutine_df = spark.read.format(\"delta\").load(routine_path)\nroutine_df.filter(\"app_id=20600000000425 and country_code='WW' and device_code='android-phone'\").select('est_average_active_users').show(3)\nroutine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date={}/'.format('2020-05-17')\nroutine_df = spark.read.format(\"delta\").load(routine_path)\nroutine_df.filter(\"app_id=20600000000425 and country_code='WW' and device_code='android-phone'\").select('est_average_active_users').show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200617-100244_1524384668","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select count(*) from store.usage_basic_kpi_fact_v6_p_{} where date='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2018, 3, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_v3_db_completeness(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        m = month[0][:4] + month[0][5:8]\n        for day in month[1]:\n            result = query(aa_dsn, sql.format(m, day))\n            db_count = result[0][0]\n\n            v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/' \\\n                      'granularity={}/date={}/'\n            v3_count = spark.read.format(\"delta\").load(v3_path.format(graularity, day)).count()\n\n            if db_count != v3_count:\n                print \"Completeness Test Fail!!!! date: {}, v3: {}, db: {}\".format(day, v3_count, db_count)\n            else:\n                print \"Completeness Test Pass! date : {}\".format(day)\n            test_result.append((graularity, day, v3_count, db_count))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'v3_count', 'db_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_v3_db_count_0616/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngraularity_list = [\"daily\"]\nfor graularity in graularity_list:\n    check_v3_db_completeness(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200617-102928_139175977","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141 -p 5432 -U citus_bdp_prod_app_int_qa -d aa_store_db << EOF\nSET search_path=store;\n\\d\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200617-103412_470983930","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        test_result = []\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        for day in month_day_list_tuple[1]:\n            dump_kpi_count = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"kpi\", \"app_id\").groupBy(\n                \"kpi\").agg(count(\"kpi\")).collect()\n            # print raw_count_with_KPI\n\n            for row in dump_kpi_count:\n                v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                          \"granularity={unified_granularity}/date={unified_date}/\"\n                v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n                v1_count = spark.read.parquet(v1_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                if row[\"count(kpi)\"] != v1_count:\n                    print 'Completeness Test Fail!!!! dump data: {}, v1 data: {}, date: {}, KPI {}'.format(\n                        row[\"count(kpi)\"], v1_count, day, kpi_mapping[row[\"kpi\"]])\n                test_result.append((_granularity, day, kpi_mapping[row[\"kpi\"]], row[\"count(kpi)\"], v1_count))\n            print \"Completeness Test Pass! date: {}\".format(day)\n        df_write_result = spark.createDataFrame(test_result,\n                                                schema=['type', 'date', 'kpi', 'dump_count', 'unified_v1_count'])\n\n        from aadatapipelinecore.core.utils.retry import retry\n\n        def write_test_result(result_df):\n            result_df.write.format(\"delta\").save(\n                \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0616/daily/\",\n                mode=\"append\",\n                partitionBy=[\"type\"])\n        retry(write_test_result, (df_write_result,), {}, interval=10)\n        print \"month={} Test complete!\".format(month_day_list_tuple[0])\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200618-041250_1424327413","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=monthly/"]},{"cell_type":"code","execution_count":0,"id":"20200618-030745_533338851","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2013, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_usage_dump_v1_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        test_result = []\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_df = spark.read.parquet(dump_path_parse)\n        for day in month_day_list_tuple[1]:\n            dump_kpi_count = dump_df.filter(\n                \"date='{}'\".format(day)).select(\"kpi\", \"app_id\").groupBy(\n                \"kpi\").agg(count(\"kpi\")).collect()\n            # print raw_count_with_KPI\n\n            for row in dump_kpi_count:\n                v1_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                          \"granularity={unified_granularity}/date={unified_date}/\"\n                v1_path_parse = v1_path.format(unified_date=day, unified_granularity=_granularity)\n                v1_count = spark.read.parquet(v1_path_parse).filter(\n                    \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                if row[\"count(kpi)\"] != v1_count:\n                    print 'Completeness Test Fail!!!! dump data: {}, v1 data: {}, date: {}, KPI {}'.format(\n                        row[\"count(kpi)\"], v1_count, day, kpi_mapping[row[\"kpi\"]])\n                test_result.append((_granularity, day, kpi_mapping[row[\"kpi\"]], row[\"count(kpi)\"], v1_count))\n            print \"Completeness Test Pass! date: {}\".format(day)\n        df_write_result = spark.createDataFrame(test_result,\n                                                schema=['type', 'date', 'kpi', 'dump_count', 'unified_v1_count'])\n\n        from aadatapipelinecore.core.utils.retry import retry\n\n        def write_test_result(result_df):\n            result_df.write.format(\"delta\").save(\n                \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_v1_count_0616/monthly/\",\n                mode=\"append\",\n                partitionBy=[\"type\"])\n        retry(write_test_result, (df_write_result,), {}, interval=10)\n        print \"month={} Test complete!\".format(month_day_list_tuple[0])\n\n\ngranularity_list = [\"monthly\"]\nfor granularity in granularity_list:\n    check_usage_dump_v1_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200618-041347_445318298","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}