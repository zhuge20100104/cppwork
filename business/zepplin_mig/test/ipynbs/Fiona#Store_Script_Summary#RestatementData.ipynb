{"cells":[{"cell_type":"code","execution_count":0,"id":"20200818-140136_158216076","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-update.v1/fact/granularity=weekly/year=2012/_update_date=2020-09-19/\n\n \n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200818-140221_180855796","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-qa/unified/qa.app-est-qa.v1/dimension/granularity=daily/test_trigger_date=2020-08-18/\").where(\"test_count is null \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200722-055045_94189250","metadata":{},"outputs":[],"source":["%%sh\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-raw/_obselete/store.app_est_20170101_20190101_dna_2020_0630_0707/fact/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/store.app-est-dna-log-pubrange.v1/fact/granularity=daily/month=2019-11/pub_range=999/device_code=ios-phone/  --recursive --human --summarize \n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/store.app-est-dna-log-pubrange.v3/fact/granularity=daily/month=2019-05/pub_range=10/device_code=ios-phone/\n\naws s3 ls s3://b2c-prod-data-pipeline-raw-store-paid/raw/store.app-est.v3/insert/_default_partition=2020-08/\n"]},{"cell_type":"code","execution_count":0,"id":"20200808-092233_661966888","metadata":{},"outputs":[],"source":["%python\nfrom datetime import timedelta, date, datetime\nimport datetime as dt\ndef datetime_to_string(date_time, convert_format=\"%Y-%m-%d\"):\n    return datetime.strftime(date_time, convert_format)\n\n\ndef string_to_datetime(date_str, convert_format=None):\n    return datetime.strptime(date_str, convert_format if convert_format else \"%Y-%m-%d\")\nprint string_to_datetime(\"2020-01-01\")\n\ndate = string_to_datetime(\"2020-01-01\")\nweekly_day_nums=5\nprint [ datetime_to_string(date - timedelta(days=x)) for x in range(weekly_day_nums)]\n"]},{"cell_type":"code","execution_count":0,"id":"20200722-062934_1400527710","metadata":{},"outputs":[],"source":["\n\n\ndf = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/store.app-est-dna-log-pubrange.v2/fact/\").where(\"granularity = 'daily' and month = '2019-06' \")\ndf.createOrReplaceTempView(\"temp\")\ndf_temp_publish_change = spark.sql(\"select sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue),  sum(est_organic_download) from temp\").cache()\ndf_temp_publish_change.createOrReplaceTempView(\"publisher_change\")\n# spark.sql(\"select * from publisher_change\").show()\n\n\ndf_date_category = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity = 'daily' and date between '2019-06-01' and '2019-06-30'\")\ndf_date_category.createOrReplaceTempView(\"temp_date_category\")\ndf_category_comparasion = spark.sql(\"select sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue), (sum(est_free_app_download) + sum(est_paid_app_download) ) as est_organic_download  from (select max(est_free_app_download) as est_free_app_download , max(est_paid_app_download) as est_paid_app_download, max(est_revenue) as est_revenue, device_code, country_code, date, app_id from temp_date_category group by device_code, country_code, date, app_id ) as prod\").cache()\ndf_category_comparasion.createOrReplaceTempView(\"category_comparsion\")\n\n\ndf_1 = spark.sql(\"select * from publisher_change except select * from category_comparsion\")\ndf_2 = spark.sql(\"select * from category_comparsion except select * from publisher_change\")\n \n \nprint df_1.first()\nprint df_2.first()\n\n# if df_1.rdd.isEmpty():\n#     print 'pass'\n# if df_2.rdd.isEmpty():\n#     print 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200724-025249_349987353","metadata":{},"outputs":[],"source":["\ndf_temp_publish_change.unpersist()\ndf_category_comparasion.unpersist()"]},{"cell_type":"code","execution_count":0,"id":"20200723-075105_1413441974","metadata":{},"outputs":[],"source":["\ndf.rdd.getNumPartitions()\n"]},{"cell_type":"code","execution_count":0,"id":"20200722-070434_2058270175","metadata":{},"outputs":[],"source":["\n\n# spark.sql(\"select  sum(est_organic_download) from (select max(est_organic_download) as est_organic_download, device_code, country_code, date, app_id from temp_date_category group by device_code, country_code, date, app_id ) as prod \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200722-065257_981634781","metadata":{},"outputs":[],"source":["\ndf_date = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/\").where(\"granularity = 'daily' and date between '2019-08-01' and '2019-08-31'\")\ndf_date.createOrReplaceTempView(\"temp_date\")\nspark.sql(\"select sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue), sum(est_paid_download), sum(est_organic_download) from temp_date \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200803-093846_952460352","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/store.app_est_20170101_20190101_dna_2020_0630_0707/fact/\n"]},{"cell_type":"code","execution_count":0,"id":"20200722-055103_1852670016","metadata":{},"outputs":[],"source":["\n\n# changed app\ndf_dna_old=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/store.app_est_20170101_20190101_dna_2020_0630_0707/fact/\").cache()\ndf_dna_old.createOrReplaceTempView(\"dna_change_old\")\nspark.sql(\"select distinct app_id, publisher_id, company_id, parent_company_id from dna_change_old\").createOrReplaceTempView(\"dna_change_old_1\")\n\n# new dna change log\ndf_dna_change_log = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.mapping-log.v1/dimension/\").filter(\"update_date = '2020-07-07'\").drop(\"_identifier\", \"update_date\")\ndf_dna_change_log.createOrReplaceTempView(\"df_dna_change_log\")\n# spark.sql(\"select app_id, publisher_id , company_id, parent_company_id, max(start_date) from df_dna_change_log  where app_id=1337585357 group by app_id, publisher_id , company_id, parent_company_id\").show()\n\n# changed data with new app, publisher, company, parent company relationship\ndf_dna_new = spark.sql(\"select a.app_id, b.publisher_id, b.company_id, b.parent_company_id, b.start_date, b.end_date from dna_change_old_1 as a join df_dna_change_log as b on a.app_id=b.app_id  \").cache()\ndf_dna_new.createOrReplaceTempView(\"df_dna_new\")\n\n# update app est \ndf = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/store.app-est-dna-log-pubrange.v1/fact\").where(\"granularity = 'daily' and month = '2019-06'  \").cache()\ndf.createOrReplaceTempView(\"temp\")\n\n# spark.sql(\"select count(*) from temp \").show()\n# spark.sql(\"select  count(*) from df_dna_new \").show()\n\n\nspark.sql('''\nSELECT DISTINCT a.app_id as app_id_a, \n                a.publisher_id AS publisher_a, \n                a.company_id as ccompany_a, \n                a.parent_company_id as parent_company_a, \n                b.app_id, \n                b.publisher_id AS publisher_b, \n                b.company_id, \n                b.parent_company_id,\n                b.start_date,\n                b.end_date\nFROM   temp a \n       JOIN df_dna_new b \n         ON a.app_id = b.app_id \n        AND a.publisher_id = b.publisher_id \n        Where (a.date >= b.start_date \n        And a.date <= b.end_date)\n        Or (a.date >= b.start_date \n        And b.end_date is null)\norder by  app_id_a desc\n''').createOrReplaceTempView(\"join_result\")\n\nspark.sql(\"select count(1) from join_result\").show()\nspark.sql(\"select * from join_result where ccompany_a != company_id or parent_company_a!=parent_company_id \").show(200)\n\n# spark.sql(\"select * from join_result\").show(20)\n# spark.sql(\"select * from df_dna_new except select distinct app_id, publisher_id, company_id, parent_company_id from temp \").show()\n# spark.sql(\"select distinct app_id, publisher_id, parent_company_id, company_id from temp except select * from df_dna_new\").show()\n# |20600004844447|20200000058031|1000200000087666| 1000200000095158|20600004844447|20200000058031|1000200000087666| 1000200000095158|2014-05-08|      null|\n# |20600004844447|20200000058031|1000200000087666| 1000200000095158|20600004844447|20200000058031|1000200000087666|                0|1970-01-01|2014-05-07|\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200722-112837_1626004223","metadata":{},"outputs":[],"source":["\n\na.company_id != b.company_id\n        OR a.parent_company_id != b.parent_company_id\n        And\n        \n        \n        \ndf_dna_change_log_06_30 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.mapping-log.v1/dimension/\").filter(\"update_date = '2020-06-30'\").drop(\"_identifier\", \"update_date\").createOrReplaceTempView(\"df_dna_change_log_06_30\")\ndf_dna_change_log_07_07 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.mapping-log.v1/dimension/\").filter(\"update_date = '2020-07-07'\").drop(\"_identifier\", \"update_date\").createOrReplaceTempView(\"df_dna_change_log_07_07\")\n\n\nspark.sql(\"select * from df_dna_change_log_06_30 where app_id = 1287987924\").show()\nspark.sql(\"select * from df_dna_change_log_07_07 where app_id = 1287987924\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200724-062302_392048250","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from temp where app_id = 1287987924\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200722-055123_209609319","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import *\ndf_2 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.publisher-company-mapping.v1/dimension/\").filter(\"update_date = '2020-07-07'\").drop(\"_identifier\", \"update_date\")\ndf_1 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.publisher-company-mapping.v1/dimension/\").filter(\"update_date = '2020-06-30'\").drop(\"_identifier\", \"update_date\")\ndf_3 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.mapping-log.v1/dimension/\").filter(\"update_date = '2020-07-07'\").drop(\"_identifier\", \"update_date\")\ndf_2.createOrReplaceTempView(\"df2\")\ndf_1.createOrReplaceTempView(\"df1\")\ndf_3.createOrReplaceTempView(\"df3\")\n# added_df = df_2.select(\"publisher_id\").subtract(df_1.select(\"publisher_id\"))\ndiff_df_ = spark.sql(\"\"\"\nselect * from (\nselect df2.*, df1.company_id as df1_company_id, df1.publisher_id as df1_publisher_id\nfrom\ndf2 full outer join df1\non df2.publisher_id = df1.publisher_id\n) tmp\nwhere company_id != df1_company_id or df1_company_id is null\n\"\"\")\n#print diff_df_.count()\n#print diff_df_.show(400, False)\ndiff_df_.createOrReplaceTempView(\"diff_df_\")\ndiff_df = spark.sql(\"\"\"\nselect \ndf3.*\nfrom df3 inner join diff_df_ on df3.publisher_id = diff_df_.publisher_id and df3.company_id = diff_df_.company_id\n\"\"\").na.fill({\"end_date\": \"2020-07-07\"}).withColumn(\"start_date\", when(col(\"start_date\") == \"1970-01-01\", \"2010-07-01\").otherwise(col(\"start_date\")))\nprint diff_df.count()\n# diff_df.show(20, False)\n# diff_df_100apps = diff_df.select(\"app_id\").distinct().orderBy(\"app_id\").limit(100).createOrReplaceTempView(\"diff_df_100apps\")\ndiff_df.createOrReplaceTempView(\"diff_df\")\nspark.sql(\"select * from df2 where publisher_id=1379847699\").show()\nspark.sql(\"select * from df1 where publisher_id=1379847699\").show()\n\nspark.sql(\"select * from diff_df where app_id=1379847700\").show()\n# diff_df_100apps_data = spark.sql(\"\"\"select a.* from diff_df a join diff_df_100apps b on a.app_id = b.app_id\"\"\")\n# diff_df_100apps_data.createOrReplaceTempView(\"diff_df\")\n# print diff_df_100apps_data.count()\n# print diff_df_100apps_data.select(\"app_id\").distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200724-040948_827656180","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/store.app-est-publisher-dna-log-pubrange.v1/fact/granularity=daily/month=2019-01/pub_range=10/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher-dna-log.v1/fact/granularity=daily/\n"]},{"cell_type":"code","execution_count":0,"id":"20200724-071326_113970592","metadata":{},"outputs":[],"source":["\n\ndf = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/store.app-est-publisher-dna-log-pubrange.v1/fact/\").where(\"granularity = 'daily' and month = '2019-06' \")\ndf.createOrReplaceTempView(\"temp\")\ndf_temp_publish_change = spark.sql(\"select sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue) from temp\").cache()\ndf_temp_publish_change.createOrReplaceTempView(\"publisher_change\")\n# spark.sql(\"select * from publisher_change\").show()\n\n\ndf_date_category = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher-dna-log.v1/fact/\").where(\"granularity = 'daily' and date between '2019-06-01' and '2019-06-30'\")\ndf_date_category.createOrReplaceTempView(\"temp_date_category\")\ndf_category_comparasion = spark.sql(\"select sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue)  from temp_date_category\").cache()\ndf_category_comparasion.createOrReplaceTempView(\"category_comparsion\")\n\n\ndf_1 = spark.sql(\"select * from publisher_change except select * from category_comparsion\")\ndf_2 = spark.sql(\"select * from category_comparsion except select * from publisher_change\")\n \n \nprint df_1.first()\nprint df_2.first()\n"]},{"cell_type":"code","execution_count":0,"id":"20200808-085015_1060692639","metadata":{},"outputs":[],"source":["\ndf = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/store.app-est-publisher-dna-log-pubrange.v1/fact/\").where(\"granularity = 'daily' and month = '2019-06' \")\ndf.createOrReplaceTempView(\"temp\")\ndata = spark.sql(\"select * from temp limit 5\").collect()\nprint data"]},{"cell_type":"code","execution_count":0,"id":"20200724-071548_2064950879","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher-dna-log.v1/fact/granularity=daily/date=2011-06-13/\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200724-071637_669336036","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}