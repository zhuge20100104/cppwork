{"cells":[{"cell_type":"code","execution_count":0,"id":"20200625-030331_918477990","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v3/fact/granularity=yearly/"]},{"cell_type":"code","execution_count":0,"id":"20200625-030312_1168307948","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\nimport datetime\nfrom pyspark.sql.functions import lit\nfrom dateutil.relativedelta import relativedelta\n\n\nstart_date = datetime.datetime.strptime('2012-09-01', '%Y-%m-%d')\nend_date = datetime.datetime.strptime('2020-04-30', '%Y-%m-%d')\ngranularity = 'monthly'\n\n\ndef get_date_range(granularity, start_date):\n    if granularity == 'weekly':\n        end_date = start_date + relativedelta(weeks=1)\n    elif granularity == 'monthly':\n        end_date = start_date + relativedelta(months=1)\n    elif granularity == 'quarterly':\n        end_date = start_date + relativedelta(months=3)\n    elif granularity == 'yearly':\n        end_date = start_date + relativedelta(months=12)\n    return end_date\n\n\nstart = start_date\nwhile start < end_date:\n    end = get_date_range(granularity, start)\n    end = end - relativedelta(days=1)\n    start = datetime.datetime.strftime(start, '%Y-%m-%d')\n    end = datetime.datetime.strftime(end, '%Y-%m-%d')\n    # print start, end\n    category_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n        \"store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\n\n    agg_df = category_daily_df.groupBy('app_id', 'country_code', 'device_code', 'category_id').agg(\n        sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), \n            sum('est_revenue').alias('revenue'))\n    agg_df.createOrReplaceTempView(\"agg_df\")\n\n    if granularity == 'weekly' or granularity == 'monthly':\n        pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n            \"store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n    else:\n        pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n            \"store.app-est-category-pre-aggr.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n\n    pre_agg_df.createOrReplaceTempView(\"pre_agg_df\")\n\n    diff_df1 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from agg_df \n                            except all \n                            select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from pre_agg_df\"\"\")\n    diff_df2 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from pre_agg_df \n                            except all \n                            select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from agg_df\"\"\")\n    print agg_df.count(), pre_agg_df.count()\n    diff_count1 = diff_df1.count()\n    diff_count2 = diff_df2.count()\n    if diff_count1 != 0 or diff_count2 != 0:\n        if diff_count1 != 0:\n            df_write_result = diff_df1.withColumn('date', lit(end)).limit(20)\n        else:\n            df_write_result = diff_df2.withColumn('date', lit(end)).limit(20)\n\n        from aadatapipelinecore.core.utils.retry import retry\n\n        def write_test_result(result_df):\n            result_df.write.format(\"delta\").save(\n                 \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_store_category_0625/monthly/\",\n                 mode=\"append\",\n                 partitionBy=[\"type\"])\n        retry(write_test_result, (df_write_result,), {}, interval=10)\n\n        print \"Store Category Test FAIL!!!!! date: {}, diff_count1: {}, diff_count2: {}\".format(end, diff_count1, diff_count2)\n    elif diff_count1 == 0 and diff_count2 == 0:\n        print \"Store Category Test PASS! date: {}, diff_count1: {}, diff_count2: {}\".format(end, diff_count1, diff_count2)\n    end = datetime.datetime.strptime(end, '%Y-%m-%d')\n    end = end + relativedelta(days=1)\n    start = end"]},{"cell_type":"code","execution_count":0,"id":"20200625-030514_195712339","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\nimport datetime\nfrom pyspark.sql.functions import lit\nfrom dateutil.relativedelta import relativedelta\n\n\nstart_date = datetime.datetime.strptime('2012-09-01', '%Y-%m-%d')\nend_date = datetime.datetime.strptime('2020-04-30', '%Y-%m-%d')\ngranularity = 'monthly'\n\n\ndef get_date_range(granularity, start_date):\n    if granularity == 'weekly':\n        end_date = start_date + relativedelta(weeks=1)\n    elif granularity == 'monthly':\n        end_date = start_date + relativedelta(months=1)\n    elif granularity == 'quarterly':\n        end_date = start_date + relativedelta(months=3)\n    elif granularity == 'yearly':\n        end_date = start_date + relativedelta(months=12)\n    return end_date\n\n\nstart = start_date\nwhile start < end_date:\n    end = get_date_range(granularity, start)\n    end = end - relativedelta(days=1)\n    start = datetime.datetime.strftime(start, '%Y-%m-%d')\n    end = datetime.datetime.strftime(end, '%Y-%m-%d')\n    print start, end\n    category_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n        \"store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\n\n    agg_df = category_daily_df.groupBy('app_id', 'country_code', 'device_code', 'category_id').agg(\n        sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), \n            sum('est_revenue').alias('revenue'))\n    agg_df.createOrReplaceTempView(\"agg_df\")\n\n    if granularity == 'weekly' or granularity == 'monthly':\n        pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n            \"store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n    else:\n        pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n            \"store.app-est-category-pre-aggr.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n\n    pre_agg_df.createOrReplaceTempView(\"pre_agg_df\")\n\n    diff_df1 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from agg_df \n                            except all \n                            select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from pre_agg_df\"\"\")\n    diff_df2 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from pre_agg_df \n                            except all \n                            select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                                from agg_df\"\"\")\n    print agg_df.count(), pre_agg_df.count()\n    diff_count1 = diff_df1.count()\n    diff_count2 = diff_df2.count()\n    if diff_count1 != 0 or diff_count2 != 0:\n        if diff_count1 != 0:\n            df_write_result = diff_df1.withColumn('date', lit(end)).limit(20)\n        else:\n            df_write_result = diff_df2.withColumn('date', lit(end)).limit(20)\n\n        print \"Store Category Test FAIL!!!!! date: {}, diff_count1: {}, diff_count2: {}\".format(end, diff_count1, diff_count2)\n    elif diff_count1 == 0 and diff_count2 == 0:\n        print \"Store Category Test PASS! date: {}, diff_count1: {}, diff_count2: {}\".format(end, diff_count1, diff_count2)\n    end = datetime.datetime.strptime(end, '%Y-%m-%d')\n    end = end + relativedelta(days=1)\n    start = end"]},{"cell_type":"code","execution_count":0,"id":"20200625-044251_914973377","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200625-094307_12844492","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\ndaily_sql = \"\"\"select count(1), \n            sum(est_free_app_download), \n            sum(est_paid_app_download), \n            sum(est_revenue)\n            from \n    (select app_id, device_code, country_code, category_id, \n            sum(est_free_app_download) as est_free_app_download, \n            sum(est_paid_app_download) as est_paid_app_download,\n            sum(est_revenue) as est_revenue\n            from store.store_est_category_fact_v1\n            where date between '{}' and '{}'\n            group by\n            app_id,\n            device_code,\n            country_code,\n            category_id) as t;\n\"\"\"\n\nsql = \"\"\"select count(1), \n        sum(est_free_app_download), \n        sum(est_paid_app_download), \n        sum(est_revenue)\n        from store.store_est_category_t_{}_fact_v1 \n        where date between '{}' and '{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndict_test = {'weekly': ['2010-07-10', '2020-06-20'],\n             'monthly': ['2010-07-31', '2020-04-30'],\n             'quarterly' : ['2010-09-30', '2020-03-31'],\n             'yearly' : ['2010-12-31', '2019-12-31']\n}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_date_range(date_list, granularity):\n    result = []\n    start = datetime.datetime.strptime(date_list[0], '%Y-%m-%d')\n    end = datetime.datetime.strptime(date_list[1], '%Y-%m-%d')\n    if granularity == 'weekly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(weeks=1)\n    elif granularity == 'monthly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=1)\n    elif granularity == 'quarterly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=3)\n    elif granularity == 'yearly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=12)\n    # print result\n    return result\n\n\ndef check_store_unified_db_completeness(date_list, graularity):\n    for date in date_list:\n        end = date\n        if graularity == 'weekly':\n            start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(weeks=1) + relativedelta(days=1)\n        elif graularity == 'monthly':\n            start = date[:7] + str('-01')\n        elif graularity == 'quarterly':\n            start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(months=3) + relativedelta(days=1)\n        elif graularity == 'yearly':\n            start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(years=1) + relativedelta(days=1)\n        # start = datetime.datetime.strftime(start, '%Y-%m-%d')\n        print start, end\n        unified_result = query(aa_dsn, daily_sql.format(start, end))\n        db_result = query(aa_dsn, sql.format(graularity[0], date, date, graularity))\n        # print date\n        # print unified_result[0][0]\n        # print db_result\n        if unified_result[0][0] == db_result[0][0] and unified_result[0][1] == db_result[0][1] and unified_result[0][2] == db_result[0][2] and unified_result[0][3] == db_result[0][3]:\n            print \"Completeness Test PASS! date: {}, unified: {}, db: {}\".format(date, unified_result[0], db_result[0])\n        else:\n            test_result.append(date)\n            print \"Completeness Test FAIL!!!!! date: {}, unified: {}, db: {}\".format(date, unified_result[0], db_result[0])\n\n\n\n\nkey = 'monthly'\nvalue = ['2010-07-31', '2020-04-30']\ncheck_store_unified_db_completeness(get_date_range(value, key), key)\nprint test_result"]},{"cell_type":"code","execution_count":0,"id":"20200625-104931_1793745828","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom pyspark.sql import Row\nfrom dateutil.relativedelta import relativedelta\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('10.2.6.141', 5432)]\nPG_AA_NAME = 'aa_store_db'\nPG_AA_ACCESS_ID = 'citus_bdp_prod_app_int_qa'\nPG_AA_SECRET_KEY = 'wZw8cfBuuklIskVG'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\ndaily_sql = \"\"\"select count(1), \n            sum(est_free_app_download), \n            sum(est_paid_app_download), \n            sum(est_revenue)\n            from \n    (select app_id, device_code, country_code, category_id, \n            sum(est_free_app_download) as est_free_app_download, \n            sum(est_paid_app_download) as est_paid_app_download,\n            sum(est_revenue) as est_revenue\n            from store.store_est_category_fact_v1\n            where date between '{}' and '{}'\n            group by\n            app_id,\n            device_code,\n            country_code,\n            category_id) as t;\n\"\"\"\n\nsql = \"\"\"select count(1), \n        sum(est_free_app_download), \n        sum(est_paid_app_download), \n        sum(est_revenue)\n        from store.store_est_category_t_{}_fact_v1 \n        where date between '{}' and '{}' and granularity='{}';\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndict_test = {'weekly': ['2010-07-10', '2020-06-20'],\n             'monthly': ['2010-07-31', '2020-04-30'],\n             'quarterly' : ['2010-09-30', '2020-03-31'],\n             'yearly' : ['2010-12-31', '2019-12-31']\n}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_date_range(date_list, granularity):\n    result = []\n    start = datetime.datetime.strptime(date_list[0], '%Y-%m-%d')\n    end = datetime.datetime.strptime(date_list[1], '%Y-%m-%d')\n    if granularity == 'weekly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(weeks=1)\n    elif granularity == 'monthly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=1)\n    elif granularity == 'quarterly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=3)\n    elif granularity == 'yearly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=12)\n    # print result\n    return result\n\n\ndef check_store_unified_db_completeness(date_list, graularity):\n    for date in date_list:\n        end = date\n        if graularity == 'weekly':\n            start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(weeks=1) + relativedelta(days=1)\n        elif graularity == 'monthly':\n            start = date[:7] + str('-01')\n        elif graularity == 'quarterly':\n            start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(months=3) + relativedelta(days=1)\n        elif graularity == 'yearly':\n            start = datetime.datetime.strptime(date, '%Y-%m-%d') - relativedelta(years=1) + relativedelta(days=1)\n        # start = datetime.datetime.strftime(start, '%Y-%m-%d')\n        print start, end\n        unified_result = query(aa_dsn, daily_sql.format(start, end))\n        db_result = query(aa_dsn, sql.format(graularity[0], date, date, graularity))\n        # print date\n        # print unified_result[0][0]\n        # print db_result\n        if unified_result[0][0] == db_result[0][0] and unified_result[0][1] == db_result[0][1] and unified_result[0][2] == db_result[0][2] and unified_result[0][3] == db_result[0][3]:\n            print \"Completeness Test PASS! date: {}, unified: {}, db: {}\".format(date, unified_result[0], db_result[0])\n        else:\n            test_result.append(date)\n            print \"Completeness Test FAIL!!!!! date: {}, unified: {}, db: {}\".format(date, unified_result[0], db_result[0])\n\n\nkey = 'monthly'\nvalue = ['2010-09-30', '2010-09-30']\ncheck_store_unified_db_completeness(get_date_range(value, key), key)\nprint test_result"]},{"cell_type":"code","execution_count":0,"id":"20200625-094538_1827095861","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}