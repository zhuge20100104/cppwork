{"cells":[{"cell_type":"code","execution_count":0,"id":"20200622-023435_697069021","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nstart = \"2020-06-20\"\nend = \"2020-06-21\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n# CSV schema\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\ndef test_monthkly_data(test_data):\n    print test_data\n    month_indicator = test_data\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2019-07-14':\n        df_ios = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%(test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_android = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/android/sbe_est_app/*/\"%(test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = df_ios.union(df_android)\n        \n\n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" % (test_data)).cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    \n    daily_est_load = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_est_load.createOrReplaceTempView(\"daily_unified_est\")\n\n\n\n    sql_text = \"\"\"\n    \n    WITH filter_top_N_raw_data AS(\n    SELECT\n     distinct\n      id,\n      Sum(est) AS est,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      platform_id,\n      vertical,\n      feed,\n      platform);\n      \n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n      \n      \n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n        );\n        \n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n    );\n    \n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n    \n    \n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n    -- map raw with country_code\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue \n     from country_code_mapping \n     inner join \n         pivot_metric_raw \n     on \n         country_code_mapping.store_id=pivot_metric_raw.store_id \n     and \n         country_code_mapping.market_code=pivot_metric_raw.platform\n    where country_name!='Global'\n    );\n\n\n      \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"ios_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n        },   \n        {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"android_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n        }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    \n    \n    diff_df1 = spark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue from daily_unified_est \")\n    diff_df2 = spark.sql(\"select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue from daily_unified_est  except all select * from country_category_mapping_raw\")\n\n    diff_df1.show()\n    diff_df2.show() \n\n    \n    eject_all_caches(spark)\n\n\nsc.parallelize(map(test_monthkly_data, dates), 1)\n    \n"]},{"cell_type":"code","execution_count":0,"id":"20200622-023657_1855484526","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\n\nstart = \"2020-06-10\"\nend =\"2020-06-15\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\n#dates.sort(reverse=True)\n\n# dates = [\"2015-04-27\", \"2013-05-10\", \"2012-11-25\", \"2012-08-13\", \"2012-07-12\"]\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\nsql_text = \"\"\"\n     WITH filter_top_N_raw_data AS(\n    SELECT\n  *\nFROM\n  (\n    SELECT\n      id,\n      Sum(est) AS est,\n      category_id,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform,\n          d2.category_id\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n          AND d1.platform = d2.platform\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      category_id,\n      platform_id,\n      platform,\n      vertical,\n      feed\n  ) \n     );\n\n\n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n\n\n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform, category_id from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform, category_id\n        );\n\n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform, category_id from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform, category_id\n    );\n\n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download, category_id as category_id_pivot\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n\n    -- map raw with category\n    WITH category_mapping_raw AS (\n\n    SELECT * from \n        ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \n    UNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n     ) as mapping \n    FULL OUTER JOIN pivot_metric_raw \n    ON \n     mapping.legacy_category_id=pivot_metric_raw.category_id_pivot \n    AND \n     mapping.mapping_platform=pivot_metric_raw.platform\n    );\n\n\n\n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue, category_id \n     from country_code_mapping \n     inner join \n         category_mapping_raw \n     on \n         country_code_mapping.store_id=category_mapping_raw.store_id \n     and \n         country_code_mapping.market_code=category_mapping_raw.platform\n    where country_name!='Global'\n    );\n\n\"\"\"\n\n\"\"\"\nget date:  [ [month, [days]], [month, [days]], [month, [days]], ....... ]\n\"\"\"\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in\n                 list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef test_daily_data(spark, test_data):\n    # CSV schema\n    from pyspark.sql import types as T\n    from pyspark.sql import functions as F\n\n    csv_schema = T.StructType(\n        [\n            T.StructField(\"store_id\", T.IntegerType(), True),\n            T.StructField(\"date\", T.DateType(), True),\n            T.StructField(\"platform_id\", T.IntegerType(), True),\n            T.StructField(\"vertical\", T.IntegerType(), True),\n            T.StructField(\"feed\", T.IntegerType(), True),\n            T.StructField(\"id\", T.LongType(), True),\n            T.StructField(\"est\", T.IntegerType(), True),\n            T.StructField(\"category_id\", T.IntegerType(), True),\n            T.StructField(\"rank\", T.IntegerType(), True)\n        ]\n    )\n\n    print test_data\n    month_indicator = test_data\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator < '2012-01-01':\n        df_1 = spark.read.option(\"basePath\",\n                                   \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % (test_data),\n            sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id',\n                                                                  'platform_id', 'vertical', 'rank', 'feed',\n                                                                  'est', 'date', 'platform').cache()\n\n\n    ### 2. only csv\n    elif month_indicator >= '2012-01-01' and month_indicator < '2019-07-14':\n        df_ios = spark.read.option(\"basePath\",\n                                   \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % (test_data),\n            sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id',\n                                                                  'platform_id', 'vertical', 'rank', 'feed',\n                                                                  'est', 'date', 'platform').cache()\n        df_android = spark.read.option(\"basePath\",\n                                       \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/android/sbe_est_app/*/\" % (\n                test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).select('id', 'store_id',\n                                                                                      'category_id',\n                                                                                      'platform_id',\n                                                                                      'vertical', 'rank',\n                                                                                      'feed', 'est', 'date',\n                                                                                      'platform').cache()\n        df_1 = df_ios.union(df_android)\n\n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\n                                 \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\n            \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" % (\n                test_data)).cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n\n    daily_category_unified = spark.read.format(\"delta\").load(\n        \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\n        \"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_category_unified.createOrReplaceTempView(\"daily_unified_category\")\n\n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"ios_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n\n                \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n            },\n            {\n                \"data_encoding\": \"csv\",\n                \"compression\": \"gzip\",\n                \"name\": \"android_country_mapping\",\n                \"data_schema\": [\n                    {\"name\": \"store_id\", \"type\": \"int\", \"nullable\": False},\n                    {\"name\": \"country_code\", \"type\": \"string\", \"nullable\": False},\n                    {\"name\": \"country_name\", \"type\": \"string\", \"nullable\": False}\n                ],\n                \"csv_options\": {\n                    'header': True,\n                    'sep': '\\t',\n                    'quote': '',\n                    'encoding': 'utf-8',\n                    'escape': ''\n                },\n\n                \"path\": [\n                    \"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n            },\n        {\n            \"data_encoding\": \"parquet\",\n            \"compression\": \"gzip\",\n            \"name\":\"category_mapping_deminsion_service\",\n            \"path\": [\"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\"],\n        }\n        ]\n    }\n\n    run(spark, ingest_msg, sql_text)\n\n    df1 = spark.sql(\n        \"select * from country_category_mapping_raw except all select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue, category_id from daily_unified_category \").persist()\n    df2 = spark.sql(\n        \"select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue, category_id from daily_unified_category  except all select * from country_category_mapping_raw\").persist()\n    df1.show()\n    df2.show()\n    print df1.take(3)\n    print df2.take(3)\n    df1.unpersist()\n    df2.unpersist()\n\n    eject_all_caches(spark)\n\nfor d in dates:\n    test_daily_data(spark, d)\n"]},{"cell_type":"code","execution_count":0,"id":"20200622-100128_916718976","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}