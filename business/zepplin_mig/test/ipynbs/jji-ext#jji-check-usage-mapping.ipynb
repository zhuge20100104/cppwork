{"cells":[{"cell_type":"code","execution_count":0,"id":"20200508-032041_1816420714","metadata":{},"outputs":[],"source":["sh\nfrom pyspark.sql import functions\ndevice_code_dict = {1: {'1': 'Android-phone', '2': 'Android-tablet'}, 2: {'1': 'ios-phone', '2': 'ios-tablet'}}\nraw_df = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2019-11-22/')\nunified_df = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-11-22/')\nprint unified_df.count(), raw_df.count()\nraw_df = raw_df.withColumn('device_code', functions.UserDefinedFunction(\n        lambda x, y: device_code_dict[x][y])(raw_df['platform'], raw_df['device_type']))\ndf_join = raw_df.join(unified_df, on=[raw_df.app_id == unified_df.app_id, raw_df.country == unified_df.country_code, raw_df.device_code == unified_df.device_code], how='inner')\nprint df_join.count()"]},{"cell_type":"code","execution_count":0,"id":"20200508-032434_691076921","metadata":{},"outputs":[],"source":["\nraw_df = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2019-11-23/')\nunified_df = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-11-23/')\nprint unified_df.count(), raw_df.count()\nprint unified_df.select('app_id').distinct().count()\njoin_df = raw_df.join(unified_df, on='app_id',how='inner')\njoin_df.show()"]},{"cell_type":"code","execution_count":0,"id":"20200508-040939_178600667","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-10-31/')\ndf2 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v2/fact/granularity=daily/date=2019-10-31/').select('app_id', 'country_code', 'device_code','est_install_base')\njoin_df = df1.join(df2, on=['app_id', 'country_code', 'device_code'], how='inner')\njoin_df.select('est_install_base', (join_df['est_install_penetration'] *\n                            join_df['est_average_active_users'] / join_df['est_usage_penetration']).\n                       alias('est_install_base_new')).na.fill(0).collect()"]},{"cell_type":"code","execution_count":0,"id":"20200508-053038_1399693340","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2019-10-31/')\ndf2 = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v2/fact/granularity=daily/date=2019-10-31/')\ndf1.join(df2, on=['app_id', 'device_code', 'country_code'], how='inner').select('est_average_bytes_per_user', 'est_average_mb_per_user').show()"]},{"cell_type":"code","execution_count":0,"id":"20200508-061549_1782740919","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\n\n\nwrong_result = []\nplatform_list = {1: 'android', 2: 'ios'}\nraw_granularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\ndevice_code_dict = {1: {'1': 'Android-phone', '2': 'Android-tablet'}, 2: {'1': 'ios-phone', '2': 'ios-tablet'}}\n\n\ndef check_new_metrics_test_result(result_list, date, metric_name):\n    for row in result_list:\n        if row[0] != row[1]:\n            wrong_result.append((metric_name, date[0], row[0], row[1]))\n            print 'Test Wrong !!!! metric: {}, unified_v2: {}, unified_v1: {}, date: {}'.format(\n                metric_name, row[0], row[1], date[0])\n\n\ndef get_path_date_list(granularity):\n    df_date = spark.read.parquet(\n        \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v2/fact/granularity=daily/\").select('date').dropDuplicates()\n    collect_date = df_date.collect()\n    return collect_date\n\n\ndef check_usage_new_metrics(date_list, _granularity):\n    unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date={}/'\n    unified_v2_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v2/fact/granularity=daily/date={}/'\n    for date in date_list:\n        unified_v1_path_parse = unified_v1_path.format(date[0])\n        unified_v2_path_parse = unified_v2_path.format(date[0])\n        unified_v1_df = spark.read.parquet(unified_v1_path_parse)\n        unified_v2_df = spark.read.parquet(unified_v2_path_parse).select(\n            'app_id', 'country_code', 'device_code', 'est_install_base', 'est_population'\n            , 'est_total_sessions', 'est_total_time')\n        join_df = unified_v1_df.join(unified_v2_df, on=['app_id', 'country_code', 'device_code'], how='inner')\n\n        IB_result_row_list = join_df.select('est_install_base', (\n                    join_df['est_install_penetration'] * join_df['est_average_active_users'] / join_df['est_usage_penetration']).\n                       alias('est_install_base_new')).na.fill(0).collect()\n\n        POP_result_row_list = join_df.select('est_population', (\n                    join_df['est_average_active_users'] / join_df['est_usage_penetration']).\n                       alias('est_population_new')).na.fill(0).collect()\n\n        TS_result_row_list = join_df.select('est_total_sessions', (\n                    join_df['est_average_active_users'] * join_df['est_average_session_per_user']).\n                       alias('est_total_sessions_new')).na.fill(0).collect()\n\n        TT_result_row_list = join_df.select('est_total_time', (\n                    join_df['est_average_active_users'] * join_df['est_average_time_per_user'] / 60).\n                       alias('est_total_sessions_new')).na.fill(0).collect()\n        check_new_metrics_test_result(IB_result_row_list, date, 'est_install_base')\n        check_new_metrics_test_result(POP_result_row_list, date, 'est_population')\n        check_new_metrics_test_result(TS_result_row_list, date, 'est_total_sessions')\n        check_new_metrics_test_result(TT_result_row_list, date, 'est_total_time')\n        print 'date={} Test Complete !'.format(date[0])\n\n\ngraularity_list = [\"daily\"]\nfor graularity in graularity_list:\n    check_usage_new_metrics(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200508-114945_381939695","metadata":{},"outputs":[],"source":["\nraw_df = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=v3.0.0/range_type=DAY/date=2019-11-22/')\nraw_df.printSchema()\nraw_df.select('device_type').show(10)"]},{"cell_type":"code","execution_count":0,"id":"20200509-020710_531710134","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls 's3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_db_count_0430/'"]},{"cell_type":"code","execution_count":0,"id":"20200509-025126_2110589779","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_db_count_0430/')\ndf.show()"]},{"cell_type":"code","execution_count":0,"id":"20200509-025230_284409158","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v2/fact/granularity=daily/date=2015-12-28/')\ndf.filter('app_id=115 and country_code=\"JP\" ').show()"]},{"cell_type":"code","execution_count":0,"id":"20200509-025939_346856948","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date=2015-12-28/')\ndf.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200509-075413_1192355677","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2015-12/device_id=1001/store_id=1/'\n"]},{"cell_type":"code","execution_count":0,"id":"20200509-080540_285437823","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2015-12/device_id=1001/store_id=1/')\ndf.printSchema()\ndf.show(5)\nprint df.filter(\"date = '2015-12-30'\").count(), df.filter(\"date = '2015-12-30'\").select('app_id', 'kpi').distinct().count()"]},{"cell_type":"code","execution_count":0,"id":"20200509-081345_664777516","metadata":{},"outputs":[],"source":["\ndf_date = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=weekly/month=2013-01/\").select('date').dropDuplicates()\ncollect_date = df_date.collect()\nprint collect_date"]},{"cell_type":"code","execution_count":0,"id":"20200509-085056_132405232","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2015-12/\"\n"]},{"cell_type":"code","execution_count":0,"id":"20200509-090746_309834832","metadata":{},"outputs":[],"source":["\ndf_date = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=weekly/month=2013-02/\").select('date').dropDuplicates()\ncollect_date = df_date.collect()\nprint collect_date"]},{"cell_type":"code","execution_count":0,"id":"20200509-091240_1944146688","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\nraw_count_with_KPI = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2015-12/device_id=1001/store_id=1/').filter(\"date='2015-12-27'\").select(\"kpi\", \"app_id\").distinct().groupBy(\"kpi\").agg(count(\"kpi\")).collect()\nprint raw_count_with_KPI"]},{"cell_type":"code","execution_count":0,"id":"20200509-101932_46085844","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2015-12/device_id=2002/store_id=0/\"\n"]},{"cell_type":"code","execution_count":0,"id":"20200509-103150_1676205706","metadata":{},"outputs":[],"source":[" \ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2015-12/')\nprint df.filter(df.kpi==17).show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200509-121650_1885930894","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/granularity=daily/date=2015-12-31/')\nprint df.columns\ndf.select('est_average_bytes_per_session').filter(df.app_id==20600005138490).show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200509-140016_516720067","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom aadatapipelinecore.core.utils.retry import retry\n\n\ntest_result = []\nkpi_mapping = {1: \"est_average_active_users\", 2: \"est_average_session_per_user\", 3: \"est_average_session_duration\",\n               4: \"est_install_penetration\", 5: \"est_average_active_days\", 6: \"est_percentage_active_days\",\n               7: \"est_average_bytes_per_user\", 8: \"est_average_time_per_user\", 9: \"est_usage_penetration\",\n               10: \"est_open_rate\", 11: \"est_total_time\", 12: \"est_share_of_category_time\", 14: \"est_total_sessions\",\n               15: \"est_share_of_category_session\", 17: \"est_average_bytes_per_session\",\n               18: \"est_share_of_category_bytes\", 20: \"est_percent_of_wifi_total\", 21: \"est_mb_per_second\",\n               22: \"est_panel_size\", 23: \"est_installs\", 24: \"est_average_active_users_country_share\",\n               25: \"est_installs_country_share\", 26: \"est_audience_index\", 27: \"est_audience_percentage\",\n               28: \"est_cross_product_affinity\"}\n\n\ndef write_test_result(df_write_result):\n    df_write_result.write.format(\"delta\").save(\n        \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_daily_count_0511/\",\n        mode=\"append\",\n        partitionBy=[\"type\"])\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 01, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 02, 15)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef check_au_app_data_count(_granularity, date_list):\n    for m in date_list:\n        try:\n            raw_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\" \\\n                       \"month={raw_month}/\"\n            unified_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/\" \\\n                           \"granularity={unified_granularity}/date={unified_date}/\"\n            raw_path_parse = raw_path.format(raw_month=m[0], raw_granularity=_granularity)\n            raw_count_with_KPI = spark.read.parquet(raw_path_parse).filter(\n                \"date='{}'\".format(m[1])).select(\"kpi\", \"app_id\").groupBy(\n                \"kpi\").agg(count(\"kpi\")).collect()\n            # print raw_count_with_KPI\n        except AnalysisException as e:\n            break\n        for row in raw_count_with_KPI:\n            unified_path_parse = unified_path.format(unified_date=m[1], unified_granularity=_granularity)\n            unified_count = spark.read.parquet(unified_path_parse).filter(\n                \"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n            if row[\"count(kpi)\"] != unified_count:\n                print 'Count Test Wrong !!!! raw data: {}, unified data: {}, date: {}, KPI {}'.format(\n                                  row[\"count(kpi)\"], unified_count, m[1], kpi_mapping[row[\"kpi\"]])\n            test_result.append((_granularity, m[1], row[\"count(kpi)\"], unified_count, kpi_mapping[row[\"kpi\"]]))\n            print row[\"count(kpi)\"], unified_count\n        print \"date={} test complete!\".format(m[1])\n        return\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_au_app_data_count(granularity, get_path_date_list(granularity))\nprint 'pass'\ndf_write_result = spark.createDataFrame(\n    test_result, schema=['type', 'date', 'dump', 'unified_v1', 'kpi'])\nwrite_test_result(df_write_result)"]},{"cell_type":"code","execution_count":0,"id":"20200511-021032_1089616487","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_daily_count_0511/\n"]},{"cell_type":"code","execution_count":0,"id":"20200511-024522_1554138866","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_dump_unified_v1_daily_count_0511/')\ndf.count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200511-024605_1989096410","metadata":{},"outputs":[],"source":["\naws s3 rm part-00001-fe64dcc4-6ec8-464b-bf6f-1ddabafb50f9-c000.snappy.parquet"]},{"cell_type":"code","execution_count":0,"id":"20200511-024855_1873007706","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}