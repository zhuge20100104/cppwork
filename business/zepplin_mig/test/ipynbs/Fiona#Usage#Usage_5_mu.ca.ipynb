{"cells":[{"cell_type":"code","execution_count":0,"id":"20191224-021704_876237833","metadata":{},"outputs":[],"source":["\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\n\nkpi_mapping={ 9:\"est_usage_penetration\"}\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return {\"2001\":\"ios-phone\",\"2002\":\"ios-tablet\"}\n    else:\n        return {\"1001\":\"android-phone\" ,\"1002\":\"android-tablet\"}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)  \n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db,current  ))\n        current += relativedelta(months=1)  \n    return result\n\n\ndef get_path_date_list(granularity):\n    df_date = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_category.v1/fact/granularity={}/\".format(granularity)).select('date').dropDuplicates()\n    collect_date= df_date.collect()\n    # collect_date=[Row(date=u'2018-12-31'), Row(date=u'2019-08-31'), Row(date=u'2015-02-28'), Row(date=u'2017-04-30'), Row(date=u'2014-08-31'), Row(date=u'2017-10-31'), Row(date=u'2018-10-31'), Row(date=u'2018-05-31'), Row(date=u'2017-08-31'), Row(date=u'2013-09-30'), Row(date=u'2016-09-30'), Row(date=u'2014-06-30'), Row(date=u'2013-07-31'), Row(date=u'2018-06-30'), Row(date=u'2014-07-31'), Row(date=u'2016-12-31'), Row(date=u'2014-09-30'), Row(date=u'2017-05-31'), Row(date=u'2019-06-30'), Row(date=u'2013-05-31'), Row(date=u'2015-03-31'), Row(date=u'2015-11-30'), Row(date=u'2019-10-31'), Row(date=u'2017-06-30'), Row(date=u'2019-02-28'), Row(date=u'2015-06-30'), Row(date=u'2018-04-30'), Row(date=u'2016-05-31'), Row(date=u'2013-03-31'), Row(date=u'2016-03-31'), Row(date=u'2019-09-30'), Row(date=u'2015-12-31'), Row(date=u'2015-01-31'), Row(date=u'2013-01-31'), Row(date=u'2014-02-28'), Row(date=u'2019-03-31'), Row(date=u'2016-08-31'), Row(date=u'2018-02-28'), Row(date=u'2013-06-30'), Row(date=u'2016-07-31'), Row(date=u'2015-10-31'), Row(date=u'2018-03-31'), Row(date=u'2014-01-31'), Row(date=u'2018-09-30'), Row(date=u'2017-07-31'), Row(date=u'2019-04-30'), Row(date=u'2014-05-31'), Row(date=u'2019-01-31'), Row(date=u'2018-08-31'), Row(date=u'2014-04-30'), Row(date=u'2016-01-31'), Row(date=u'2017-12-31'), Row(date=u'2019-05-31'), Row(date=u'2017-09-30'), Row(date=u'2018-11-30'), Row(date=u'2018-01-31'), Row(date=u'2016-06-30'), Row(date=u'2015-04-30'), Row(date=u'2015-05-31'), Row(date=u'2018-07-31'), Row(date=u'2016-02-29'), Row(date=u'2015-09-30'), Row(date=u'2013-12-31'), Row(date=u'2014-12-31'), Row(date=u'2013-08-31'), Row(date=u'2013-04-30'), Row(date=u'2019-07-31'), Row(date=u'2013-02-28'), Row(date=u'2017-01-31'), Row(date=u'2017-11-30'), Row(date=u'2013-11-30'), Row(date=u'2013-10-31'), Row(date=u'2017-02-28'), Row(date=u'2016-11-30'), Row(date=u'2016-04-30'), Row(date=u'2014-03-31'), Row(date=u'2014-11-30'), Row(date=u'2015-07-31'), Row(date=u'2017-03-31'), Row(date=u'2014-10-31'), Row(date=u'2016-10-31'), Row(date=u'2015-08-31')]\n    date_list = [(x[0][:7],x[0]) for x in collect_date]\n    print date_list\n    return date_list\n    \n    \n\n    \nimport traceback\ndef check_mu_ca_data_count(store_id_list, device_id_list, _granularity, date_list):\n    t = unittest.TestCase('run')\n    for id,country_code in store_id_list.items():\n        for device,device_code in device_id_list.items():\n            for m in date_list:\n                raw_count_with_KPI=''\n                # print id, device, m[0] , m[1]\n                try:\n                    raw_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_category.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={raw_device_id}/store_id={raw_store_id}/\"\n                    unified_path=\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.usage-penetration-rank.v1//fact/granularity={unified_granularity}/date={unified_date}/\"\n                    raw_path_parse=raw_path.format(raw_device_id=device,raw_store_id=id, raw_month=m[0], raw_granularity=_granularity)\n                    raw_count_with_KPI=spark.read.parquet(raw_path_parse).filter(\"date='{}'\".format(m[1])).select(\"kpi\",\"app_id\",\"category_id\",\"rank\").distinct().groupBy(\"kpi\").agg(count(\"kpi\")).collect()\n                    # print raw_count_with_KPI\n                except AnalysisException as e: \n                    break\n                    # traceback.print_exc()\n                # print 'raw count', raw_count\n                for row in raw_count_with_KPI:\n                    # print 'row _ test', row[\"kpi\"], row[\"count(kpi)\"]\n                    unified_path_parse=unified_path.format(unified_date=m[1], unified_granularity=_granularity)\n                    unified_count= spark.read.parquet(unified_path_parse).filter(\"device_code='{}' and country_code='{}'\".format(device_code,country_code)).filter(\"{} is not null\".format(kpi_mapping[row[\"kpi\"]])).select(kpi_mapping[row[\"kpi\"]]).count()\n                    # print 'unified count' , unified_count\n                    t.assertEqual(row[\"count(kpi)\"], unified_count, \" raw: {} ~ unified data: {},device:{},  store_id:{} , month: {}, KPI {}\".format(row[\"count(kpi)\"], unified_count, device, id , m, row[\"kpi\"]))\n\ngraularity_list=[\"daily\"]\nfor graularity in graularity_list:\n    print graularity\n    check_mu_ca_data_count(IOS_COUNTRY_ID_CODES, get_device_list('ios'),graularity, get_path_date_list(graularity))\n    check_mu_ca_data_count(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),graularity, get_path_date_list(graularity))\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200109-015548_567412992","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_category.v1/fact/"]},{"cell_type":"code","execution_count":0,"id":"20191224-021416_1346295417","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom aaintdatapipeline.application.app_qa.db_check_v1.constants import pg_settings\nfrom aaintdatapipeline.core.urn import Urn\nfrom aaintdatapipeline.application.app_qa.conf import settings\nfrom aaintdatapipeline.core.loader.plproxy import build_db_settings\nfrom aaplproxy.connection import ClusterConnection\nimport unittest\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport psycopg2\n\nANDROID_COUNTRY_ID_CODES = { 1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB', 8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA', 15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID', 22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL', 29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT', 36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO', 43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE', 50: 'KW', 51: 'SA', 52: 'CO', 65: 'LB', 56: 'PE', 80: 'HR', 54: 'PK', 62: 'EC', 73: 'QA', 102: 'MO', 103: 'LU', 53: 'KZ', 1000: 'WW' }\n\n\nIOS_COUNTRY_ID_CODES={143460:'AU',143455:'CA',143465:'CN',143443:'DE',143454:'ES',143442:'FR',143444:'GB',143450:'IT',143462:'JP',143441:'US',143446:'BE',143459:'CH',143483:'CL',143472:'ZA',143471:'VN',143463:'HK',143505:'AR',143503:'BR',143467:'IN',143447:'FI',143476:'ID',143469:'RU',143452:'NL',143473:'MY',143480:'TR',143468:'MX',143466:'KR',143478:'PL',143475:'TH',143470:'TW',143474:'PH',143464:'SG',143516:'EG',143456:'SE',143445:'AT',143489:'CZ',143482:'HU',143458:'DK',143449:'IE',143491:'IL',143461:'NZ',143457:'NO',143453:'PT',143487:'RO',143496:'SK',143448:'GR',143526:'BG',143492:'UA',143481:'AE',143493:'KW',143479:'SA',143501:'CO',143451:'LU',143497:'LB',143515:'MO',143507:'PE',143494:'HR',143477:'PK',143509:'EC',143498:'QA',0:'WW'}\n\ndef get_device_list(device):\n    if device=='ios':\n        return ['2001','2002']\n    else:\n        return ['1001','1002']\n\ndef get_month_list():\n    result = []\n    today = datetime.date(2019, 10, 1) \n    current = datetime.date(2013, 1, 1)    \n    while current <= today:\n        month_data_raw=datetime.datetime.strftime(current,'%Y-%m')\n        month_data_leg_db=datetime.datetime.strftime(current,'%Y%m')\n        result.append((month_data_raw, month_data_leg_db ))\n        current += relativedelta(months=1)    \n    return result\n    \n    \ndef pg_settings(urn, sql, granularity, schema):\n    result = {}\n    db_settings = build_db_settings(urn, schema)\n    connection = ClusterConnection(db_settings)\n    master_runner = connection.master_runner\n    rows, _ = master_runner.select(sql)\n    result[granularity] = rows\n    return result\n\n\nurn = Urn(\n    namespace='app-qa.db-validation.v1',\n    owner='app_qa'\n)\n\npath=\"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_category.v1/fact/granularity={raw_granularity}/month={raw_month}/device_id={device}/store_id={sid}/\"\n\n\nimport traceback\ndef check_mu_category_dump_data(store_id_list, device_id_list, _granularity):\n    t = unittest.TestCase('run')\n    for id in store_id_list:\n        for device in device_id_list:\n            for m in get_month_list():\n                # print id, device, m[0] \n                try:\n                    weekly_basic = \"\"\"\n                        SELECT app_id_count FROM plproxy.execute_select_nestloop($$select count(app_id) from mu.category_{granularity}_{device_id}_{store_id}_{month} $$) t (app_id_count BIGINT)\n                    \"\"\".format(device_id=device,store_id=id, month=m[1], granularity=_granularity)\n                    granularity_ = _granularity.capitalize()\n                    result = pg_settings(urn, weekly_basic, granularity_,'usage')\n                    l = [int(x[0]) for x in result[granularity_]]\n                    if sum(l)==0:\n                        pass\n                        # print 'the table is empty  - device:{}  store_id:{} month:{}'.format(device, id, m[0])\n                    else:\n                        dump_data_path=path.format(device=device, raw_month=m[0], sid=id, raw_granularity=_granularity)\n                        dump_count=spark.read.parquet(dump_data_path).count()\n                        t.assertEqual(dump_count, sum(l), \"{}:{} ~ raw: {} ~ dumped data: {} \".format(granularity_, m[0], dump_count, sum(l)))\n                except psycopg2.ProgrammingError : \n                    pass\n                    # traceback.print_exc()\n                    # print(e)\n                    # print 'the table is not existed - granularity {}, device:{}  store_id:{} month:{}'.format(granularity_, device, id, m[0])\n                \n\ncheck_mu_category_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'monthly')\ncheck_mu_category_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'monthly')\ncheck_mu_category_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'weekly')\ncheck_mu_category_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'weekly')\ncheck_mu_category_dump_data(IOS_COUNTRY_ID_CODES, get_device_list('ios'),'daily')\ncheck_mu_category_dump_data(ANDROID_COUNTRY_ID_CODES, get_device_list('android'),'daily')\nprint 'pass'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191224-022741_209938303","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_category.v1/fact/granularity=daily/month=2016-03/device_id=2002/\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage"]},{"cell_type":"code","execution_count":0,"id":"20191224-022758_386128088","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}