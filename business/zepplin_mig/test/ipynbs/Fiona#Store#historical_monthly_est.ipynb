{"cells":[{"cell_type":"code","execution_count":0,"id":"20200611-063122_2035145411","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\"\"\"\nget date:  [ [month, [days]], [month, [days]], [month, [days]], ....... ]\n\"\"\"\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\n# start = \"2010-07-31\"  # prod\nstart = \"2019-01-31\"    # test\nend = \"2019-11-30\"\n\nmonthly = get_date_list(start, end, freq='M')\nprint monthly\n\n\ndate_list = []\nfor m in monthly:\n    d = get_date_list(m[:8]+'01', m, freq='D')  # start = the first day of each month; end = each month\n    month_and_day = [m, d]\n    date_list.append(month_and_day)\n\nprint date_list\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n# CSV schema\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\ndef test_monthkly_data(test_data):\n    print test_data[0]\n    print test_data[1]\n    \n    month_indicator = test_data[0]\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2019-07-01':\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 3. half is csv, half is parquet ###\n    elif month_indicator == '2019-07-31':\n        # First half of 2019-07\n        temp_date_range = get_date_list('2019-07-01', '2019-07-14')\n        first_half_month_df = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        # Second half of 2019-07\n        temp_date_range = get_date_list('2019-07-15', '2019-07-31')\n        second_half_month = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(temp_date_range)).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = first_half_month_df.union(second_half_month)\n        \n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(test_data[1])) .cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    \n    weekly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v3/fact/\").where(\"granularity='monthly' and date='{}' and data_stage='final'\".format(test_data[0])).cache()\n    weekly_df_ho.createOrReplaceTempView(\"unified_monthly\")\n\n    sql_text = \"\"\"\n    \n    WITH filter_top_N_raw_data AS(\n    SELECT\n     distinct\n      id,\n      Sum(est) AS est,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      platform_id,\n      vertical,\n      feed,\n      platform);\n      \n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n      \n      \n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n        );\n        \n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n    );\n    \n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n    \n    \n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n    -- map raw with country_code\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue \n     from country_code_mapping \n     inner join \n         pivot_metric_raw \n     on \n         country_code_mapping.store_id=pivot_metric_raw.store_id \n     and \n         country_code_mapping.market_code=pivot_metric_raw.platform\n    where country_name!='Global'\n    );\n\n\n      \"\"\"\n      \n    diff_df1 = spark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_monthly \")\n    diff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_monthly  except all select * from country_category_mapping_raw\")\n\n    diff_df1.show()\n    diff_df2.show() \n\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"ios_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n        },   \n        {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"android_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n        }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    eject_all_caches(spark)\n\n\nsc.parallelize(map(test_monthkly_data, date_list), 1)\n    \n"]},{"cell_type":"code","execution_count":0,"id":"20200611-065919_7701332","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}