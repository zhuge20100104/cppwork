{"cells":[{"cell_type":"code","execution_count":0,"id":"20200430-032112_1651318282","metadata":{},"outputs":[],"source":["\n\nspark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/\").csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/2018-08-{04,11,18,25}/ios/sbe_est_app/143441/\", sep=\"\\t\").createOrReplaceTempView(\"weekly_data\")\nspark.sql(\"select * from weekly_data where _c0=1015763729\").show()\n\n# spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2010-07-{04,05,06,07,08,09,10}/ios/sbe_est_app/143441/\", sep=\"\\t\").createOrReplaceTempView(\"raw_data_daily\")\n# spark.sql(\"select *, cast(_c4 as int) as t1, cast(_c7 as int) as _c7 from raw_data_daily where _c5=373998688 order by t1 , _c7 desc   \").show(200)\n\n\n# monthly data\nspark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/MONTH/\").csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/MONTH/2018-08-31/ios/sbe_est_app/143441/\", sep=\"\\t\").createOrReplaceTempView(\"monthly_data\")\nspark.sql(\"select * from monthly_data where _c0=1015763729\").show()\n\nspark.sql('''\nSELECT _c1,_c0,_c2, SUM(est) \nFROM\n(\n    SELECT *, CAST(_c10 as int) AS est FROM weekly_data\n)\nWHERE _c0=1015763729\nGROUP BY est,_c1,_c0  ''').show()\n\n# spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/\").csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/2018-08-{05,06,07,08,09,10,11}/ios/sbe_est_app/143441/\", sep=\"\\t\").createOrReplaceTempView(\"raw_data_ha\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200507-065049_1042670288","metadata":{},"outputs":[],"source":["\nimport datetime\nstart_week = \"2015-01-01\"\nend_week = \"2015-02-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(str(real_date1 + datetime.timedelta(days)))\n\nprint dates"]},{"cell_type":"code","execution_count":0,"id":"20200430-032218_1028098037","metadata":{},"outputs":[],"source":["\n\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ndaily_csv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.StringType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\nmonthly_csv_schema = T.StructType(\n    [\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"category\", T.IntegerType(), True),\n        T.StructField(\"year\", T.StringType(), True),\n        T.StructField(\"month\", T.StringType(), True),\n        T.StructField(\"iphone_free\", T.IntegerType(), True),\n        T.StructField(\"iphone_paid\", T.IntegerType(), True),\n        T.StructField(\"iphone_revenue\", T.IntegerType(), True),\n        T.StructField(\"ipad_free\", T.LongType(), True),\n        T.StructField(\"ipad_paid\", T.IntegerType(), True),\n        T.StructField(\"ipad_revenue\", T.IntegerType(), True)\n    ]\n)\n\n\nweekly_csv_schema = T.StructType(\n    [\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"category\", T.IntegerType(), True),\n        T.StructField(\"start_date\", T.StringType(), True),\n        T.StructField(\"end_date\", T.StringType(), True),\n        T.StructField(\"iphone_free\", T.IntegerType(), True),\n        T.StructField(\"iphone_paid\", T.IntegerType(), True),\n        T.StructField(\"iphone_revenue\", T.IntegerType(), True),\n        T.StructField(\"ipad_free\", T.LongType(), True),\n        T.StructField(\"ipad_paid\", T.IntegerType(), True),\n        T.StructField(\"ipad_revenue\", T.IntegerType(), True)\n    ]\n)\n       \n        \ndef check_diff(dates):\n    print 'daily_data', dates\n\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(daily_csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % \",\".join(dates), sep=\"\\t\")\n    df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/MONTH/\").schema(monthly_csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/MONTH/{%s}/ios/sbe_est_app/*/\" % dates[-1], sep=\"\\t\")\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    df_2.createOrReplaceTempView(\"monthly_data\")\n    # spark.sql(\"select * from daily_data\").show(2)\n    # spark.sql(\"select * from monthly_data\").show(2)\n\nsc.parallelize(map(check_diff, [dates] ), 1)\n"]},{"cell_type":"code","execution_count":0,"id":"20200508-075712_1579038001","metadata":{},"outputs":[],"source":["\ntest_df = spark.sql('''\nSELECT * \nFROM   ( \n                SELECT   id, \n                         store_id, \n                         category, \n                         platform_id, \n                         vertical, \n                         feed, \n                         Sum(est) AS est\n                FROM     daily_data \n                WHERE    feed IN ( 0, \n                                  1, \n                                  2, \n                                  101, \n                                  100, \n                                  102 ) \n                GROUP BY id, \n                         store_id, \n                         category, \n                         platform_id, \n                         vertical, \n                         feed\n                          ) PIVOT ( max(est) FOR feed IN (0, \n                                                          1, \n                                                          2, \n                                                          101, \n                                                          100, \n                                                          102) ) ''').withColumnRenamed(\"0\", platform_feed_to_metric('ios',0)).withColumnRenamed(\"1\", platform_feed_to_metric('ios',1)).withColumnRenamed(\"2\", platform_feed_to_metric('ios',2)).withColumnRenamed(\"101\", platform_feed_to_metric('ios',101)).withColumnRenamed(\"100\", platform_feed_to_metric('ios',100)).withColumnRenamed(\"102\", platform_feed_to_metric('ios',102)).na.fill(0).cache()\ntest_df.createOrReplaceTempView(\"transfer\")\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200508-095258_452829983","metadata":{},"outputs":[],"source":["\n\ndef platform_feed_to_metric(platform, feed):\n    mapping = [\n        ['ios', 0, 'iphone_free'],\n        ['ios',1, 'iphone_paid'],\n        ['ios',2, 'iphone_revenue'],\n        ['ios',101,'ipad_free'],\n        ['ios',100,'ipad_paid'],\n        ['ios',102,'ipad_revenue'],\n        ['android',0,'est_free_app_download'],\n        ['android',1,'est_paid_app_download'],\n        ['android',2,'est_revenue'],\n    ]\n    return [x for x in mapping if (x[0], x[1]) == (platform, feed)][0][2]\n    \nplatform_feed_to_metric(\"ios\",0)"]},{"cell_type":"code","execution_count":0,"id":"20200507-071329_1613309194","metadata":{},"outputs":[],"source":["\ntest_df = spark.sql('''\nSELECT * \nFROM   ( \n                SELECT   id, \n                         store_id, \n                         category, \n                         Sum(iphone_free) AS iphone_free,\n                         Sum(iphone_paid) AS iphone_paid,\n                         Sum(iphone_revenue) AS iphone_revenue,\n                         Sum(ipad_free) AS ipad_free,\n                         Sum(ipad_paid) AS ipad_paid,\n                         Sum(ipad_revenue) AS ipad_revenue\n\n                FROM      \n                GROUP BY id, \n                         store_id, \n                         category ) as t ''')\ntest_df.createOrReplaceTempView(\"transfer\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200507-072949_1784855771","metadata":{},"outputs":[],"source":["\n# diff_df_1 = spark.sql(\"select distinct id, store_id from (select id, category, store_id, iphone_free, iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer ) as t1 except all select distinct id, store_id from (select id, category, store_id, iphone_free, iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from monthly_data ) as t2\").cache()\n# diff_df_2 = spark.sql(\"select distinct id, store_id from (select id, category, store_id, iphone_free, iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from monthly_data ) as t1 except all select distinct id, store_id from ( select id, category, store_id, iphone_free, iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer ) as t2\").cache()\n\n\ndiff_df_1 = spark.sql(\"select id, category, store_id, iphone_free, iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer except all select id, category, store_id, iphone_free, iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from monthly_data \").cache()\ndiff_df_2 = spark.sql(\"select id, category, store_id, iphone_free, iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from monthly_data except all select id, category, store_id, iphone_free, iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer\").cache()\nprint diff_df_1.take(10)\nprint diff_df_2.take(10)\n\n# print diff_df_1.show()\n# print diff_df_2.show()\n\n# diff_df_1.show()\n# diff_df_2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200509-032419_917428440","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nstart_week = \"2015-01-01\"\nend_week = \"2015-02-01\"\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append(str(real_date1 + datetime.timedelta(days)))\n\nprint sar_list\n# test_path=list()\n# for x in sar_list:\n#     for key,item in x.items():\n#         test_path.append((key.strftime(\"%Y-%m-%d\"),[d.strftime(\"%Y-%m-%d\") for d in item]))\n\nweekly_csv_schema = T.StructType(\n    [\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"category\", T.IntegerType(), True),\n        T.StructField(\"start_date\", T.StringType(), True),\n        T.StructField(\"end_date\", T.StringType(), True),\n        T.StructField(\"iphone_free\", T.IntegerType(), True),\n        T.StructField(\"iphone_paid\", T.IntegerType(), True),\n        T.StructField(\"iphone_revenue\", T.IntegerType(), True),\n        T.StructField(\"ipad_free\", T.LongType(), True),\n        T.StructField(\"ipad_paid\", T.IntegerType(), True),\n        T.StructField(\"ipad_revenue\", T.IntegerType(), True)\n    ]\n)\n \ndf_weekly = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/\").schema(weekly_csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/{%s}/ios/sbe_est_app/*/\" % \",\".join(sar_list), sep=\"\\t\")\n\ndf_weekly.createOrReplaceTempView(\"weekly_data\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200509-033205_902758286","metadata":{},"outputs":[],"source":["\n\n# spark.sql(\"select * from weekly_data where id=378736412 and store_id=143462   \").show()\nspark.sql(\"select * from weekly_data where id=585544408 and store_id=143496 and category=6014 \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200508-105335_1726548379","metadata":{},"outputs":[],"source":["\nspark.sql(\"select sum(est), feed from daily_data where id=585544408 and store_id=143496 group by feed \").show()\n# spark.sql(\"select * from daily_data where id=378736412 and store_id=143462 and category=100  \").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200509-040038_1299752553","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from daily_data where id=585544408 and store_id=143496 and feed=101 \").show(20000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200509-043149_825975714","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from daily_data where id=585544408 and store_id=143496 and category=100 \").show(20000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200509-041053_1834362090","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from weekly_data where id=585544408 and store_id=143496 \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200507-074224_626632125","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from daily_data where id=378736412 and store_id=143462 and category=100 \").show(2)\nspark.sql(\"select * from weekly_data where id=378736412 and store_id=143462 and category=100 \").show()\nspark.sql(\"select * from monthly_data where id=378736412 and store_id=143462 and category=100 \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200507-072959_1108297861","metadata":{},"outputs":[],"source":["\n\nspark.sql(\"select * from monthly_data where id=378736412 and store_id=143462 and category=6014 \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200508-101707_1257650937","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from daily_data where id=378736412 and store_id=143462 \").show(200)"]},{"cell_type":"code","execution_count":0,"id":"20200507-073941_1613266917","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2010-07-04/ios/sbe_est_app/143449/  --recursive\n\naws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/2010-07-10/ios/sbe_est_app/143449/ --recursive\n\naws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/MONTH/2010-07-31/ios/sbe_est_app/143449/ --recursive\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200507-074536_349948440","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}