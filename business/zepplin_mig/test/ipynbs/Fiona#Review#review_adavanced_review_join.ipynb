{"cells":[{"cell_type":"code","execution_count":0,"id":"20200224-014415_1607433044","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count, avg\n# print spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join_20200218_20200223_2/\").show() #.groupBy(\"\")\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join_20200218_20200223_2/\").filter(\"process_date='2020-02-22'\").show()  # groupBy(\"process_date\").agg({\"process_date\":\"count\"}).show()\n# \n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join_20200218_20200221_1/\").groupBy(\"event_month\").agg({\"event_month\":\"count\"}).show()\n# df1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join_20200218_20200223_2/\")\n# df2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\")\ndf2 = df2.drop(\"process_date\")\ndf3 = df1.join(df2, \"review_id\")\n# df3.show()\ndf3.groupBy(\"product_id\", \"process_date\").agg({\"product_id\":\"count\",\"process_date\":\"count\"}).orderBy(\"process_date\").show()\n\n# df3.filter(\"product_id='1475474300'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200731-014515_1061024780","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200731-023105_1185580733","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\ndef citus_row():\n    def get_data_in_citus():\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_citus_db\",\n                user=\"citus_bdp_usage_qa\",\n                host=\"10.2.10.132\",\n                password=\"dNzWtSV3pKTx\",\n                port=5432\n            )\n        )\n        sql = \" select review_id from advancedreview.advancedreview_topic_fact_v3 where _identifier ='120200730080215828' and country_code='JP' \"\n        db_data = query(citus_dsn_, sql)\n        return db_data\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    result = get_data_in_citus()\n    return [Row(review_id=r[0] ) for r in result]\ndf = citus_row()\n"]},{"cell_type":"code","execution_count":0,"id":"20200731-014502_770292751","metadata":{},"outputs":[],"source":["\ndf_db = spark.createDataFrame(df)\ndf_db.createOrReplaceTempView(\"db\")\n\n\ndf1 =  spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\").where(\"process_date='2020-07-29'\").cache()\ndf1.createOrReplaceTempView(\"temp\")\n# spark.sql(\"select date, device_code, language, product_id, review_id, count(review_id) as review_id_count from temp where country_code='JP' group by date, device_code, language, product_id, review_id order by review_id_count desc\").show()\n# spark.sql(\"select  _identifier, country_code, count(1) from temp group by country_code, _identifier\").show()\n\nspark.sql(\"select * from temp where review_id in (select review_id from temp where country_code='JP' except all select * from db)\").show()\nspark.sql(\"select review_id from temp where review_id in (select review_id from temp where country_code='JP' except all select * from db)\").show()\n\n# spark.sql(\"select * from db except all select review_id from temp where country_code='JP'\").show()\n\nspark.sql(\"select * from temp where country_code='JP' and product_id=1064917658 \").show()\nspark.sql(\"select * from temp where country_code='JP' and review_id=6250300160 \").show()\n\n\n\n\n# spark.sql(\"select * from temp where review_id = 6250300160\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200731-022109_2125942650","metadata":{},"outputs":[],"source":["\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\").where(\"process_date = '2020-07-29'\").createOrReplaceTempView(\"adv_data\")\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/\").where(\"process_date = '2020-07-29'\").createOrReplaceTempView(\"review_data\")\nspark.sql(\"\"\"\nselect * from adv_data a left join review_data rv\non a.review_id = rv.review_id \nwhere rv.review_id is null\n\"\"\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200309-031047_714846268","metadata":{},"outputs":[],"source":["\ndf3= spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact\").where(\"process_date='2020-03-07' and process_hour='23'\").filter(\"review_id='5623338514'\")\ndf3.show()\n\npath=\"process_granularity={hourly,daily,weekly}/process_date=2020-03-07/process_hour=23\"\nspark.read.option('basePath','s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/').parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v4/fact/{}\".format(path)).where(\"market_code in ('apple-store','google-play') and device_code in ('android-all', 'ios-all')\").filter(\"review_id='5623338514'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200309-024634_279575929","metadata":{},"outputs":[],"source":["\n\ndf1 =  spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join/process_date=2020-03-07/\")\ndf2 =  spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\").where(\"process_date='2020-03-07'\")\n\ndf1.createOrReplaceTempView(\"not_join\")\ndf2.createOrReplaceTempView(\"topic\")\n\nspark.sql(\"select * from topic where review_id = '5623338514' \").show()\nspark.sql(\"select not_join.review_id,topic.product_id  from not_join inner join topic on not_join.review_id = topic.review_id \").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200612-090503_863731304","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join/ \n"]},{"cell_type":"code","execution_count":0,"id":"20200222-114240_660725819","metadata":{},"outputs":[],"source":["\n# df1=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/\").where(\"review_id='5549512716' \").show()\nprint spark.read.format(\"delta\").parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\").createOrReplaceTempView(\"topic\")\nspark.sql(\"select distinct Cast(_identifier AS long) as number_identifer from topic where _identifier like '%12020061%' order by number_identifer desc\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200219-094933_269229138","metadata":{},"outputs":[],"source":["\nimport unittest\nimport boto3\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3 \nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom datetime import datetime\nfrom elasticsearch import Elasticsearch\nimport json\nimport pyspark.sql.functions as F\nimport json\nimport time\nfrom datetime import datetime\nfrom elasticsearch import Elasticsearch\nimport json\n\n\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-advancedreview'))\npath_list= s3_bucket_list.all(prefix=\"unified/aa.old_es_0211_before_2/\", depth_is_1=True )\nprint path_list[0].split(\"=\")[1].replace(\"-\",\"\").strip(\"/\")\n\nsamples=[]\ndef test_detail_of_unified_db(path_list):\n    for path in path_list:\n        print path\n        unified_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/{}\".format(path)).sample(False, 0.1, seed=0)\n        unified_df.registerTempTable(\"advancedreviews\")\n\n        df = spark.sql(\"select * from advancedreviews limit 10\")\n        \n        rows = (df\n        # .withColumn(\"date\", F.date_format(\"time\", \"yyyy-MM-dd\"))\n        .withColumn(\"chinese_content\", F.when((df.content_language == \"zh\") | (df.content_language == \"zh_Hant\"), df.content))\n        .withColumn(\"chinese_reply\", F.when((df.reply_language == \"zh\") | (df.reply_language == \"zh_Hant\"), df.reply))\n        .withColumn(\"chinese_title\", F.when((df.title_language == \"zh\") | (df.title_language == \"zh_Hant\"), df.title))\n        .withColumn(\"english_content\", F.when(df.content_language == \"en\", df.content))\n        .withColumn(\"english_reply\", F.when(df.reply_language == \"en\", df.reply))\n        .withColumn(\"english_title\", F.when(df.title_language == \"en\", df.title))\n        .withColumn(\"japanese_content\", F.when(df.content_language == \"ja\", df.content))\n        .withColumn(\"japanese_reply\", F.when(df.reply_language == \"ja\", df.reply))\n        .withColumn(\"japanese_title\", F.when(df.title_language == \"ja\", df.title))\n        .withColumn(\"korean_content\", F.when(df.content_language == \"ko\", df.content))\n        .withColumn(\"korean_reply\", F.when(df.reply_language == \"ko\", df.reply))\n        .withColumn(\"korean_title\", F.when(df.title_language == \"ko\", df.title))\n        .withColumn(\"other_language_content\", F.when(\n            (df.content_language != \"zh\") & (df.content_language != \"zh_Hant\") & (df.content_language != \"en\") &\n            (df.content_language != \"ja\") & (df.content_language != \"ko\"), df.content))\n        .withColumn(\"other_language_reply\", F.when(\n            (df.reply_language != \"zh\") & (df.reply_language != \"zh_Hant\") & (df.reply_language != \"en\") &\n            (df.reply_language != \"ja\") & (df.reply_language != \"ko\"), df.reply))\n        .withColumn(\"other_language_title\", F.when(\n            (df.title_language != \"zh\") & (df.title_language != \"zh_Hant\") & (df.title_language != \"en\") &\n            (df.title_language != \"ja\") & (df.title_language != \"ko\"), df.title))\n        .collect()\n        )\n        print 'get several data',len(rows)\n        samples.append(rows)\n    return samples\n    print 'sample size is : {}'.format(len(sample))\n\n\n\n\ndef es_doc(review_id):\n    es_connection = Elasticsearch([\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200\"],http_auth=(\"bdp\",\"C38vEJEuraCw\"),port=19200)\n    doc = es_connection.search(index=\"int-ss-advancedreview_v1-apple-store-ios-all*\", body={\"query\":{\"match\":{\"_id\":\"{}\".format(review_id)}}})\n    return doc\n\n\n\ndef compare(doc, row):\n    keys = [ u'_identifier', u'app_id', u'chinese_content', u'chinese_reply', u'chinese_title', u'content', u'content_language', u'country_code', u'date', u'device_code', u'english_content', u'english_reply', u'english_title', u'japanese_content', u'japanese_reply', u'japanese_title', u'korean_content', u'korean_reply', u'korean_title', u'language', u'market_code', u'other_language_content', u'other_language_reply', u'other_language_title', u'product_version', u'rating', u'reply', u'reply_date', u'reply_language', u'title', u'title_language', u'user_device', u'user_id', u'user_language', u'user_name', u'user_purchased', u'topic_ids']\n    row_dict =  row\n    print 'row dict', row_dict\n     \n    doc = doc[\"hits\"][\"hits\"][0][\"_source\"]\n    print doc\n    for key in keys:\n        if key in (\"app_id\", \"_identifier\"):\n            assert str(doc[key]) == str(row_dict[key]), \"{} is diffrent\".format(key)\n        else:\n            assert doc[key] == row_dict[key], \"{} is diffrent\".format(key)\n# test_detail_of_unified_db(path_list)\nfor sample in samples[1]:\n    print sample\n    doc = es_doc(sample.review_id)\n    compare(doc, sample)\n    print 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200224-084702_1191656028","metadata":{},"outputs":[],"source":["\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/\")\ndf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/\").select(\"review_id\")#.drop(\"doc_id\").withColumn(\"topic_ids\", cast(ArrayType(DataTypes.IntegerType)))\ndf2 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact\").select(\"review_id\")\n\n# df3 = df1.union(df2)\n# print df3.select(\"review_id\").distinct().count()\n\n# print df1.columns\n# print df2.columns\n"]},{"cell_type":"code","execution_count":0,"id":"20200302-004551_1296544724","metadata":{},"outputs":[],"source":["\ndf3 = df1.union(df2)\nprint df3.distinct().count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200302-004719_1387875524","metadata":{},"outputs":[],"source":["%%sh\ncurl -XGET -u bdp:C38vEJEuraCw  -H 'Content-Type: application/json'   http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-advancedreview_v1*/_count?\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200302-004805_1048822341","metadata":{},"outputs":[],"source":["%python\n16060027-16059966"]},{"cell_type":"code","execution_count":0,"id":"20200302-004838_224312605","metadata":{},"outputs":[],"source":["\n\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join/\").filter(\"process_date='2020-02-25'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200219-090102_542315326","metadata":{},"outputs":[],"source":["\nfrom elasticsearch import Elasticsearch\nimport json\n# review_miss_list = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join_201908_20200211_4/\").groupBy(\"process_date\").agg({\"process_date\":\"count\"}).show()\n# # print review_miss_list\n# l = []\n# for x in review_miss_list:\n#     l.append(str(x.review_id))\n# print l\n\ndef es_doc(review_id):\n    es_connection = Elasticsearch([\"http://internal-aa-int-ss-review-elasticsearch-126620990.us-east-1.elb.amazonaws.com:19200/\"],http_auth=(\"bdp_read\",\"E9vjhU2qG1bAcM83\"),port=19200)\n    doc = es_connection.search(index=\"int-reviews-*\", body={\"query\":{\"match\":{\"_id\":\"{}\".format(review_id)}}})\n    return doc\n    \nprint es_doc(\"5550015704\")"]},{"cell_type":"code","execution_count":0,"id":"20200219-041539_208723627","metadata":{},"outputs":[],"source":["\nfrom elasticsearch import Elasticsearch\nimport json\n\nfrom pyspark.sql.functions import count, avg\n\n# review_miss_list = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.old_es_0211_before_2/event_month={2019-10,2019-11,2019-12,2020-01}/\").collect()\nreview_miss_list = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join/process_date=2020-03-08\").collect()\n\n# print review_miss_list\nl = []\nfor x in review_miss_list:\n    l.append(str(x.review_id))\nprint len(l)\n\nnot_find = []\ndef es_doc(review_id):\n    # print 'review_id ', review_id\n    es_connection = Elasticsearch([\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200\"],http_auth=(\"bdp\",\"C38vEJEuraCw\"),port=19200)\n    doc = es_connection.search(index=\"int-ss-advancedreview_v1-apple-store-ios-all-202003*\", body={\"query\":{\"match\":{\"_id\":\"{}\".format(review_id)}}})\n    if len(doc[\"hits\"][\"hits\"]) < 1:\n        not_find.append(review_id)\n    # return doc\n    \nprint 'start to test es'\nfor review_id in l:\n    es_doc(review_id)\nprint not_find\n# 2020-03-07 - ['5583818364']\n# 2020-03-08- ['5513989363', '5544648603', '5587852245', '5513608208', '5549250577', '5591995058', '5513186983', '5535403661', '5584841050', '5586886540', '5531232282', '5580175938', '5498922564', '5591153214', '5597883095', '5515830987', '5589648055', '5565223614', '5531491918', '5571782488', '5573295267', '5549059820']\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200228-055622_1883133483","metadata":{},"outputs":[],"source":["\nimport unittest\nimport boto3\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3 \nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom datetime import datetime\nfrom elasticsearch import Elasticsearch\nimport json\n\n\n# get up level data set\n\n# s3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-advancedreview'))\n# # path_list= s3_bucket_list.all(prefix=\"unified/advancedreview.topic.v1/\", depth_is_1=True )\npath_list=[1,2]#= s3_bucket_list.all(prefix=\"unified/advancedreview.topic.v1/fact/\", depth_is_1=True )\n\n\n\nprint path_list.pop(0)\n# # print path_list[0].split(\"=\")[-1].strip(\"/\")\ndef get_identifer_count_list():\n    count_list=[]\n    for d in path_list:\n        process_date = d.split(\"=\")[-1].strip(\"/\")\n        identifer_count_list = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.topic.v1/fact/process_date={}/\".format(process_date)).dropDuplicates([\"review_id\"]).groupBy(\"_identifier\").agg({\"*\":\"count\"}).collect()\n        print identifer_count_list\n        count_list.extend(identifer_count_list)\n    return count_list\n    \ndef es_doc(_identifer):\n    # print 'review_id ', review_id\n    es_connection = Elasticsearch([\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200\"],http_auth=(\"bdp\",\"C38vEJEuraCw\"),port=19200)\n    doc = es_connection.count(index=\"int-ss-advancedreview_v1-apple-store-ios-all-*\", body={\"query\":{\"match\":{\"_identifier\":\"{}\".format(_identifer)}}})\n    return doc[\"count\"]\n    \ncount_list = get_identifer_count_list()\nprint 'count_list length is : ', len(count_list)\n# print count_list[0][\"_identifier\"]\nprint len(count_list)\nfor r in count_list :\n    # print r\n    ident =  r[\"_identifier\"]\n    count =  r[\"count(1)\"]\n    db_count = es_doc(ident)\n    print db_count, count\n    if db_count != count:\n        print \"db count is {}, unified count: {}, diff is : {}, identiferis {} \".format(db_count, count, db_count- count, r)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200218-041512_1207660206","metadata":{},"outputs":[],"source":["\nimport unittest\nimport boto3\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3 \nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom datetime import datetime\nfrom elasticsearch import Elasticsearch\nimport json\nimport pyspark.sql.functions as F\nimport json\nimport time\nfrom datetime import datetime\nfrom elasticsearch import Elasticsearch\nimport json\n\n\n# get up level data set\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-advancedreview'))\npath_list= s3_bucket_list.all(prefix=\"unified/aa.adv_join_201908_20200211_2/\", depth_is_1=True )\nprint path_list[0].split(\"=\")[1].replace(\"-\",\"\").strip(\"/\")\n\nsamples=[]\ndef test_detail_of_unified_db(path_list):\n    for path in path_list:\n        print path\n        unified_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/{}\".format(path)).sample(False, 0.1, seed=0)\n        unified_df.registerTempTable(\"advancedreviews\")\n\n        df = spark.sql(\"select * from advancedreviews limit 10\")\n        \n        rows = (df\n        # .withColumn(\"date\", F.date_format(\"time\", \"yyyy-MM-dd\"))\n        .withColumn(\"chinese_content\", F.when((df.content_language == \"zh\") | (df.content_language == \"zh_Hant\"), df.content))\n        .withColumn(\"chinese_reply\", F.when((df.reply_language == \"zh\") | (df.reply_language == \"zh_Hant\"), df.reply))\n        .withColumn(\"chinese_title\", F.when((df.title_language == \"zh\") | (df.title_language == \"zh_Hant\"), df.title))\n        .withColumn(\"english_content\", F.when(df.content_language == \"en\", df.content))\n        .withColumn(\"english_reply\", F.when(df.reply_language == \"en\", df.reply))\n        .withColumn(\"english_title\", F.when(df.title_language == \"en\", df.title))\n        .withColumn(\"japanese_content\", F.when(df.content_language == \"ja\", df.content))\n        .withColumn(\"japanese_reply\", F.when(df.reply_language == \"ja\", df.reply))\n        .withColumn(\"japanese_title\", F.when(df.title_language == \"ja\", df.title))\n        .withColumn(\"korean_content\", F.when(df.content_language == \"ko\", df.content))\n        .withColumn(\"korean_reply\", F.when(df.reply_language == \"ko\", df.reply))\n        .withColumn(\"korean_title\", F.when(df.title_language == \"ko\", df.title))\n        .withColumn(\"other_language_content\", F.when(\n            (df.content_language != \"zh\") & (df.content_language != \"zh_Hant\") & (df.content_language != \"en\") &\n            (df.content_language != \"ja\") & (df.content_language != \"ko\"), df.content))\n        .withColumn(\"other_language_reply\", F.when(\n            (df.reply_language != \"zh\") & (df.reply_language != \"zh_Hant\") & (df.reply_language != \"en\") &\n            (df.reply_language != \"ja\") & (df.reply_language != \"ko\"), df.reply))\n        .withColumn(\"other_language_title\", F.when(\n            (df.title_language != \"zh\") & (df.title_language != \"zh_Hant\") & (df.title_language != \"en\") &\n            (df.title_language != \"ja\") & (df.title_language != \"ko\"), df.title))\n        .collect()\n        )\n        print 'get several data',len(rows)\n        samples.append(rows)\n    print 'sample size is : {}'.format(len(sample))\n\n\n\n\ndef es_doc(review_id):\n    es_connection = Elasticsearch([\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200\"],http_auth=(\"bdp\",\"C38vEJEuraCw\"),port=19200)\n    doc = es_connection.search(index=\"int-ss-advancedreview_v1-apple-store-ios-all*\", body={\"query\":{\"match\":{\"_id\":\"{}\".format(review_id)}}})\n    return doc\n\n\n\ndef compare(doc, row):\n    keys = [ u'_identifier', u'app_id', u'chinese_content', u'chinese_reply', u'chinese_title', u'content', u'content_language', u'country_code', u'date', u'device_code', u'english_content', u'english_reply', u'english_title', u'japanese_content', u'japanese_reply', u'japanese_title', u'korean_content', u'korean_reply', u'korean_title', u'language', u'market_code', u'other_language_content', u'other_language_reply', u'other_language_title', u'product_version', u'rating', u'reply', u'reply_date', u'reply_language', u'title', u'title_language', u'user_device', u'user_id', u'user_language', u'user_name', u'user_purchased', u'topic_ids']\n    row_dict =  row\n    print 'row dict', row_dict\n     \n    doc = doc[\"hits\"][\"hits\"][0][\"_source\"]\n    print doc\n    for key in keys:\n        if key in (\"app_id\", \"_identifier\"):\n            assert str(doc[key]) == str(row_dict[key]), \"{} is diffrent\".format(key)\n        else:\n            assert doc[key] == row_dict[key], \"{} is diffrent\".format(key)\n\ntest_detail_of_unified_db(path_list)\nfor sample in samples:\n    doc = es_doc(sample[0].review_id)\n    compare(doc, sample[0])\n    print 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200218-015523_1610985081","metadata":{},"outputs":[],"source":["\nimport unittest\nimport boto3\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3 \nfrom aadatapipelinecore.core.fs import Conf\nimport json\nimport time\nfrom datetime import datetime\nfrom elasticsearch import Elasticsearch\nimport json\n\n\n# get up level data set\n\ns3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-advancedreview'))\n# path_list= s3_bucket_list.all(prefix=\"unified/aa.adv_topic_201907_20200219/\", depth_is_1=True )\npath_list= s3_bucket_list.all(prefix=\"unified/aa.adv_join_20200218_20200221_1/\", depth_is_1=True )\nprint path_list[0].split(\"=\")[1].replace(\"-\",\"\").strip(\"/\")\n\ndef es_count(month):\n    es_connection = Elasticsearch([\"http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200\"],http_auth=(\"bdp\",\"C38vEJEuraCw\"),port=19200)\n    doc = es_connection.count(index=\"int-ss-advancedreview_v1-*-{}\".format(month))\n    return int(doc[\"count\"])\n\ndef test_compare_unified_db_count(path_list):\n    for path in path_list:\n        unified_count = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/{}\".format(path)).select(\"review_id\").distinct().count()\n        month = path.split(\"=\")[1].replace(\"-\",\"\").strip(\"/\")\n        db_count = es_count(month)\n        if unified_count != db_count:\n            print 'not equal!!! unified {} db {} month is {}'.format(unified_count, db_count, month)\n        else:\n            print 'pass!!! unified {} db {} month is {}'.format(unified_count, db_count, month)\n\n\n\n\n# not equal!!! unified 43273 db 0 month is 201906\n# not equal!!! unified 2108164 db 0 month is 201907\n# not equal!!! unified 2013740 db 0 month is 201908\n# not equal!!! unified 2004049 db 0 month is 201909\n# not equal!!! unified 2277383 db 0 month is 201910\n# not equal!!! unified 1998511 db 0 month is 201911\n# not equal!!! unified 1970965 db 0 month is 201912\n# not equal!!! unified 1901957 db 0 month is 202001\n# not equal!!! unified 1068169 db 0 month is 202002\n#  SPARK JOBS FINISHED   \n# Took 1 min 3 sec. Last updated by fzhang at February 21 2020, 6:21:33 PM. (outdated)\n\ntest_compare_unified_db_count(path_list)"]},{"cell_type":"code","execution_count":0,"id":"20200218-030642_374575620","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132 -p 5432 -U citus_bdp_usage_qa -d aa_citus_db << EOF\nSET search_path=advancedreview;\n\nBEGIN;\n-- SELECT count(*) FROM  advancedreview.advancedreview_topic_fact_v2 where country_code='US' and language='en' and array_length(topic_ids, 1)>=2 and topic_ids@>'{1}';\n--SELECT * FROM  advancedreview.advancedreview_topic_fact_v2 where country_code='IN' and review_id='4395695885';\n-- select * from advancedreview_topic_fact_v2 where  cardinality(topic_ids)=0 order by date desc limit 3;\n-- select * from advancedreview_topic_fact_v2 where topic_ids <@ ARRAY[10]::smallint[] limit 10 ;\n-- select _identifier, count(*) from advancedreview_topic_fact_v2 group by _identifier order by _identifier ;\nselect * from advancedreview_topic_fact_v3 order by date desc limit 3 ;\n\nCOMMIT;\nEOF \n"]},{"cell_type":"code","execution_count":0,"id":"20200218-023655_1820003783","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count, avg\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join_20200218_20200221_1/\").groupBy(\"event_month\").agg({\"event_month\":\"count\"}).show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200217-093416_1579907110","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count, avg\n\nprocess_date_result = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview_post.topic.v1/\").groupBy(\"process_date\").agg({\"process_date\":\"count\"})\nprint  process_date_result.show()\n\nprocess_date_result_list = process_date_result.collect()\n\nprint process_date_result_list[1]"]},{"cell_type":"code","execution_count":0,"id":"20200217-103315_1953695026","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132 -p 5432 -U citus_bdp_usage_qa -d aa_citus_db << EOF\nSET search_path=advancedreview;\n\nBEGIN;\n-- SELECT count(*) FROM  advancedreview.advancedreview_topic_fact_v2 where country_code='US' and language='en' and array_length(topic_ids, 1)>=2 and topic_ids@>'{1}';\n--SELECT * FROM  advancedreview.advancedreview_topic_fact_v2 where country_code='IN' and review_id='4395695885';\n-- select * from advancedreview_topic_fact_v2 where  cardinality(topic_ids)=0 order by date desc limit 3;\n-- select * from advancedreview_topic_fact_v2 where topic_ids <@ ARRAY[10]::smallint[] limit 10 ;\n-- select _identifier, count(*) from advancedreview_topic_fact_v2 group by _identifier order by _identifier ;\nselect count(*) from advancedreview_topic_fact_v3 where date between '2020-02-01' and '2020-02-22' ;\n\nCOMMIT;\nEOF "]},{"cell_type":"code","execution_count":0,"id":"20200222-124025_430393827","metadata":{},"outputs":[],"source":["%%sh\ncurl -XGET -u bdp:C38vEJEuraCw -H 'Content-Type: application/json' http://internal-internel-review-es-v7-1286999769.us-east-1.elb.amazonaws.com:19200/int-ss-advancedreview_v1-*202002/_stats?pretty"]},{"cell_type":"code","execution_count":0,"id":"20200217-103657_865256927","metadata":{},"outputs":[],"source":["\nnot_join_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join_201908_20200211/\").show()\n\nnot_join_df.groupBy(\"event_month\").agg({\"event_month\":\"count\"}).show()"]},{"cell_type":"code","execution_count":0,"id":"20200217-104859_1649296739","metadata":{},"outputs":[],"source":["\n\n\nspark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/\").parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review.v1/fact/process_granularity=*/process_date=2020-0{1-16,1-17,1-18,1-19,1-20,1-21,1-22,1-23,1-24,1-25,1-26,1-27,1-28,1-29,1-30,1-31,2*}/\").registerTempTable(\"review_2020_01_16_\")\nval sql = (\n    \"\"\"\nSELECT a.*,date_format(a.time, 'yyyy-MM') as event_month FROM\n    review_2020_01_16_ a\n    INNER JOIN (\n        SELECT review_id,\n        max(unix_timestamp(process_date, 'yyyy-MM-dd') + cast(process_hour as  INT)) AS process_time\n        FROM review_2020_01_16_\n        GROUP BY review_id\n    ) b\n    ON\n        a.review_id = b.review_id\n        AND unix_timestamp(a.process_date, 'yyyy-MM-dd') + cast(a.process_hour as  INT) = b.process_time\n\"\"\")\nval df = spark.sql(sql).cache()\ndf.registerTempTable(\"review_data\")\ndf.write.option(\"compression\", \"gzip\").partitionBy(\"event_month\").parquet(\"s3://b2c-prod-data-pipeline-unified-review/unified/review_2020_01_16_2020_02_11_aggr\")\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview_post.topic.v1/\").registerTempTable(\"topic_data\")\nval sql = (\"\"\"\n    SELECT\n        topic._identifier,\n        date_format(topic.event_date, 'yyyy-MM') as event_month,\n        topic.event_date as date,\n        review.country_code,\n        review.product_version,\n        review.user_name,\n        review.rating,\n        review.language,\n        review.market_code,\n        review.device_code,\n        review.user_device,\n        review.user_purchased,\n        review.user_id,\n        review.app_id,\n        review.user_language,\n        review.title,\n        review.title_language,\n        CASE WHEN review.title_language == \"ko\" THEN review.title END AS korean_title,\n        CASE WHEN review.title_language == \"ja\" THEN review.title END AS japanese_title,\n        CASE WHEN review.title_language IN (\"zh\",\"zh_Hant\") THEN review.title END AS chinese_title,\n        CASE WHEN review.title_language == \"en\" THEN review.title END AS english_title,\n        CASE WHEN review.title_language NOT IN (\"zh\",\"zh_Hant\",\"en\",\"ja\", \"ko\") THEN review.title END AS other_language_title,\n        review.content,\n        review.content_language,\n        CASE WHEN review.content_language == \"ko\" THEN review.content END AS korean_content,\n        CASE WHEN review.content_language == \"ja\" THEN review.content END AS japanese_content,\n        CASE WHEN review.content_language IN (\"zh\",\"zh_Hant\") THEN review.content END AS chinese_content,\n        CASE WHEN review.content_language == \"en\" THEN review.content END AS english_content,\n        CASE WHEN review.content_language NOT IN (\"zh\",\"zh_Hant\",\"en\",\"ja\", \"ko\") THEN review.content END AS other_language_content,\n        review.reply,\n        review.reply_language,\n        CASE WHEN review.reply_language == \"ko\" THEN review.reply END AS korean_reply,\n        CASE WHEN review.reply_language == \"ja\" THEN review.reply END AS japanese_reply,\n        CASE WHEN review.reply_language IN (\"zh\",\"zh_Hant\") THEN review.reply END AS chinese_reply,\n        CASE WHEN review.reply_language == \"en\" THEN review.reply END AS english_reply,\n        CASE WHEN review.reply_language NOT IN (\"zh\",\"zh_Hant\",\"en\",\"ja\", \"ko\") THEN review.reply END AS other_language_reply,\n        review.reply_date,\n        topic.topic_ids,\n        topic.review_id,\n        review.review_id as review_id_\n    FROM\n        topic_data AS topic\n        LEFT JOIN\n        review_data AS review\n        ON topic.review_id = review.review_id\n\"\"\")\nval df = spark.sql(sql).cache()\ndf.registerTempTable(\"joined_data\")\nspark.sql(\"select * from joined_data where review_id_ is null\").write.option(\"compression\", \"gzip\").partitionBy(\"event_month\").parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/adv_join_202016_20200211_review_null\")\nspark.sql(\"select * from joined_data where review_id_ is not null\").write.option(\"compression\", \"gzip\").partitionBy(\"event_month\").parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/adv_join_202016_20200211_review_is_not_null\")\n//2020-01-16 - 2020-02-11 process data review data join topic data 2.11 before\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_join_201908_20200211/\").registerTempTable(\"old_data\")\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/adv_join_202016_20200211_review_is_not_null\").registerTempTable(\"new_data\")\nval sql1 = \"select a.* from new_data a left join old_data b on a.review_id = b.review_id\"\nspark.sql(sql1).registerTempTable(\"new_review_\")\nval sql2 = \"select b.* from new_data a right join old_data b on a.review_id = b.review_id where a.review_id is null\"\nspark.sql(sql2).registerTempTable(\"old_review_\")\nval columns = spark.sql(\"select * from old_review_\").columns.mkString(\",\")\nval df = spark.sql(\"select %s from new_review_ union all select %s from old_review_\".format(columns,columns)).cache()\ndf.registerTempTable(\"joined_data\")\ndf.write.option(\"compression\", \"gzip\").mode(\"overwrite\").partitionBy(\"event_month\").parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_join_201908_20200211_2/\")\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview_post.topic.v1/\").registerTempTable(\"topic_data\")\n(\n    spark.sql(\"\"\"select a.* ,b.review_id as review_id_ \n    from topic_data a left join joined_data b on a.review_id = b.review_id \n    where b.review_id is null\"\"\")\n    .write\n    .mode(\"overwrite\")\n    .option(\"compression\", \"gzip\")\n    .partitionBy(\"event_month\")\n    .parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/aa.adv_not_join_201908_20200211_2\")\n)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}