{"cells":[{"cell_type":"code","execution_count":0,"id":"20211213-033227_546898217","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2020-01-01\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-12-31\", '%Y-%m-%d')\n\nRETENTION_D_LIST = [\"D7\",\"D14\",\"D30\",\"D60\",\"D90\"]\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\n\n\nfor retention_d in RETENTION_D_LIST:\n    for granularity in DATE_DICT:\n        for date in DATE_DICT[granularity]:\n                try:\n                    s3path =\"s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_{retention_d}_routine/version=v1.0.0/range_type=MONTH/date={date}/\".format(retention_d=retention_d, date=date)\n                    spark.read.parquet(s3path).createOrReplaceTempView(\"test_retention_day\")\n                    df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type, sum(value) as sum from test_retention_day where value is not null\".format(date=date, granularity=granularity))\n                    df.show(100)\n                    # df = spark.sql(\"select distinct(device_type) from test_retention_day\")\n                    # df.show(100)\n                    # df = spark.sql(\"select * from test_retention_week limit 20\")\n                    # print(df)\n                    df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/2021-12-13/{}/\".format(retention_d), mode=\"append\") #append\n                    print(\"PASS on {} {}\".format(granularity, date))\n                except Exception as e:\n                    print(\"ERROR on {} {}\".format(granularity, date))\n"]},{"cell_type":"code","execution_count":0,"id":"20211214-071458_2062091657","metadata":{},"outputs":[],"source":["\n\n\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D7_routine/version=v1.0.0/range_type=MONTH/date=2020-10-31\").createOrReplaceTempView(\"tom_test7\")\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D14_routine/version=v1.0.0/range_type=MONTH/date=2020-10-31\").createOrReplaceTempView(\"tom_test14\")\nspark.read.parquet(\"s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D30_routine/version=v1.0.0/range_type=MONTH/date=2020-10-31\").createOrReplaceTempView(\"tom_test30\")\nspark.sql(\"\"\"select count(distinct(app_id)) from (\nselect distinct(app_id) from tom_test7 \nunion all \nselect distinct(app_id) from tom_test14\nunion all \nselect distinct(app_id) from tom_test30\n)\n\"\"\").show()\n\nspark.sql(\"select * from tom_test7 limit 10 \").show()"]},{"cell_type":"code","execution_count":0,"id":"20211213-033840_752350395","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://aardvark-prod-pdx-mdm-to-int/usage_retention/batch=retention_batch_D7_routine/version=v1.0.0/range_type=MONTH/date=2021-10-31/\n\n\n#batch=retention_batch_D7_routine/version=v1.0.0/range_type=MONTH/\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20211213-034244_758928480","metadata":{},"outputs":[],"source":["\nimport datetime\nimport pandas as pd\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\n\ndef get_date_granularity_mapping_list(begin_date, end_date):\n    date_granularity_mapping_list = {\n        \"monthly\": get_date_list(begin_date, end_date, \"M\"),\n    }\n    return date_granularity_mapping_list\n\nbegin_date = datetime.datetime.strptime(\"2020-01-01\", '%Y-%m-%d')\nend_date = datetime.datetime.strptime(\"2021-12-31\", '%Y-%m-%d')\n\n\nDATE_DICT = get_date_granularity_mapping_list(begin_date, end_date)\n\n\n\nfor granularity in DATE_DICT:\n    for date in DATE_DICT[granularity]:\n            try:\n                s3path =\"s3://aardvark-prod-pdx-mdm-to-int/usage_long_term_retention/version=v1.0.0/range_type=MONTH/date={date}/\".format(retention_d=retention_d, date=date)\n                spark.read.parquet(s3path).createOrReplaceTempView(\"test_retention_w_m\")\n                df = spark.sql(\"select  '{granularity}' as granularity, '{date}' as date, count(distinct(app_id)) as count_app_id, count(distinct(country)) as count_country, count(distinct(device_type)) as count_device_type, sum(value) as sum from test_retention_w_m where value is not null\".format(date=date, granularity=granularity))\n                df.show(100)\n                # df = spark.sql(\"select distinct(device_type) from test_retention_day\")\n                # df.show(100)\n                # df = spark.sql(\"select * from test_retention_week limit 20\")\n                # print(df)\n                df.write.format(\"parquet\").save(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/2021-12-13/retention_W_M/\", mode=\"append\") #append\n                print(\"PASS on {} {}\".format(granularity, date))\n            except Exception as e:\n                print(\"ERROR on {} {}\".format(granularity, date))\n"]},{"cell_type":"code","execution_count":0,"id":"20211214-063930_1094438420","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls \"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/2021-12-13/\"\n"]},{"cell_type":"code","execution_count":0,"id":"20211213-041432_1149350651","metadata":{},"outputs":[],"source":["\n\n\ndate='2021-12-13'\n\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/{}/D*/\".format(date)).createOrReplaceTempView(\"tom_test\")\nspark.sql(\"select date,count_app_id,count_country, sum(sum) from tom_test group by date,count_app_id,count_country order by date asc\").show(1000)\n# df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/{}/\".format(date)).coalesce(1).write.csv(\"s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/adhoc/retention_day.csv\".format(date),header = 'true')\n\n"]},{"cell_type":"code","execution_count":0,"id":"20211214-062527_1920198207","metadata":{},"outputs":[],"source":["%%sh\nDATE='2021-12-13'\n\nrm -rf /tmp/$DATE/\nmkdir -p /tmp/$DATE/\n\naws s3 cp --recursive s3://b2c-prod-data-pipeline-qa/aa.tom.s6usage/adhoc/$DATE/ /tmp/$DATE/retention_day/\nls -al /tmp/$DATE/retention_day/\n\ncat /tmp/$DATE/retention_day/*.csv\n\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}