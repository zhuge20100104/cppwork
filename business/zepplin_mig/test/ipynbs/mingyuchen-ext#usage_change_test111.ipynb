{"cells":[{"cell_type":"code","execution_count":0,"id":"20201116-055736_713004833","metadata":{},"outputs":[],"source":["\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom pyspark.sql import Row\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\nfrom pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType, ShortType\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\n# legacy_category_list = [36, 6014, 7012, 6004, 6008]\nunified_category_list = [800000, 800001, 800005, 800035, 800031]\nlegacy_category_list = [36]\ncategory_mapping = {\n    36: 800000,\n    6014:800001,\n    7012:800005,\n    6004:800035,\n    6008:800031\n}\n\naggr_sql = \"\"\"select  app_id, rank, kpi, estimate from plproxy.execute_select($proxy$\nSELECT app_id, rank, kpi, estimate\nFROM mu.category_monthly_2001_143441 where \ndate in ('2020-01-31', '2020-02-29','2020-03-31') \nand rank <= 1000 \nand category_id = {legacy_category_id} $proxy$)\n t (app_id bigint, rank integer, kpi smallint, estimate double precision) order by rank asc;\n\"\"\"\n\naggr_sql_aa = \"\"\"\nSELECT app_id\n    FROM plproxy.execute_select_nestloop($proxy$\n        SELECT app_id, MAX(estimate) AS estimate\n        FROM mu.category_weekly_2001_143441\n        WHERE date BETWEEN '2020-08-21' AND '2020-08-30' AND category_id = 6014 AND rank <= 1000\n        GROUP BY app_id\n        ORDER BY MAX(estimate) DESC\n        LIMIT 1000\n    $proxy$) t (app_id BIGINT, estimate FLOAT8)\n    GROUP BY app_id\n    ORDER BY MAX(estimate) DESC, app_id ASC\n    LIMIT 1000\n\"\"\"\n\nsingle_sql =  \"\"\"select  app_id, rank, kpi, estimate from plproxy.execute_select($proxy$\nSELECT app_id, rank, kpi, estimate\nFROM mu.category_daily_2001_143441\nwhere date = '2020-07-01'\nand rank <= 1000 \nand category_id = {legacy_category_id} $proxy$)\n t (app_id bigint, rank integer, kpi smallint, estimate double precision) order by rank asc;\n\"\"\"\n\ndomain_single_dql = \"\"\"\nselect  domain_id, rank_est_usage_penetration, est_usage_penetration from plproxy.execute_select($proxy$\nSELECT domain_id, rank_est_usage_penetration, est_usage_penetration\nFROM mw.category_m_ip_us where \ndate = '2020-09-30' \nand rank_est_usage_penetration <= 1000 \nand category_id = {unified_category_id} $proxy$)\n t (domain_id bigint, rank_est_usage_penetration integer, est_usage_penetration double precision) order by rank_est_usage_penetration asc;\n\"\"\" \n\n\n\ndef get_plproxy_result(sql_str):\n    plproxy_result = []\n    result = query(PLPROXY_DSN, sql_str)\n    # print result\n    # distinct_domain_id = result[1][0]\n    # for _r in result:\n    #     plproxy_result.append(_r[0])\n        \n    df_data = [Row(app_id=r[0], rank=r[1], kpi=r[2], estimate=r[3]) for r in result]\n    # # print df_data[1]\n    _schema =StructType([StructField(\"app_id\", LongType(), False), \n    StructField(\"rank\", IntegerType(), False),\n    StructField(\"kpi\", ShortType(), False),\n    StructField(\"estimate\", DoubleType(), False)])\n    df_plproxy = spark.createDataFrame(data=df_data, schema=_schema)\n    # df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n    # spark.sql(\"select * from plproxy_df_new\").show(10000, False)\n    return df_plproxy\n    # return df_plproxy\n    \ndef get_unified_data():\n    domain_unified_source_path = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity=w/month=202005/date=2020-05-16\"\n    \n    spark.read.format(\"delta\").load(unified_source_path).createOrReplaceTempView(\"test_unified\")\n    spark.sql(\"select distinct domain_id from test_unified where  est_average_active_users <> 0 and est_average_active_users is not null order by domain_id asc\").createOrReplaceTempView(\"unified_df_new\")\n\ndef get_plproxy_data():\n     df_plproxy=get_plproxy_result()\n     df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n     spark.sql(\"select count(distinct app_id) from plproxy_df_new\").show(10, False)\n    #  spark.sql(\"select * from plproxy_df_new order by rank asc\").show(10000, False)\n     \n     spark.sql(\"select app_id, avg(estimate) as estimate from plproxy_df_new group by app_id order by estimate desc\").show(10000, False)\n\n\n\ndef compare_single():\n    \n    for category in legacy_category_list:\n        df_plproxy=get_plproxy_result(single_sql.format(legacy_category_id=category))\n        df_plproxy.createOrReplaceTempView(\"plproxy_df\")\n        spark.sql(\"\"\"\n            select product_key as app_id \n            from basic_with_category_daily \n            where date ='2020-07-01'\n            and device_code = 'ios-phone' \n            and country_code='US' \n            and unified_category_key={unified_category} \n            order by est_usage_penetration desc limit 1000\n            \"\"\"\n            .format(unified_category=category_mapping[category])).createOrReplaceTempView(\"apps_new\")\n        spark.sql(\"\"\"\n            select app_id \n            from plproxy_df \n            except \n            select app_id \n            from apps_new\"\"\").createOrReplaceTempView(\"plproxy_diff_new\")\n        spark.sql(\"\"\"\n            select app_id \n            from apps_new \n            except \n            select app_id \n            from plproxy_df\"\"\").createOrReplaceTempView(\"new_diff_plproxy\")\n        \n        print \"category is {category}\".format(category=category)\n        # spark.sql(\"select app_id from plproxy_df\").show(1000, False)\n        # spark.sql(\"select app_id from apps_new\").show(1000, False)\n        \n        spark.sql(\"\"\"\n            select count(1) as plproxy_diff_new \n            from plproxy_diff_new\"\"\").show(10, False)\n        spark.sql(\"\"\"\n            select count(1) as new_diff_plproxy \n            from new_diff_plproxy\"\"\").show(10, False)\n        # spark.sql(\"\"\"\n        #     select app_id as plproxy_diff_new \n        #     from plproxy_diff_new\"\"\").show(10, False)\n        # spark.sql(\"\"\"\n        #     select app_id as new_diff_plproxy \n        #     from new_diff_plproxy\"\"\").show(10, False)\n\n\ndef compare_aggr():\n    # basic_with_category_{granularity}; .format(date='2020-08-31', legacy_category_id=36)\n    \n    for category in legacy_category_list:\n        df_plproxy=get_plproxy_result(aggr_sql.format(legacy_category_id=category))\n        df_plproxy.createOrReplaceTempView(\"plproxy_df\")\n        spark.sql(\"\"\"\n            select app_id,\n                max(estimate) as estimate \n            from plproxy_df \n            group by app_id \n            order by estimate desc \n            limit 1000\"\"\").createOrReplaceTempView(\"plproxy_aggr\")\n        \n        # spark.sql(\"\"\"\n        \n        #     select product_key as app_id, \n        #         sum(est_active_users)/sum(est_population) as aggr_up \n        #     from basic_with_category_daily\n        #     where date between '2020-01-31' and '2020-03-31'  \n        #     and device_code = 'ios-phone'\n        #     and granularity_code = 'monthly'\n        #     and country_code = 'US'\n        #     and product_type_code='app'\n        #     and unified_category_key={unified_category} \n        #     group by app_id \n        #     order by aggr_up desc \n        #     limit 1000\"\"\"\n            \n        #     .format(unified_category=category_mapping[category])).createOrReplaceTempView(\"apps_new\")\n        # spark.sql(\"select app_id from plproxy_aggr except select app_id from apps_new\").createOrReplaceTempView(\"plproxy_diff_new\")\n        # spark.sql(\"select app_id from apps_new except select app_id from plproxy_aggr\").createOrReplaceTempView(\"new_diff_plproxy\")\n        # print \"category is {category}\".format(category=category)\n        spark.sql(\"select app_id,estimate from plproxy_aggr\").show(1000, False)\n        # spark.sql(\"select app_id from apps_new\").show(1000, False)\n        #spark.sql(\"select count(1) as plproxy_diff_new from plproxy_diff_new\").show(10, False)\n        #spark.sql(\"select count(1) as new_diff_plproxy from new_diff_plproxy\").show(10, False)\n        # # spark.sql(\"select app_id from plproxy_df order by app_id limit 10\").show(10, False)\n        # spark.sql(\"select app_id from apps_new order by app_id limit 10\").show(10, False)\n        \n        # spark.sql(\"select app_id as plproxy_diff_new from plproxy_diff_new\").show(10, False)\n        # spark.sql(\"select app_id as new_diff_plproxy from new_diff_plproxy\").show(10, False)\n        \n    \n# compare_single()   \ncompare_aggr()\n# unified_source_path = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity=w/date=2020-05-16\"\n# spark.read.format(\"delta\").load(unified_source_path).show(10)\n"]},{"cell_type":"code","execution_count":0,"id":"20201116-061556_1507376524","metadata":{},"outputs":[],"source":["\nimport pandas as pd\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom applications.db_check_v1.common.db_check_utils import query\nfrom pyspark.sql import Row\nfrom conf.settings import PG_USAGE_HOSTS, PG_USAGE_NAME, PG_USAGE_ACCESS_ID, PG_USAGE_SECRET_KEY, \\\n    CITUS_USAGE_NAME, CITUS_USAGE_ACCESS_ID, CITUS_USAGE_HOSTS, CITUS_USAGE_SECRET_KEY\nfrom pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType, ShortType\n\nPLPROXY_DSN = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_USAGE_NAME,\n        user=PG_USAGE_ACCESS_ID,\n        host=PG_USAGE_HOSTS[0][0],\n        password=PG_USAGE_SECRET_KEY,\n        port=PG_USAGE_HOSTS[0][1]\n    )\n)\n\n# legacy_category_list = [36, 6014, 7012, 6004, 6008]\nunified_category_list = [800000, 800001, 800005, 800035, 800031]\nlegacy_category_list = [36]\ncategory_mapping = {\n    36: 800000,\n    6014:800001,\n    7012:800005,\n    6004:800035,\n    6008:800031,\n    6016:800022\n}\n\n\n \nsum_active_days=\"\"\"\n   select sum(PAD*AU)/sum(AU) AS est_percentage_of_active_days_in_granularity from plproxy.execute_select($proxy$\nSELECT distinct sum(PAD*AU)/sum(AU) AS est_percentage_of_active_days_in_granularity\nFROM mu.category_monthly_2001_143441 where\ndate between '2020-01-31' and '2020-02-29'\nand app_id in (20600010054776)\nand country_code='US'\nand category_id = 36 $proxy$)\n t (est_percentage_of_active_days_in_granularity double);\"\"\"\n\n\ndef get_plproxy_result(sql_str):\n    plproxy_result = []\n    result = query(PLPROXY_DSN, sql_str)\n    # print result\n    # distinct_domain_id = result[1][0]\n    # for _r in result:\n    #     plproxy_result.append(_r[0])\n        \n    df_data = [Row(category_id=r[0]) for r in result]\n    # # print df_data[1]\n    _schema =StructType([StructField(\"est_percentage_of_active_days_in_granularity\", DoubleType(), False)])\n    df_plproxy = spark.createDataFrame(data=df_data, schema=_schema)\n    # df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n    # spark.sql(\"select * from plproxy_df_new\").show(10000, False)\n    return df_plproxy\n    # return df_plproxy\n\ndef get_plproxy_data():\n     df_plproxy=get_plproxy_result()\n     df_plproxy.createOrReplaceTempView(\"plproxy_df_new\")\n     spark.sql(\"select count(distinct app_id) from plproxy_df_new\").show(10, False)\n    #  spark.sql(\"select * from plproxy_df_new order by rank asc\").show(10000, False)\n     \n     spark.sql(\"select app_id, avg(estimate) as estimate from plproxy_df_new group by app_id order by estimate desc\").show(10000, False)\n\n\n\ndef compare_single():\n    \n    for category in legacy_category_list:\n        df_plproxy=get_plproxy_result(single_sql.format(legacy_category_id=category))\n        df_plproxy.createOrReplaceTempView(\"plproxy_df\")\n        spark.sql(\"\"\"\n            select product_key as app_id \n            from basic_with_category_daily \n            where date ='2020-07-01'\n            and device_code = 'ios-phone' \n            and country_code='US' \n            and unified_category_key={unified_category} \n            order by est_usage_penetration desc limit 1000\n            \"\"\"\n            .format(unified_category=category_mapping[category])).createOrReplaceTempView(\"apps_new\")\n        spark.sql(\"\"\"\n            select app_id \n            from plproxy_df \n            except \n            select app_id \n            from apps_new\"\"\").createOrReplaceTempView(\"plproxy_diff_new\")\n        spark.sql(\"\"\"\n            select app_id \n            from apps_new \n            except \n            select app_id \n            from plproxy_df\"\"\").createOrReplaceTempView(\"new_diff_plproxy\")\n        \n        print \"category is {category}\".format(category=category)\n        # spark.sql(\"select app_id from plproxy_df\").show(1000, False)\n        # spark.sql(\"select app_id from apps_new\").show(1000, False)\n        \n        spark.sql(\"\"\"\n            select count(1) as plproxy_diff_new \n            from plproxy_diff_new\"\"\").show(10, False)\n        spark.sql(\"\"\"\n            select count(1) as new_diff_plproxy \n            from new_diff_plproxy\"\"\").show(10, False)\n        # spark.sql(\"\"\"\n        #     select app_id as plproxy_diff_new \n        #     from plproxy_diff_new\"\"\").show(10, False)\n        # spark.sql(\"\"\"\n        #     select app_id as new_diff_plproxy \n        #     from new_diff_plproxy\"\"\").show(10, False)\n\n\ndef compare_aggr():\n  \n    for category in legacy_category_list:\n        plproxy_store=get_plproxy_result(sum_active_days)\n        plproxy_store.createOrReplaceTempView(\"plproxy_store\")\n        \n        # plproxy_usage=get_plproxy_result(plproxy_usage_rank_category)\n        # plproxy_usage.createOrReplaceTempView(\"plproxy_store\")\n\n        spark.sql(\"select category_id from plproxy_store except select category_id from plproxy_usage\").createOrReplaceTempView(\"store_diff_usage\")\n        spark.sql(\"select category_id from plproxy_usage except select category_id from plproxy_store\").createOrReplaceTempView(\"usage_diff_store\")\n        # print \"category is {category}\".format(category=category)\n        # spark.sql(\"select app_id,estimate from plproxy_aggr\").show(1000, False)\n        # spark.sql(\"select app_id from apps_new\").show(1000, False)\n        spark.sql(\"select count(1) as store_diff_usage from store_diff_usage\").show(10, False)\n        # spark.sql(\"select count(1) as usage_diff_store from usage_diff_store\").show(10, False)\n        # # spark.sql(\"select app_id from plproxy_df order by app_id limit 10\").show(10, False)\n        # spark.sql(\"select app_id from apps_new order by app_id limit 10\").show(10, False)\n        \n        # spark.sql(\"select category_id as plproxy_diff_new from plproxy_diff_new\").show(10, False)\n        # spark.sql(\"select category_id as new_diff_plproxy from new_diff_plproxy\").show(10, False)\n        \n    \n# compare_single()   \ncompare_aggr()\n# unified_source_path = \"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.basic.v4/fact/granularity=w/date=2020-05-16\"\n# spark.read.format(\"delta\").load(unified_source_path).show(10)\n"]},{"cell_type":"code","execution_count":0,"id":"20201116-064618_1674514829","metadata":{},"outputs":[],"source":["\npath = \"s3://b2c-prod-data-pipeline-unified-store-free/unified/store.app.v1/dimension/\"\nspark.read.format(\"parquet\").load(path).createOrReplaceTempView(\"test\")\nspark.sql(\"select last_updated, release_date,current_release_date, category_id  from test where id in  (1448228271)\").show(10, False)"]},{"cell_type":"code","execution_count":0,"id":"20201116-074936_746274371","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF\n            select  distinct category_id from plproxy.execute_select(\\$proxy\\$\nSELECT distinct category_id\nFROM aa.app_store_daily_estimate_1_143441 where\ndate between '2020-07-01' and '2020-07-31'\nand app_id in (1447489158)\nand category_id = 6016\nand feed_id in (0,1,2)\\$proxy\\$)\n t (category_id int );\n \nEOF"]},{"cell_type":"code","execution_count":0,"id":"20201116-075154_854621026","metadata":{},"outputs":[],"source":["\npath = \"s3://b2c-prod-data-pipeline-unified-store-free/unified/store.app.v1/dimension/\"\nspark.read.format(\"parquet\").load(path).createOrReplaceTempView(\"test\")\nspark.sql(\"select last_updated, release_date,current_release_date, category_id  from test where id in  (1264195462)\").show(10, False)"]},{"cell_type":"code","execution_count":0,"id":"20201116-082220_424661399","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}