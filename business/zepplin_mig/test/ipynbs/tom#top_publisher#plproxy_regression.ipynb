{"cells":[{"cell_type":"code","execution_count":0,"id":"20200623-113735_79630297","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport pandas as pd\npd.set_option('expand_frame_repr', False)\nimport datetime\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df, etl_skip\nfrom applications.db_check_v1.common.constants import COUNTRY_CODE_MAPPING_BY_MARKET_CODE as COUNTRY_CODE_MAPPING, \\\n    CATEGORY_ID_MAPPING_BY_MARLKET_AND_DEVICE_CODE as CATEGORY_ID_MAPPING\nfrom applications.db_check_v1.common.utils import get_week_start_end_date, get_date_list\n#from applications.db_check_v1.cases.store.publisher_est_v1.constants import MARKET_SIZE_DSN\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\nimport boto3\ns3 = boto3.resource('s3')\ns3object = s3.Object('b2c-prod-data-pipeline-qa', 'tom/top_publisher/regression_plproxy.txt')\n\ndef write_log(strobj, s3obj):\n    s3obj.put(Body=str(strobj))\n\n\n# Copyright (c) 2018 App Annie Inc. All rights reserved.\n# pylint: disable=E1101,C0412,C1801,C0201\n\n\"\"\"\nDB Check modules\n\"\"\"\n\nimport datetime\n\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df, etl_skip\nfrom applications.db_check_v1.common.constants import COUNTRY_CODE_MAPPING_BY_MARKET_CODE as COUNTRY_CODE_MAPPING, \\\n    CATEGORY_ID_MAPPING_BY_MARLKET_AND_DEVICE_CODE as CATEGORY_ID_MAPPING\nfrom applications.db_check_v1.common.utils import get_week_start_end_date, get_date_list\nfrom applications.db_check_v1.cases.usage.basic_kpi_v3.test_basic_kpi_v3_routine_plproxy import CITUS_DSN as PUB_EST_DSN\nfrom applications.db_check_v1.cases.store.app_est_publisher_v1.constants import PUB_EST_DSN, PUB_EST_DB_METRICS\n\n\nclass PublisherEstRawData(object):\n    raw_s3_path = \"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_{}/version=2.0.0/range_type=DAY\" \\\n                  \"/date={}/\"\n    device_code_mapping = {\n        \"00\": \"android-all\",\n        \"01\": \"android-all\",\n        \"02\": \"android-all\",\n        \"10\": \"ios-phone\",\n        \"11\": \"ios-phone\",\n        \"12\": \"ios-phone\",\n        \"1100\": \"ios-tablet\",\n        \"1101\": \"ios-tablet\",\n        \"1102\": \"ios-tablet\",\n        # \"21000\": \"ios-all\",\n        # \"21001\": \"ios-all\",\n        # \"21002\": \"ios-all\",\n    }\n\n    metric_mapping = {\n        0: \"free_app_download\",\n        1: \"paid_app_download\",\n        2: \"revenue\",\n        101: \"free_app_download\",\n        100: \"paid_app_download\",\n        102: \"revenue\",\n        # 1000: \"free_app_download\",\n        # 1001: \"paid_app_download\",\n        # 1002: \"revenue\"\n    }\n\n    dimension_mapping = {\n        \"id\": \"publisher_id\",\n    }\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code, stage=\"final\"):\n        df = self._get_raw_data_by_date_country(date, country_code, stage)\n        df = self._parse_mapping(df)\n        df = self._parse_unified_format(df)\n        df = self._data_clean_up(df)\n        return df\n\n    def _data_clean_up(self, df):\n        # clean unknown mapping\n        category_id_list = list(set(CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].values() +\n                                    CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].values()))\n\n        country_code_list = list(set(COUNTRY_CODE_MAPPING[\"apple-store\"].values() +\n                                     COUNTRY_CODE_MAPPING[\"google-play\"].values()))\n\n        df = df[(df['category_id'].isin(category_id_list)) & (df['country_code'].isin(country_code_list))]\n        return df\n\n    def _parse_mapping(self, df):\n        # country_code mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"google-play\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"apple-store\"]})\n        df = df.rename(columns={'store_id': 'country_code'})\n\n        # category_id mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"]})\n\n        # device_code mapping\n        df[\"device_code\"] = df[\"platform_id\"].astype(str) + df[\"feed\"].astype(str)\n        df = df.replace({\"device_code\": self.device_code_mapping})\n\n        # granularity\n        df[\"granularity\"] = \"daily\"\n\n        # metrics mapping (from feed)\n        df = df.replace({\"feed\": self.metric_mapping})\n        return df\n\n    def _parse_unified_format(self, df):\n        df = df.rename(columns=self.dimension_mapping)\n        df = df.pivot_table(index=[\"publisher_id\", \"category_id\", \"device_code\", \"country_code\", \"granularity\"],\n                            columns='feed', values='est')\n        df.reset_index(inplace=True)\n        df.columns.name = None\n        return df\n\n    def _get_raw_data_by_date_country(self, date, country_code, stage):\n        \"\"\"\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        |        id|store_id|category_id|platform_id|vertical|rank|feed|  est|platform|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        | 284417353|       0|       6006|          1|       1|   1|1002|45235|     ios|\n        | 349554266|       0|       6006|          1|       1|   2|1002|20732|     ios|\n        |1316153435|       0|       6006|          1|       1|   3|1002|15136|     ios|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        \"\"\"\n        ios_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"apple-store\"].items() if v == country_code]\n        gp_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"google-play\"].items() if v == country_code]\n        raw_df = self.spark.read.parquet(self.raw_s3_path.format(date, stage.upper())).\\\n            filter('store_id in ({})'.format(\",\".join(ios_store_ids + gp_store_ids))).toPandas()\n        return raw_df\n\n    def get_metrics_count(self, date, stage=\"final\"):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n        # ios_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].keys()]\n        # gp_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].keys()]\n\n        fillter_sql = \"platform_id = {} and store_id in ({}) and feed in ({})\"\n        df = self.spark.read.parquet(self.raw_s3_path.format(stage.upper(), date))\n        feed_ids_sql = \",\".join([str(x) for x in self.metric_mapping.keys()])\n\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id), feed_ids_sql)).count() + \\\n            df.filter(fillter_sql.format(0, \",\".join(gp_store_id), feed_ids_sql)).count()\n        return count_all\n\n    def get_v1_raw_metrics_count(self, date):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n\n        df = self.spark.read.option(\"delimiter\", \"\\t\").csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/*/sbe_est_publisher/*/*.csv.gz\".format(date))\n        fillter_sql = \"_c2 = {} and _c0 in ({})\"  # _c2 > platform_id, _c0 > store_id\n\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id))).count() + \\\n            df.filter(fillter_sql.format(0, \",\".join(gp_store_id))).count()\n        return count_all\n\n\nclass PublisherEstUnifiedData(object):\n    unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher-dna-log.v1/\" \\\n                      \"fact/granularity=daily/date={}/\"\n    available_device_codes = ['ios-phone', 'ios-tablet', 'android-all']\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code):\n        unified_df = self.spark.read.format(\"delta\").\\\n            load(self.unified_s3_path.format(date)).filter(\"country_code = '{}'\".format(country_code)).toPandas()\n        unified_df = unified_df.drop([\"_identifier\", \"revenue_iap\", \"revenue_non_iap\", \"date\"], axis=1)\n        return unified_df\n\n    def get_metrics_count(self, date):\n        df = self.spark.read.format(\"delta\").load(self.unified_s3_path.format(date))\n        metrics_count = 0\n        device_code_list_sql = \"','\".join(self.available_device_codes)\n        for metric in PUB_EST_DB_METRICS:\n            metrics_count += df.filter(\"device_code in ('{}') and {} is not null\".\n                                       format(device_code_list_sql, metric)).count()\n        return metrics_count\n\n\nclass PublisherEstDBData(object):\n    def get(self, date):\n        sql = \"SELECT * FROM store.store_est_publisher_fact_v2 WHERE date='{}'\".format(date)\n        return query_df(PUB_EST_DSN, sql)\n\n    def get_metrics_count(self, date):\n        metrics_count = 0\n        for metric in PUB_EST_DB_METRICS:\n            sql = \"SELECT count(*) AS metrics_count FROM store.store_est_publisher_fact_v2 \" \\\n                  \"WHERE date='{}' AND {} IS NOT NULL\".format(date, metric)\n            data = query_df(PUB_EST_DSN, sql)\n            metrics_count += data.loc[0].metrics_count\n        return metrics_count\n\n\nclass TestPublisherEstFinalWeekly(PipelineTest):\n    # Every Tuesday 15:00 UTC(23:00 BJ) time will refresh the data of last Full Week.\n    trigger_date_config = ('* 15 * * 2', 3)\n\n    @etl_skip()\n    def test_publisher_est_final_weekly_etl_completeness(self):\n        start_date, end_date = get_week_start_end_date(self.check_date_str)\n        date_list = get_date_list(start_date, end_date)\n        for date in date_list:\n            raw_count = PublisherEstRawData(self.spark).get_metrics_count(date, \"final\")\n            unified_count = PublisherEstUnifiedData(self.spark).get_metrics_count(date)\n            db_count = PublisherEstDBData().get_metrics_count(date)\n            self.assertEqual(raw_count, unified_count,\n                             'raw count:{}, unified count:{}, date:{}'.format(raw_count, unified_count, date))\n            self.assertEqual(raw_count, db_count,\n                             'raw count:{}, db count:{}, date:{}'.format(raw_count, db_count, date))\n            self.assertTrue(db_count > 0, \"db count is 0\")\n\n    def test_publisher_est_etl_final_timelines(self):\n        # Every Tuesday 15:00 UTC(23:00 BJ) time will refresh the data of last Full Week.\n        # E.g. 2020-02-12 10:00 the data of 2020-02-02 ~ 2020-02-08 will be ready\n        trigger_datetime = datetime.datetime.strptime(\"2020-02-12 9:00:00\", '%Y-%m-%d %H:%M:%S')\n        check_date_str_actual = self._get_check_date_from_routing_config(trigger_datetime).strftime(\"%Y-%m-%d\")\n        self.assertEqual(\"2020-02-08\", check_date_str_actual)\n\n    # @etl_skip()\n    # def test_publisher_est_etl_accuracy(self):\n    #     # Every Tuesday 16:00 UTC time will refresh the data of last Full Week.\n    #     country_code = 'US'\n    #     start_date, end_date = get_week_start_end_date(self.check_date_str)\n    #     date_list = get_date_list(start_date, end_date)\n    #     for date in date_list:\n    #         raw_df = PublisherEstRawData(self.spark).get(date, country_code)\n    #         unified_df = PublisherEstUnifiedData(self.spark).get(date, country_code)\n    #         db_df = PublisherEstDBData().get(date)\n    #\n    #         self._compare_df(raw_df, unified_df, log=\"raw / unified - {}\".format(date))\n    #         self._compare_df(unified_df, db_df, log=\"unified / db - {}\".format(date))\n\n\nclass TestPublisherEstPreviewDaily(PipelineTest):\n    # Every day 15:00 UTC(23:00 BJ) time will refresh the data of last Full Week.\n    trigger_date_config = ('* 15 * * *', 1)\n\n    def test_publisher_est_preview_daily_etl_completeness(self):\n        raw_count = PublisherEstRawData(self.spark).get_metrics_count(self.check_date_str, 'preview')\n        unified_count = PublisherEstUnifiedData(self.spark).get_metrics_count(self.check_date_str)\n        db_count = PublisherEstDBData().get_metrics_count(self.check_date_str)\n        self.assertEqual(raw_count, unified_count)\n        self.assertEqual(raw_count, db_count)\n        self.assertTrue(db_count > 0)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200623-115017_1880486251","metadata":{},"outputs":[],"source":["\n\n\nfrom applications.db_check_v1.common.db_check_utils import query\n\nbegin_date = datetime.datetime(2020, 8, 9)\nend_date = datetime.datetime(2020, 8, 18)\n\nsql_str = \"\"\"\nselect sum(count_a) from plproxy.execute_select_nestloop($proxy$ \n    select count(pub_id) as counta\n    from pp.pub_store_daily_estimate_{}\n    where \n        date = '{}' and {}\n$proxy$) tbl (count_a BIGINT);\n\n\"\"\"\n\nplproxy_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=\"dailyest\",\n        user=\"app_bdp_usage_qa\",\n        host=\"internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com\",\n        password=\"2mHdFW6%#REu\",\n        port=7432\n    )\n)\n\ndate_list = get_date_list(begin_date, end_date, \"D\")\n\n# 143547 143601\nios_c_f = \"store_id in (0,143441,143442,143443,143444,143445,143446,143447,143448,143449,143450,143451,143452,143453,143454,143455,143456,143457,143458,143459,143460,143461,143462,143463,143464,143465,143466,143467,143468,143469,143470,143471,143472,143473,143474,143475,143476,143477,143478,143479,143480,143481,143482,143483,143484,143485,143486,143487,143488,143489,143491,143492,143493,143494,143495,143496,143497,143498,143499,143500,143501,143502,143503,143504,143505,143506,143507,143508,143509,143510,143511,143512,143513,143514,143515,143516,143517,143518,143519,143520,143521,143523,143524,143525,143526,143527,143528,143529,143530,143531,143532,143533,143534,143535,143536,143537,143538,143539,143540,143541,143542,143543,143544,143545,143546,143548,143549,143550,143551,143552,143553,143554,143555,143556,143557,143558,143559,143560,143561,143562,143563,143564,143565,143566,143567,143568,143570,143571,143572,143573,143574,143575,143576,143577,143578,143579,143580,143581,143582,143583,143584,143585,143586,143587,143588,143589,143590,143591,143592,143593,143594,143595,143597,143598,143599,143600,143602,143603,143604,143605,143606,143608,143609,143610,143612,143613,143614,143615,143617,143619,143620,143621,143622,143624)\" \ngp_c_f =\"store_id in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,1000)\"\n\nlog= ''\nfor date in date_list:\n    try:\n        count_ios = query(plproxy_dsn, sql_str.format( 0, date, gp_c_f))[0][0]\n        count_gp = query(plproxy_dsn, sql_str.format( 1, date, ios_c_f))[0][0]\n        # count_agg = query(plproxy_dsn, sql_str.format( 2, date, ios_c_f))[0][0]\n        # log_tmp =  \"{},{}\".format(date,count_ios+count_gp+count_agg)\n        # log = log +log_tmp\n        # write_log(log, s3object)\n        print \"{},{},{},{},{}\".format(date, PublisherEstRawData(spark).get_metrics_count(date, stage='preview'), PublisherEstUnifiedData(spark).get_metrics_count(date), PublisherEstDBData().get_metrics_count(date), count_ios+count_gp)\n    except Exception, e:\n        log_tmp =  \"{},{}\".format(date,'error')\n        # log = log + log_tmp\n        # write_log(log, s3object)\n        print e.message\n        # date_list.append(date)\n        print log_tmp\n\n    \n\n\n    \n\n"]},{"cell_type":"code","execution_count":0,"id":"20200623-113754_1821189896","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \nselect date, sum(count_a) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select date, count(pub_id) as counta\n    from pp.pub_store_daily_estimate\n    where \n        date between '2020-07-18' and '2020-07-31' group by date\n\\$proxy\\$) tbl (date Date, count_a BIGINT)  group by date  order by date asc;\n\n\n\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200628-061805_947230371","metadata":{},"outputs":[],"source":["%%sh\n\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \nselect store_id, sum(count_a) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select store_id,count(pub_id) as counta\n    from pp.pub_store_daily_estimate_2\n    where \n        date = '2020-03-22' and store_id= 143547\n    group by store_id\n\\$proxy\\$) tbl ( store_id INT, count_a BIGINT) group by store_id order by  sum(count_a) asc;\n\nEOF\n\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \n# select category_id, sum(count_a) from plproxy.execute_select_nestloop(\\$proxy\\$ \n#     select category_id,count(pub_id) as counta\n#     from pp.pub_store_daily_estimate_0\n#     where \n#         date = '2020-03-22' AND store_id=50\n#     group by category_id\n# \\$proxy\\$) tbl ( category_id INT, count_a BIGINT) group by category_id order by  sum(count_a) asc;\n\n# EOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200628-062644_1194693365","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \n\nselect date,sum(count_a) as count_b from (\n (select date,count(publisher_id) as count_a from store.store_est_publisher_fact_v1 where date between '2020-07-18' and '2020-07-31' and device_code in ('ios-phone', 'ios-tablet') and est_free_app_download is not null group by date) \n UNION ALL\n (select date,count(publisher_id) as count_a from store.store_est_publisher_fact_v1 where date between '2020-07-18' and '2020-07-31' and device_code in ('ios-phone', 'ios-tablet') and est_paid_app_download is not null group by date)\n UNION ALL\n (select date,count(publisher_id) as count_a from store.store_est_publisher_fact_v1 where date between '2020-07-18' and '2020-07-31' and device_code in ('ios-phone', 'ios-tablet') and est_revenue is not null group by date)\n) as b where country_code='MS' group by b.date order by b.date asc;\n\n-- select category_id,count(publisher_id) as count_a from store.store_est_publisher_fact_v1 where date='2020-03-22' and device_code in ('android-all') and country_code='KW' group by category_id order by count_a asc;\n\n-- select * from store.store_est_publisher_fact_v1 where  where date='2020-03-22' and device_code in ('android-all') and country_code='KW' and category_id=400004 order by publisher_id asc;\n \nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200628-074301_69661782","metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":0,"id":"20200628-062854_1195669040","metadata":{},"outputs":[],"source":["\n\nunified_s3_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher.v1/fact/\" \\\n                  \"granularity=daily/date={}/\"\n\nspark.read.format(\"delta\").load(unified_s3_path.format(date))"]},{"cell_type":"code","execution_count":0,"id":"20200623-120701_31860717","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 cp s3://b2c-prod-data-pipeline-qa/tom/top_publisher/regression_plproxy.txt /tmp/regression_plproxy.txt\ncat /tmp/regression_plproxy.txt\n"]},{"cell_type":"code","execution_count":0,"id":"20200623-131954_1847636783","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}