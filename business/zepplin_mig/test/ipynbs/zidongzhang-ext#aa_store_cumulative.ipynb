{"cells":[{"cell_type":"code","execution_count":0,"id":"20200427-065226_902145856","metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":0,"id":"20200427-065247_923613905","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\nsql_text = \"\"\"\n-- mapping feed as metrc in raw\nWITH feed_metric AS (\nselect *, 'free_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='0' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-phone\" as device_code from rank_raw where  feed='1' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric , \"ios-phone\" as device_code from rank_raw where  feed='2' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-tablet\" as device_code from rank_raw where  feed='101' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='100' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-tablet\" as device_code from rank_raw  where  feed='102' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1000' and platform='ios'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1001' and platform='ios'\nUNION ALL\nselect *, 'revenue' as metric, \"ios-all\" as device_code from rank_raw  where  feed='1002' and platform='ios'\nUNION ALL\nselect *, 'free_app_download' as metric , \"android-all\" as device_code from rank_raw   where  feed='0' and platform='android'\nUNION ALL\nselect *, 'paid_app_download' as metric, \"android-all\" as device_code from rank_raw  where  feed='1' and platform='android'\nUNION ALL\nselect *, 'revenue' as metric,  \"android-all\" as device_code from rank_raw  where  feed='2' and platform='android'\n);\n-- select tested column from raw data\nWITH metric_raw_data AS (\nSELECT id, category_id as raw_category_id,rank,store_id as raw_store_id , metric,device_code,date , platform from feed_metric where store_id not in (3,4,5,6, 1002,1003, 1005,1004, 1006,1007)\n);\n-- group by and count data in raw data\nWITH group_by_raw AS (\nSELECT count(id) AS total_count , raw_category_id, raw_store_id, metric,device_code,date,platform from metric_raw_data where raw_store_id not in (3,4,5,6, 1002,1003, 1004, 1005, 1006,1007) group by raw_category_id, raw_store_id, metric,device_code,date, platform\n);\n-- pivot metric column\nWITH pivot_metric_rank_raw AS (\nSELECT \nfree_app_download,revenue, paid_app_download, raw_category_id,raw_store_id,device_code, platform,date\nFROM\n      group_by_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n-- select tested column from raw data\nWITH metric_raw_store_data AS (\nSELECT distinct id, est, store_id as raw_store_id , metric,device_code, date , platform from feed_metric where store_id not in (3,4,5,6, 1002,1003, 1005,1004, 1006,1007)\n);\n-- group by and count data in raw data\nWITH group_by_store_raw AS (\nSELECT count(est) AS total_count ,raw_store_id, metric,device_code,date,platform from metric_raw_store_data where raw_store_id not in (3,4,5,6,1002,1003, 1004, 1005, 1006,1007) group by raw_store_id, metric,device_code,date, platform\n);\n-- pivot metric column\nWITH pivot_metric_store_raw AS (\nSELECT \nfree_app_download,revenue, paid_app_download,raw_store_id,device_code, platform, date\nFROM\n      group_by_store_raw\n PIVOT (\n    max(total_count) \n\tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n  )\n);\n-- map raw with category\nWITH category_mapping_raw AS (\nSELECT * from ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \nUNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n ) as mapping right join pivot_metric_rank_raw on mapping.legacy_category_id=pivot_metric_rank_raw.raw_category_id and \nmapping.mapping_platform=pivot_metric_rank_raw.platform\n);\n-- map raw with rank country_code\nWITH country_category_mapping_rank_raw AS (\nselect date, raw_store_id, country_code,device_code,category_id,free_app_download,paid_app_download,revenue from country_code_mapping right join category_mapping_raw on country_code_mapping.country_code_store_id=category_mapping_raw.raw_store_id and country_code_mapping.market_code=category_mapping_raw.platform\n);\n-- map raw with store country_code\nWITH country_mapping_store_raw AS (\nselect date, raw_store_id, country_code,device_code,free_app_download,paid_app_download,revenue from country_code_mapping right join pivot_metric_store_raw on country_code_mapping.country_code_store_id=pivot_metric_store_raw.raw_store_id and country_code_mapping.market_code=pivot_metric_store_raw.platform\n);\n-- group by unified data\nWITH unified_group_data AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code, category_id as unified_category_id from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code, category_id from  rank_unified ) as unified\ngroup by  category_id,  country_code,  device_code );\n-- group by unified data\nWITH unified_group_data_store AS (\nselect count(app_id) as unified_count_app_id, count(free_app_download) as unified_count_free_app_download, count(paid_app_download) as unified_count_paid_app_download, count(revenue) as unified_count_revenue,\n  country_code as unified_country_code, device_code as unified_device_code from ( select distinct  app_id, free_app_download, paid_app_download, revenue, country_code, device_code from  store_unified ) as unified\ngroup by   country_code,  device_code );\n-- compare raw vs unified data\nWITH compared_data_rank AS (\n    SELECT * from country_category_mapping_rank_raw left join unified_group_data on unified_group_data.unified_country_code==country_category_mapping_rank_raw.country_code and unified_group_data.unified_category_id==country_category_mapping_rank_raw.category_id and unified_group_data.unified_device_code==country_category_mapping_rank_raw.device_code\n);\nWITH miss_data_rank AS (\nselect * from compared_data_rank where unified_count_paid_app_download!=paid_app_download or unified_count_free_app_download != free_app_download  or unified_count_revenue != revenue or unified_count_app_id is null\n)\n-- compare raw vs unified data store\nWITH compared_store_data AS (\n    SELECT * from country_mapping_store_raw left join unified_group_data_store on unified_group_data_store.unified_country_code==country_mapping_store_raw.country_code and unified_group_data_store.unified_device_code==country_mapping_store_raw.device_code\n);\nWITH miss_data_store AS (\nselect * from compared_store_data where free_app_download!=unified_count_free_app_download or paid_app_download!=unified_count_paid_app_download or revenue!=unified_count_revenue or unified_count_app_id is null\n)\n\"\"\"\nstart = '2020-03-03'\nend = '2020-03-04'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n# dates=['2020-03-01','2020-01-24',\"2019-12-06\",\"2020-01-02\",\"2020-01-05\",\"2020-01-10\",\"2020-01-11\",\"2020-01-13\",\"2020-01-20\"]\nd1 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"ios\"))\nd1 = spark.createDataFrame([(0, 'WW', 'Worldwide', 'ios')],\n                           schema=[\"store_id\", \"country_code\", \"_c2\", \"market_code\"]).union(d1)\nd2 = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\",\n                    sep=\"\\t\").withColumnRenamed(\"_c0\", \"store_id\").withColumnRenamed(\"_c1\",\n                                                                                     \"country_code\").withColumn(\n    \"market_code\", F.lit(\"android\"))\ncountry_code_df = d1.union(d2).where(\"country_code is not null\").cache()\ncountry_code_df = country_code_df.withColumnRenamed(\"store_id\", \"country_code_store_id\")\nprint 'country mapping table'\ncountry_code_df.show(2)\ncountry_code_df.createOrReplaceTempView(\"country_code_mapping\")\ncategory_mapping_table = spark.read.parquet(\n    \"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\")\ncategory_mapping_table.createOrReplaceTempView(\"category_mapping_deminsion_service\")\nnamespace = \"aa.store.market-size.v1\"\nfor test_date in dates:\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"name\": \"rank_raw\",\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"path\": [\n                    \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={}/\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n            }\n        ]\n    }\n    run(spark, ingest_msg, sql_text)\n    result = spark.sql(\"select * from miss_data_rank\").show()\n    # result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_rank_data_v2_{}/\".format(start),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n    result_store = spark.sql(\"select * from miss_data_store\").show()\n    # result_store.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_data_v2_{}/\".format(start),\n    #                                   mode=\"append\",\n    #                                   partitionBy=[\"date\"])\n    eject_all_caches(spark)"]},{"cell_type":"code","execution_count":0,"id":"20200511-123831_906522104","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20200427-070058_546641732","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/"]},{"cell_type":"code","execution_count":0,"id":"20200427-081744_141832811","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200427-070307_1513178833","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/ | tail -n 4"]},{"cell_type":"code","execution_count":0,"id":"20200427-081851_1009410659","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/\").show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200427-070319_1050275825","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/ | head -n 60"]},{"cell_type":"code","execution_count":0,"id":"20200506-035556_1406248941","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/ | head -n 5\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2010-07-04/ --summarize --human --recursive | tail -4 \n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date=2010-07-31/"]},{"cell_type":"code","execution_count":0,"id":"20200506-070549_529951076","metadata":{},"outputs":[],"source":["%md\n## Cumulative Test"]},{"cell_type":"code","execution_count":0,"id":"20200506-070546_1145030958","metadata":{},"outputs":[],"source":["\ndate = '2010-07-04'\ndf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).orderBy('free_app_download', ascending=0)\ndf1.show(10)"]},{"cell_type":"code","execution_count":0,"id":"20200508-021555_65033994","metadata":{},"outputs":[],"source":["\ndate = '2010-07-04'\ndf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).where(\"app_id='377194688' and country_code='US'\")\ndf1.show(10)"]},{"cell_type":"code","execution_count":0,"id":"20200512-101846_1252396744","metadata":{},"outputs":[],"source":["%md\n2010-07-04 选 5 ios (phone, tablet)\nfree/paid/ download, revenue top 3, + facebook\ndownload value =2\n2012-01-01 选 5 android 数据\nfree download top 1\ndownload value =2\ncountry code WW, US (edited) \n\n\n\n\n\n\nFiona Zhang(opens in new tab)  10:32 AM\nfacebook: 20600000009072   android\nfacebook: 284882215 ios"]},{"cell_type":"code","execution_count":0,"id":"20200506-071424_861290349","metadata":{},"outputs":[],"source":["\ndate = '2012-07-04'\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).where(\"device_code='android-all'\").orderBy('free_app_download', ascending=0)\ndf2.show(5)\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).where(\"device_code='android-all'\").orderBy('paid_app_download', ascending=0)\ndf2.show(5)\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).where(\"device_code='android-all'\").orderBy('revenue', ascending=0)\ndf2.show(5)\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).where(\"device_code='android-all' and free_app_download=2\").orderBy('free_app_download', ascending=1)\ndf2.show(5)\napp = [\n    ('20600000025034', 'android-all', 'WW'),\n    ('20600000357382', 'android-all', 'WW'),\n    ('20600000246936', 'android-all', 'WW'),\n    ('20600000348221', 'android-all', 'TH')\n    ]"]},{"cell_type":"code","execution_count":0,"id":"20200506-072312_44477189","metadata":{},"outputs":[],"source":["\ndate = '2010-07-04'\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).where(\"app_id='339739007'\").orderBy('free_app_download', ascending=0)\ndf2.show(10)\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format('2020-01-01')).where(\"app_id='339739007'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200506-072815_736435982","metadata":{},"outputs":[],"source":["\ndate = '2010-07-04'\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).where(\"app_id='284882215'\").orderBy('free_app_download', ascending=0)\ndf2.show(10)\ndf2.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200506-071743_115639654","metadata":{},"outputs":[],"source":["\n# mapping:  ios-phone,    ios-all,     ios-tablet,  free_app_download=2, facebook\napp_list = ['377194688', '364709193', '379174209',  '339739007',         '284882215']\napp_list\n\n# app_dict = {\n#     'ios-phone': '377194688',\n#     'ios-all': '364709193',\n#     'ios-tablet': '379174209',\n    \n# }"]},{"cell_type":"code","execution_count":0,"id":"20200506-065824_1094672727","metadata":{},"outputs":[],"source":["\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list"]},{"cell_type":"code","execution_count":0,"id":"20200507-020555_918571155","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20200507-020756_764635305","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/"]},{"cell_type":"code","execution_count":0,"id":"20200507-020427_588202422","metadata":{},"outputs":[],"source":["\ndate = \"2010-07-04\"\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(date)).where(\"app_id='284882215'\").orderBy(\"free_app_download\", ascending=0)\ndf.show(10)"]},{"cell_type":"code","execution_count":0,"id":"20200507-031127_1916116673","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nschema = StructType([StructField(\"app_id\", StringType(), True), \n                 StructField(\"device_code\", StringType(), True),\n                 StructField(\"country_code\", StringType(), True),\n                 StructField(\"free_app_download\", IntegerType(), True),\n                 StructField(\"paid_app_download\", IntegerType(), True),\n                 StructField(\"revenue\", IntegerType(), True)])\ndf = spark.createDataFrame([], schema=schema)\ndf.createOrReplaceTempView('table')\nspark.sql(\"SELECT * FROM table\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200508-012830_235394775","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/zidong/"]},{"cell_type":"code","execution_count":0,"id":"20200507-021743_700461970","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nimport pandas as pd\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \ndates = get_date_list('2010-07-04', '2010-07-31', freq='D')    # >> ['2010-07-04', '2010-07-05', '2010-07-06']\ndates = dates + get_date_list('2010-08-01', '2020-04-18', freq='D')\n# dates = dates + get_date_list('2010-08-01', '2010-09-30', freq='D')\nprint dates\n# mapping:  ios-phone,    ios-all,     ios-tablet,  free_app_download=2, facebook\napp_list = ['377194688', '364709193', '379174209',  '339739007',         '284882215', '20600000025034', '20600000357382', '20600000246936', '20600000009072']\ndevice_code = ['ios-phone','ios-tablet','ios-all', 'android-all']    # exclude 'android-all'\ncountries = ['WW', 'US']\n# app = [\n#     ('377194688', 'ios-phone', 'WW'),\n#     ('364709193', 'ios-all', 'US'),\n#     ('379174209', 'ios-tablet', 'US'),\n#     ('339739007', 'ios-phone', 'WW'),\n#     ('284882215', 'ios-all', 'WW'),    # facebook\n#     ('20600000025034', 'android-all', 'WW'),\n#     ('20600000357382', 'android-all', 'WW'),\n#     ('20600000246936', 'android-all', 'WW'),\n#     ('20600000348221', 'android-all', 'TH'),\n#     ('20600000009072', 'android-all', 'WW')\n#     ]\n\ndef main_test(dates, app, device_code, countries):\n    where_clause = \"app_id in ({}) and device_code in ('{}') and country_code in ('{}')\".format(\",\".join(map(str,app_list)),   \"','\".join(device_code), \"','\".join(countries) )\n    # where_clause = \"app_id={} and device_code={} and country_code={}\".format(app_id, device_code, country_code)\n    # >> \"app_id in (377194688,364709193,379174209,339739007,284882215) and device_code in ('ios-phone','ios-tablet','ios-all') and country_code in ('WW','US')\"\n    print where_clause\n\n    concat_list = []\n    for date in dates:\n    # for app_id, device_code, country_code in app:\n        # where_clause = \"app_id='{}' and device_code='{}' and country_code='{}'\".format(app_id, device_code, country_code)\n        unified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\")\\\n        .parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date))\\\n        .where(where_clause).select('app_id', 'device_code', 'country_code', 'free_app_download').toPandas()\n        \n        concat_list.append(unified_data)\n        \n    temp_df = pd.concat(concat_list).groupby(['app_id', 'device_code', 'country_code'], sort=True).sum(level=['free_app_download', 'paid_app_download', 'revenue'])\n    \n    # fetch data from cumulative\n    # cum_concat_list = []\n    # for app_id, device_code, country_code in app:\n    # where_clause = \"app_id='{}' and device_code='{}' and country_code='{}'\".format(app_id, device_code, country_code)\n    cum_df = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(dates[-1])).where(where_clause).select('app_id', 'device_code', 'country_code', 'free_app_download').toPandas()\n    # cum_concat_list.append(cum_df)\n    # cum_temp_df = pd.concat(cum_concat_list).groupby(['app_id', 'device_code', 'country_code'], sort=True).sum(level=['free_app_download', 'paid_app_download', 'revenue'])\n    cum_temp_df = cum_df.groupby(['app_id', 'device_code', 'country_code'], sort=True).sum(level=['free_app_download', 'paid_app_download', 'revenue'])\n    return temp_df, cum_temp_df\n\ntemp_df, cum_temp_df = main_test(dates, app, device_code, countries)\nprint temp_df\nprint cum_temp_df\n\n# compare\nimport numpy as np\ndef temp_log_to_s3(log, name, file_format):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'zidong/aa.store.cum/{}.{}'.format(name, file_format))\n    if file_format == 'json':\n        s3object.put(Body=json.dumps(log))\n        return json.dumps(log)\n    else:\n        s3object.put(Body=str(log))    \n        return log\n\n# print sorted(list(temp_df['free_app_download'])) == sorted(list(cum_df['free_app_download']))\n\n# joined_df = temp_df.merge(cum_df, on=['app_id', 'device_code', 'country_code'])\n# print joined_df.T\n\ndef _compare_df(df1, df2, on=None):\n    for diff_type in [\"left\", \"right\"]:\n        diff_df = df1.merge(df2, indicator=True, how=diff_type, on=on)  # .loc[lambda x : x['_merge']!='both']\n        print diff_df\n        diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n        print diff_df.empty\n        if not diff_df.empty:\n            print diff_type\n            print diff_df\n    temp_log_to_s3(diff_df, 'diff_df', 'csv')\n\n# temp_df['free_app_download'] = [np.nan if i==5 else i for i in temp_df['free_app_download']]\n# temp_df['free_app_download'] = [i if str(i).isdigit() else 0 for i in temp_df['free_app_download']]\n# print [i.__repr__() for i in temp_df['free_app_download']]\n\ntemp_df[temp_df['free_app_download'].isnull()] = 0\n# temp_df[temp_df['paid_app_download'].isnull()] = 0\n# temp_df[temp_df['revenue'].isnull()] = 0\n\n# cum_temp_df['free_app_download'] = [0 if l is None else l for l in cum_temp_df['free_app_download']]\n# cum_temp_df['paid_app_download'] = [0 if l is None else l for l in cum_temp_df['paid_app_download']]\n# cum_temp_df['revenue'] = [0 if l is None else l for l in cum_temp_df['revenue']]\nprint temp_df\n\n_compare_df(temp_df, cum_temp_df, on=['app_id', 'device_code', 'country_code', 'free_app_download'])\n# diff_df = cum_temp_df.merge(temp_df, indicator=True, how='left', on=['app_id'])\n# print diff_df.T\n\n# temp_df.reset_index(inplace=True)\n# # temp_df.drop(['index'], axis=1, inplace=True)\n# print temp_df\n# # # save\n# spark.createDataFrame(temp_df).write.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\", mode=\"overwrite\")\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\").show(100)\n\n# # # compare\n# import numpy as np\n# def temp_log_to_s3(log, name, file_format):\n#     import boto3\n#     import json\n#     s3 = boto3.resource('s3')\n#     s3object = s3.Object('b2c-prod-data-pipeline-qa', 'zidong/aa.store.cum/{}.{}'.format(name, file_format))\n#     if file_format == 'json':\n#         s3object.put(Body=json.dumps(log))\n#         return json.dumps(log)\n#     else:\n#         s3object.put(Body=str(log))    \n#         return log\n\n# # print sorted(list(temp_df['free_app_download'])) == sorted(list(cum_df['free_app_download']))\n\n# # joined_df = temp_df.merge(cum_df, on=['app_id', 'device_code', 'country_code'])\n# # print joined_df.T\n\n# def _compare_df(df1, df2, on=None):\n#     for diff_type in [\"left\", \"right\"]:\n#         diff_df = df1.merge(df2, indicator=True, how=diff_type, on=on)  # .loc[lambda x : x['_merge']!='both']\n#         diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n#         print diff_df.empty\n#         if not diff_df.empty:\n#             print diff_type\n#             print diff_df\n#     print \"diff_df\", diff_df\n    \n\n# # temp_df['free_app_download'] = [np.nan if i==5 else i for i in temp_df['free_app_download']]\n# # temp_df['free_app_download'] = [i if str(i).isdigit() else 0 for i in temp_df['free_app_download']]\n# # print [i.__repr__() for i in temp_df['free_app_download']]\n\n# temp_df[temp_df['free_app_download'].isnull()] = 0\n# temp_df[temp_df['paid_app_download'].isnull()] = 0\n# temp_df[temp_df['revenue'].isnull()] = 0\n\n# cum_temp_df['free_app_download'] = [0 if l is None else l for l in cum_temp_df['free_app_download']]\n# cum_temp_df['paid_app_download'] = [0 if l is None else l for l in cum_temp_df['paid_app_download']]\n# cum_temp_df['revenue'] = [0 if l is None else l for l in cum_temp_df['revenue']]\n# # cum_temp_df.set_index(['app_id', 'device_code', 'country_code'])\n# print temp_df\n\n# _compare_df(temp_df, cum_temp_df, on=['app_id', 'device_code', 'country_code', 'free_app_download', 'paid_app_download', 'revenue'])\n# # diff_df = cum_temp_df.merge(temp_df, indicator=True, how='left', on=['app_id'])\n# # print diff_df.T"]},{"cell_type":"code","execution_count":0,"id":"20200514-011804_780024553","metadata":{},"outputs":[],"source":["\n# test_df = temp_df.reset_index()\n# # temp_df.drop(['index'], axis=1, inplace=True)\n# print temp_df\n# # save\nprint test_df\nspark.createDataFrame(test_df).write.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\", mode=\"overwrite\")\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\").show(100)"]},{"cell_type":"code","execution_count":0,"id":"20200514-012919_387916872","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\")\ndf.printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200513-103409_234739181","metadata":{},"outputs":[],"source":["\nprint u\"\\nmismatched input '-' expecting <EOF>(line 1, pos 41)\\n\\n== SQL ==\\napp_id='379174209' and device_code='['ios-phone', 'ios-tablet', 'ios-all']' and country_code='US'\\n-----------------------------------------^^^\\n\""]},{"cell_type":"code","execution_count":0,"id":"20200513-014958_378985644","metadata":{},"outputs":[],"source":["\nprint temp_df\n# temp_df.reset_index(inplace=True)\n# temp_df.drop(['index'], axis=1, inplace=True)\n# print temp_df\n# spark.createDataFrame(temp_df).write.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\", mode=\"overwrite\")\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\").show(100)"]},{"cell_type":"code","execution_count":0,"id":"20200513-021711_517740497","metadata":{},"outputs":[],"source":["\nprint temp_df.drop(['index'], axis=1)"]},{"cell_type":"code","execution_count":0,"id":"20200512-122830_1096247319","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\naws s3 cp s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/diff_df.csv -"]},{"cell_type":"code","execution_count":0,"id":"20200507-122114_527638095","metadata":{},"outputs":[],"source":["\n# compare\nimport numpy as np\ndef temp_log_to_s3(log, name, file_format):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'zidong/aa.store.cum/{}.{}'.format(name, file_format))\n    if file_format == 'json':\n        s3object.put(Body=json.dumps(log))\n        return json.dumps(log)\n    else:\n        s3object.put(Body=str(log))    \n        return log\n\n# print sorted(list(temp_df['free_app_download'])) == sorted(list(cum_df['free_app_download']))\n\n# joined_df = temp_df.merge(cum_df, on=['app_id', 'device_code', 'country_code'])\n# print joined_df.T\n\ndef _compare_df(df1, df2, on=None):\n    for diff_type in [\"left\", \"right\"]:\n        diff_df = df1.merge(df2, indicator=True, how=diff_type, on=on)  # .loc[lambda x : x['_merge']!='both']\n        print diff_df\n        diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n        print diff_df.empty\n        if not diff_df.empty:\n            print diff_type\n            print diff_df\n    temp_log_to_s3(diff_df, 'diff_df', 'csv')\n\n# temp_df['free_app_download'] = [np.nan if i==5 else i for i in temp_df['free_app_download']]\n# temp_df['free_app_download'] = [i if str(i).isdigit() else 0 for i in temp_df['free_app_download']]\n# print [i.__repr__() for i in temp_df['free_app_download']]\n\ntemp_df[temp_df['free_app_download'].isnull()] = 0\n# temp_df[temp_df['paid_app_download'].isnull()] = 0\n# temp_df[temp_df['revenue'].isnull()] = 0\n\n# cum_temp_df['free_app_download'] = [0 if l is None else l for l in cum_temp_df['free_app_download']]\n# cum_temp_df['paid_app_download'] = [0 if l is None else l for l in cum_temp_df['paid_app_download']]\n# cum_temp_df['revenue'] = [0 if l is None else l for l in cum_temp_df['revenue']]\nprint temp_df\n\n_compare_df(temp_df, cum_temp_df, on=['app_id', 'device_code', 'country_code', 'free_app_download'])\n# diff_df = cum_temp_df.merge(temp_df, indicator=True, how='left', on=['app_id'])\n# print diff_df.T"]},{"cell_type":"code","execution_count":0,"id":"20200508-013921_1857325290","metadata":{},"outputs":[],"source":["%%sh\naws s3 cp s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/diff_df.csv -"]},{"cell_type":"code","execution_count":0,"id":"20200513-072330_485510513","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nimport pandas as pd\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \ndates = get_date_list('2010-07-04', '2010-07-31', freq='D')    # >> ['2010-07-04', '2010-07-05', '2010-07-06']\n# dates = dates + get_date_list('2010-08-01', '2020-04-18', freq='D')\nprint dates\n# mapping:  ios-phone,    ios-all,     ios-tablet,  free_app_download=2, facebook\napp_list = ['377194688', '364709193', '379174209',  '339739007',         '284882215']\ndevice_code = ['ios-phone','ios-tablet','ios-all']    # exclude 'android-all'\ncountries = ['WW', 'US']\napp = [\n    ('377194688', 'ios-phone', 'WW'),\n    ('364709193', 'ios-all', 'US'),\n    ('379174209', 'ios-tablet', 'US'),\n    ('339739007', 'ios-phone', 'WW'),\n    ('284882215', 'ios-all', 'WW'),    # facebook\n    ('20600000025034', 'android-all', 'WW'),\n    ('20600000357382', 'android-all', 'WW'),\n    ('20600000246936', 'android-all', 'WW'),\n    ('20600000348221', 'android-all', 'TH'),\n    ('20600000009072', 'android-all', 'WW')\n    ]\n\n\n\nconcat_list = []\n\nfor app_id, device_code, country_code in app:\n    where_clause = \"app_id='{}' and device_code='{}' and country_code='{}'\".format(app_id, device_code, country_code)\n    unified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\")\\\n    .parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=20[10-11]*/\")\\\n    .where(where_clause).select('app_id', 'device_code', 'country_code', 'free_app_download', 'paid_app_download', 'revenue')\n    # unified_data.createOrReplaceTempView('uni_table')\n    # uni_df = spark.sql(\"SELECT *, sum(free_app_download) free_app_download, sum(paid_app_download) paid_app_download, sum(revenue) revenue FROM uni_table GROUPBY app_id, device_code, country_code\")\n    uni_df = unified_data.groupby('app_id', 'device_code', 'country_code').agg({'free_app_download': 'sum', 'paid_app_download': 'sum', 'revenue': 'sum'}).withColumnRenamed('sum(free_app_download)', 'free_app_download').withColumnRenamed('sum(paid_app_download)', 'paid_app_download').withColumnRenamed('sum(revenue)', 'revenue')\n    \n    # cumulative data\n    cum_df = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format('2011-12-31')).where(where_clause).select('app_id', 'device_code', 'country_code', 'free_app_download', 'paid_app_download', 'revenue')\n    \n    join_df = uni_df.join(cum_df, ['app_id', 'device_code', 'country_code', 'free_app_download', 'paid_app_download', 'revenue'], 'full_outer')\n    join_df.show(10)\n\n# save\n# spark.createDataFrame(temp_df).write.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\", mode=\"overwrite\")\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_sample_result/\").show(100)"]},{"cell_type":"code","execution_count":0,"id":"20200507-040928_352786310","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nschema = StructType([StructField(\"app_id\", StringType(), True), \n                 StructField(\"device_code\", StringType(), True),\n                 StructField(\"country_code\", StringType(), True),\n                 StructField(\"free_app_download\", IntegerType(), True),\n                 StructField(\"paid_app_download\", IntegerType(), True),\n                 StructField(\"revenue\", IntegerType(), True)])\nrdd = sc.parallelize([{'app_id': '111', 'device_code': '222', 'country_code': 'WW', 'free_app_download': 1, 'paid_app_download': 2, 'revenue': 3}])\ndf1 = spark.createDataFrame(rdd, schema=schema)\ndf1.show()\n\nrdd = sc.parallelize([{'app_id': '111', 'device_code': '222', 'country_code': 'WW', 'free_app_download': 1, 'paid_app_download': 2, 'revenue': 3}])\ndf2 = spark.createDataFrame(rdd, schema=schema)\ndf2.show()\n\njoin_df = df1.join(df2, ['app_id', 'device_code', 'country_code', 'free_app_download', 'paid_app_download', 'revenue'], 'full_outer')\njoin_df.show()"]},{"cell_type":"code","execution_count":0,"id":"20200507-024811_1558260093","metadata":{},"outputs":[],"source":["\na.groupby('app_id', 'device_code', 'country_code').agg({'free_app_download': 'sum'}).orderBy('app_id', 'device_code', 'country_code').show(200)"]},{"cell_type":"code","execution_count":0,"id":"20200507-033820_502670965","metadata":{},"outputs":[],"source":["\ndate = '2010-07-05'\ndf = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(date)).where(\"app_id='284882215' and device_code='ios-all' and country_code='US'\")\ndf.show()"]},{"cell_type":"code","execution_count":0,"id":"20200507-085323_1933457798","metadata":{},"outputs":[],"source":["\ndf = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2010-07-0[4-5]/\").where(\"app_id='284882215' and device_code='ios-all' and country_code='US'\")\ndf.show(10)"]},{"cell_type":"code","execution_count":0,"id":"20200507-024223_1366267029","metadata":{},"outputs":[],"source":["\nprint u\"\\nmismatched input ':' expecting <EOF>(line 1, pos 4)\\n\\n== SQL ==\\ndate::text in (2010-07-04) and app_id in (377194688,364709193,379174209,339739007,284882215) and device_code in ('ios-phone','ios-tablet','ios-all') and country_code in ('WW','US')\\n----^^^\\n\""]},{"cell_type":"code","execution_count":0,"id":"20200506-035700_1092750872","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n# start = \"2020-01-01\"\n# end = \"2020-04-19\"\nstart = '2010-07-04'\nend = '2010-07-04'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n\n\n# test_ios_app_list = [377194688 , 375875657, 366236510, 354990881,365691077 , 343200656 ,20600000009072,20600000000553]\ntest_ios_app_list = ['377194688', '364709193', '379174209',  '339739007',         '284882215']\ndevice_code=['ios-phone','ios-tablet','ios-all','android-all']\ncountry_list=['WW', 'US']\n# test_list = [(app, device, country ) for app in test_ios_app_list  for device in device_code for country in country_list ]\n\n\nsql_where = \"app_id in ({}) and device_code in ('{}') and country_code in ('{}')\".format(\",\".join(map(str,test_ios_app_list)),   \"','\".join(device_code), \"','\".join(country_list) )\nprint sql_where\n\n\n_unified_data = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=20{10,11,12,13,14,15,16,17,18,19}-*\").where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\")\n\nresult=list()\nfor day in dates:\n    _unified_data_single_date = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % (day.strftime(\"%Y-%m-%d\"))).where(sql_where).select(\"app_id\", \"device_code\", \"country_code\",\"free_app_download\",\"paid_app_download\",\"revenue\").collect()\n    # print _unified_data\n    result.extend(_unified_data_single_date)\n\nschema = StructType([StructField(\"app_id\", StringType(), True), \n                     StructField(\"device_code\", StringType(), True),\n                     StructField(\"country_code\", StringType(), True),\n                     StructField(\"free_app_download\", IntegerType(), True),\n                     StructField(\"paid_app_download\", IntegerType(), True),\n                     StructField(\"revenue\", IntegerType(), True)])\n\n_unified_data.createOrReplaceTempView(\"unified_data_view\")\nspark.createDataFrame(result, schema=schema).createOrReplaceTempView(\"unified_data_singe_date_view\")\nspark.sql(\"select * from unified_data_view union select * from unified_data_singe_date_view\").createOrReplaceTempView(\"union_unified_data\")\nspark.sql(\"select app_id, sum(free_app_download) from union_unified_data group by app_id, device_code, country_code\").show(2)\n\n\n\ncdf1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(dates[-1])).where(sql_where).cache()\ncdf1.createOrReplaceTempView(\"cumu\")\n\nprint \"compare sum data: \", dates[-1]\n\n# df1 = spark.sql(\"select app_id, sum(free_app_download) as free_app_download, device_code, country_code from unified_data_view  group by app_id , device_code, country_code EXCEPT ALL select app_id , device_code, country_code, free_app_download from cumu  \").show()\n# spark.sql(\"select app_id, free_app_download, device_code, country_code from cumu EXCEPT ALL select app_id , device_code, country_code , sum(free_app_download) as free_app_download  from unified_data_view  group by app_id , device_code, country_code   \").show()\n\nspark.sql(\"select app_id, sum(free_app_download) as free_app_download, sum(paid_app_download) as paid_app_download, sum(revenue) as revenue, device_code, country_code from union_unified_data  group by app_id , device_code, country_code order by app_id , device_code, country_code\").createOrReplaceTempView(\"df1\")\nspark.sql(\"select app_id, free_app_download, paid_app_download, revenue, device_code, country_code from cumu order by app_id , device_code, country_code\").createOrReplaceTempView(\"df2\")\nspark.sql(\"select * from df1 except all select * from df2\").show()\nspark.sql(\"select * from df2 except all select * from df1\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200506-065857_1894109472","metadata":{},"outputs":[],"source":["\nfrom collections import deque\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nimport pandas as pd\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \ncum_dates = get_date_list('2010-07-04', '2010-07-31', freq='D') + \\\n            get_date_list('2010-08-31', '2020-03-31', freq='M') + \\\n            get_date_list('2020-04-01', '2020-04-18', freq='D')     # prod\n# cum_dates = get_date_list('2010-07-04', '2010-07-31', freq='D') + \\\n#             get_date_list('2010-08-31', '2011-09-30', freq='M')       # test\n# cum_dates = get_date_list('2010-07-04', '2010-07-06', freq='D')\nstore_dates = get_date_list('2010-07-04', '2020-04-18', freq='D')   # prod\n# store_dates = get_date_list('2010-07-04', '2011-09-30', freq='D')     # test\n# store_dates = get_date_list('2010-07-04', '2010-07-06', freq='D')\n\nprint cum_dates\nprint store_dates\n\n\ntemp_df = pd.DataFrame({'app_id': []})\nfor cum_date in cum_dates:\n    concat_list = [pd.DataFrame({'app_id': []})]\n    store_dates_range = range(len(store_dates))\n    # print store_dates\n    for i in store_dates_range:\n        unified_data = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(store_dates[i])).select('app_id').distinct().toPandas()\n        concat_list.append(unified_data)\n\n        if store_dates[i] == cum_date:\n            store_dates = store_dates[i+1:]\n            break\n    # print \"temp_df\", temp_df\n    concat_list.append(temp_df)\n    temp_df = pd.concat(concat_list).drop_duplicates()\n\n    # fetch data from cumulative\n    cum_df = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(cum_date)).select('app_id').distinct().toPandas()\n\n    # compare\n    joined_df = temp_df.merge(cum_df, on=['app_id'], indicator=True)\n    if not joined_df.loc[joined_df['_merge'] != 'both'].empty:\n        print cum_date\n"]},{"cell_type":"code","execution_count":0,"id":"20200509-083545_1476996149","metadata":{},"outputs":[],"source":["\na = ['2010-07-04', '2010-07-05', '2010-07-06', '2010-07-07', '2010-07-08', '2010-07-09', '2010-07-10', '2010-07-11', '2010-07-12', '2010-07-13', '2010-07-14', '2010-07-15', '2010-07-16', '2010-07-17', '2010-07-18', '2010-07-19', '2010-07-20', '2010-07-21', '2010-07-22', '2010-07-23', '2010-07-24', '2010-07-25', '2010-07-26', '2010-07-27', '2010-07-28', '2010-07-29', '2010-07-30', '2010-07-31', '2010-08-01', '2010-08-02', '2010-08-03', '2010-08-04', '2010-08-05', '2010-08-06', '2010-08-07', '2010-08-08', '2010-08-09', '2010-08-10', '2010-08-11', '2010-08-12', '2010-08-13', '2010-08-14', '2010-08-15', '2010-08-16', '2010-08-17', '2010-08-18', '2010-08-19', '2010-08-20', '2010-08-21', '2010-08-22', '2010-08-23', '2010-08-24', '2010-08-25', '2010-08-26', '2010-08-27', '2010-08-28', '2010-08-29', '2010-08-30', '2010-08-31', '2010-09-01', '2010-09-02', '2010-09-03', '2010-09-04', '2010-09-05', '2010-09-06', '2010-09-07', '2010-09-08', '2010-09-09', '2010-09-10', '2010-09-11', '2010-09-12', '2010-09-13', '2010-09-14', '2010-09-15', '2010-09-16', '2010-09-17', '2010-09-18', '2010-09-19', '2010-09-20', '2010-09-21', '2010-09-22', '2010-09-23', '2010-09-24', '2010-09-25', '2010-09-26', '2010-09-27', '2010-09-28', '2010-09-29', '2010-09-30', '2010-10-01', '2010-10-02', '2010-10-03', '2010-10-04', '2010-10-05', '2010-10-06', '2010-10-07', '2010-10-08', '2010-10-09', '2010-10-10', '2010-10-11', '2010-10-12', '2010-10-13', '2010-10-14', '2010-10-15', '2010-10-16', '2010-10-17', '2010-10-18', '2010-10-19', '2010-10-20', '2010-10-21', '2010-10-22', '2010-10-23', '2010-10-24', '2010-10-25', '2010-10-26', '2010-10-27', '2010-10-28', '2010-10-29', '2010-10-30', '2010-10-31', '2010-11-01', '2010-11-02', '2010-11-03', '2010-11-04', '2010-11-05', '2010-11-06', '2010-11-07', '2010-11-08', '2010-11-09', '2010-11-10', '2010-11-11', '2010-11-12', '2010-11-13', '2010-11-14', '2010-11-15', '2010-11-16', '2010-11-17', '2010-11-18', '2010-11-19', '2010-11-20', '2010-11-21', '2010-11-22', '2010-11-23', '2010-11-24', '2010-11-25', '2010-11-26', '2010-11-27', '2010-11-28', '2010-11-29', '2010-11-30', '2010-12-01', '2010-12-02', '2010-12-03', '2010-12-04', '2010-12-05', '2010-12-06', '2010-12-07', '2010-12-08', '2010-12-09', '2010-12-10', '2010-12-11', '2010-12-12', '2010-12-13', '2010-12-14', '2010-12-15', '2010-12-16', '2010-12-17', '2010-12-18', '2010-12-19', '2010-12-20', '2010-12-21', '2010-12-22', '2010-12-23', '2010-12-24', '2010-12-25', '2010-12-26', '2010-12-27', '2010-12-28', '2010-12-29', '2010-12-30', '2010-12-31', '2011-01-01', '2011-01-02', '2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07', '2011-01-08', '2011-01-09', '2011-01-10', '2011-01-11', '2011-01-12', '2011-01-13', '2011-01-14', '2011-01-15', '2011-01-16', '2011-01-17', '2011-01-18', '2011-01-19', '2011-01-20', '2011-01-21', '2011-01-22', '2011-01-23', '2011-01-24', '2011-01-25', '2011-01-26', '2011-01-27', '2011-01-28', '2011-01-29', '2011-01-30', '2011-01-31', '2011-02-01', '2011-02-02', '2011-02-03', '2011-02-04', '2011-02-05', '2011-02-06', '2011-02-07', '2011-02-08', '2011-02-09', '2011-02-10', '2011-02-11', '2011-02-12', '2011-02-13', '2011-02-14', '2011-02-15', '2011-02-16', '2011-02-17', '2011-02-18', '2011-02-19', '2011-02-20', '2011-02-21', '2011-02-22', '2011-02-23', '2011-02-24', '2011-02-25', '2011-02-26', '2011-02-27', '2011-02-28', '2011-03-01', '2011-03-02', '2011-03-03', '2011-03-04', '2011-03-05', '2011-03-06', '2011-03-07', '2011-03-08', '2011-03-09', '2011-03-10', '2011-03-11', '2011-03-12', '2011-03-13', '2011-03-14', '2011-03-15', '2011-03-16', '2011-03-17', '2011-03-18', '2011-03-19', '2011-03-20', '2011-03-21', '2011-03-22', '2011-03-23', '2011-03-24', '2011-03-25', '2011-03-26', '2011-03-27', '2011-03-28', '2011-03-29', '2011-03-30', '2011-03-31', '2011-04-01', '2011-04-02', '2011-04-03', '2011-04-04', '2011-04-05', '2011-04-06', '2011-04-07', '2011-04-08', '2011-04-09', '2011-04-10', '2011-04-11', '2011-04-12', '2011-04-13', '2011-04-14', '2011-04-15', '2011-04-16', '2011-04-17', '2011-04-18', '2011-04-19', '2011-04-20', '2011-04-21', '2011-04-22', '2011-04-23', '2011-04-24', '2011-04-25', '2011-04-26', '2011-04-27', '2011-04-28', '2011-04-29', '2011-04-30', '2011-05-01', '2011-05-02', '2011-05-03', '2011-05-04', '2011-05-05', '2011-05-06', '2011-05-07', '2011-05-08', '2011-05-09', '2011-05-10', '2011-05-11', '2011-05-12', '2011-05-13', '2011-05-14', '2011-05-15', '2011-05-16', '2011-05-17', '2011-05-18', '2011-05-19', '2011-05-20', '2011-05-21', '2011-05-22', '2011-05-23', '2011-05-24', '2011-05-25', '2011-05-26', '2011-05-27', '2011-05-28', '2011-05-29', '2011-05-30', '2011-05-31', '2011-06-01', '2011-06-02', '2011-06-03', '2011-06-04', '2011-06-05', '2011-06-06', '2011-06-07', '2011-06-08', '2011-06-09', '2011-06-10', '2011-06-11', '2011-06-12', '2011-06-13', '2011-06-14', '2011-06-15', '2011-06-16', '2011-06-17', '2011-06-18', '2011-06-19', '2011-06-20', '2011-06-21', '2011-06-22', '2011-06-23', '2011-06-24', '2011-06-25', '2011-06-26', '2011-06-27', '2011-06-28', '2011-06-29', '2011-06-30', '2011-07-01', '2011-07-02', '2011-07-03', '2011-07-04', '2011-07-05', '2011-07-06', '2011-07-07', '2011-07-08', '2011-07-09', '2011-07-10', '2011-07-11', '2011-07-12', '2011-07-13', '2011-07-14', '2011-07-15', '2011-07-16', '2011-07-17', '2011-07-18', '2011-07-19', '2011-07-20', '2011-07-21', '2011-07-22', '2011-07-23', '2011-07-24', '2011-07-25', '2011-07-26', '2011-07-27', '2011-07-28', '2011-07-29', '2011-07-30', '2011-07-31', '2011-08-01', '2011-08-02', '2011-08-03', '2011-08-04', '2011-08-05', '2011-08-06', '2011-08-07', '2011-08-08', '2011-08-09', '2011-08-10', '2011-08-11', '2011-08-12', '2011-08-13', '2011-08-14', '2011-08-15', '2011-08-16', '2011-08-17', '2011-08-18', '2011-08-19', '2011-08-20', '2011-08-21', '2011-08-22', '2011-08-23', '2011-08-24', '2011-08-25', '2011-08-26', '2011-08-27', '2011-08-28', '2011-08-29', '2011-08-30', '2011-08-31', '2011-09-01', '2011-09-02', '2011-09-03', '2011-09-04', '2011-09-05', '2011-09-06', '2011-09-07', '2011-09-08', '2011-09-09', '2011-09-10', '2011-09-11', '2011-09-12', '2011-09-13', '2011-09-14', '2011-09-15', '2011-09-16', '2011-09-17', '2011-09-18', '2011-09-19', '2011-09-20', '2011-09-21', '2011-09-22', '2011-09-23', '2011-09-24', '2011-09-25', '2011-09-26', '2011-09-27', '2011-09-28', '2011-09-29', '2011-09-30']\nlen(a)"]},{"cell_type":"code","execution_count":0,"id":"20200509-034930_1346268889","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nimport pandas as pd\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \ndates = get_date_list('2010-07-04', '2010-07-05', freq='D')    # >> ['2010-07-04', '2010-07-05', '2010-07-06']\n# dates = dates + get_date_list('2010-08-01', '2020-04-18', freq='D')\n\ntemp_set = {}\n\n# concat_list = []\nfor date in dates:\n    unified_data = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).select('app_id').distinct().collect()\n\n    temp_set = pd.concat([unified_data, temp_df]).drop_duplicates()\n\n    # fetch data from cumulative\n    cum_df = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(date)).select('app_id').distinct().collect()\n\n    # compare\n    joined_df = temp_df.merge(cum_df, on=['app_id'], indicator=True)\n    print joined_df.loc[joined_df['_merge'] != 'both'].empty\n    # print temp_df\n    # print cum_df"]},{"cell_type":"code","execution_count":0,"id":"20200509-035055_622320960","metadata":{},"outputs":[],"source":["\nunified_data = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format('2020-01-01')).select('app_id').distinct().collect()\nprint unified_data[:10]"]},{"cell_type":"code","execution_count":0,"id":"20200509-034023_965788466","metadata":{},"outputs":[],"source":["\n# unified_data = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).select('app_id').distinct().toPandas()\ntemp_df = pd.DataFrame({'app_id': []})\ndf = pd.concat([unified_data, temp_df]).drop_duplicates()\nprint temp_df\nprint df"]},{"cell_type":"code","execution_count":0,"id":"20200509-025932_1222236085","metadata":{},"outputs":[],"source":["\n\nimport numpy as np\njoined_df = temp_df.merge(cum_df, on=['app_id'], indicator=True)\njoined_df['_merge'] = np.nan\njoined_df['_merge'].isnull()"]},{"cell_type":"code","execution_count":0,"id":"20200509-030338_1851954411","metadata":{},"outputs":[],"source":["\ndef temp_log_to_s3(log, name, file_format):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'zidong/aa.store.cum/{}.{}'.format(name, file_format))\n    if file_format == 'json':\n        s3object.put(Body=json.dumps(log))\n        return json.dumps(log)\n    else:\n        s3object.put(Body=str(log))    \n        return log\n        \ndef s3_to_temp_log(name, file_format):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'zidong/aa.store.cum/{}.{}'.format(name, file_format))\n    body = s3object.get()['Body'].read()\n    print body\n    if file_format == 'json':\n        body = json.loads(body)\n        return body\n    return body\n\ndf1 = pd.DataFrame({'angles': [1, 2, 3]})\nprint df1\n\ndf2 = pd.DataFrame({'angles': [2, 3, 6]})\nprint df2\n\ndf3 = pd.concat([df1, df2]).drop_duplicates()\n\ndf3.reset_index(drop=True, inplace=True)\nprint df3\n\ncsv = df3.to_csv(index=False)\ntemp_log_to_s3(csv, 'test_csv', 'csv')\nbody = s3_to_temp_log('test_csv', 'csv')\n\ndf = pd.read_csv(body)\nprint df"]},{"cell_type":"code","execution_count":0,"id":"20200511-071250_855948044","metadata":{},"outputs":[],"source":["\nimport sys\nfrom StringIO import StringIO\n\ndf = pd.read_csv(StringIO(body), sep=\",\")\nprint df"]},{"cell_type":"code","execution_count":0,"id":"20200509-061140_129430076","metadata":{},"outputs":[],"source":["\nfrom collections import deque\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nimport pandas as pd\n\ndef temp_log_to_s3(log, name, file_format):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'zidong/aa.store.cum/{}.{}'.format(name, file_format))\n    if file_format == 'json':\n        s3object.put(Body=json.dumps(log))\n        return json.dumps(log)\n    else:\n        s3object.put(Body=str(log))    \n        return log\n\ndef s3_to_temp_log(name, file_format):\n    import boto3\n    import json\n    s3 = boto3.resource('s3')\n    s3object = s3.Object('b2c-prod-data-pipeline-qa', 'zidong/aa.store.cum/{}.{}'.format(name, file_format))\n    body = s3object.get()['Body'].read()\n\n    if file_format == 'json':\n        body = json.loads(body)\n        return body\n    return body\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \ncum_dates = get_date_list('2010-07-04', '2010-07-31', freq='D') + \\\n            get_date_list('2010-08-31', '2020-03-31', freq='M') + \\\n            get_date_list('2020-04-01', '2020-04-18', freq='D')     # prod\n# cum_dates = get_date_list('2010-07-04', '2010-07-31', freq='D') + \\\n#             get_date_list('2010-08-31', '2011-09-30', freq='M')       # test monthly\n# cum_dates = get_date_list('2010-07-04', '2010-07-04', freq='D')\nstore_dates = get_date_list('2010-07-04', '2020-04-18', freq='D')   # prod\n# store_dates = get_date_list('2010-07-04', '2011-09-30', freq='D')     # test monthly\n# store_dates = get_date_list('2010-07-04', '2010-07-04', freq='D')\n\nprint cum_dates\nprint store_dates\n\n\ntemp_df = pd.DataFrame({'app_id': []})\nfor cum_date in cum_dates:\n    concat_list = [pd.DataFrame({'app_id': []})]\n    store_dates_range = range(len(store_dates))\n    # print store_dates\n    for i in store_dates_range:\n        unified_data = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(store_dates[i])).select('app_id').distinct().toPandas()\n\n        temp_df = pd.concat([temp_df, unified_data]).drop_duplicates()\n        # temp_df.reset_index(drop=True, inplace=True)\n        \n        if store_dates[i] == cum_date:\n            store_dates = store_dates[i+1:]\n            break\n\n    # fetch data from cumulative\n    cum_df = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(cum_date)).select('app_id').distinct().toPandas()\n\n    # compare\n    joined_df = temp_df.merge(cum_df, on=['app_id'], indicator=True)\n    if not joined_df.loc[joined_df['_merge'] != 'both'].empty:\n        print \"failed data is\", cum_date\n        break\n\ntemp_df.reset_index(drop=True, inplace=True)\ncsv = temp_df.to_csv(index=False)\ntemp_log_to_s3(csv, 'cum_test_csv', 'csv')\n\n# read from csv file and convert to pandas dataframe\nbody = s3_to_temp_log('cum_test_csv', 'csv')\nimport sys\nfrom StringIO import StringIO\n# saved_pandas_csv_df = pd.read_csv(StringIO(body), sep=\",\")\n# print saved_pandas_csv_df\n\n# # save\ntemp_df['app_id'] = temp_df['app_id'].astype(str)\ntemp_df['app_id'] = [i[:-2] for i in temp_df['app_id']]\nspark.createDataFrame(temp_df).write.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store.app-est-cum.v1/store_cum_completeness_result/\", mode=\"overwrite\")\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store.app-est-cum.v1/store_cum_completeness_result/\").show(100)\n\nprint \"END\""]},{"cell_type":"code","execution_count":0,"id":"20200514-024822_141092920","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/aa.store.app-est-cum.v1/store_cum_completeness_result/"]},{"cell_type":"code","execution_count":0,"id":"20200514-023955_1801226326","metadata":{},"outputs":[],"source":["\nprint temp_df\nspark.createDataFrame(temp_df).write.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_completeness_result/\", mode=\"overwrite\")\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/store_cum_completeness_result/\")\ndf.show(100)"]},{"cell_type":"code","execution_count":0,"id":"20200514-024053_68703462","metadata":{},"outputs":[],"source":["\nprint temp_df\ntemp_df['app_id'] = [i[:-2] for i in temp_df['app_id']]\nprint temp_df"]},{"cell_type":"code","execution_count":0,"id":"20200511-063554_155715892","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/\n\naws s3 cp s3://b2c-prod-data-pipeline-qa/zidong/aa.store.cum/cum_test_csv.csv -"]},{"cell_type":"code","execution_count":0,"id":"20200511-072222_1762211741","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-free/unified/\n"]},{"cell_type":"code","execution_count":0,"id":"20200511-073021_1667573661","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-free/unified/store.app.v1/dimension/\").show(10, True, True)"]},{"cell_type":"code","execution_count":0,"id":"20200511-111940_201542996","metadata":{},"outputs":[],"source":["\nimport pandas as pd\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\n# cum_dates = get_date_list('2010-07-04', '2010-07-31', freq='D') + \\\n#             get_date_list('2010-08-31', '2020-03-31', freq='M') + \\\n#             get_date_list('2020-04-01', '2020-04-18', freq='D')     # prod\n# cum_dates = get_date_list('2010-07-04', '2010-07-31', freq='D') + \\\n            # get_date_list('2010-08-31', '2011-09-30', freq='M')       # test monthly\ncum_dates = get_date_list('2010-07-04', '2010-07-04', freq='D')\n# store_dates = get_date_list('2010-07-04', '2020-04-18', freq='D')   # prod\n# store_dates = get_date_list('2010-07-04', '2011-09-30', freq='D')     # test monthly\nstore_dates = get_date_list('2010-07-04', '2010-07-04', freq='D')\n\nbase_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/\"\n\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nschema = StructType([StructField(\"app_id\", StringType(), True)])\nprev_df = spark.createDataFrame([], schema=schema)\n\nfor cum_date in cum_dates:\n    paths = []\n    # date_range = range(len(store_dates))\n    print store_dates\n    for i in range(len(store_dates)):\n        print i\n        path = base_path + \"granularity=daily/date=\" + store_dates[i] + \"/\"\n        paths.append(path)\n        \n        if store_dates[i] == cum_date:\n            store_dates = store_dates[i+1 : ]\n            break\n    print paths\n\n    next_df = spark.read.option('basePath', base_path).parquet(*paths).select('app_id').distinct()\n    prev_df = next_df.union(prev_df).distinct()\n    \n    # compare\n    cum_df = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/\").where(\"date='{}'\".format(cum_date)).select('app_id').distinct()\n    except_df = prev_df.subtract(cum_df)\n    if except_df.first():\n        print \"failed date:\", cum_date\nprint \"SUCCESSED\"\nprev_df.write.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store.app-est-cum.v1/store_cum_completeness_result/\", mode=\"overwrite\")\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200514-075531_1399156354","metadata":{},"outputs":[],"source":["\nprev_df.write.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store.app-est-cum.v1/store_cum_completeness_result/\", mode=\"overwrite\")\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store.app-est-cum.v1/store_cum_completeness_result/\").show(100)"]},{"cell_type":"code","execution_count":0,"id":"20200511-120602_1282434599","metadata":{},"outputs":[],"source":["\nbase_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/\"\npath_list = [\n    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2010-07-04/\",\n    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2010-07-05/\"\n    ]\ndf = spark.read.option('basePath', base_path).parquet(*path_list).select('app_id')\n\ndf.show(10)\n"]},{"cell_type":"code","execution_count":0,"id":"20200511-114314_527397907","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2010-07-05/\").show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200511-115052_467353508","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/"]},{"cell_type":"code","execution_count":0,"id":"20200514-085256_1413468424","metadata":{},"outputs":[],"source":["%%sh\n"]},{"cell_type":"code","execution_count":0,"id":"20200512-074922_2073362032","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='TMV!PYT02X*w' psql -h 54.210.244.2 -p 5433 -U app_tomcat -d aa_android << EOF\n\\d app;\nSELECT id, name, company FROM app LIMIT 1;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200511-120313_37519524","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='TMV!PYT02X*w' psql -h 54.210.244.2 -p 5432 -U app_tomcat -d aa << EOF\n\\d aa_app;\nSELECT id, name, company FROM aa_app LIMIT 1;\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200512-081817_741593543","metadata":{},"outputs":[],"source":["\nunified_data = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2010-07*/\")\nuni_df = unified_data.select('app_id').distinct()\nuni_df.show(10)"]},{"cell_type":"code","execution_count":0,"id":"20200514-062732_1956943730","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}