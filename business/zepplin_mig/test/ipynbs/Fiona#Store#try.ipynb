{"cells":[{"cell_type":"code","execution_count":0,"id":"20200602-082219_1834531089","metadata":{},"outputs":[],"source":["\ndf1 = spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-05-10\").cache()\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-05-10\").cache()\n\ndf1.filter(\"est=10290 \").show()\ndf2.filter(\"app_id=1426252715 and free_app_download=10290\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200512-132401_990029724","metadata":{},"outputs":[],"source":["\nnums = range(0, 10)\nprint(nums)\nrdd = sc.parallelize(nums, 10)\nrdd.map(lambda (x, y) : (x, x*2) ).collect()\nprint(\"Default parallelism: {}\".format(sc.defaultParallelism))\nprint(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(rdd.partitioner))\nprint(\"Partitions structure: {}\".format(rdd.glom().collect()))\n"]},{"cell_type":"code","execution_count":0,"id":"20200512-134529_621402614","metadata":{},"outputs":[],"source":["\ntest_data = [(x, y*y) for x in xrange(1,1000) for y in xrange(1,100)]\n# print test_data"]},{"cell_type":"code","execution_count":0,"id":"20200512-132359_968876976","metadata":{},"outputs":[],"source":["\nrdd = sc.parallelize(test_data, 3)\nresult1 = rdd.reduceByKey(lambda a, b: a + b).collect()\nresult2 = rdd.groupByKey().collect()\n# print result"]},{"cell_type":"code","execution_count":0,"id":"20200512-141218_1026386377","metadata":{},"outputs":[],"source":["\nprint result1[5]\nprint result2[5]"]},{"cell_type":"code","execution_count":0,"id":"20200510-072554_402198610","metadata":{},"outputs":[],"source":["\n\nprint dir(df2)"]},{"cell_type":"code","execution_count":0,"id":"20200510-070814_2061251360","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import broadcast\n\ndf1.unpersist()\ndf2.unpersist()\ndf1 = spark.read.option(\"basePath\",\n                      \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-{01,03}/device_code=ios-tablet\").cache()\n# df1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-01/device_code=ios-tablet\").cache()\ndf2 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2020-01-01/device_code=ios-tablet/\").cache()\n\ndf1.join(broadcast(df2), df1.app_id == df2.app_id).count()"]},{"cell_type":"code","execution_count":0,"id":"20200510-070820_2122359454","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-01/device_code=ios-tablet/ --human\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2020-01-01/device_code=ios-tablet/  --human\n \n"]},{"cell_type":"code","execution_count":0,"id":"20200412-134558_493552932","metadata":{},"outputs":[],"source":["\n# sc.parallelize([(\"Alice\",\"10\",\"135215\"),(\"Lisa\",\"12\",\"12456\")])\nlines = sc.parallelize([\"hello world\", \"hi\"])\nwords = lines.flatMap(lambda line: line.split(\" \")) \nprint words\nprint words.first()\n# sc.parallelize\ndf1 = spark.createDataFrame([(\"12\",\"12456\",\"Lisa\"),(\"10\",\"135215\",\"Alice\")], schema=[\"age\",\"tel\",\"name\"])\ndf2 = spark.createDataFrame([(\"Alice\",\"10\",\"135215\"),(\"Lisa\",\"12\",\"12456\"),(\"Lisa\",\"12\",\"12456\")], schema=[\"name\",\"age\",\"tel\"])\n\ndf1.printSchema()\ndf1.createOrReplaceTempView(\"view1\")\ndf2.createOrReplaceTempView(\"view2\")\n\nspark.sql(\"select * from view1 except select * from view2\").show()\nspark.sql(\"select * from view2 except select * from view1\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200412-134708_1540192088","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-qa/aa.store/result_store_est_unified_db_count/"]},{"cell_type":"code","execution_count":0,"id":"20200414-115554_104755391","metadata":{},"outputs":[],"source":["%%sh\naws s3 rm s3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_monthly_category_count/ --recursive"]},{"cell_type":"code","execution_count":0,"id":"20200420-150047_1917006650","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nstart = \"2018-02-11\"\nend = \"2018-02-18\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\n\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append(\n            (\n                [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n                [i.strftime(\"%Y-%m-%d\") for i in item], \n                [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n            )\n        )\nprint test_path[0][2]\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\nsql_text =\"\"\"\n\n\"\"\"\nnamespace = \"aa.store.market-size.v1\"\ningest_msg = {\n    \"namespace\": \"aa.store.market-size.v1\",\n    \"job_type\": \"routine\",\n    \"options\":{},\n    \"source\": [\n        {\n            \"path\": [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2018-02-15\"]\n        }\n    ]\n}\nrun(spark, ingest_msg, sql_text)\nspark.sql(\"select distinct date from raw_data where country_code='US'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200426-043754_1978984893","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}