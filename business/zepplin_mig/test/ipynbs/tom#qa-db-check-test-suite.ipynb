{"cells":[{"cell_type":"code","execution_count":0,"id":"20191125-070857_1009936315","metadata":{},"outputs":[],"source":["\n\n#!/usr/bin/env python\n# coding=utf-8\n# Copyright (c) 2017 App Annie Inc. All rights reserved.\n\"\"\"\n\nImplement test cases here\n\"\"\"\n\nimport os\nimport shutil\nimport unittest\n\nfrom aaintdatapipeline.core.conf.settings import ROOT\nfrom aaintdatapipeline.core.fs.device import meta_bucket, raw_bucket, unified_bucket\nfrom aaintdatapipeline.core.fs.device.bucket import unified_data_system_config_bucket\nfrom aaintdatapipeline.core.utils.commandline import env\nfrom aaintdatapipeline.core.utils.encode import activate_system_utf8\nfrom aaintdatapipeline.core.utils.spark import create_spark, eject_all_caches\n\n\nclass PySparkTest(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"Setup a basic Spark context for testing\"\"\"\n        activate_system_utf8()\n        env(PYTHONIOENCODING='utf8')\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext\n        cls._release_resource()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls._release_resource()\n        cls.sc = None\n        # comment below code for performance\n        # cls.spark.stop()\n\n    @classmethod\n    def _release_resource(cls):\n        # clear spark cached and persisted files\n        eject_all_caches(cls.spark)\n        # clear physical files\n        cls._empty_bucket(meta_bucket())\n        cls._empty_bucket(raw_bucket())\n        cls._empty_bucket(unified_bucket())\n        cls._empty_bucket(unified_data_system_config_bucket())\n        cls._empty_spark_db_dir(\"{}/metastore_db\".format(ROOT))\n        cls._empty_spark_db_dir(\"{}/spark-warehouse\".format(ROOT))\n\n    @classmethod\n    def _empty_bucket(cls, bucket):\n        try:\n            if os.environ.get('EMPTY_BUCKET', \"true\").lower() != \"false\":\n                bucket.empty()\n        except OSError:\n            pass\n\n    @classmethod\n    def _empty_spark_db_dir(cls, db_dir):\n        try:\n            shutil.rmtree(db_dir, ignore_errors=False)\n        except OSError:\n            pass\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-060657_323725826","metadata":{},"outputs":[],"source":["\nimport unittest\nimport sys\nimport time\nimport datetime\n\nclass TestDemo(PySparkTest):\n    checkdate = \"2019-10-11\"\n    granularity = \"weekly\"\n    \n    def test_fail1(self):\n        print \"fail1\"\n        assert False==True\n        \n    def test_fail2(self):\n        print \"fail2\"\n        assert False==True\n\n    def test_fail3(self):\n        print \"fail3\"\n        assert False==True\n        \n    def test_pass(self):\n        print \"pass\"\n        assert True==True\n        \n    def test_pass_log(self):\n        print \"pass log\"\n        assert True==True\n        \n    def test_fail_log(self):\n        print \"tom is working\"\n        print \"fail\"\n        assert False==True\n        \n    def test_spark_log(self):\n        spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/process_date=2019-10-14/country=AU/language=en/part-00000-fab743d5-e8ff-4f23-86ff-92bacb3fdccd-c000.snappy.parquet\").show()\n        assert False==True\n        \n    def test_check_date_with_doc_string(self):\n        \"\"\"DEMO CHECK DATE{}\"\"\"\n        utcnow = datetime.datetime.utcnow() if not self.trigger_date else self.trigger_date\n        print utcnow\n        assert False==True\n\n    def test_set_doc_string(self):\n        \"\"\"Mobile Web Led\"\"\"\n        assert False==True\n        \n    def shortDescription(self):\n        \"\"\"Returns a one-line description of the test, or None if no\n        description has been provided.\n\n        The default implementation of this method returns the first line of\n        the specified test method's docstring.\n        \"\"\"\n        _doc = self._testMethodDoc\n        if not _doc:\n            _doc = self.__doc__ or \"\"\n        doc = _doc.split(\"\\n\")[0].strip()\n        checkdate = self.checkdate\n        granularity = self.granularity\n        return \"{} - {} - {}\".format(doc, granularity, checkdate )\n\n        \n"]},{"cell_type":"code","execution_count":0,"id":"20191125-091718_1660573210","metadata":{},"outputs":[],"source":["\n\n\ndef debug(case):\n    std_out_origin= sys.stdout\n    try:\n        suite =  unittest.TestSuite()\n        suite.addTest(case)\n        runner = unittest.TextTestRunner(verbosity=2, buffer=True)\n        runner.run(suite)\n    finally:\n        sys.stdout = std_out_origin\n    \n\ntestcase = TestDemo(\"test_set_doc_string\")\ndebug(testcase)\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-072430_1253587396","metadata":{},"outputs":[],"source":["\n\nstd_out_origin_tom= sys.stdout\n\nlog_file = \"/tmp/test.log\"\nwith open(log_file, \"w+\") as file:\n    suite =  unittest.TestSuite()\n    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestDemo))\n    runner = unittest.TextTestRunner(file, verbosity=2, buffer=True)\n    runner.run(suite)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-065534_910844728","metadata":{},"outputs":[],"source":["%%sh\ncat /tmp/test.log"]},{"cell_type":"code","execution_count":0,"id":"20191125-074028_1033983566","metadata":{},"outputs":[],"source":["\n\nsys.stdout = std_out_origin\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-061459_1940185297","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"qa-data-db-check-debug\")\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-071611_741321918","metadata":{},"outputs":[],"source":["%%sh\n\necho $PYTHONPATH\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-075552_1574650124","metadata":{},"outputs":[],"source":["%%sh\nls -al /home/hadoop/bdp/application/aaintdatapipeline/\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-075608_802267674","metadata":{},"outputs":[],"source":["\n\nimport os\n\nprint os. environ\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-075744_318665666","metadata":{},"outputs":[],"source":["\n\n# Copyright (c) 2018 App Annie Inc. All rights reserved.\n\n\"\"\"\nDB Check modules\n\"\"\"\nimport unittest\n\nimport datetime\nimport croniter\nfrom sqlalchemy.dialects.postgresql import psycopg2\n\nfrom aaintdatapipeline.application.app_qa.common.db_check_utils import query\nfrom aaintdatapipeline.application.app_qa.conf.settings import CITUS_MKT_NAME\nfrom aaintdatapipeline.application.app_qa.conf.settings_local_prod import CITUS_AA_CITUS_DB_NAME, \\\n    CITUS_AA_CITUS_DB_ACCESS_ID, CITUS_AA_CITUS_DB_HOSTS, CITUS_AA_CITUS_DB_SECRET_KEY\nfrom aaintdatapipeline.application.app_qa.db_check_v1.pyspark_test import PySparkTest\nfrom aaintdatapipeline.core.conf import Conf\nfrom aaintdatapipeline.core.fs.device import S3Bucket, specified_bucket\nimport zlib\nimport pandas as pd\n\n\n# UTILS\ndef rank_bucket(bucket_str):\n    \"\"\"\n    Get Bucket Object from bucket string\n    :param bucket_str: string like: s3://xxx.xxx\n    :type bucket_str: str\n    :return: bucket_object\n    :rtype: bucket_object\n    \"\"\"\n    conf = Conf(\n        bucket_name=bucket_str,\n        bucket_class=S3Bucket\n    )\n    return specified_bucket(conf)\n\n\nclass AppStoreRankRawData():\n    bucket_name = \"\"\n    bucket_path = \"\"\n    data_split_str = \"\"\n    rank_list_split_str = \"\"\n    rank_split_str = \"\"\n    accept_feeds = []\n    country_code_mapping = {}\n    category_id_mapping = {}\n\n    def get_raw_data_by(self, date):\n        \"\"\"\n        :return: raw_data_frame\n        :rtype: list_of_dic\n        raw_data:\n        _________________________________________________________________________________\n        |    date    |   country_id   |  category_id  |   feed_id   |   rank (app_id)   |\n        |------------|----------------|---------------|-------------|-------------------|\n        | 2019-04-27 | 143441(bigint) |   6016 (int)  |   0 (int)   | 376510438(bigint) |\n        ---------------------------------------------------------------------------------\n        unified_data:\n        _____________________________________________________________________________________\n        |  country_code  |   category_id   |       app_id       | feed_name (free_download) |\n        |----------------|-----------------|--------------------|---------------------------|\n        |      'US'      | 100026 (bigint) | 376510438 (bigint) |    25 (int) (app_rank)    |\n        -------------------------------------------------------------------------------------\n        \"\"\"\n        path = \"{_bucket_path}/{_date}/23/\".format(_date=date, _bucket_path=self.bucket_path)\n        bucket = rank_bucket(self.bucket_name)\n        columns = ['date', 'country_code', 'category_id', 'metrics', 'app_rank_list']\n\n        _raw_data_list = []\n        df_list = []\n        for key in bucket.list(path):\n            _raw_data = zlib.decompress(bucket.get(key))\n            for _line in _raw_data.splitlines():\n                line_data = _line.split(self.data_split_str)\n                app_rank_list = line_data.split(self.rank_list_split_str)\n                if self.rank_split_str:\n                    app_rank_list = [rank.split(self.rank_split_str)[1] for rank in app_rank_list]\n                _raw_data_list.append(app_rank_list)\n            _df = pd.DataFrame(_raw_data_list, columns=columns, )\n            df_list.append(_df)\n        return pd.concat(df_list)\n\n\nclass IPhoneRaw(AppStoreRankRawData):\n    bucket_name = \"prod_appannie_ios\"\n    bucket_path = \"country-ranks\"\n    data_split_str = \"\\t\"\n    rank_split_str = \" \"\n    accept_feeds = [0, 1, 2]\n\n\nclass IPadRaw(AppStoreRankRawData):\n    bucket_name = \"prod_appannie_ios\"\n    bucket_path = \"country-ranks\"\n    data_split_str = \"\\t\"\n    rank_split_str = \" \"\n    accept_feeds = [101, 100, 102]\n\n\nclass MacRaw(AppStoreRankRawData):\n    bucket_name = \"prod_appannie_ios\"\n    bucket_path = \"mac/country-ranks\"\n    data_split_str = \"\\t\"\n    rank_split_str = \" \"\n    accept_feeds = [200, 201, 202]\n\n\nclass AppleTvRaw(AppStoreRankRawData):\n    bucket_name = \"prod_appannie_appletv\"\n    bucket_path = \"country-ranks\"\n    data_split_str = \",\"\n    rank_split_str = \" \"\n    accept_feeds = [0, 1, 2]\n\n\nclass AmazonRaw(AppStoreRankRawData):\n    bucket_name = \"prod_appannie_appletv\"\n    bucket_path = \"country-ranks\"\n    data_split_str = \",\"\n    rank_split_str = \" \"\n    accept_feeds = [0, 1, 2]\n\n\nclass GooglePlayRaw(AppStoreRankRawData):\n    bucket_name = \"prod_appannie_appletv\"\n    bucket_path = \"country-ranks\"\n    data_split_str = \",\"\n    rank_split_str = \" \"\n    accept_feeds = [0, 1, 2]\n\n\ndef get_app_rank_cases(params):\n    project_name = params['project_name']\n    if project_name == 'All_Products':\n        check_date = datetime.date.today() - datetime.timedelta(days=2)\n\n\n# CONSTANTS\nAPP_STORE_RANK_METRICS = [\"free_download\", \"new_paid_download\", \"revenue\", \"paid_download\", \"new_free_download\"]\ncitus_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=CITUS_AA_CITUS_DB_NAME,\n        user=CITUS_AA_CITUS_DB_ACCESS_ID,\n        host=CITUS_AA_CITUS_DB_HOSTS[0][0],\n        password=CITUS_AA_CITUS_DB_SECRET_KEY,\n        port=CITUS_AA_CITUS_DB_HOSTS[0][1]\n    )\n)\n\n\nclass AppleTvUnified():\n    bucket_name = \"prod_appannie_appletv\"\n    bucket_path = \"country-ranks\"\n    data_split_str = \",\"\n    rank_split_str = \" \"\n    accept_feeds = [0, 1, 2]\n    s3_path = \"s3://b2c-prod-data-pipeline-unified-store-free/unified/app-tech.store.app-rank.v1/fact/\"\n\n    def get(self, date, device_code, country_code, category_id):\n        df = spark.read.parquet(\"date={}/device_code={}/\".format(date, device_code))\n        return df.filter(\"country_code='{country_code}' and category_id='{category_id}'\".format(\n            country_code=country_code, category_id=category_id)).toPandas()\n\n\nclass AppleTvDB():\n    schema = \"prod_appannie_appletv\"\n    table = \"country-ranks\"\n    device_code = 'tv-os-tv'\n\n    def get(self, date, device_code, country_code, category_id):\n        sql = \"SELECT * from {schema}.{table} where date ='' and \"\n        result = query(citus_dsn, sql)\n        return pd.DataFrame(result)\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                df = pd.DataFrame(cur.fetchall())\n                df.columns = cur.keys()\n                conn.commit()\n        return df\n\n\n# CASES\n\nclass AppStoreRankDailyTest(PySparkTest):\n    check_date = ''\n    trigger_date = ''\n    routing_config = ('* 9 * * *', 1)\n\n    @classmethod\n    def setUpClass(cls):\n        super(PySparkTest, cls).setUpClass()\n\n    def setUp(self):\n        super(PySparkTest, self).setUp()\n        self.trigger_date = datetime.datetime.utcnow()\n        self.check_date = self._get_date_from_refresh_routing_config(self.routing_config, self.trigger_date)\n\n    @unittest.skip\n    def test_etl_process(self):\n        tv_raw_df = AppleTvRaw().get_raw_data_by(self.check_date)\n        tv_unified_df = AppleTvUnified().get(self.check_date, \"tv-os-tv\", \"US\", 300000)\n        tv_db_df = AppleTvDB().get(self.check_date, \"tv-os-tv\", \"US\", 300000)\n\n    @unittest.skip\n    def test_completeness_country(self):\n        pass\n\n    @unittest.skip\n    def test_completeness_category(self):\n        pass\n\n    @unittest.skip\n    def test_completeness_rank(self):\n        pass\n\n    @unittest.skip\n    def test_mode_rank(self):\n        pass\n\n    def _get_date_from_refresh_routing_config(self, config, trigger_date=None):\n        \"\"\"\n        return the date of : <days_delta> ago from previous scheduled date&time of <cron_time>.\n        e.g.\n        config = (\"0 9 * * *\", 1), today is 2019-10-27 8:00\n        so previous scheduled date&time is 2019-10-26 9:00\n        will return \"2019-10-25\"\n\n        :param config: config format: (<cron_time>, <days_delta>)\n        :type config: tuple\n        the cron_time please refer to https://support.acquia.com/hc/en-us/articles/360004224494-Cron-time-string-format\n        the days_delta is the days ago from the expected date.\n        :return: date string of \"%Y-%m-%d\"\n        :type return: str\n        \"\"\"\n        schedule, days_delta = config\n        # here use UTC now\n        cron = croniter.croniter(schedule, trigger_date or datetime.datetime.utcnow())\n        date = cron.get_prev(datetime.datetime) - datetime.timedelta(days=days_delta)\n        return date.strftime(\"%Y-%m-%d\")\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191125-080058_376327378","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}