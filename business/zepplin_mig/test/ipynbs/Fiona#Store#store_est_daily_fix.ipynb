{"cells":[{"cell_type":"code","execution_count":0,"id":"20200522-063504_2004061765","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=MONTH/date=2020-03-31/\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v3/fact/granularity=monthly/\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v3/fact/granularity=monthly/ --recursive --summarize --human | tail -5\n"]},{"cell_type":"code","execution_count":0,"id":"20200602-084217_636021660","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\"\"\"\nget date:  [ [month, [days]], [month, [days]], [month, [days]], ....... ]\n\"\"\"\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\n# start = \"2010-07-31\"  # prod\nstart = \"2019-08-31\"    # test\nend = \"2019-08-31\"\n\nmonthly = get_date_list(start, end, freq='M')\nprint monthly\n\n\ndate_list = []\nfor m in monthly:\n    d = get_date_list(m[:8]+'01', m, freq='D')  # start = the first day of each month; end = each month\n    month_and_day = [m, d]\n    date_list.append(month_and_day)\n\nprint date_list\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n# CSV schema\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\ndef test_monthkly_data(test_data):\n    print test_data[0]\n    print test_data[1]\n    \n    month_indicator = test_data[0]\n    if month_indicator == '2010-07-31':\n        \n        # only csv, but date range is '2010-07-04' to '2010-07-31'\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n\n    elif month_indicator > '2010-08-01' and month_indicator < '2019-07-01':\n\n        # only csv\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n\n    elif month_indicator == '2019-07-31':\n        # half is csv, half is parquet\n        # First half of 2019-07\n        temp_date_range = get_date_list('2019-07-01', '2019-07-14')\n        first_half_month_df = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n\n        # Second half of 2019-07\n        temp_date_range = get_date_list('2019-07-15', '2019-07-31')\n        second_half_month = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(temp_date_range)).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = first_half_month_df.union(second_half_month)\n        \n\n    else:  # month_indicator >= '2019-08-31'\n\n        # only parquet\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(test_data[1])) .cache()\n        df_1.show(10)\n\n    # df_1.createOrReplaceTempView(\"daily_data\")\n    \n    # weekly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v3/fact/\").where(\"granularity='monthly' and date='{}' and data_stage='final'\".format(test_data[0])).cache()\n    # weekly_df_ho.createOrReplaceTempView(\"unified_monthly\")\n    \n    eject_all_caches(spark)\n\n\nsc.parallelize(map(test_monthkly_data, date_list), 1)\n\n# diff_df1 = spark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_monthly \")\n# diff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_monthly  except all select * from country_category_mapping_raw\")\n# if diff_df1.take(1) or diff_df2.take(1):\n#     print \"FAILED: \", sar_list[0]\n#     diff_df1.show(100)\n#     diff_df2.show(100)\nprint \"END\""]},{"cell_type":"code","execution_count":0,"id":"20200522-032726_48414521","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nstart = \"2017-04-01\"\nend = \"2020-03-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n# CSV schema\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\ndef test_monthkly_data(test_data):\n    print test_data\n    month_indicator = test_data\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2019-07-14':\n        df_ios = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%(test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_android = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/android/sbe_est_app/*/\"%(test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = df_ios.union(df_android)\n        \n\n    # ### 3. half is csv, half is parquet ###\n    # elif month_indicator == '2019-07-31':\n    #     # First half of 2019-07\n    #     temp_date_range = get_date_list('2019-07-01', '2019-07-14')\n    #     first_half_month_df = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n    #     # Second half of 2019-07\n    #     temp_date_range = get_date_list('2019-07-15', '2019-07-31')\n    #     second_half_month = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" % (temp_date_range)).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n    #     df_1 = first_half_month_df.union(second_half_month)\n        \n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" % (test_data)).cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    \n    daily_est_load = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_est_load.createOrReplaceTempView(\"daily_unified_est\")\n\n\n\n    sql_text = \"\"\"\n    \n    WITH filter_top_N_raw_data AS(\n    SELECT\n     distinct\n      id,\n      Sum(est) AS est,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      platform_id,\n      vertical,\n      feed,\n      platform);\n      \n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n      \n      \n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n        );\n        \n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n    );\n    \n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n    \n    \n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n    -- map raw with country_code\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue \n     from country_code_mapping \n     inner join \n         pivot_metric_raw \n     on \n         country_code_mapping.store_id=pivot_metric_raw.store_id \n     and \n         country_code_mapping.market_code=pivot_metric_raw.platform\n    where country_name!='Global'\n    );\n\n\n      \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"ios_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n        },   \n        {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"android_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n        }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    \n    \n    diff_df1 = spark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue from daily_unified_est \")\n    diff_df2 = spark.sql(\"select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue from daily_unified_est  except all select * from country_category_mapping_raw\")\n\n    diff_df1.show()\n    diff_df2.show() \n\n    \n    eject_all_caches(spark)\n\n\nsc.parallelize(map(test_monthkly_data, dates), 1)\n    \n"]},{"cell_type":"code","execution_count":0,"id":"20200609-103536_1272925380","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-03-31/ --recursive | sort -n\necho '*****'\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-03-31/ --recursive  | sort -n\n"]},{"cell_type":"code","execution_count":0,"id":"20200609-081006_490008642","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\nstart = \"2020-05-02\"\nend = \"2020-05-03\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n# CSV schema\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\ndef test_monthkly_data(test_data):\n    print test_data\n    month_indicator = test_data\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2019-07-01':\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 3. half is csv, half is parquet ###\n    elif month_indicator == '2019-07-31':\n        # First half of 2019-07\n        temp_date_range = get_date_list('2019-07-01', '2019-07-14')\n        first_half_month_df = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        # Second half of 2019-07\n        temp_date_range = get_date_list('2019-07-15', '2019-07-31')\n        second_half_month = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(temp_date_range)).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = first_half_month_df.union(second_half_month)\n        \n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" % (test_data)).cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    \n    daily_est_load = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_est_load.createOrReplaceTempView(\"daily_unified_est\")\n\n\n    daily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_download_attr.createOrReplaceTempView(\"daily_unified_attr_est\")\n\n    daily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(\"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_download_attr.createOrReplaceTempView(\"daily_pre_load_data\")\n\n\n    sql_text = \"\"\"\n    \n    WITH filter_top_N_raw_data AS(\n    SELECT\n     distinct\n      id,\n      Sum(est) AS est,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      platform_id,\n      vertical,\n      feed,\n      platform);\n      \n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n      \n      \n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n        );\n        \n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n    );\n    \n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n    \n    \n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n    -- map raw with country_code\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue \n     from country_code_mapping \n     inner join \n         pivot_metric_raw \n     on \n         country_code_mapping.store_id=pivot_metric_raw.store_id \n     and \n         country_code_mapping.market_code=pivot_metric_raw.platform\n    where country_name!='Global'\n    );\n\n    WITH download_unified_attr AS(\n    SELECT app_id, coalesce(free_app_download, 0 ) as free_app_download, coalesce(paid_app_download, 0 ) as paid_app_download, coalesce(revenue, 0 ) as revenue, device_code, country_code from country_category_mapping_raw\n\n    );\n    \n    \n    WITH daily_unified_attr_est_join_est_table AS(\n    SELECT a.app_id, a.free_app_download, a.paid_app_download, a.revenue, a.country_code, a.device_code, b.organic_download_share\n    FROM download_unified_attr a\n    JOIN daily_unified_attr_est b\n    ON a.app_id=b.app_id\n    AND a.country_code=b.country_code\n    AND a.device_code=b.device_code\n    \n    );\n    \n    \n    WITH caculate_data AS (\n        SELECT CAST(ROUND (organic_download_share * (free_app_download+paid_app_download)) AS int) AS est_organic_download, \n        CAST(free_app_download+paid_app_download - ROUND(organic_download_share * (free_app_download+paid_app_download)) AS int) as est_paid_download,\n        app_id, \n        free_app_download AS est_free_app_download, \n        paid_app_download AS est_paid_app_download, \n        revenue AS est_revenue, \n        device_code, \n        country_code\n        FROM daily_unified_attr_est_join_est_table\n    );\n\n\n    WITH daily_pre_load_data_coalesce AS(\n    SELECT app_id, coalesce(est_paid_download, 0 ) as est_paid_download, coalesce(est_organic_download, 0 ) as est_organic_download, coalesce(est_free_app_download, 0 ) as est_free_app_download, coalesce(est_paid_app_download, 0 ) as est_paid_app_download, coalesce(est_revenue, 0 ) as est_revenue, device_code, country_code from daily_pre_load_data\n\n    );\n    \n\n      \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"ios_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n        },   \n        {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"android_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n        }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    # eject_all_caches(spark)\n\n\nsc.parallelize(map(test_monthkly_data, dates), 1)\n\n# spark.sql(\"select * from caculate_data\").show()\n  \ndiff_df1 = spark.sql(\"select app_id, est_free_app_download, est_paid_app_download, est_organic_download, est_paid_download, est_revenue, device_code, country_code from caculate_data except all select app_id, est_free_app_download, est_paid_app_download, est_organic_download, est_paid_download, est_revenue, device_code, country_code from daily_pre_load_data_coalesce \")\ndiff_df2 = spark.sql(\"select app_id, est_free_app_download, est_paid_app_download, est_organic_download, est_paid_download, est_revenue, device_code, country_code from daily_pre_load_data_coalesce except all select app_id, est_free_app_download, est_paid_app_download, est_organic_download, est_paid_download, est_revenue, device_code, country_code from caculate_data \")\n\ndiff_df1.show()\ndiff_df2.show() \n"]},{"cell_type":"code","execution_count":0,"id":"20200522-053436_651512660","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\nstart = \"2019-07-14\"\nend = \"2020-05-31\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\ndates.sort(reverse=True)\n\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n# CSV schema\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\ndef test_monthly_category_data(test_data):\n    print test_data\n\n    month_indicator = test_data\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2019-07-01':\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 3. half is csv, half is parquet ###\n    elif month_indicator == '2019-07-31':\n        # First half of 2019-07\n        temp_date_range = get_date_list('2019-07-01', '2019-07-14')\n        first_half_month_df = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        # Second half of 2019-07\n        temp_date_range = get_date_list('2019-07-15', '2019-07-31')\n        second_half_month = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(temp_date_range)).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = first_half_month_df.union(second_half_month)\n        \n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  (test_data)) .cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    \n    daily_category_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_category_unified.createOrReplaceTempView(\"daily_unified_category\")\n\n\n    \n    \n    # /date=2020-03-28\n    sql_text = \"\"\"\n     WITH filter_top_N_raw_data AS(\n    SELECT\n  *\nFROM\n  (\n    SELECT\n      id,\n      Sum(est) AS est,\n      category_id,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform,\n          d2.category_id\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n          AND d1.platform = d2.platform\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      category_id,\n      platform_id,\n      platform,\n      vertical,\n      feed\n  ) \n     );\n\n      \n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n      \n      \n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform, category_id from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform, category_id\n        );\n        \n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform, category_id from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform, category_id\n    );\n    \n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download, category_id as category_id_pivot\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n    \n    -- map raw with category\n    WITH category_mapping_raw AS (\n\n    SELECT * from \n        ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \n    UNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n     ) as mapping \n    FULL OUTER JOIN pivot_metric_raw \n    ON \n     mapping.legacy_category_id=pivot_metric_raw.category_id_pivot \n    AND \n     mapping.mapping_platform=pivot_metric_raw.platform\n    );\n    \n    \n    \n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue, category_id \n     from country_code_mapping \n     inner join \n         category_mapping_raw \n     on \n         country_code_mapping.store_id=category_mapping_raw.store_id \n     and \n         country_code_mapping.market_code=category_mapping_raw.platform\n    where country_name!='Global'\n    );\n\n\n\n\n    \n      \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            # {\n            #     \"data_encoding\": \"parquet\",\n            #     \"compression\": \"gzip\",\n            #     \"name\": \"store_unified_weekly_data\",\n            #     \"path\": [\n            #         \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/granularity=weekly/date=2020-03-28/\"],\n            #     # \"path\": est_list,\n\n            # }, \n            {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"ios_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n           'header': True,\n           'sep': '\\t',\n           'quote': '',\n           'encoding': 'utf-8',\n           'escape': ''\n           },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n        },   \n        {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"android_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n           'header': True,\n           'sep': '\\t',\n           'quote': '',\n           'encoding': 'utf-8',\n           'escape': ''\n           },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n        },\n        {\n            \"data_encoding\": \"parquet\",\n            \"compression\": \"gzip\",\n            \"name\":\"category_mapping_deminsion_service\",\n            \"path\": [\"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\"],\n        }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n\n    spark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue, category_id from daily_unified_category \").show()\n    spark.sql(\"select app_id, country_code, device_code, est_free_app_download as free_app_download, est_paid_app_download as paid_app_download, est_revenue as revenue, category_id from daily_unified_category  except all select * from country_category_mapping_raw\").show()\n    eject_all_caches(spark)\n\n    \n\n    \nsc.parallelize(map(test_monthly_category_data, dates), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200609-092603_177172270","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\nstart = \"2020-05-02\"\nend = \"2020-05-03\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\n\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n# CSV schema\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\ndef test_monthly_category_data(test_data):\n    print test_data[0]\n    print test_data[1]\n    \n    month_indicator = test_data\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2019-07-01':\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 3. half is csv, half is parquet ###\n    elif month_indicator == '2019-07-31':\n        # First half of 2019-07\n        temp_date_range = get_date_list('2019-07-01', '2019-07-14')\n        first_half_month_df = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        # Second half of 2019-07\n        temp_date_range = get_date_list('2019-07-15', '2019-07-31')\n        second_half_month = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(temp_date_range)).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = first_half_month_df.union(second_half_month)\n        \n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  (test_data)) .cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    \n    daily_category_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_category_unified.createOrReplaceTempView(\"daily_unified_category\")\n\n    daily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_download_attr.createOrReplaceTempView(\"daily_unified_attr_est\")\n\n    daily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-category-load.v3/fact/\").where(\"granularity='daily' and date='{}'\".format(test_data)).cache()\n    daily_download_attr.createOrReplaceTempView(\"daily_pre_load_data\")\n\n    \n    \n    # /date=2020-03-28\n    sql_text = \"\"\"\n     WITH filter_top_N_raw_data AS(\n    SELECT\n  *\nFROM\n  (\n    SELECT\n      id,\n      Sum(est) AS est,\n      category_id,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform,\n          d2.category_id\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n          AND d1.platform = d2.platform\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      category_id,\n      platform_id,\n      platform,\n      vertical,\n      feed\n  ) \n     );\n\n      \n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n      \n      \n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform, category_id from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform, category_id\n        );\n        \n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform, category_id from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform, category_id\n    );\n    \n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download, category_id as category_id_pivot\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n    \n    -- map raw with category\n    WITH category_mapping_raw AS (\n\n    SELECT * from \n        ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \n    UNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n     ) as mapping \n    FULL OUTER JOIN pivot_metric_raw \n    ON \n     mapping.legacy_category_id=pivot_metric_raw.category_id_pivot \n    AND \n     mapping.mapping_platform=pivot_metric_raw.platform\n    );\n    \n    \n    \n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue, category_id \n     from country_code_mapping \n     inner join \n         category_mapping_raw \n     on \n         country_code_mapping.store_id=category_mapping_raw.store_id \n     and \n         country_code_mapping.market_code=category_mapping_raw.platform\n    where country_name!='Global'\n    );\n\n\n\n\n\n    WITH download_unified_attr AS(\n    SELECT app_id, category_id, coalesce(free_app_download, 0 ) as free_app_download, coalesce(paid_app_download, 0 ) as paid_app_download, coalesce(revenue, 0 ) as revenue, device_code, country_code from country_category_mapping_raw\n\n    );\n    \n    \n    WITH daily_unified_attr_est_join_est_table AS(\n    SELECT a.app_id, a.free_app_download, a.paid_app_download, a.revenue, a.country_code, a.device_code, a.category_id, b.organic_download_share\n    FROM download_unified_attr a\n    JOIN daily_unified_attr_est b\n    ON a.app_id=b.app_id\n    AND a.country_code=b.country_code\n    AND a.device_code=b.device_code\n    \n    );\n    \n    \n    WITH caculate_data AS (\n        SELECT CAST(ROUND (organic_download_share * (free_app_download+paid_app_download)) AS int) AS est_organic_download, \n        CAST(free_app_download+paid_app_download - ROUND(organic_download_share * (free_app_download+paid_app_download)) AS int) as est_paid_download,\n        app_id, \n        category_id,\n        free_app_download AS est_free_app_download, \n        paid_app_download AS est_paid_app_download, \n        revenue AS est_revenue, \n        device_code, \n        country_code\n        FROM daily_unified_attr_est_join_est_table\n    );\n\n\n    WITH daily_pre_load_data_coalesce AS(\n    SELECT app_id, category_id, coalesce(est_paid_download, 0 ) as est_paid_download, coalesce(est_organic_download, 0 ) as est_organic_download, coalesce(est_free_app_download, 0 ) as est_free_app_download, coalesce(est_paid_app_download, 0 ) as est_paid_app_download, coalesce(est_revenue, 0 ) as est_revenue, device_code, country_code from daily_pre_load_data\n\n    );\n\n\n    \n      \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            # {\n            #     \"data_encoding\": \"parquet\",\n            #     \"compression\": \"gzip\",\n            #     \"name\": \"store_unified_weekly_data\",\n            #     \"path\": [\n            #         \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/granularity=weekly/date=2020-03-28/\"],\n            #     # \"path\": est_list,\n\n            # }, \n            {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"ios_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n           'header': True,\n           'sep': '\\t',\n           'quote': '',\n           'encoding': 'utf-8',\n           'escape': ''\n           },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n        },   \n        {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"android_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n           'header': True,\n           'sep': '\\t',\n           'quote': '',\n           'encoding': 'utf-8',\n           'escape': ''\n           },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n        },\n        {\n            \"data_encoding\": \"parquet\",\n            \"compression\": \"gzip\",\n            \"name\":\"category_mapping_deminsion_service\",\n            \"path\": [\"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\"],\n        }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n\n    diff_df1 = spark.sql(\"select app_id, category_id, est_free_app_download, est_paid_app_download, est_organic_download, est_paid_download, est_revenue, device_code, country_code from caculate_data except all select app_id, category_id, est_free_app_download, est_paid_app_download, est_organic_download, est_paid_download, est_revenue, device_code, country_code from daily_pre_load_data_coalesce \")\n    diff_df2 = spark.sql(\"select app_id, category_id, est_free_app_download, est_paid_app_download, est_organic_download, est_paid_download, est_revenue, device_code, country_code from daily_pre_load_data_coalesce except all select app_id, category_id, est_free_app_download, est_paid_app_download, est_organic_download, est_paid_download, est_revenue, device_code, country_code from caculate_data \")\n\n    diff_df1.show()\n    diff_df2.show() \n\n    \n\n    \nsc.parallelize(map(test_monthly_category_data, dates), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200609-094421_433674603","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from daily_pre_load_data_coalesce where country_code='US' and app_id=341456761\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200515-073654_1007526994","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\nstart = \"2014-12-28\"\nend = \"2017-01-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append((real_date1 + datetime.timedelta(days), temp))\n\n# test_path=list()\n\n\n        \n\nprint sar_list[0][0]\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\n\n\n\ndef test_weekly_data(test_data):\n    print test_data\n    \n    month_indicator = str(test_data[0]) # DT.strptime(test_data[0], '%Y-%m-%d')\n\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2013-01-01':\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n\n\n    ### 2. only csv\n    elif month_indicator > '2013-01-01' and month_indicator < '2019-07-01':\n        df_ios = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_android = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/android/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = df_ios.union(df_android)\n\n    ### 3. half is csv, half is parquet ###\n    elif month_indicator == '2019-07-31':\n        # First half of 2019-07\n        temp_date_range = get_date_list('2019-07-01', '2019-07-14')\n        first_half_month_df = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        # Second half of 2019-07\n        temp_date_range = get_date_list('2019-07-15', '2019-07-31')\n        second_half_month = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(temp_date_range)).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = first_half_month_df.union(second_half_month)\n        \n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(test_data[1])) .cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    \n    print test_data[0]\n    weekly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v3/fact/\").where(\"granularity='weekly' and date='{}' and data_stage='final'\".format(test_data[0])).cache()\n    weekly_df_ho.createOrReplaceTempView(\"unified_weekly\")\n\n\n    sql_text = \"\"\"\n    \n    WITH filter_top_N_raw_data AS(\n    SELECT\n     distinct\n      id,\n      Sum(est) AS est,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      platform_id,\n      vertical,\n      feed,\n      platform);\n      \n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n      \n      \n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n        );\n        \n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n    );\n    \n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n    \n    \n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n    -- map raw with country_code\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue \n     from country_code_mapping \n     inner join \n         pivot_metric_raw \n     on \n         country_code_mapping.store_id=pivot_metric_raw.store_id \n     and \n         country_code_mapping.market_code=pivot_metric_raw.platform\n    where country_name!='Global'\n    );\n\n\n      \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"ios_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n           'header': True,\n           'sep': '\\t',\n           'quote': '',\n           'encoding': 'utf-8',\n           'escape': ''\n           },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n        },   \n        {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"android_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n           'header': True,\n           'sep': '\\t',\n           'quote': '',\n           'encoding': 'utf-8',\n           'escape': ''\n           },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n        }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    \n    spark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_weekly \").show()\n    spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_weekly  except all select * from country_category_mapping_raw\").show()\n    \n    eject_all_caches(spark)\n\n    \n\n    \nsc.parallelize(map(test_weekly_data, sar_list), 1)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200605-130631_1431724899","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from country_category_mapping_raw where app_id=1079292817 and country_code='NG'  \").show()\nspark.sql(\"select * from unified_weekly where app_id=1079292817 and country_code='NG'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200605-135403_863191913","metadata":{},"outputs":[],"source":["\n\n# spark.read.option(\"basePath\",\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2018-05-{06,07,08,09,10,11,12}\").where(\"app_id=1079292817 and country_code='NG'\").show()\n\nspark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2018-05-10/ios/sbe_est_app/143561/\").csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2018-05-10/ios/sbe_est_app/143561/\",sep=\"\\t\").where(\"_c5=1079292817\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200518-055320_1885749019","metadata":{},"outputs":[],"source":["\n# weekly_df_ha = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v3/fact/\").where(\"granularity='weekly' and date='2020-03-28' and data_stage='final'\").cache()\n# weekly_df_ha.createOrReplaceTempView(\"unified_weekly\")\n\n# weekly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='weekly' and date='2020-03-28' and data_stage='final'\").cache()\n# weekly_df_ho.createOrReplaceTempView(\"unified_category_weekly\")\n\nspark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_weekly \").show()\nspark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_weekly  except all select * from country_category_mapping_raw\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200518-122509_1617206110","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom datetime import datetime as DT\n\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\nstart = \"2017-09-17\"\nend = \"2019-07-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append((real_date1 + datetime.timedelta(days), temp))\n\n# test_path=list()\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n        \n\n\"\"\"\nget date:  [ [month, [days]], [month, [days]], [month, [days]], ....... ]\n\"\"\"\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\n# start = \"2010-07-31\"  # prod\nstart = \"2010-08-31\"    # test\nend = start\n\nmonthly = get_date_list(start, end, freq='M')\nprint monthly\n\n\ndate_list = []\nfor m in monthly:\n    d = get_date_list(m[:8]+'01', m, freq='D')  # start = the first day of each month; end = each month\n    month_and_day = [m, d]\n    date_list.append(month_and_day)\n\nprint date_list\n\n\nprint sar_list[0][1]\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n\n\ndef test_weekly_data(test_data):\n    print test_data[1]\n    \n    month_indicator = str(test_data[0]) # DT.strptime(test_data[0], '%Y-%m-%d')\n\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2013-01-01':\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n\n\n    ### 2. only csv\n    elif month_indicator > '2013-01-01' and month_indicator < '2019-07-01':\n        df_ios = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_android = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/android/sbe_est_app/*/\"%\",\".join(test_data[1]), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = df_ios.union(df_android)\n\n    ### 3. half is csv, half is parquet ###\n    elif month_indicator == '2019-07-31':\n        # First half of 2019-07\n        temp_date_range = get_date_list('2019-07-01', '2019-07-14')\n        first_half_month_df = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        # Second half of 2019-07\n        temp_date_range = get_date_list('2019-07-15', '2019-07-31')\n        second_half_month = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(temp_date_range)).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = first_half_month_df.union(second_half_month)\n        \n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(test_data[1])) .cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    \n    weekly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='weekly' and date='{}' and data_stage='final'\".format(test_data[0])).cache()\n    weekly_df_ho.createOrReplaceTempView(\"unified_category_weekly\")\n\n    \n    \n    # /date=2020-03-28\n    sql_text = \"\"\"\n     WITH filter_top_N_raw_data AS(\n    SELECT\n  *\nFROM\n  (\n    SELECT\n      id,\n      Sum(est) AS est,\n      category_id,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform,\n          d2.category_id\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n          AND d1.platform = d2.platform\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      category_id,\n      platform_id,\n      platform,\n      vertical,\n      feed\n  ) \n     );\n\n      \n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n      \n      \n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform, category_id from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform, category_id\n        );\n        \n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform, category_id from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform, category_id\n    );\n    \n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download, category_id as category_id_pivot\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n    \n    -- map raw with category\n    WITH category_mapping_raw AS (\n\n    SELECT * from \n        ( select *, 'ios' as mapping_platform from category_mapping_deminsion_service where market_code='apple-store' \n    UNION ALL select *, 'android' as mapping_platform from category_mapping_deminsion_service where market_code='google-play'\n     ) as mapping \n    FULL OUTER JOIN pivot_metric_raw \n    ON \n     mapping.legacy_category_id=pivot_metric_raw.category_id_pivot \n    AND \n     mapping.mapping_platform=pivot_metric_raw.platform\n    );\n    \n    \n    \n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue, category_id \n     from country_code_mapping \n     inner join \n         category_mapping_raw \n     on \n         country_code_mapping.store_id=category_mapping_raw.store_id \n     and \n         country_code_mapping.market_code=category_mapping_raw.platform\n    where country_name!='Global'\n    );\n\n\n\n\n    \n      \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            # {\n            #     \"data_encoding\": \"parquet\",\n            #     \"compression\": \"gzip\",\n            #     \"name\": \"store_unified_weekly_data\",\n            #     \"path\": [\n            #         \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/granularity=weekly/date=2020-03-28/\"],\n            #     # \"path\": est_list,\n\n            # }, \n            {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"ios_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n           'header': True,\n           'sep': '\\t',\n           'quote': '',\n           'encoding': 'utf-8',\n           'escape': ''\n           },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n        },   \n        {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"android_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n           'header': True,\n           'sep': '\\t',\n           'quote': '',\n           'encoding': 'utf-8',\n           'escape': ''\n           },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n        },\n        {\n            \"data_encoding\": \"parquet\",\n            \"compression\": \"gzip\",\n            \"name\":\"category_mapping_deminsion_service\",\n            \"path\": [\"s3://b2c-prod-data-pipeline-qa/aa.store/store_cateogry_mapping\"],\n        }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    \n    # to_country_code = udf(lambda x, y : id_to_country_code(x,y), StringType())\n\n    # df_raw = spark.sql(\"select * from pivot_metric_raw\").select(\"app_id\", to_country_code(\"platform\",\"country_code\").alias(\"country_code\"), \"platform\", \"device_code\", \"free_app_download\",\"revenue\", \"paid_app_download\").cache()\n    # df_raw.createOrReplaceTempView(\"raw_weekly_data\")\n    # spark.sql(\"select * from country_category_mapping_raw where app_id=1464081043 and country_code='AU'\").show()\n    # print spark.sql(\"select * from store_unified_weekly_data\").take(2)\n    # spark.sql(\"select * from compare_data_raw where app_id is not null except all select * from compare_data_unified where app_id is not null\").show()\n    # spark.sql(\"select * from compare_data_unified except all select * from compare_data_raw\").show()\n    # count_1 = spark.sql(\"select count(*) from compare_data_raw where app_id is not null\").take(1)\n    # count_2 = spark.sql(\"select count(*) from compare_data_unified \").take(1)\n    # if count_1[0][0] != count_2[0][0]:\n    #     print 'failed!!!!!!!!!!!!!'\n    spark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue, category_id from unified_category_weekly \").show()\n    spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue, category_id from unified_category_weekly  except all select * from country_category_mapping_raw\").show()\n    eject_all_caches(spark)\n\n    \n\n    \nsc.parallelize(map(test_weekly_data, sar_list), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200519-023808_1808091417","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue, category_id from unified_category_weekly \").show()\nspark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue, category_id from unified_category_weekly  except all select * from country_category_mapping_raw\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200519-030539_1904646743","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from country_category_mapping_raw where app_id = 20600001302870 and country_code='WW' and device_code='android-all' \").show() ## 我算出来哒\nspark.sql(\"select * from unified_category_weekly where app_id = 20600001302870 and country_code='WW'  and device_code='android-all'\").show()  ## 你算出来哒  \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/granularity=weekly/date=2020-03-28/\"],\n"]},{"cell_type":"code","execution_count":0,"id":"20200519-033004_1166173652","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from daily_data where id=20600001302870 and store_id=1000 and feed = 2 and rank<=4000 \").show(200)  ## raw data"]},{"cell_type":"code","execution_count":0,"id":"20200519-031432_42238342","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from filter_top_N_raw_data where id=971304016 and store_id=143489\").show(200)"]},{"cell_type":"code","execution_count":0,"id":"20200515-073046_2028476649","metadata":{},"outputs":[],"source":["\n\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\n\nstart_week = \"2020-03-22\"\nend_week = \"2020-03-29\"\n\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(str(real_date1 + datetime.timedelta(days)))\n\ndates_temp=list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates_temp.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates_temp:\n            temp.append(dates_temp.pop())\n        sar_list.append(str(real_date1 + datetime.timedelta(days)))\n\n\n\n\nprint 'test date'\nprint dates\nprint sar_list\n \n\n\ndf_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=ios/*/\" %  \",\".join(dates)).cache()\n\n# df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/MONTH/\").schema(monthly_csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/MONTH/{%s}/ios/sbe_est_app/*/\" % dates[-1], sep=\"\\t\").cache()\n# df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/\").schema(weekly_csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/{%s}/ios/sbe_est_app/*/\" % \",\".join(sar_list), sep=\"\\t\").cache()\n\ndf_1.createOrReplaceTempView(\"daily_data\")\n# df_2.createOrReplaceTempView(\"monthly_data\")\n# df_3.createOrReplaceTempView(\"weekly_data\")\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200517-060741_1895115733","metadata":{},"outputs":[],"source":["\nspark.sql('''\n    SELECT\n      id,\n      Sum(est) AS est,\n      store_id,\n      platform_id,\n      feed,\n      vertical\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      platform_id,\n      vertical,\n      feed\n  ''').create\n  \n  \nspark.sql('''select *, \n    case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='3' and platform='ios' then 'revenue_iap' \n        when feed='4' and platform='ios' then 'revenue_non_iap' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='103' and platform='ios' then 'revenue_iap' \n        when feed='104' and platform='ios' then 'revenue_non_iap' \n        when feed='1000' and platform='ios' then 'free_app_download' \n        when feed='1001' and platform='ios' then 'paid_app_download' \n        when feed='1002' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        when feed='3' and platform='android' then 'revenue_iap' \n        when feed='4' and platform='android' then 'revenue_non_iap' \n        end as metric from raw_map_data_df''')"]},{"cell_type":"code","execution_count":0,"id":"20200517-055535_1894795005","metadata":{},"outputs":[],"source":["\nnew_transfor_df = spark.sql('''\nSELECT\n  *\nFROM\n  (\n    SELECT\n      id,\n      Sum(est) AS est,\n      store_id,\n      platform_id,\n      feed,\n      vertical\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      platform_id,\n      vertical,\n      feed\n  ) PIVOT (\n    max(est) FOR feed IN (\n      0,\n      1,\n      2,\n      101,\n      100,\n      102\n    )\n  )\n''').withColumnRenamed(\"0\", platform_feed_to_metric('ios',0)).withColumnRenamed(\"1\", platform_feed_to_metric('ios',1)).withColumnRenamed(\"2\", platform_feed_to_metric('ios',2)).withColumnRenamed(\"101\", platform_feed_to_metric('ios',101)).withColumnRenamed(\"100\", platform_feed_to_metric('ios',100)).withColumnRenamed(\"102\", platform_feed_to_metric('ios',102)).na.fill(0).cache()\n                                                        \nnew_transfor_df.createOrReplaceTempView(\"transfer_category_est\")\n\nspark.sql(\"select * from transfer_new\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200514-033825_1263651644","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\nstart = \"2020-03-08\"\nend = \"2020-03-15\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\n\n\n        \nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append(\n            (\n                [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n                [i.strftime(\"%Y-%m-%d\") for i in item], \n                [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n            )\n        )\n# print test_path\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n\n\ndef test_weekly_data(test_data):\n    print 'test_date[0]', test_data[0]\n    print 'test_data[1]', test_data[1]\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={%s}/\" % \",\".join(test_data[1]))\n    df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}/\" % \",\".join(test_data[1]))\n    df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v3/fact/granularity=weekly/date=%s/\" % test_data[0])\n\n    df_1.createOrReplaceTempView(\"daily_rank\")\n    df_2.createOrReplaceTempView(\"daily_est\")\n    df_3.createOrReplaceTempView(\"weekly_rank\")\n\n    sql_text = \"\"\"\n    \n    -- rank_unified,store_unified\n    WITH unified_data_test AS \n    ( \n                    SELECT          store_unified.country_code, \n                                    store_unified.device_code, \n                                    store_unified.free_app_download AS est_free_app_download , \n                                    store_unified.paid_app_download AS est_paid_app_download, \n                                    store_unified.revenue           AS est_revenue, \n                                    store_unified.revenue_iap       AS est_revenue_iap, \n                                    store_unified.revenue_non_iap   AS est_revenue_non_iap, \n                                    rank_unified.category_id, \n                                    rank_unified.app_id, \n                                    rank_unified.free_app_download, \n                                    rank_unified.paid_app_download, \n                                    rank_unified.revenue, \n                                    rank_unified.revenue_iap, \n                                    rank_unified.revenue_non_iap, \n                                    rank_unified.granularity, \n                                    rank_unified.date \n                    FROM            rank_unified \n                    FULL OUTER JOIN store_unified \n                    ON              rank_unified.app_id = store_unified.app_id \n                    AND             rank_unified.country_code = store_unified.country_code \n                    AND             rank_unified.device_code = store_unified.device_code \n                    AND             rank_unified.date = store_unified.date );\n\n\n\n    WITH unified_rank_filter_data_free_app_download AS \n    ( \n           SELECT * \n           FROM   unified_data_test \n           WHERE ( ( ( \n                                free_app_download<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                free_app_download<=4000 \n                         AND    country_code==\"WW\" ) ) \n           OR     ( ( \n                                paid_app_download<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                paid_app_download<=4000 \n                         AND    country_code==\"WW\" ) ) \n           OR     ( ( \n                                revenue<=1000 \n                         AND    country_code!=\"WW\" ) \n                  OR     ( \n                                revenue<=4000 \n                         AND    country_code==\"WW\" ) ) )\n           AND    device_code!='ios-all'\n    );\n\n\n\n           WITH unified_category_filter_data_free_app_download AS \n    ( \n           SELECT * ,\n           CASE WHEN (free_app_download > 1000 and country_code !='WW') or (free_app_download > 4000 and country_code =='WW' ) or (free_app_download is null or free_app_download <= 0) Then null else est_free_app_download END as est_free_app_download_category,\n           CASE WHEN (paid_app_download > 1000 and country_code !='WW') or (paid_app_download > 4000 and country_code =='WW' ) or (paid_app_download is null or paid_app_download <= 0) Then null else est_paid_app_download END as est_paid_app_download_category,\n           CASE WHEN (revenue > 1000 and country_code !='WW') or (revenue > 4000 and country_code =='WW') or (revenue is null or revenue <= 0) Then null else est_revenue  END as est_revenue_category\n           FROM   unified_rank_filter_data_free_app_download \n\n\n    );    \n\n    \n\n    \n\n    \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n                # \"path\": est_list,\n\n            }, {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"rank_unified\",\n                \"path\": [\n                    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={}\".format(\n                        test_date)],\n                # \"path\": rank_list,\n\n            }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    \n    # spark.sql(\"select * from download_attribution where product_id=284035177 and country_code='ww'\").show()\n    # spark.sql(\"select * from compare_data_unified where device_code='ios-tablet' \").show()\n    # spark.sql(\"select * from compare_data_raw\").show()\n\n    # spark.sql(\"select * from caculate_data where app_id=20600000009072 and unified_country_code='WW' and unified_device_code='android-all'\").show()\n    spark.sql(\"select * from compare_data_raw where app_id is not null except all select * from compare_data_unified where app_id is not null\").show()\n    spark.sql(\"select * from compare_data_unified except all select * from compare_data_raw\").show()\n    count_1 = spark.sql(\"select count(*) from compare_data_raw where app_id is not null\").take(1)\n    count_2 = spark.sql(\"select count(*) from compare_data_unified \").take(1)\n    if count_1[0][0] != count_2[0][0]:\n        print 'failed!!!!!!!!!!!!!'\n\n    # spark.sql(\"select * from compare_data_unified_add_to_est where diff !=0  \").show()\n    eject_all_caches(spark)\n\n    \n\n    \nsc.parallelize(map(test_download_attribution, test_path), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200514-034304_1114603625","metadata":{},"outputs":[],"source":["\nprint test_path"]},{"cell_type":"code","execution_count":0,"id":"20200513-131533_537294181","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/granularity=weekly/date=2020-05-02/\").createOrReplaceTempView(\"test\")\n# s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/granularity=monthly/date=2020-04-30/\n\nspark.sql(\"select * from test\")"]},{"cell_type":"code","execution_count":0,"id":"20200514-015246_1511556513","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\ndf = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],), ([6, 20, None, 3],)], ['data'])\ndf.show()\ndf.select(F.shuffle(df.data).alias('s')).collect()"]},{"cell_type":"code","execution_count":0,"id":"20200513-011629_1159221477","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\nimport datetime\n\nstart_week = \"2020-04-12\"\nend_week = \"2020-04-19\"\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append((key.strftime(\"%Y-%m-%d\"),[d.strftime(\"%Y-%m-%d\") for d in item]))\n        \n        \n        \ndef check_diff(weekly_data):\n    print 'weekly_data[0]', weekly_data[0]\n    print 'weekly_data[1]', weekly_data[1]\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=weekly/date=%s/\" % weekly_data[0])\n\n    df_1.createOrReplaceTempView(\"daily_rank\")\n    df_2.createOrReplaceTempView(\"daily_est\")\n    df_3.createOrReplaceTempView(\"weekly_rank\")\n\n    spark.sql('''select daily_est.date, daily_est.free_app_download, daily_est.paid_app_download, daily_est.revenue, daily_est.app_id, daily_est.device_code, daily_est.country_code, daily_rank.category_id from daily_rank \n                full outer join daily_est \n                on daily_rank.country_code= daily_est.country_code\n                and daily_rank.device_code=daily_est.device_code\n                and daily_rank.app_id = daily_est.app_id \n                and daily_rank.date = daily_est.date''').createOrReplaceTempView(\"daily_est_rank_join\")\n\n\n    spark.sql(\"select app_id, country_code, device_code, category_id, sum(free_app_download) as free_app_download, sum(paid_app_download) as paid_app_download, sum(revenue) as revenue from daily_est_rank_join group by country_code, device_code, category_id, app_id \").createOrReplaceTempView(\"sum_daily\")\n        \n    spark.sql(''' select *, \n                    ROW_NUMBER() OVER (PARTITION BY device_code, country_code, category_id ORDER BY free_app_download DESC, app_id DESC) free_app_download_rank,\n                    ROW_NUMBER() OVER (PARTITION BY device_code, country_code, category_id ORDER BY paid_app_download DESC, app_id DESC) paid_app_download_rank,\n                    ROW_NUMBER() OVER (PARTITION BY device_code, country_code, category_id ORDER BY revenue DESC, app_id DESC) revenue_rank\n                    from sum_daily \n                    ''').createOrReplaceTempView(\"sum_rank_daily\")\n                        \n    # spark.sql('''select app_id, country_code, free_app_download, device_code, category_id \n    #                 from weekly_rank \n    #                 where country_code='US' \n    #                     and device_code='ios-all' \n    #                     and category_id=100012  \n    #                     and free_app_download is not null \n    #                     order by free_app_download desc''').createOrReplaceTempView(\"weekly_rank_1\")\n\n    # spark.sql(\"select * from sum_rank_daily where country_code='US' and category_id=100000 and device_code='ios-phone' order by free_app_download desc\").show(1000)\n    spark.sql(\"select * from sum_rank_daily where country_code='US' and category_id=100022 and device_code='ios-phone' order by paid_app_download desc\").show(1005)\n    # spark.sql(\"select * from sum_rank_daily where country_code='US' and category_id=100000 and device_code='ios-phone' order by revenue desc\").show(1000)\n\nsc.parallelize(map(check_diff, test_path), 3)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200514-055717_133104599","metadata":{},"outputs":[],"source":["\n1368840 - 1617652\n"]},{"cell_type":"code","execution_count":0,"id":"20200421-104608_646720036","metadata":{},"outputs":[],"source":["\nimport datetime\nstart = \"2010-07-04\"\nend = \"2010-07-31\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\n\nmonth_day=list()\nfor days in xrange(date_range.days):\n    month_day.append(real_date1 + datetime.timedelta(days))\n\n\ntest_list= sorted(list(set([ d.strftime(\"%Y-%m-%d\")[:7] for d in month_day ])))\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200423-102339_386349427","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\nimport datetime\n\nstart_week = \"2010-07-04\"\nend_week = \"2010-07-12\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append((key.strftime(\"%Y-%m-%d\"),[d.strftime(\"%Y-%m-%d\") for d in item]))\n        \n        \n        \ndef check_diff(weekly_data):\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=weekly/date=%s/\" % weekly_data[0])\n\n    df_1.createOrReplaceTempView(\"daily_rank\")\n    df_2.createOrReplaceTempView(\"daily_est\")\n    df_3.createOrReplaceTempView(\"weekly\")\n    \n    print \"compare app id: \" , weekly_data\n    # spark.sql(\"select distinct app_id from monthly except all select distinct app_id from  daily \").show(2)\n    # spark.sql(\"select distinct app_id from daily except all select distinct app_id from  monthly \").show(2)\n    spark.sql('''select daily_est.free_app_download, daily_est.paid_app_download, daily_est.revenue, daily_est.app_id, daily_est.device_code, daily_est.country_code, daily_rank.category_id from daily_rank \n                join daily_est \n                on daily_rank.country_code= daily_est.country_code\n                and daily_rank.device_code=daily_est.device_code\n                and daily_rank.app_id = daily_est.app_id \n                and daily_rank.date = daily_est.date ''').createOrReplaceTempView(\"join_category_daily\")\n                \n                # sum(paid_app_download) as paid_app_download_count , sum(revenue) as revenue_count, \n    spark.sql(\"select app_id, country_code, sum(free_app_download) as free_app_download_count, device_code, category_id from join_category_daily group by country_code,device_code, category_id, app_id \").createOrReplaceTempView(\"sum_daily\")\n\n    spark.sql(\"select country_code, device_code, category_id, count(*) as total_daily_count from sum_daily group by country_code,device_code, category_id \").createOrReplaceTempView(\"count_daily\")\n    spark.sql(\"select app_id, country_code,device_code, category_id, free_app_download_count from sum_daily where free_app_download_count is not null \").createOrReplaceTempView(\"test_sum_daily\")\n    spark.sql('''select * from\n                    (select app_id, weekly.country_code, weekly.device_code, weekly.category_id, free_app_download, total_daily_count from weekly \n                     join count_daily on\n                    weekly.country_code=count_daily.country_code\n                    and weekly.device_code=count_daily.device_code\n                    and weekly.category_id=count_daily.category_id) as prod where free_app_download<=total_daily_count ''').createOrReplaceTempView(\"unified_weekly_data\")\n\n    spark.sql('''select app_id from unified_weekly_data except all select app_id from test_sum_daily ''').show()\n    spark.sql('''select app_id from test_sum_daily except all select app_id from test_sum_daily ''').show()\n\n    # spark.sql(\"select * from weekly\")\n    # spark.sql(\"select app_id, country_code, free_app_download, device_code ,category_id left join sum_daily \")\n    # spark.sql(\"select app_id, country_code, device_code ,category_id from sum_daily where free_app_download is not null\").createOrReplaceTempView(\"daily_free_app\")\n    # total_count = spark.sql(\"select count(*) from daily_free_app\").take(1)\n    # print total_count\n    # spark.sql(\"select app_id, country_code, device_code ,category_id from sum_daily where paid_app_download is not null\").createOrReplaceTempView(\"daily_paid_app\")\n    # spark.sql(\"select app_id, country_code, device_code ,category_id from sum_daily where revenue is not null\").createOrReplaceTempView(\"daily_revenue\")\n\n\n\n    # spark.sql(\"select count(*) from daily_free_app\").take(1)\n\n    # spark.sql(\"select app_id, country_code, device_code ,category_id from daily_free_app except all select app_id, country_code, device_code ,category_id from weekly where free_app_download<= {} \".format(total_count[0][0])).show()\n    # spark.sql(\"select app_id, country_code, device_code ,category_id from weekly where free_app_download<={}  except all select app_id, country_code, device_code ,category_id from daily_free_app \".format(total_count[0][0])).show()\n\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from sum_daily except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from monthly\").show(2)\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from monthly except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from sum_daily\").show(2)\n\n    # except_1 = spark.sql(\"select app_id, country_code,free_app_download,device_code,category_id from order_sum_daily_free_download except all select app_id, country_code,free_app_download,device_code,category_id from weekly\").withColumn(\"date\", F.lit(weekly_data[0]) ).withColumn(\"type\",F.lit(\"daily_weekly\"))\n    # except_2 = spark.sql(\"select app_id, country_code,paid_app_download,device_code,category_id from order_sum_daily_paid_download except all select app_id, country_code,paid_app_download,device_code,category_id from weekly\").withColumn(\"date\", F.lit(weekly_data[0]) ).withColumn(\"type\",F.lit(\"daily_weekly\"))\n    # except_3 = spark.sql(\"select app_id, country_code,revenue,device_code,category_id from order_sum_daily_revenue except all select app_id, country_code,revenue,device_code,category_id from weekly\").withColumn(\"date\", F.lit(weekly_data[0]) ).withColumn(\"type\",F.lit(\"daily_weekly\"))\n\n    # except_1.show(30)\n    # except_2.show()\n    # except_3.show()\n    \n    # df_write_result = except_2.union(except_1).cache()\n    # df_write_result.createOrReplaceTempView(\"df_write_result\")\n    #if df_write_result.rdd.isEmpty():\n    #    print 'pass'\n#    else:\n #       print 'failed!!!!!!!' , month\n\n    # from aadatapipelinecore.core.utils.retry import retry\n    # def write_test_result(df_write_result):\n    #     df_write_result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_monthly_category_count/\",\n    #                                       mode=\"append\",\n    #                                       partitionBy=[\"date\"])\n    # retry(write_test_result,(df_write_result,),{},interval=10)\n\nsc.parallelize(map(check_diff, test_path), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200421-104414_2132516666","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\nimport datetime\n\nstart_week = \"2010-07-04\"\nend_week = \"2010-07-12\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append((key.strftime(\"%Y-%m-%d\"),[d.strftime(\"%Y-%m-%d\") for d in item]))\n        \n        \n        \ndef check_diff(weekly_data):\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=weekly/date=%s/\" % weekly_data[0])\n    df_4 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=weekly/date=%s/\" % weekly_data[0])\n\n    df_1.createOrReplaceTempView(\"daily_rank\")\n    df_2.createOrReplaceTempView(\"daily_est\")\n    df_3.createOrReplaceTempView(\"weekly_rank\")\n    df_3.createOrReplaceTempView(\"weekly_est\")\n    \n    print \"compare app id: \" , weekly_data\n    # spark.sql(\"select distinct app_id from monthly except all select distinct app_id from  daily \").show(2)\n    # spark.sql(\"select distinct app_id from daily except all select distinct app_id from  monthly \").show(2)\n    spark.sql('''select daily_est.free_app_download, daily_est.paid_app_download, daily_est.revenue, daily_est.app_id, daily_est.device_code, daily_est.country_code, daily_rank.category_id from daily_rank \n                join daily_est \n                on daily_rank.country_code= daily_est.country_code\n                and daily_rank.device_code=daily_est.device_code\n                and daily_rank.app_id = daily_est.app_id \n                and daily_rank.date = daily_est.date ''').createOrReplaceTempView(\"sum_category_daily\")\n\n    spark.sql(\"select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download , sum(revenue) as revenue, device_code ,category_id from sum_category_daily group by country_code,device_code, category_id, app_id \").createOrReplaceTempView(\"sum_daily\")\n        \n    spark.sql(\"select app_id, country_code, free_app_download, device_code, category_id from sum_daily where free_app_download is not null\").createOrReplaceTempView(\"sum_daily_free_download\")\n    # spark.sql(\"select app_id, country_code, paid_app_download, device_code, category_id from sum_daily where paid_app_download is not null \").createOrReplaceTempView(\"sum_daily_paid_app_download\")\n    # spark.sql(\"select app_id, country_code, revenue , device_code ,category_id from sum_daily where revenue is not null \").createOrReplaceTempView(\"sum_daily_revenue\")\n\n\n    spark.sql('''select app_id, country_code, device_code, category_id, free_app_download, ROW_NUMBER() OVER (PARTITION BY device_code, country_code, category_id ORDER BY free_app_download DESC) free_app_download_rank from sum_daily_free_download    ''').createOrReplaceTempView(\"order_sum_daily_free_download_rank\")\n\n\n    \n\n\n    \n    # spark.sql('''select app_id, country_code, device_code, category_id, free_app_download, dense_rank() OVER (PARTITION BY device_code, country_code, category_id ORDER BY free_app_download DESC) free_app_download_rank from sum_daily_free_download''').createOrReplaceTempView(\"order_sum_daily_free_download_dense_rank\")\n    \n    # spark.sql('''select app_id, country_code, device_code, category_id, free_app_download, row_number() OVER (PARTITION BY device_code, country_code, category_id ORDER BY free_app_download DESC) free_app_download_rank from sum_daily_free_download''').createOrReplaceTempView(\"order_sum_daily_free_download_row_number\")\n\n\n    # spark.sql('''select app_id, country_code, device_code, category_id, ROW_NUMBER() OVER (PARTITION BY device_code, country_code, category_id ORDER BY paid_app_download DESC) paid_app_download from sum_daily_paid_app_download''').createOrReplaceTempView(\"order_sum_daily_paid_download\")\n    # spark.sql('''select app_id, country_code, device_code, category_id, ROW_NUMBER() OVER (PARTITION BY device_code, country_code, category_id ORDER BY revenue DESC) revenue from sum_daily_revenue''').createOrReplaceTempView(\"order_sum_daily_revenue\")\n\n\n\n\n    # spark.sql(\"select app_id, country_code, free_app_download , paid_app_download , revenue, device_code,category_id from daily group by country_code,device_code,app_id,category_id \").createOrReplaceTempView(\"sum_daily\")\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from sum_daily except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from monthly\").show(2)\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from monthly except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code,category_id from sum_daily\").show(2)\n\n    spark.sql('''select count(*) from \n                    (select app_id, country_code,free_app_download,device_code,category_id from order_sum_daily_free_download_rank except all select app_id, country_code,free_app_download,device_code,category_id from weekly) as test''').withColumn(\"date\", F.lit(weekly_data[0]) ).withColumn(\"type\",F.lit(\"daily_weekly\")).show()\n    # except_2 = spark.sql(\"select app_id, country_code,paid_app_download,device_code,category_id from order_sum_daily_paid_download except all select app_id, country_code,paid_app_download,device_code,category_id from weekly\").withColumn(\"date\", F.lit(weekly_data[0]) ).withColumn(\"type\",F.lit(\"daily_weekly\"))\n    # except_3 = spark.sql(\"select app_id, country_code,revenue,device_code,category_id from order_sum_daily_revenue except all select app_id, country_code,revenue,device_code,category_id from weekly\").withColumn(\"date\", F.lit(weekly_data[0]) ).withColumn(\"type\",F.lit(\"daily_weekly\"))\n\n    # except_1.show(30)\n    # except_2.show()\n    # except_3.show()\n    \n    # df_write_result = except_2.union(except_1).cache()\n    # df_write_result.createOrReplaceTempView(\"df_write_result\")\n    #if df_write_result.rdd.isEmpty():\n    #    print 'pass'\n#    else:\n #       print 'failed!!!!!!!' , month\n\n    # from aadatapipelinecore.core.utils.retry import retry\n    # def write_test_result(df_write_result):\n    #     df_write_result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_monthly_category_count/\",\n    #                                       mode=\"append\",\n    #                                       partitionBy=[\"date\"])\n    # retry(write_test_result,(df_write_result,),{},interval=10)\n\nsc.parallelize(map(check_diff, test_path), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200507-020202_1347560282","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\nimport datetime\n\nstart_week = \"2020-01-12\"\nend_week = \"2020-01-19\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append((key.strftime(\"%Y-%m-%d\"),[d.strftime(\"%Y-%m-%d\") for d in item]))\n        \n        \n        \ndef check_diff(weekly_data):\n    print 'weekly_data[0]', weekly_data[0]\n    print 'weekly_data[1]', weekly_data[1]\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}\" % \",\".join(weekly_data[1]))\n    df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=WEEK/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=WEEK/date={%s}/platform=ios/\" % weekly_data[0])\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    df_2.createOrReplaceTempView(\"weekly_data\")\n    spark.sql(\"select * from daily_data\").show(2)\n    spark.sql(\"select * from weekly_data\").show(2)\n\nsc.parallelize(map(check_diff, test_path), 1)\n"]},{"cell_type":"code","execution_count":0,"id":"20200507-021129_1101823707","metadata":{},"outputs":[],"source":["\ntest_df = spark.sql('''\nSELECT * \nFROM   ( \n                SELECT   id, \n                         store_id, \n                         category_id, \n                         platform_id, \n                         vertical, \n                         feed, \n                         platform, \n                         Sum(est) AS est\n                FROM     daily_data \n                WHERE    feed IN ( 0, \n                                  1, \n                                  2, \n                                  101, \n                                  100, \n                                  102 ) \n                AND      platform='ios' \n                GROUP BY id, \n                         store_id, \n                         category_id, \n                         platform_id, \n                         vertical, \n                         feed,\n                         platform ) PIVOT ( max(est) FOR feed IN (0, \n                                                                  1, \n                                                                  2, \n                                                                  101, \n                                                                  100, \n                                                                  102) ) ''').withColumnRenamed(\"0\", platform_feed_to_metric('ios',0)).withColumnRenamed(\"1\", platform_feed_to_metric('ios',1)).withColumnRenamed(\"2\", platform_feed_to_metric('ios',2)).withColumnRenamed(\"101\", platform_feed_to_metric('ios',101)).withColumnRenamed(\"100\", platform_feed_to_metric('ios',100)).withColumnRenamed(\"102\", platform_feed_to_metric('ios',102)).na.fill(0).cache()\ntest_df.createOrReplaceTempView(\"transfer\")\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200507-024019_8008430","metadata":{},"outputs":[],"source":["\n\ndef platform_feed_to_metric(platform, feed):\n    mapping = [\n        ['ios', 0, 'iphone_free'],\n        ['ios',1, 'iphone_paid'],\n        ['ios',2, 'iphone_revenue'],\n        ['ios',101,'ipad_free'],\n        ['ios',100,'ipad_paid'],\n        ['ios',102,'ipad_revenue'],\n        ['android',0,'est_free_app_download'],\n        ['android',1,'est_paid_app_download'],\n        ['android',2,'est_revenue'],\n    ]\n    return [x for x in mapping if (x[0], x[1]) == (platform, feed)][0][2]\n    \nplatform_feed_to_metric(\"ios\",0)"]},{"cell_type":"code","execution_count":0,"id":"20200507-023856_223739973","metadata":{},"outputs":[],"source":["\n\n# print spark.sql(\"select id from (select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer except all select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from weekly_data where platform='ios' ) as t1  except select id from (select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from weekly_data where platform='ios' except all select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer) as t2 \").show()\n\n\n\nprint spark.sql(\"select * from (select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from weekly_data where platform='ios' except all select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer) as t2 except select * from (select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer except all select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from weekly_data where platform='ios' ) as t1 \").show()\n\n\n# print spark.sql(\"select count(*) from (select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer except all select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from weekly_data where platform='ios' ) as t1 \").show()\n# print spark.sql(\"select  count(*)  from (select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from weekly_data where platform='ios' except all select id, store_id, category_id, iphone_free,iphone_paid, iphone_revenue, ipad_free, ipad_paid, ipad_revenue from transfer) as t2 \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200507-054922_1666902281","metadata":{},"outputs":[],"source":["\n# print spark.sql(\"select count(*) from weekly_data \").show()\n# spark.sql(\"select * from daily_data where id=281747159 and store_id=143451 and category_id=7001 and feed=102\").show()\n# spark.sql(\"select * from weekly_data where id=281747159 and store_id=143451 and category_id=7001\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200507-061316_1831064616","metadata":{},"outputs":[],"source":["\n1166880.000/33192067.000\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200507-022443_944403749","metadata":{},"outputs":[],"source":["\n\nspark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=WEEK/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=WEEK/date=2020-01-11/platform=ios/\").where(\"category_id=36 and id=304878510 and store_id=0  and platform='ios'\").show()\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200507-023229_1076828816","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=WEEK/date=2020-01-11/platform=ios/\n\naws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-11/"]},{"cell_type":"code","execution_count":0,"id":"20200424-083501_1254883968","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\nimport datetime\n\nstart_week = \"2010-07-04\"\nend_week = \"2010-07-11\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append((key.strftime(\"%Y-%m-%d\"),[d.strftime(\"%Y-%m-%d\") for d in item]))\n        \n        \n        \ndef check_diff(weekly_data):\n    print 'weekly_data[0]', weekly_data[0]\n    print 'weekly_data[1]', weekly_data[1]\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=weekly/date=%s/\" % weekly_data[0])\n\n    df_1.createOrReplaceTempView(\"daily_rank\")\n    df_2.createOrReplaceTempView(\"daily_est\")\n    df_3.createOrReplaceTempView(\"weekly_rank\")\n\n    spark.sql('''select daily_est.free_app_download, daily_est.paid_app_download, daily_est.revenue, daily_est.app_id, daily_est.device_code, daily_est.country_code, daily_rank.category_id from daily_rank \n                full outer join daily_est \n                on daily_rank.country_code= daily_est.country_code\n                and daily_rank.device_code=daily_est.device_code\n                and daily_rank.app_id = daily_est.app_id \n                and daily_rank.date = daily_est.date''').createOrReplaceTempView(\"daily_est_rank_join\")\n\n\n    spark.sql(\"select app_id, country_code, sum(free_app_download) as free_app_download, device_code, category_id from daily_est_rank_join group by country_code, device_code, category_id, app_id \").createOrReplaceTempView(\"sum_daily\")\n        \n    spark.sql(''' select *, \n                    ROW_NUMBER() OVER (PARTITION BY device_code, country_code, category_id ORDER BY free_app_download DESC) rank \n                    from sum_daily \n                    where country_code='US' and category_id=100012 and device_code='ios-all' \n                        order by free_app_download desc''').createOrReplaceTempView(\"sum_rank_daily\")\n                        \n    spark.sql('''select app_id, country_code, free_app_download, device_code, category_id \n                    from weekly_rank \n                    where country_code='US' \n                        and device_code='ios-all' \n                        and category_id=100012  \n                        and free_app_download is not null \n                        order by free_app_download desc''').createOrReplaceTempView(\"weekly_rank_1\")\n\n\nsc.parallelize(map(check_diff, test_path), 1)\n\nspark.sql('''\nSELECT * FROM (SELECT * FROM weekly_rank_1 \nwhere free_app_download <= 70\n) AS test_unified\nEXCEPT\nSELECT app_id, country_code, rank as free_app_download, device_code, category_id  FROM (\nSELECT * FROM sum_rank_daily \nINNER JOIN\n( SELECT free_app_download \nFROM   (SELECT Count(1) AS free_app_download_count, \n              free_app_download \n        FROM   sum_rank_daily \n        WHERE  free_app_download IS NOT NULL \n        GROUP  BY free_app_download) AS t \nWHERE  free_app_download_count = 1  \n) AS filter_count\n\nON \nsum_rank_daily.free_app_download = filter_count.free_app_download\nWHERE rank  <= 70\n) test_raw order by free_app_download asc\n''').show()\n\n\nspark.sql('''\nSELECT app_id, country_code, rank as free_app_download, device_code, category_id  FROM (\nSELECT * FROM sum_rank_daily \nINNER JOIN\n( SELECT free_app_download \nFROM   (SELECT Count(1) AS free_app_download_count, \n               free_app_download \n        FROM   sum_rank_daily \n        WHERE  free_app_download IS NOT NULL \n        GROUP  BY free_app_download) AS t \nWHERE  free_app_download_count = 1  \n) AS filter_count\n\nON \nsum_rank_daily.free_app_download = filter_count.free_app_download\nWHERE rank <= 70\n) test_raw \n\nEXCEPT\n\nSELECT * FROM (SELECT * FROM weekly_rank_1 \nwhere free_app_download <= 70\n) AS test_unified order by free_app_download asc\n\n''').show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200424-084400_1450481519","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from daily_rank where app_id in (373998688) and country_code='US' and device_code='ios-all' order by category_id, date desc  \").show(200)\nspark.sql(\"select * from daily_est where app_id in (373998688) and country_code='US' and device_code='ios-all' order by date desc \").show()\nspark.sql(\"select * from daily_est_rank_join where app_id in (373998688) and country_code='US' and device_code='ios-all' and category_id=100012 \").show()\nspark.sql(\"select * from sum_rank_daily where app_id in (373998688)\").show()\nspark.sql(\"select * from weekly_rank where app_id in (373998688) and country_code='US' and device_code='ios-all' and category_id=100012\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200427-064807_1023960174","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from weekly_rank where  country_code='US' and device_code='ios-all' and category_id=100012 order by free_app_download asc \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200427-055931_96809991","metadata":{},"outputs":[],"source":["\ndf_raw_debug = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={2019-07-27,2019-07-26,2019-07-25,2019-07-24,2019-07-23,2019-07-22,2019-07-21}\").cache()\ndf_raw_debug.where(\"store_id=143441 and id=1030437345 and platform='ios' and category_id=7011\").orderBy(\"feed\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200426-123735_548283318","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.StringType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\ntest_date='2010-07-04,2010-07-05,2010-07-06,2010-07-07,2010-07-08,2010-07-09,2010-07-10'\nraw2= spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%(test_date), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).cache()\nraw2.filter(\"store_id=143441 and id=373998688 and platform='ios'  \").show(1000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200426-152814_34824737","metadata":{},"outputs":[],"source":["\n\nspark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/\").csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/2018-08-{04,11,18,25}/ios/sbe_est_app/143441/\", sep=\"\\t\").createOrReplaceTempView(\"weekly_data\")\nspark.sql(\"select * from weekly_data where _c0=1015763729\").show()\n\n# spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2010-07-{04,05,06,07,08,09,10}/ios/sbe_est_app/143441/\", sep=\"\\t\").createOrReplaceTempView(\"raw_data_daily\")\n# spark.sql(\"select *, cast(_c4 as int) as t1, cast(_c7 as int) as _c7 from raw_data_daily where _c5=373998688 order by t1 , _c7 desc   \").show(200)\n\n\n# monthly data\nspark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/MONTH/\").csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/MONTH/2018-08-31/ios/sbe_est_app/143441/\", sep=\"\\t\").createOrReplaceTempView(\"monthly_data\")\nspark.sql(\"select * from monthly_data where _c0=1015763729\").show()\n\nspark.sql('''\nSELECT _c1,_c0,_c2, SUM(est) \nFROM\n(\n    SELECT *, CAST(_c10 as int) AS est FROM weekly_data\n)\nGROUP BY est,_c1,_c0,_c2 where _c0= 1015763729 ''').show()\n# spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/\").csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/WEEK/2018-08-{05,06,07,08,09,10,11}/ios/sbe_est_app/143441/\", sep=\"\\t\").createOrReplaceTempView(\"raw_data_ha\")\n# spark.sql(\"select * from raw_data_ha where _c0=1332651082 and _c7='7011'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200426-151111_1018462968","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from sum_rank_daily\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200426-094901_1440699799","metadata":{},"outputs":[],"source":["\n\n\n\nprint spark.sql('''\nSELECT * FROM (SELECT * FROM weekly_rank_1 \nwhere free_app_download <= 60\n) AS test_unified\nEXCEPT\nSELECT app_id, country_code, rank as free_app_download, device_code, category_id  FROM (\nSELECT * FROM sum_rank_daily \nINNER JOIN\n( SELECT free_app_download \nFROM   (SELECT Count(1) AS free_app_download_count, \n              free_app_download \n        FROM   sum_rank_daily \n        WHERE  free_app_download IS NOT NULL \n        GROUP  BY free_app_download) AS t \nWHERE  free_app_download_count = 1  \n) AS filter_count\n\nON \nsum_rank_daily.free_app_download = filter_count.free_app_download\nWHERE rank  <= 60\n) test_raw order by free_app_download asc\n''').show()\n\n\nprint spark.sql('''\nSELECT app_id, country_code, rank as free_app_download, device_code, category_id  FROM (\nSELECT * FROM sum_rank_daily \nINNER JOIN\n( SELECT free_app_download \nFROM   (SELECT Count(1) AS free_app_download_count, \n               free_app_download \n        FROM   sum_rank_daily \n        WHERE  free_app_download IS NOT NULL \n        GROUP  BY free_app_download) AS t \nWHERE  free_app_download_count = 1  \n) AS filter_count\n\nON \nsum_rank_daily.free_app_download = filter_count.free_app_download\nWHERE rank <= 60\n) test_raw \n\nEXCEPT\n\nSELECT * FROM (SELECT * FROM weekly_rank_1 \nwhere free_app_download <= 60\n) AS test_unified order by free_app_download asc\n\n''').show()\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200426-150731_178583146","metadata":{},"outputs":[],"source":["\n\n\n\nprint spark.sql('''\nSELECT * FROM (\nSELECT * FROM sum_rank_daily \nINNER JOIN\n( SELECT free_app_download \nFROM   (SELECT Count(1) AS free_app_download_count, \n               free_app_download \n        FROM   sum_rank_daily \n        WHERE  free_app_download IS NOT NULL \n        GROUP  BY free_app_download) AS t \nWHERE  free_app_download_count = 1  \n) AS filter_count\n\nON \nsum_rank_daily.free_app_download = filter_count.free_app_download\n) test_raw EXCEPT \nSELECT * FROM (SELECT * FROM weekly_rank \nINNER JOIN\n( SELECT free_app_download \nFROM   (SELECT COUNT(1) AS free_app_download_count, \n               free_app_download \n        FROM   weekly_rank \n        WHERE  free_app_download IS NOT NULL \n        GROUP  BY free_app_download) AS t \nWHERE  free_app_download_count = 1  \n) AS filter_count_weekly\n\nON \nweekly_rank.free_app_download = filter_count_weekly.free_app_download\n) AS test_unified\n''').show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200426-111113_1263931138","metadata":{},"outputs":[],"source":["\nprint spark.sql(\"select * from sum_rank_daily where free_app_download is not null and country_code='US' and category_id=100012 and device_code='ios-phone'  order by free_app_download desc    \").show(250)\nprint spark.sql(\"select * from weekly_rank where free_app_download is not null and country_code='US' and category_id=100012 and device_code='ios-phone'  order by free_app_download asc \").show(250)\n"]},{"cell_type":"code","execution_count":0,"id":"20200426-111515_1305050443","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from daily_join  where app_id=373998688 and country_code='US' and device_code='ios-all' and category_id=100012\").show()\n# spark.sql('''select * from sum_rank_daily where free_app_download is not null order by free_app_download desc''').show(70)\n# spark.sql('''select * from weekly_rank where free_app_download is not null order by free_app_download desc''').show(70)\n"]},{"cell_type":"code","execution_count":0,"id":"20200426-103831_992513991","metadata":{},"outputs":[],"source":["\nprint spark.sql('''\nSELECT * FROM weekly_rank \nINNER JOIN\n( SELECT free_app_download \nFROM   (SELECT COUNT(1) AS free_app_download_count, \n               free_app_download \n        FROM   weekly_rank \n        WHERE  free_app_download IS NOT NULL \n        GROUP  BY free_app_download) AS t \nWHERE  free_app_download_count = 1  \n) AS filter_count_weekly\n\nON \nweekly_rank.free_app_download = filter_count_weekly.free_app_download\n''').show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200426-104446_720806393","metadata":{},"outputs":[],"source":["\nspark.sql('''\nSELECT COUNT(1) AS free_app_download_count, \n               free_app_download \n        FROM   weekly_rank \n        WHERE  free_app_download IS NOT NULL \n        GROUP  BY free_app_download\n        ORDER BY free_app_download_count ASC\n''').show()"]},{"cell_type":"code","execution_count":0,"id":"20200426-094345_803068015","metadata":{},"outputs":[],"source":["\nprint spark.sql(\"select * from sum_rank_daily where free_app_download is not null and country_code='US' and category_id=100012 and device_code='ios-all' order by  free_app_download desc   \").show(250)\nprint spark.sql(\"select * from weekly_join where free_app_download is not null and country_code='US' and category_id=100012 and device_code='ios-all'  order by  free_app_download desc \").show(250)"]},{"cell_type":"code","execution_count":0,"id":"20200421-104516_268875916","metadata":{},"outputs":[],"source":["\n# df_write_result.createOrReplaceTempView(\"df_write_result\")\nspark.sql(\"select * from count_daily where category_id = 100034 and device_code='ios-all' and country_code='AR' order by total_daily_count desc\").show(2264)\n# spark.sql(\"select * from join_category_daily where country_code='AR' and category_id='100010' and device_code='ios-all' and app_id=336904996 \").show()\n# spark.sql(\"select count(*) from daily_free_app\").show()\n# spark.sql(\"select * from weekly where country_code='AR' and category_id='100010' and device_code='ios-all' and app_id=336904996 \").show(20)\n# spark.sql(\"select * from daily_free_app where country_code='AR' and category_id='100010' and device_code='ios-all' and app_id=336904996 \").show(20)\n\n# spark.sql(\"select * from order_sum_daily_free_download_dense_rank where country_code='CO' and category_id='100009' and device_code='ios-all'  \").show(20)\n# spark.sql(\"select * from order_sum_daily_free_download_row_number where country_code='CO' and category_id='100009' and device_code='ios-all'  \").show(20)\n\n\n# spark.sql(\"select * from weekly where country_code='AE' and category_id='100009'  and device_code='ios-all' order by free_app_download , app_id asc \").show(50)\n# spark.sql(\"select * from sum_daily where country_code='CO' and category_id='100009' and app_id in (311992257,369522274) and device_code='ios-all'  \").show()\n# spark.sql(\"select * from order_sum_daily_free_download where country_code='CO' and category_id='100009' and app_id in (311992257,369522274) and device_code='ios-all'  \").show()\n# spark.sql(\"select * from weekly where country_code='CO' and category_id='100009' and app_id in (311992257,369522274) and device_code='ios-all' \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200421-124252_740782087","metadata":{},"outputs":[],"source":["\n\nspark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-03-07/ios/sbe_est_app/*/\").registerTempTable(\"raw_cat_weekly_data\")\nspark.sql(\"\"\"select * from raw_cat_weekly_data where id=335545504 and store_id=143481 and feed\"\"\").show()\n\n\nspark.read.option(\"basePath\",\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=weekly/date={2020-03-07}/\").registerTempTable(\"cat_weekly_data\")\nspark.sql(\"\"\"select * from cat_weekly_data where app_id=335545504 and country_code=  'AE' and device_code='ios-tablet'\"\"\").show()\n\n\nspark.read.option(\"basePath\",\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={2020-03-04}/\").registerTempTable(\"cat_data\")\nspark.sql(\"\"\"select * from cat_data where app_id=335545504 and country_code=  'AE' and device_code='ios-tablet'\"\"\").show()\n\nspark.read.option(\"basePath\",\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={2020-03-04}/\").registerTempTable(\"estt_data\")\nspark.sql(\"\"\"select * from estt_data where app_id=335545504 and country_code=  'AE' and device_code='ios-tablet'\"\"\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200424-041121_1610641544","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date=2020-03-04  --recursive\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-03-04  --recursive"]},{"cell_type":"code","execution_count":0,"id":"20200424-041133_2142486956","metadata":{},"outputs":[],"source":["   \nANDROID_CATEGORIES = [\n    (1, 400000), (2, 400001), (3, 400022), (4, 400023), (5, 400024),\n    (6, 400008), (7, 400011), (8, 400014), (9, 400017), (10, 400020),\n    (11, 400025), (12, 400030), (13, 400031), (14, 400032), (15, 400033),\n    (16, 400035), (17, 400036), (18, 400038), (19, 400040), (20, 400042),\n    (21, 400043), (22, 400044), (23, 400058), (24, 400046), (25, 400047),\n    (26, 400048), (27, 400050), (28, 400051), (29, 400052), (30, 400053),\n    (31, 400054), (32, 400055), (33, 400056), (34, 400045), (35, 400057),\n    (36, 400059), (37, 400060), (38, 400002), (39, 400003), (40, 400021),\n    (41, 400004), (42, 400005), (43, 400006), (44, 400007), (46, 400009),\n    (47, 400010), (48, 400012), (49, 400013), (51, 400015), (52, 400016),\n    (54, 400018), (55, 400019), (56, 400061), (57, 400063), (58, 400064),\n    (59, 400065), (60, 400062), (61, 400066), (62, 400067), (63, 400068),\n    (64, 400069), (65, 400070), (66, 400026), (67, 400027), (68, 400041),\n    (69, 400028), (70, 400029), (71, 400034), (72, 400037), (73, 400039),\n    (75, 400049)\n]\n\n\nIOS_CATEGORIES = [\n    (36, 100000), (100, 100021), (360, 100030), (361, 100031), (362, 100032),\n    (363, 100033), (6000, 100023), (6001, 100077), (6002, 100076), (6003, 100075),\n    (6004, 100073), (6005, 100072), (6006, 100070), (6007, 100069), (6008, 100068),\n    (6009, 100067), (6010, 100066), (6011, 100065), (6012, 100034), (6013, 100029),\n    (6014, 100001), (6015, 100027), (6016, 100026), (6017, 100025), (6018, 100022),\n    (6020, 100064), (6021, 100035), (6022, 100024), (6023, 100028), (6024, 100071),\n    (6025, 100074), (7001, 100002), (7002, 100003), (7003, 100004), (7004, 100005),\n    (7005, 100006), (7006, 100007), (7007, 100008), (7008, 100009), (7009, 100010),\n    (7010, 100011), (7011, 100012), (7012, 100013), (7013, 100014), (7014, 100015),\n    (7015, 100016), (7016, 100017), (7017, 100018), (7018, 100019), (7019, 100020),\n    (13001, 100053), (13002, 100046), (13003, 100049), (13004, 100054), (13005, 100060),\n    (13006, 100037), (13007, 100036), (13008, 100038), (13009, 100039), (13010, 100040),\n    (13011, 100041), (13012, 100042), (13013, 100043), (13014, 100044), (13015, 100045),\n    (13017, 100047), (13018, 100048), (13019, 100050), (13020, 100051), (13021, 100052),\n    (13023, 100055), (13024, 100056), (13025, 100057), (13026, 100058), (13027, 100059),\n    (13028, 100061), (13029, 100062), (13030, 100063)\n]\n\n\nIOS_STORE_COUNTRY_MAPPING = [\n    (0, 'WW'), (143575, 'AL'), (143563, 'DZ'), (143564, 'AO'), (143538, 'AI'),\n    (143540, 'AG'), (143505, 'AR'), (143524, 'AM'), (143460, 'AU'), (143445, 'AT'),\n    (143568, 'AZ'), (143539, 'BS'), (143559, 'BH'), (143541, 'BB'), (143565, 'BY'),\n    (143446, 'BE'), (143555, 'BZ'), (143576, 'BJ'), (143542, 'BM'), (143577, 'BT'),\n    (143556, 'BO'), (143525, 'BW'), (143503, 'BR'), (143543, 'VG'), (143560, 'BN'),\n    (143526, 'BG'), (143578, 'BF'), (143579, 'KH'), (143455, 'CA'), (143580, 'CV'),\n    (143544, 'KY'), (143581, 'TD'), (143483, 'CL'), (143465, 'CN'), (143501, 'CO'),\n    (143582, 'CG'), (143495, 'CR'), (143494, 'HR'), (143557, 'CY'), (143489, 'CZ'),\n    (143458, 'DK'), (143545, 'DM'), (143508, 'DO'), (143509, 'EC'), (143516, 'EG'),\n    (143506, 'SV'), (143518, 'EE'), (143583, 'FJ'), (143447, 'FI'), (143442, 'FR'),\n    (143584, 'GM'), (143443, 'DE'), (143573, 'GH'), (143448, 'GR'), (143546, 'GD'),\n    (143504, 'GT'), (143585, 'GW'), (143553, 'GY'), (143510, 'HN'), (143463, 'HK'),\n    (143482, 'HU'), (143558, 'IS'), (143467, 'IN'), (143476, 'ID'), (143449, 'IE'),\n    (143491, 'IL'), (143450, 'IT'), (143511, 'JM'), (143462, 'JP'), (143528, 'JO'),\n    (143517, 'KZ'), (143529, 'KE'), (143493, 'KW'), (143586, 'KG'), (143587, 'LA'),\n    (143519, 'LV'), (143497, 'LB'), (143588, 'LR'), (143520, 'LT'), (143451, 'LU'),\n    (143515, 'MO'), (143530, 'MK'), (143531, 'MG'), (143589, 'MW'), (143473, 'MY'),\n    (143532, 'ML'), (143521, 'MT'), (143590, 'MR'), (143533, 'MU'), (143468, 'MX'),\n    (143591, 'FM'), (143523, 'MD'), (143592, 'MN'), (143547, 'MS'), (143593, 'MZ'),\n    (143594, 'NA'), (143484, 'NP'), (143452, 'NL'), (143461, 'NZ'), (143512, 'NI'),\n    (143534, 'NE'), (143561, 'NG'), (143457, 'NO'), (143562, 'OM'), (143477, 'PK'),\n    (143595, 'PW'), (143485, 'PA'), (143597, 'PG'), (143513, 'PY'), (143507, 'PE'),\n    (143474, 'PH'), (143478, 'PL'), (143453, 'PT'), (143498, 'QA'), (143487, 'RO'),\n    (143469, 'RU'), (143598, 'ST'), (143479, 'SA'), (143535, 'SN'), (143599, 'SC'),\n    (143600, 'SL'), (143464, 'SG'), (143496, 'SK'), (143499, 'SI'), (143601, 'SB'),\n    (143472, 'ZA'), (143466, 'KR'), (143454, 'ES'), (143486, 'LK'), (143548, 'KN'),\n    (143549, 'LC'), (143550, 'VC'), (143554, 'SR'), (143602, 'SZ'), (143456, 'SE'),\n    (143459, 'CH'), (143470, 'TW'), (143603, 'TJ'), (143572, 'TZ'), (143475, 'TH'),\n    (143551, 'TT'), (143536, 'TN'), (143480, 'TR'), (143604, 'TM'), (143552, 'TC'),\n    (143537, 'UG'), (143492, 'UA'), (143481, 'AE'), (143444, 'GB'), (143441, 'US'),\n    (143514, 'UY'), (143566, 'UZ'), (143502, 'VE'), (143471, 'VN'), (143571, 'YE'),\n    (143605, 'ZW')]\n    \n# IOS_STORE_COUNTRY_MAPPING = [\n#     (0, 'WW'), (143441, 'US'), (143465, 'CN'), (143460, 'AU'), (143444, 'GB')\n#     ]\n\nANDROID_STORE_COUNTRY_MAPPING = [\n    (17, 'AR'), (1, 'AU'), (35, 'AT'), (61, 'AZ'), (11, 'BE'), (18, 'BR'), (47, 'BG'),\n    (2, 'CA'), (13, 'CL'), (3, 'CN'), (52, 'CO'), (64, 'CR'), (80, 'HR'), (36, 'CZ'),\n    (38, 'DK'), (62, 'EC'), (33, 'EG'), (20, 'FI'), (6, 'FR'), (4, 'DE'), (46, 'GR'),\n    (16, 'HK'), (37, 'HU'), (19, 'IN'), (21, 'ID'), (39, 'IE'), (40, 'IL'), (8, 'IT'),\n    (9, 'JP'), (53, 'KZ'), (95, 'KE'), (50, 'KW'), (86, 'LV'), (65, 'LB'), (78, 'LT'),\n    (24, 'MY'), (26, 'MX'), (23, 'NL'), (41, 'NZ'), (74, 'NG'), (42, 'NO'), (54, 'PK'),\n    (56, 'PE'), (31, 'PH'), (28, 'PL'), (43, 'PT'), (84, 'PR'), (73, 'QA'), (44, 'RO'),\n    (22, 'RU'), (51, 'SA'), (32, 'SG'), (45, 'SK'), (14, 'ZA'), (27, 'KR'), (5, 'ES'),\n    (34, 'SE'), (12, 'CH'), (30, 'TW'), (29, 'TH'), (25, 'TR'), (48, 'UA'), (49, 'AE'),\n    (7, 'GB'), (10, 'US'), (15, 'VN'), (1000, 'WW')\n]\n# ANDROID_STORE_COUNTRY_MAPPING = [\n#   (12, 'CH'), (30, 'TW'), (25, 'TR'), (48, 'UA'),\n#     (7, 'GB'), (10, 'US'), (1000, 'WW')\n# ]\n\n\n\ndef device_code_to_feed(market_code, device_code, metric_name):\n    mapping = [\n        ['apple-store',0,'ios-phone','est_free_app_download'],\n        ['apple-store',1,'ios-phone','est_paid_app_download'],\n        ['apple-store',2,'ios-phone','est_revenue'],\n        ['apple-store',101,'ios-tablet','est_free_app_download'],\n        ['apple-store',100,'ios-tablet','est_paid_app_download'],\n        ['apple-store',102,'ios-tablet','est_revenue'],\n        ['google-play',0,'android-all','est_free_app_download'],\n        ['google-play',1,'android-all','est_paid_app_download'],\n        ['google-play',2,'android-all','est_revenue'],\n    ]\n    return [x for x in mapping if (x[0], x[2], x[3]) == (market_code, device_code, metric_name)][0][1]\n\n\n\ndef country_code_to_id(market_code, code):\n    if market_code == 'apple-store':\n        ios_mapping = {_code:_id for (_id, _code) in IOS_STORE_COUNTRY_MAPPING}\n        return ios_mapping[code]\n    else:\n        gp_mapping = {_code:_id for (_id, _code) in ANDROID_STORE_COUNTRY_MAPPING}\n        return gp_mapping[code]\n\n\ndef category_to_legacy_category(market_code, legacy):\n    if market_code == 'apple-store':\n        ios_category = {_category:_legacy_category for (_legacy_category,_category) in IOS_CATEGORIES }\n        return ios_category[legacy]\n    else:\n        gp_category =  {_category:_legacy_category for (_legacy_category,_category) in ANDROID_CATEGORIES }\n        return gp_category[legacy]\n\n\n\ndef id_to_country_code(market_code, store_id):\n    if market_code == 'ios':\n        ios_mapping = {store_id:_code for (store_id, _code) in IOS_STORE_COUNTRY_MAPPING}\n        print ios_mapping\n        return ios_mapping[store_id]\n    else:\n        gp_mapping = {store_id:_code for (store_id, _code) in ANDROID_STORE_COUNTRY_MAPPING}\n        return gp_mapping[store_id]\n\n\n\nid_to_country_code(\"test\",15)\n# 13028, 100061\n# 71, 400034\n# category_to_legacy_category(\"apple-store\",100000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200426-124246_1858597604","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\nimport datetime\n\nstart_week = \"2019-07-21\"\nend_week = \"2019-07-28\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append((key.strftime(\"%Y-%m-%d\"),[d.strftime(\"%Y-%m-%d\") for d in item]))\n        \n        \n        \ndef check_diff(weekly_data):\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_2 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}/\" % \",\".join(weekly_data[1]))\n    df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=weekly/date=%s/\" % weekly_data[0])\n    # df_4 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=weekly/date=%s/\" % weekly_data[0])\n\n    df_1.createOrReplaceTempView(\"daily_rank\")\n    df_2.createOrReplaceTempView(\"daily_est\")\n    df_3.createOrReplaceTempView(\"weekly_rank\")\n    # df_4.createOrReplaceTempView(\"weekly_est\")\n    \n    # print \"compare app id: \" , weekly_data\n    \n    spark.sql('''select daily_est.free_app_download, daily_est.paid_app_download, daily_est.revenue, daily_est.app_id, daily_est.device_code, daily_est.country_code, daily_rank.category_id from daily_rank \n                full outer join daily_est \n                on daily_rank.country_code= daily_est.country_code\n                and daily_rank.device_code=daily_est.device_code\n                and daily_rank.app_id = daily_est.app_id \n                and daily_rank.date = daily_est.date''').createOrReplaceTempView(\"daily_join\")\n\n    # spark.sql('''select weekly_est.free_app_download, weekly_rank.free_app_download as rank, weekly_est.paid_app_download, weekly_est.revenue, weekly_est.app_id, weekly_est.device_code, weekly_est.country_code, weekly_rank.category_id from weekly_rank \n    #             join weekly_est \n    #             on weekly_rank.country_code= weekly_est.country_code\n    #             and weekly_rank.device_code=weekly_est.device_code\n    #             and weekly_rank.app_id = weekly_est.app_id \n    #             and weekly_rank.date = weekly_est.date ''').createOrReplaceTempView(\"weekly_join\")\n\n\n\n    spark.sql(\"select app_id, country_code, sum(free_app_download) as free_app_download, device_code, category_id from daily_join group by country_code, device_code, category_id, app_id \").createOrReplaceTempView(\"sum_daily\")\n        \n    # # spark.sql(\"select app_id, country_code, free_app_download, device_code, category_id from sum_daily where free_app_download is not null\").createOrReplaceTempView(\"sum_daily_free_download\")\n    # spark.sql(\"select * from weekly_join\").show()\n\n    # spark.sql('''select app_id, country_code, device_code, category_id, free_app_download, ROW_NUMBER() OVER (PARTITION BY device_code, country_code, category_id ORDER BY free_app_download DESC) free_app_download_rank from sum_daily''').createOrReplaceTempView(\"order_sum_daily_free_download_rank\")\n\n\n    \n\n    # spark.sql('''select count(*) from \n    #                 (select app_id, country_code,free_app_download,device_code,category_id from order_sum_daily_free_download_rank except all select app_id, country_code,free_app_download,device_code,category_id from weekly) as test''').withColumn(\"date\", F.lit(weekly_data[0]) ).withColumn(\"type\",F.lit(\"daily_weekly\")).show()\n\n    spark.sql(''' select *, \n                    ROW_NUMBER() OVER (PARTITION BY device_code, country_code, category_id ORDER BY free_app_download DESC) rank \n                    from sum_daily \n                    where country_code='US' and category_id=100013 and device_code='ios-phone' \n                        order by free_app_download desc''').createOrReplaceTempView(\"sum_rank_daily\")\n    spark.sql('''select app_id, country_code, free_app_download, device_code, category_id \n                    from weekly_rank \n                    where country_code='US' \n                        and device_code='ios-phone' \n                        and category_id=100013  \n                        and free_app_download is not null \n                        order by free_app_download desc''').createOrReplaceTempView(\"weekly_rank_1\")\n                        \n    # spark.sql('''select count(*) from ( select * from sum_rank_daily where free_app_download is not null except select * from weekly_rank ) as t1 ''').show()\n    # spark.sql('''select count(*) from ( select * from weekly_rank except select * from sum_rank_daily where free_app_download is not null ) as t2 ''').show()\n    # spark.sql('''select count(*) from sum_rank_daily where free_app_download is not null ''').show()\n\n\nsc.parallelize(map(check_diff, test_path), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200507-020103_166814216","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\ndates = get_date_list('2010-07-04', '2010-07-31', freq='D')    # >> ['2010-07-04', '2010-07-05', '2010-07-06']\ndates = dates + get_date_list('2010-08-31', '2018-12-31', freq='M')\nprint dates\n# mapping:  ios-phone,    ios-all,     ios-tablet,  free_app_download=2, facebook\napp_list = ['377194688', '364709193', '379174209',  '339739007',         '284882215']\ndevice_code = ['ios-phone','ios-tablet','ios-all']    # exclude 'android-all'\ncountries = ['WW', 'US']\ndef main_test(dates, app_list, device_code, countries):\n    where_clause = \"app_id in ({}) and device_code in ('{}') and country_code in ('{}')\".format(\",\".join(map(str,app_list)),   \"','\".join(device_code), \"','\".join(countries) )\n    # >> \"app_id in (377194688,364709193,379174209,339739007,284882215) and device_code in ('ios-phone','ios-tablet','ios-all') and country_code in ('WW','US')\"\n    print where_clause\n    # fetch data from store-est and aggregrate \"free_app_download\"\n    schema = StructType([\n        StructField(\"app_id\", StringType(), True), \n        StructField(\"device_code\", StringType(), True),\n        StructField(\"country_code\", StringType(), True),\n        StructField(\"free_app_download\", IntegerType(), True),\n        # StructField(\"paid_app_download\", IntegerType(), True)\n        ])\n    uni_df = spark.createDataFrame([], schema=schema)\n    for date in dates:\n        unified_data = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(date)).where(where_clause).select('app_id', 'device_code', 'country_code', 'free_app_download')\n        uni_df = uni_df.union(unified_data).groupby('app_id', 'device_code', 'country_code').agg({'free_app_download': 'sum'}).withColumnRenamed('sum(free_app_download)', 'free_app_download')\n    # fetch data from cumulative\n    cum_df = spark.read.format('delta').load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-cum.v1/fact/date={}/\".format(dates[-1])).where(where_clause).select('app_id', 'device_code', 'country_code', 'free_app_download')\n    res_df = uni_df.subtract(cum_df)\n    return res_df\na = main_test(dates, app_list, device_code, countries)\na.show()\nprint len(a.collect())\n\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}