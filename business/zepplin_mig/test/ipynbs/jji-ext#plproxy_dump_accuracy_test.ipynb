{"cells":[{"cell_type":"code","execution_count":0,"id":"20200616-031928_1060023912","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        date_list = [('2018-12', '2018-12-27'), ('2019-06', '2019-06-27'),\n                     ('2019-12', '2019-12-27')]\n    if granularity == 'weekly':\n        date_list = [('2013-12', '2013-12-28'), ('2014-12', '2014-12-27'),\n                     ('2015-12', '2015-12-26'), ('2016-12', '2016-12-24'),\n                     ('2017-12', '2017-12-23')]\n    if granularity == 'monthly':\n        date_list = [('2013-12', '2013-12-31'), ('2014-12', '2014-12-31'),\n                     ('2015-12', '2015-12-31'), ('2016-12', '2016-12-31'),\n                     ('2017-12', '2017-12-31')]\n    return date_list\n\n\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select device_id,store_id,date,app_id,kpi,estimate from plproxy.execute_select_nestloop($proxy$\n    select device_id,store_id,date,app_id,kpi,estimate\n    from mu.app_{}\n    where\n        date = '{}'\n$proxy$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT) limit 50;\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_plproxy_dump_accuracy(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        result = query(aa_dsn, sql.format(graularity, date[1]))\n        plproxy_df = spark.createDataFrame(result,\n                                           schema=['device_id', 'store_id', 'date', 'app_id', 'kpi', 'estimate'])\n\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={}/month={}/\"\n        dump_df = spark.read.parquet(dump_path.format(graularity, date[0]))\n\n        join_df_count = dump_df.filter(\"date='{}'\".format(date[1])).join(\n            plproxy_df, on=[plproxy_df.device_id == dump_df.device_id, plproxy_df.store_id == dump_df.store_id,\n                            plproxy_df.app_id == dump_df.app_id, plproxy_df.kpi == dump_df.kpi,\n                            plproxy_df.estimate == dump_df.estimate], how='inner').count()\n        if join_df_count != 50:\n            print \"Accuracy Test Fail!!!! graularity: {}, date: {}\".format(graularity, date[1])\n        else:\n            print \"Accuracy Test Pass! graularity: {},date : {}\".format(graularity, date[1])\n\n\ngraularity_list = [\"daily\"]\nfor graularity in graularity_list:\n    check_plproxy_dump_accuracy(get_path_date_list(graularity), graularity)"]},{"cell_type":"code","execution_count":0,"id":"20200616-032121_1091685923","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200616-032409_162202968","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}