{"cells":[{"cell_type":"code","execution_count":0,"id":"20200521-085350_874623390","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select date, sum(count_a) from plproxy.execute_select_nestloop($proxy$ \n    select date, count(*) as count_a\n    from mu.app_daily\n    where \n        date between '{start_day}' and '{end_day}'\n    group by date\n$proxy$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 01, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 03, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 03, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_dump_completeness(_granularity, date_list):\n    for month_day_list_tuple in date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity={raw_granularity}/\" \\\n                   \"month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_count_list = spark.read.parquet(dump_path_parse).groupBy('date').agg(count('app_id').alias('count')).collect()\n        dump_count_list = sorted(dump_count_list, key=lambda x: x['date'], reverse=False)\n\n        plproxy_count_list = query(aa_dsn, sql.format(start_day=month_day_list_tuple[1][0], end_day=month_day_list_tuple[1][len(month_day_list_tuple[1])-1]))\n        # print month_day_list_tuple[1]\n        # print dump_count_list\n        # print plproxy_count_list\n        for dump_count, plproxy_count in zip(dump_count_list, plproxy_count_list):\n            if dump_count['count'] != plproxy_count[1]:\n                print \"Completeness Test Fail!!!! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            else:\n                print \"Completeness Test Pass! date: {}\".format(dump_count['date'])\n        break\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_dump_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200521-090847_16162598","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import count\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2015-12/\")\nprint df.groupBy('date').agg(count('app_id').alias('count')).collect()"]},{"cell_type":"code","execution_count":0,"id":"20200521-091051_2033340445","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200521-101123_534065138","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\n\ntest_result = []\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select date, sum(count_a) from plproxy.execute_select_nestloop($proxy$ \n    select date, count(*) as count_a\n    from mu.app_daily\n    where \n        date between '{start_day}' and '{end_day}'\n    group by date\n$proxy$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 01, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 03, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 03, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_dump_completeness(_granularity, date_list):\n    for month_day_list_tuple in date_list:\n        plproxy_count_list = query(aa_dsn, sql.format(start_day=month_day_list_tuple[1][0], end_day=month_day_list_tuple[1][len(month_day_list_tuple)-1]))\n        print plproxy_count_list\n        break\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_dump_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200521-101521_381959979","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact/"]},{"cell_type":"code","execution_count":0,"id":"20200605-032034_1874817282","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select date, sum(count_a) from plproxy.execute_select_nestloop($proxy$ \n    select date, count(*) as count_a\n    from mu.app_daily\n    where \n        date between '{start_day}' and '{end_day}'\n    group by date\n$proxy$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 01, 31)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 03, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_dump_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_count_list = spark.read.parquet(dump_path_parse).groupBy('date').agg(count('app_id').alias('count')).collect()\n        dump_count_list = sorted(dump_count_list, key=lambda x: x['date'], reverse=False)\n\n        plproxy_count_list = query(aa_dsn, sql.format(\n            start_day=month_day_list_tuple[1][0], end_day=month_day_list_tuple[1][len(month_day_list_tuple[1])-1]))\n        for dump_count, plproxy_count in zip(dump_count_list, plproxy_count_list):\n            if dump_count['count'] != plproxy_count[1]:\n                print \"Completeness Test Fail!!!! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            else:\n                print \"Completeness Test Pass! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            test_result.append((_granularity, dump_count['date'], plproxy_count[1], dump_count['count']))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'plproxy_count', 'dump_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_plproxy_dump_count_0605/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_dump_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200605-033531_774577005","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select date, sum(count_a) from plproxy.execute_select_nestloop($proxy$ \n    select date, count(*) as count_a\n    from mu.app_{}\n    where \n        date between '{start_day}' and '{end_day}'\n    group by date\n$proxy$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_dump_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_count_list = spark.read.parquet(dump_path_parse).groupBy('date').agg(count('app_id').alias('count')).collect()\n        dump_count_list = sorted(dump_count_list, key=lambda x: x['date'], reverse=False)\n\n        plproxy_count_list = query(aa_dsn, sql.format(_granularity, start_day=month_day_list_tuple[1][0],\n                                                      end_day=month_day_list_tuple[1][len(month_day_list_tuple[1])-1]))\n        for dump_count, plproxy_count in zip(dump_count_list, plproxy_count_list):\n            if dump_count['count'] != plproxy_count[1]:\n                print \"Completeness Test Fail!!!! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            else:\n                print \"Completeness Test Pass! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            test_result.append((_granularity, dump_count['date'], plproxy_count[1], dump_count['count']))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'plproxy_count', 'dump_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_plproxy_dump_count_0605/weekly/\",\n            mode=\"overwrite\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"weekly\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_dump_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'\n"]},{"cell_type":"code","execution_count":0,"id":"20200605-041647_1426015336","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select date, sum(count_a) from plproxy.execute_select_nestloop($proxy$ \n    select date, count(*) as count_a\n    from mu.app_{}\n    where \n        date between '{start_day}' and '{end_day}'\n    group by date\n$proxy$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2015, 12, 27)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    date_list = {}\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if date_list.has_key(x[0][:7]):\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda x: datetime.datetime.strptime(x[0] + str(-01), '%Y-%m-%d'),\n                        reverse=False)\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_dump_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/app-tech.usage.legacy-mu_app.v2/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_count_list = spark.read.parquet(dump_path_parse).groupBy('date').agg(count('app_id').alias('count')).collect()\n        dump_count_list = sorted(dump_count_list, key=lambda x: x['date'], reverse=False)\n\n        plproxy_count_list = query(aa_dsn, sql.format(_granularity, start_day=month_day_list_tuple[1][0],\n                                                      end_day=month_day_list_tuple[1][len(month_day_list_tuple[1])-1]))\n        for dump_count, plproxy_count in zip(dump_count_list, plproxy_count_list):\n            if dump_count['count'] != plproxy_count[1]:\n                print \"Completeness Test Fail!!!! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            else:\n                print \"Completeness Test Pass! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            test_result.append((_granularity, dump_count['date'], plproxy_count[1], dump_count['count']))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'plproxy_count', 'dump_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_plproxy_dump_count_0605/monthly/\",\n            mode=\"overwrite\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"monthly\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_dump_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200605-045603_2095652786","metadata":{},"outputs":[],"source":["\ndf = spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_plproxy_dump_count_0605/monthly/').orderBy('date').collect()\nfor row in df:\n    print row['date'],'\\t',int(row['plproxy_count']),'\\t',row['dump_count']"]},{"cell_type":"code","execution_count":0,"id":"20200605-090003_1855691320","metadata":{},"outputs":[],"source":["\n# Copyright (c) 2020 App Annie Inc. All rights reserved.\n\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        date_list = [('2015-12', '2015-12-27'), ('2016-06', '2016-06-27'),\n                     ('2016-12', '2016-12-27'), ('2017-06', '2017-06-27'),\n                     ('2017-12', '2017-12-27')]\n    if granularity == 'weekly':\n        date_list = [('2013-12', '2013-12-28'), ('2014-12', '2014-12-27'),\n                     ('2015-12', '2015-12-26'), ('2016-12', '2016-12-24'),\n                     ('2017-12', '2017-12-23')]\n    if granularity == 'monthly':\n        date_list = [('2013-12', '2013-12-31'), ('2014-12', '2014-12-31'),\n                     ('2015-12', '2015-12-31'), ('2016-12', '2016-12-31'),\n                     ('2017-12', '2017-12-31')]\n    return date_list\n\n\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\nsql = \"\"\"select device_id,store_id,date,app_id,kpi,estimate from plproxy.execute_select_nestloop($proxy$\n    select device_id,store_id,date,app_id,kpi,estimate\n    from mu.app_{}\n    where\n        date = '{}'\n$proxy$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT) limit 50;\"\"\"\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_plproxy_dump_accuracy(date_list, graularity):\n    \"\"\"\n        date_list:\n                [(month1,day1), (month1,day2), (month2,day1), (month2,day2)]\n        sample:\n            [('2015-12', '2015-12-27'), ('2015-12', '2015-12-28'),\n            ('2016-12', '2016-12-27'), ('2016-12', '2016-12-28')]\n    \"\"\"\n    for date in date_list:\n        result = query(aa_dsn, sql.format(graularity, date[1]))\n        plproxy_df = spark.createDataFrame(result,\n                                           schema=['device_id', 'store_id', 'date', 'app_id', 'kpi', 'estimate'])\n\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={}/month={}/\"\n        dump_df = spark.read.parquet(dump_path.format(graularity, date[0]))\n\n        join_df_count = dump_df.filter(\"date='{}'\".format(date[1])).join(\n            plproxy_df, on=[plproxy_df.device_id == dump_df.device_id, plproxy_df.store_id == dump_df.store_id,\n                            plproxy_df.app_id == dump_df.app_id, plproxy_df.kpi == dump_df.kpi,\n                            plproxy_df.estimate == dump_df.estimate], how='inner').count()\n        if join_df_count != 50:\n            print \"Accuracy Test Fail!!!! graularity: {}, date: {}\".format(graularity, date[1])\n        else:\n            print \"Accuracy Test Pass! graularity: {},date : {}\".format(graularity, date[1])\n\n\ngraularity_list = [\"daily\", \"weekly\", \"monthly\"]\nfor graularity in graularity_list:\n    check_plproxy_dump_accuracy(get_path_date_list(graularity), graularity)\n"]},{"cell_type":"code","execution_count":0,"id":"20200605-090447_303949074","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet('s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2019-10-01/')\ndf.filter(\"app_id=20600001429645 and country_code='CN' and device_code='android-phone'\").select('est_average_bytes_per_user').show()"]},{"cell_type":"code","execution_count":0,"id":"20200608-094456_1259582794","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nkpi_mapping_dict = {1: 'AU', 2: 'AFU', 3: 'ADU', 4: 'IP', 5: 'AAD', 6: 'PAD', 7: 'MBPU',\n                    8: 'ATU', 9: 'UP', 10: 'OR', 17: 'MBPS', 23: 'IS', 24: 'SOU', 25: 'SOI'}\ngranularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select kpi, sum(kpi_count) from plproxy.execute_select_nestloop($proxy$ \n    select kpi, count(app_id) as kpi_count\n    from mu.app_{}\n    where \n        date = '{}'\n    group by kpi\n$proxy$) t (kpi SMALLINT, kpi_count BIGINT) group by kpi order by kpi asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 3, 31)\n    start = datetime.date(2020, 3, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_routine_consistency(_granularity, date_list):\n    routine_path = 's3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type={}/date={}/'\n    for date in date_list:\n        plproxy_kpi_count_list = query(aa_dsn, sql.format(_granularity, date[1]))\n        routine_path_parse = routine_path.format(granularity_dict[_granularity], date[1])\n        routine_df = spark.read.parquet(routine_path_parse)\n        for plproxy_kpi_count in plproxy_kpi_count_list:\n            routine_kpi_count = routine_df.filter(\n                \"{} is not null\".format(kpi_mapping_dict[plproxy_kpi_count[0]])).count()\n            if plproxy_kpi_count[1] != routine_kpi_count:\n                print \"Consistency Test Fail!!!! date: {}, plproxy_count: {}, routine_count: {}, kpi: {}\".format(\n                    date[1], plproxy_kpi_count[1], routine_kpi_count, kpi_mapping_dict[plproxy_kpi_count[0]]\n                )\n            else:\n                print \"Consistency Test Pass! date: {}, plproxy_count: {}, routine_count: {}, kpi: {}\".format(\n                    date[1], plproxy_kpi_count[1], routine_kpi_count, kpi_mapping_dict[plproxy_kpi_count[0]]\n                )\n\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_routine_consistency(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200610-031811_1234000108","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom conf import settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nkpi_mapping_dict = {1: 'est_average_active_users', 2: 'est_average_session_per_user', 3: 'est_average_session_duration',\n                    4: 'est_install_penetration', 5: 'est_average_active_days', 6: 'est_percentage_active_days',\n                    7: 'est_average_bytes_per_user', 8: 'est_average_time_per_user', 9: 'est_usage_penetration',\n                    10: 'est_open_rate', 17: 'est_average_bytes_per_session', 23: 'est_installs', 24: 'est_share_of_users',\n                    25: 'est_share_of_installs'}\ngranularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select kpi, sum(kpi_count) from plproxy.execute_select_nestloop($proxy$ \n    select kpi, count(app_id) as kpi_count\n    from mu.app_{}\n    where \n        date = '{}'\n    group by kpi\n$proxy$) t (kpi SMALLINT, kpi_count BIGINT) group by kpi order by kpi asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 5, 17)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_routine_consistency(_granularity, date_list):\n    plproxy_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/'\n    routine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/'\n    for date in date_list:\n        plproxy_path_parse = plproxy_path.format(_granularity, date[1])\n        routine_path_parse = routine_path.format(_granularity, date[1])\n        plproxy_df = spark.read.format(\"delta\").load(plproxy_path).where(\"granularity='daily' and date='{}'\".format(date[1]))\n        routine_df = spark.read.format(\"delta\").load(routine_path).where(\"granularity='daily' and date='{}'\".format(date[1]))\n        for kpi in kpi_mapping_dict:\n            if kpi==1:\n                plproxy_kpi_count = plproxy_df.filter(\n                    \"{}!=0\".format(kpi_mapping_dict[kpi])).count()\n                routine_kpi_count = routine_df.filter(\n                    \"{}!=0\".format(kpi_mapping_dict[kpi])).count()\n            else:\n                plproxy_kpi_count = plproxy_df.filter(\n                    \"est_average_active_users!=0 and {}!=0\".format(kpi_mapping_dict[kpi])).count()\n                routine_kpi_count = routine_df.filter(\n                    \"est_average_active_users!=0 and {}!=0\".format(kpi_mapping_dict[kpi])).count()\n            if plproxy_kpi_count != routine_kpi_count:\n                print \"Consistency Test Fail!!!! date: {}, plproxy_count: {}, routine_count: {}, kpi: {}\".format(\n                    date[1], plproxy_kpi_count, routine_kpi_count, kpi_mapping_dict[kpi]\n                )\n            else:\n                print \"Consistency Test Pass! date: {}, plproxy_count: {}, routine_count: {}, kpi: {}\".format(\n                    date[1], plproxy_kpi_count, routine_kpi_count, kpi_mapping_dict[kpi]\n                )\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_routine_consistency(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200702-023400_104474575","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom conf import settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nkpi_mapping_dict = {1: 'est_average_active_users', 2: 'est_average_session_per_user', 3: 'est_average_session_duration',\n                    4: 'est_install_penetration', 5: 'est_average_active_days', 6: 'est_percentage_active_days',\n                    7: 'est_average_bytes_per_user', 8: 'est_average_time_per_user', 9: 'est_usage_penetration',\n                    10: 'est_open_rate', 17: 'est_average_bytes_per_session', 23: 'est_installs', 24: 'est_share_of_users',\n                    25: 'est_share_of_installs'}\ngranularity_dict = {'daily': 'DAY', 'monthly': 'MONTH', 'weekly': 'WEEK'}\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select kpi, sum(kpi_count) from plproxy.execute_select_nestloop($proxy$ \n    select kpi, count(app_id) as kpi_count\n    from mu.app_{}\n    where \n        date = '{}'\n    group by kpi\n$proxy$) t (kpi SMALLINT, kpi_count BIGINT) group by kpi order by kpi asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 6, 20)\n    start = datetime.date(2020, 6, 14)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(granularity):\n    if granularity == 'daily':\n        collect_date = get_daily_date_list()\n    if granularity == 'weekly':\n        collect_date = get_weekly_date_list()\n    if granularity == 'monthly':\n        collect_date = get_monthly_date_list()\n    date_list = [(x[0][:7], x[0]) for x in collect_date]\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_routine_consistency(_granularity, date_list):\n    plproxy_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/'\n    routine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/'\n    for date in date_list:\n        plproxy_path_parse = plproxy_path.format(_granularity, date[1])\n        routine_path_parse = routine_path.format(_granularity, date[1])\n        plproxy_df = spark.read.format(\"delta\").load(plproxy_path).where(\"granularity='daily' and date='{}'\".format(date[1]))\n        routine_df = spark.read.format(\"delta\").load(routine_path).where(\"granularity='daily' and date='{}'\".format(date[1]))\n        for kpi in kpi_mapping_dict:\n            if kpi==1:\n                plproxy_kpi_count = plproxy_df.filter(\n                    \"{}!=0\".format(kpi_mapping_dict[kpi])).count()\n                routine_kpi_count = routine_df.filter(\n                    \"{}!=0\".format(kpi_mapping_dict[kpi])).count()\n            else:\n                plproxy_kpi_count = plproxy_df.filter(\n                    \"est_average_active_users!=0 and {}!=0\".format(kpi_mapping_dict[kpi])).count()\n                routine_kpi_count = routine_df.filter(\n                    \"est_average_active_users!=0 and {}!=0\".format(kpi_mapping_dict[kpi])).count()\n            if plproxy_kpi_count != routine_kpi_count:\n                print \"Consistency Test Fail!!!! date: {}, plproxy_count: {}, routine_count: {}, kpi: {}\".format(\n                    date[1], plproxy_kpi_count, routine_kpi_count, kpi_mapping_dict[kpi]\n                )\n            else:\n                print \"Consistency Test Pass! date: {}, plproxy_count: {}, routine_count: {}, kpi: {}\".format(\n                    date[1], plproxy_kpi_count, routine_kpi_count, kpi_mapping_dict[kpi]\n                )\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_routine_consistency(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200610-031946_884928801","metadata":{},"outputs":[],"source":["\nplproxy_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date=2020-05-17/'\nroutine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2020-05-17/'\nplproxy_df = spark.read.format(\"delta\").load(plproxy_path)\nroutine_df = spark.read.format(\"delta\").load(routine_path)\nprint plproxy_df.filter(\"est_average_active_users!=0\").count()\nprint routine_df.filter(\"est_average_active_users!=0\").count()\n# print plproxy_df.filter(\"est_average_active_users!=0\").select('app_id', 'country_code', 'device_code', 'est_average_active_users').subtract(routine_df.filter(\"est_average_active_users!=0\").select('app_id', 'country_code', 'device_code', 'est_average_active_users')).show()\nplproxy_df.filter(\"est_average_active_users!=0\").select('app_id', 'country_code', 'device_code').subtract(routine_df.filter(\"est_average_active_users!=0\").select('app_id', 'country_code', 'device_code')).show()\nroutine_df.filter(\"est_average_active_users!=0\").select('app_id', 'country_code', 'device_code').subtract(plproxy_df.select('app_id', 'country_code', 'device_code')).show()"]},{"cell_type":"code","execution_count":0,"id":"20200610-065617_56701031","metadata":{},"outputs":[],"source":["\nplproxy_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date=2020-05-17/'\nroutine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2020-05-17/'\nplproxy_df = spark.read.format(\"delta\").load(plproxy_path)\nroutine_df = spark.read.format(\"delta\").load(routine_path)\nplproxy_df.createOrReplaceTempView(\"plproxy_df\")\nroutine_df.createOrReplaceTempView(\"routine_df\")\nsql = \"\"\"\n        select\n            plproxy_df.app_id, plproxy_df.device_code, plproxy_df.country_code,\n            plproxy_df.{kpi} as v3_{kpi},\n            v5.{kpi} as v5_{kpi}\n        from plproxy_df inner join\n            (select app_id, device_code, country_code, {kpi}\n             from routine_df\n             ) AS v5\n        on (plproxy_df.app_id=v5.app_id) and (plproxy_df.device_code=v5.device_code) and (plproxy_df.country_code=v5.country_code) and (plproxy_df.{kpi}!=v5.{kpi})\n        \"\"\".format(kpi='est_average_active_users')\nunified_v1 = spark.sql(sql)\nunified_v1.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200610-075358_414637898","metadata":{},"outputs":[],"source":["\nplproxy_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date=2020-06-17/'\nroutine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date=2020-05-17/'\nplproxy_df = spark.read.format(\"delta\").load(plproxy_path)\nroutine_df = spark.read.format(\"delta\").load(routine_path)\nplproxy_df.createOrReplaceTempView(\"plproxy_df\")\nroutine_df.createOrReplaceTempView(\"routine_df\")\nkpi_mapping_dict = {1: 'est_average_active_users', 2: 'est_average_session_per_user', 3: 'est_average_session_duration',\n                    4: 'est_install_penetration', 5: 'est_average_active_days', 6: 'est_percentage_active_days',\n                    7: 'est_average_bytes_per_user', 8: 'est_average_time_per_user', 9: 'est_usage_penetration',\n                    10: 'est_open_rate', 17: 'est_average_bytes_per_session', 23: 'est_installs', 24: 'est_share_of_users',\n                    25: 'est_share_of_installs'}\nfor i,kpi in kpi_mapping_dict.items():\n    plproxy_count = plproxy_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).count()\n    routine_count =  routine_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).count()\n    subtract_count = plproxy_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).select('app_id', 'country_code', 'device_code', kpi).subtract(routine_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).select('app_id', 'country_code', 'device_code', kpi)).count()\n    print plproxy_count, routine_count, subtract_count\n    if subtract_count!=0:\n        print float(subtract_count) / plproxy_count\n    sql = \"\"\"\n            select\n                plproxy_df.app_id, plproxy_df.device_code, plproxy_df.country_code,\n                plproxy_df.{kpi} as v3_{kpi},\n                v5.{kpi} as v5_{kpi}\n            from plproxy_df inner join\n                (select app_id, device_code, country_code, {kpi}\n                 from routine_df\n                 ) AS v5\n            on (plproxy_df.app_id=v5.app_id) and (plproxy_df.device_code=v5.device_code) and (plproxy_df.country_code=v5.country_code) and (plproxy_df.{kpi}!=v5.{kpi})\n            \"\"\".format(kpi=kpi)\n    unified_v1 = spark.sql(sql)\n    # unified_v1 = unified_v1= unified_v1.withColumn('difference', unified_v1['v3_{}'.format(kpi)] - unified_v1['v5_{}'.format(kpi)])\n    unified_v1.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200610-080715_1041994394","metadata":{},"outputs":[],"source":["\nroutine_df = spark.read.parquet('s3://aardvark-prod-pdx-mdm-to-int/basic_kpi/version=1.0.0/range_type=DAY/date=2020-05-17')\n# routine_df.filter(\"app_id=20600000015411\").show()\nroutine_df.filter(\"app_id=100 and country='RO' and platform=2 and device_type=1\").select('app_id', 'device_type', 'country', 'platform', 'AU').show()"]},{"cell_type":"code","execution_count":0,"id":"20200611-101816_387611971","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 17)\n    start = datetime.date(2020, 5, 17)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndate_list = get_daily_date_list()\nfor date in date_list:\n    plproxy_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date={}/'.format(date[0])\n    routine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date={}/'.format(date[0])\n    plproxy_df = spark.read.format(\"delta\").load(plproxy_path)\n    routine_df = spark.read.format(\"delta\").load(routine_path)\n    kpi_mapping_dict = {1: 'est_average_active_users', 2: 'est_average_session_per_user', 3: 'est_average_session_duration',\n                        4: 'est_install_penetration', 5: 'est_average_active_days', 6: 'est_percentage_active_days',\n                        7: 'est_average_bytes_per_user', 8: 'est_average_time_per_user', 9: 'est_usage_penetration',\n                        10: 'est_open_rate', 17: 'est_average_bytes_per_session', 23: 'est_installs', 24: 'est_share_of_users',\n                        25: 'est_share_of_installs'}\n    for i,kpi in kpi_mapping_dict.items():\n        # plproxy_count = plproxy_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).count()\n        # routine_count =  routine_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).count()\n        plproxy_df = plproxy_df.withColumn(kpi, functions.UserDefinedFunction(lambda x: format(x, '.6f'))(plproxy_df[kpi]))\n        routine_df = routine_df.withColumn(kpi, functions.UserDefinedFunction(lambda x: format(x, '.6f'))(routine_df[kpi]))\n        plproxy_df.createOrReplaceTempView(\"plproxy_df\")\n        routine_df.createOrReplaceTempView(\"routine_df\")\n        subtract_count = plproxy_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).select('app_id', 'country_code', 'device_code', kpi).subtract(routine_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).select('app_id', 'country_code', 'device_code', kpi)).count()\n        print date[0], kpi, subtract_count\n        if subtract_count!=0:\n            print float(subtract_count) / plproxy_count\n        sql = \"\"\"\n                select\n                    plproxy_df.app_id, plproxy_df.device_code, plproxy_df.country_code,\n                    plproxy_df.{kpi} as v3_{kpi},\n                    v5.{kpi} as v5_{kpi}\n                from plproxy_df inner join\n                    (select app_id, device_code, country_code, {kpi}\n                     from routine_df\n                     ) AS v5\n                on (plproxy_df.app_id=v5.app_id) and (plproxy_df.device_code=v5.device_code) and (plproxy_df.country_code=v5.country_code) and (plproxy_df.{kpi}!=v5.{kpi})\n                \"\"\".format(kpi=kpi)\n        unified_v1 = spark.sql(sql)\n        unified_v1 = unified_v1= unified_v1.withColumn('difference', unified_v1['v3_{}'.format(kpi)] - unified_v1['v5_{}'.format(kpi)])\n        unified_v1.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200612-021001_293100244","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n\nselect count(uniqlo_id) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select max(app_id) as uniqlo_id\n    from mu.app_daily\n    where \n        date='2020-06-20'\n    group by\n    app_id,\n    store_id,\n    device_id\n\\$proxy\\$) t (uniqlo_id BIGINT);\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200709-083919_1236659266","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=usage;\nselect date, count(1) from usage_basic_kpi_fact_v6 where granularity='daily' and date between '2020-05-24' and '2020-06-20' group by date order by date asc;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200702-024008_1742997530","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\nselect device_id,store_id,date,app_id,kpi,estimate from plproxy.execute_select_nestloop(\\$proxy\\$\n    select device_id,store_id,date,app_id,kpi,estimate\n    from mu.app_daily \n    where\n        date = '2020-06-14'\n\\$proxy\\$) t (device_id SMALLINT,store_id INT,date DATE,app_id BIGINT,kpi SMALLINT, estimate FLOAT) limit 50;\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200701-075207_1767740865","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=usage;\nselect * from usage_basic_kpi_fact_v6 where granularity='daily' and date='2020-06-14' and app_id=284220417 and device_code='ios-phone' and country_code='AU';\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200612-021700_68730442","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/"]},{"cell_type":"code","execution_count":0,"id":"20200612-022542_1792547426","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql import Row\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 5, 17)\n    start = datetime.date(2020, 5, 17)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndate_list = get_daily_date_list()\nfor date in date_list:\n    print date[0]\n    plproxy_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/granularity=daily/date={}/'.format(date[0])\n    routine_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v5/fact/granularity=daily/date={}/'.format(date[0])\n    plproxy_df = spark.read.format(\"delta\").load(plproxy_path)\n    routine_df = spark.read.format(\"delta\").load(routine_path)\n    kpi_mapping_dict = {1: 'est_average_active_users', 2: 'est_average_session_per_user', 3: 'est_average_session_duration',\n                        4: 'est_install_penetration', 5: 'est_average_active_days', 6: 'est_percentage_active_days',\n                        7: 'est_average_bytes_per_user', 8: 'est_average_time_per_user', 9: 'est_usage_penetration',\n                        10: 'est_open_rate', 17: 'est_average_bytes_per_session', 23: 'est_installs', 24: 'est_share_of_users',\n                        25: 'est_share_of_installs'}\n    for i,kpi in kpi_mapping_dict.items():\n        # plproxy_count = plproxy_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).count()\n        # routine_count =  routine_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).count()\n        plproxy_df = plproxy_df.withColumn(kpi, functions.UserDefinedFunction(lambda x: format(x, '.0f'))(plproxy_df[kpi]))\n        routine_df = routine_df.withColumn(kpi, functions.UserDefinedFunction(lambda x: format(x, '.0f'))(routine_df[kpi]))\n        plproxy_df.createOrReplaceTempView(\"plproxy_df\")\n        routine_df.createOrReplaceTempView(\"routine_df\")\n        subtract_count = plproxy_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).select('app_id', 'country_code', 'device_code', kpi).subtract(routine_df.filter(\"est_average_active_users!=0 and {}!=0\".format(kpi)).select('app_id', 'country_code', 'device_code', kpi)).count()\n        print kpi, '\\t', float(subtract_count) / plproxy_count\n        if subtract_count!=0:\n            print float(subtract_count) / plproxy_count\n            sql = \"\"\"\n                select\n                    plproxy_df.app_id, plproxy_df.device_code, plproxy_df.country_code,\n                    plproxy_df.{kpi} as v3_{kpi},\n                    v5.{kpi} as v5_{kpi}\n                from plproxy_df inner join\n                    (select app_id, device_code, country_code, {kpi}\n                     from routine_df\n                     ) AS v5\n                on (plproxy_df.app_id=v5.app_id) and (plproxy_df.device_code=v5.device_code) and (plproxy_df.country_code=v5.country_code) and (plproxy_df.{kpi}!=v5.{kpi})\n                \"\"\".format(kpi=kpi)\n            unified_v1 = spark.sql(sql)\n            unified_v1 = unified_v1= unified_v1.withColumn('difference', unified_v1['v3_{}'.format(kpi)] - unified_v1['v5_{}'.format(kpi)])\n            unified_v1.show(3)"]},{"cell_type":"code","execution_count":0,"id":"20200612-074128_110204822","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select date, sum(count_a) from plproxy.execute_select_nestloop($proxy$\n    select date, count(*) as count_a\n    from mu.app_{}\n    where\n        date between '{start_day}' and '{end_day}'\n    group by date\n$proxy$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_dump_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_count_list = spark.read.parquet(dump_path_parse).groupBy(\n            'date').agg(count('app_id').alias('count')).collect()\n        dump_count_list = sorted(dump_count_list, key=lambda x: x['date'], reverse=False)\n\n        plproxy_count_list = query(aa_dsn, sql.format(\n            _granularity, start_day=month_day_list_tuple[1][0],\n            end_day=month_day_list_tuple[1][len(month_day_list_tuple[1]) - 1]))\n        for dump_count, plproxy_count in zip(dump_count_list, plproxy_count_list):\n            if dump_count['count'] != plproxy_count[1]:\n                print \"Completeness Test Fail!!!! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            else:\n                print \"Completeness Test Pass! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            test_result.append((_granularity, dump_count['date'], plproxy_count[1], dump_count['count']))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'plproxy_count', 'dump_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_plproxy_dump_count_0616/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_dump_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200616-025917_188053293","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import Row\nimport random\nfrom pyspark.sql.functions import lit, coalesce\nfrom pyspark.sql.types import DoubleType\nimport datetime\nfrom dateutil.relativedelta import relativedelta\n\ntest_result = []\ndevice_code_agg_mapping = {'android-phone': 'android-all', 'android-tablet': 'android-all',\n                           'ios-phone': 'ios-all', 'ios-tablet': 'ios-all'}\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 4, 30)\n    start = datetime.date(2020, 1, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 5, 23)\n    start = datetime.date(2020, 1, 4)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2019, 12, 31)\n    start = datetime.date(2018, 4, 1)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef check_not_empty(df, date):\n    empty_count = df.filter(\"est_average_active_users is null\").count()\n    if empty_count != 0:\n        print \"AU is Not Empty Test Fail!!! empty_count: {}, date: {}\".format(empty_count, date)\n    else:\n        print \"AU is Not Empty Test Pass! date: {}\".format(date)\n\n\ndef check_percentage_accuracy(df, date):\n    illegal_percentage_count = df.filter(\n        \"est_install_penetration>1 or est_open_rate>1 or est_percent_of_wifi_total>1 \"\n        \"or est_percentage_active_days>1 or est_share_of_category_bytes>1 or est_share_of_category_session>1 \"\n        \"or est_share_of_category_time>1 or est_share_of_installs>1 or est_share_of_users>1 \"\n        \"or est_usage_penetration>1\").count()\n    if illegal_percentage_count != 0:\n        print \"Percentage<1 Test Fail!!! illegal_percentage_count: {}, date: {}\".format(illegal_percentage_count, date)\n    else:\n        print \"Percentage<1 Test Pass! date: {}\".format(date)\n\n\ndef check_usage_unified_v1_v3_accuracy(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month in date_list:\n        unified_v1_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v1/fact/' \\\n                          'granularity={v1_granularity}/date={v1_date}/'\n        unified_v3_path = 's3://b2c-prod-data-pipeline-unified-usage/unified/usage.basic-kpi.v3/fact/' \\\n                          'granularity={v3_granularity}/date={v3_date}/'\n        sample_index = random.randint(0, len(month[1]) - 1)\n        date = month[1][sample_index]\n        unified_v1_path_parse = unified_v1_path.format(v1_granularity=_granularity, v1_date=date)\n        unified_v3_path_parse = unified_v3_path.format(v3_granularity=_granularity, v3_date=date)\n\n        unified_v1 = spark.read.parquet(unified_v1_path_parse)\n        unified_v1 = (\n            unified_v1\n            .drop(\"est_mb_per_second\", \"est_share_of_device_time\", \"est_share_of_device_session\",\n                  \"est_share_of_device_mb\", \"est_panel_size\")\n            .withColumn('est_total_time',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_time_per_user'] / 60)\n            .withColumn('est_average_time_per_user', coalesce(unified_v1['est_average_time_per_user'],\n                                                              unified_v1['est_average_session_duration'] *\n                                                              unified_v1['est_average_session_per_user']) * 1000)\n            .withColumn('est_average_session_duration', unified_v1['est_average_session_duration'] * 1000)\n        )\n        unified_v1 = (\n            unified_v1\n            .withColumn('est_average_bytes_per_session', unified_v1['est_average_bytes_per_session'] * 1024 * 1024)\n            .withColumn('est_average_bytes_per_user', unified_v1['est_average_bytes_per_user'] * 1024 * 1024)\n            .withColumn('est_share_of_category_bytes', unified_v1['est_share_of_category_bytes'] * 1024 * 1024)\n            .withColumn('est_install_base',\n                        unified_v1['est_install_penetration'] *\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumn('est_population',\n                        unified_v1['est_average_active_users'] / unified_v1['est_usage_penetration'])\n            .withColumnRenamed('est_average_active_users_country_share', 'est_share_of_users')\n            .withColumnRenamed('est_installs_country_share', 'est_share_of_installs')\n            .withColumn('est_total_sessions',\n                        unified_v1['est_average_active_users'] * unified_v1['est_average_session_per_user'])\n        )\n        unified_v1.createOrReplaceTempView(\"v1_df\")\n        unified_v1 = spark.sql(\"\"\"\n                    select\n                        v1_df.*,\n                        ww.est_average_active_users as est_average_active_users_worldwide,\n                        ww.est_installs as est_installs_worldwide\n                    from v1_df left join\n                        (select device_code, app_id, est_installs, est_average_active_users\n                         from v1_df where country_code ='WW'\n                         ) AS ww\n                    on (v1_df.app_id=ww.app_id) and (v1_df.device_code = ww.device_code)\n                    \"\"\")\n        unified_v1 = unified_v1.na.fill(0).drop('_identifier')\n\n        unified_v3 = spark.read.format(\"delta\").load(unified_v3_path_parse).drop('_identifier', 'date', 'granularity')\n\n        check_percentage_accuracy(unified_v3, date)\n        check_not_empty(unified_v3, date)\n\n        subtract_count = unified_v1.select(unified_v3.columns).subtract(unified_v3).count()\n        subtract_count_reverse = unified_v3.select(unified_v1.columns).subtract(unified_v1).count()\n        if subtract_count != 0 or subtract_count_reverse != 0:\n            print 'Accuracy Test FAIL!!!! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n        else:\n            print 'Accuracy Test PASS! subtract count: {}, date: {}'.format(\n                max(subtract_count, subtract_count_reverse), date)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_unified_v1_v3_accuracy(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200709-070242_1592837541","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/"]},{"cell_type":"code","execution_count":0,"id":"20200709-070040_1899643310","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200618-011439_1664061646","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import count\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nimport datetime as d\nimport datetime\nfrom datetime import timedelta\n\n\ntest_result = []\nPG_AA_HOSTS = [('internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com', 7432)]\nPG_AA_NAME = 'cohort'\nPG_AA_ACCESS_ID = 'app_bdp_usage_qa'\nPG_AA_SECRET_KEY = '2mHdFW6%#REu'\n\naa_dsn = (\n    \"dbname='{db}' user='{user}' password='{password}' \"\n    \"host='{host}' port='{port}'\".format(\n        db=PG_AA_NAME,\n        user=PG_AA_ACCESS_ID,\n        host=PG_AA_HOSTS[0][0],\n        password=PG_AA_SECRET_KEY,\n        port=PG_AA_HOSTS[0][1]\n    )\n)\n\nurn = Urn(\n    namespace=\"app-qa.db-check.v1\",\n    owner=\"app_qa\"\n)\n\nsql = \"\"\"select date, sum(count_a) from plproxy.execute_select_nestloop($proxy$\n    select date, count(*) as count_a\n    from mu.app_{}\n    where\n        date between '{start_day}' and '{end_day}'\n    group by date\n$proxy$) tpl (date DATE, count_a BIGINT) group by date order by date asc;\"\"\"\n\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_monthly_date_list():\n    result = []\n    end = datetime.date(2020, 04, 30)\n    start = datetime.date(2013, 01, 31)\n    while start <= end:\n        start = last_day_of_month(start)\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(months=1)\n    return result\n\n\ndef get_weekly_date_list():\n    result = []\n    end = datetime.date(2020, 05, 31)\n    start = datetime.date(2013, 01, 12)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(weeks=1)\n    return result\n\n\ndef get_daily_date_list():\n    result = []\n    end = datetime.date(2020, 06, 20)\n    start = datetime.date(2020, 05, 24)\n    while start <= end:\n        month_data_raw = datetime.datetime.strftime(start, '%Y-%m-%d')\n        result.append(Row(month_data_raw))\n        start += relativedelta(days=1)\n    return result\n\n\ndef get_path_date_list(gran):\n    date_list = {}\n    if gran == 'daily':\n        collect_date = get_daily_date_list()\n    if gran == 'weekly':\n        collect_date = get_weekly_date_list()\n    if gran == 'monthly':\n        collect_date = get_monthly_date_list()\n    for x in collect_date:\n        if x[0][:7] in date_list:\n            date_list[x[0][:7]].append(x[0])\n        else:\n            date_list[x[0][:7]] = [x[0]]\n    date_list = sorted(date_list.items(), key=lambda d: datetime.datetime.strptime(d[0] + str(-1), '%Y-%m-%d'),\n                       reverse=False)\n    return date_list\n\n\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\n\n\ndef check_usage_plproxy_dump_completeness(_granularity, date_list):\n    \"\"\"\n        date_list:\n                [(month,[day1,day2,day3])]\n        sample:\n            [('2015-12', ['2015-12-27', '2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31'])]\n    \"\"\"\n    for month_day_list_tuple in date_list:\n        dump_path = \"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/\" \\\n                    \"granularity={raw_granularity}/month={raw_month}/\"\n        dump_path_parse = dump_path.format(raw_month=month_day_list_tuple[0], raw_granularity=_granularity)\n        dump_count_list = spark.read.parquet(dump_path_parse).groupBy(\n            'date').agg(count('app_id').alias('count')).collect()\n        dump_count_list = sorted(dump_count_list, key=lambda x: x['date'], reverse=False)\n\n        plproxy_count_list = query(aa_dsn, sql.format(\n            _granularity, start_day=month_day_list_tuple[1][0],\n            end_day=month_day_list_tuple[1][len(month_day_list_tuple[1]) - 1]))\n        for dump_count, plproxy_count in zip(dump_count_list, plproxy_count_list):\n            if dump_count['count'] != plproxy_count[1]:\n                print \"Completeness Test Fail!!!! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            else:\n                print \"Completeness Test Pass! date: {}, plproxy_count: {}, dump_count: {}\".format(\n                    dump_count['date'], plproxy_count[1], dump_count['count'])\n            test_result.append((_granularity, dump_count['date'], plproxy_count[1], dump_count['count']))\n    df_write_result = spark.createDataFrame(test_result, schema=['type', 'date', 'plproxy_count', 'dump_count'])\n\n    from aadatapipelinecore.core.utils.retry import retry\n\n    def write_test_result(df_write_result):\n        df_write_result.write.format(\"delta\").save(\n            \"s3://b2c-prod-data-pipeline-qa/aa.usage/result_usage_plproxy_dump_count_0708/daily/\",\n            mode=\"append\",\n            partitionBy=[\"type\"])\n    retry(write_test_result, (df_write_result,), {}, interval=10)\n\n\ngranularity_list = [\"daily\"]\nfor granularity in granularity_list:\n    check_usage_plproxy_dump_completeness(granularity, get_path_date_list(granularity))\nprint 'pass'"]},{"cell_type":"code","execution_count":0,"id":"20200708-093450_1439975536","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\n# reload dependencies from temp\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\n# spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy"]},{"cell_type":"code","execution_count":0,"id":"20200708-093620_1353079362","metadata":{},"outputs":[],"source":["\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-usage/unified/usage.legacy-mu_app.v1/fact/granularity=daily/month=2020-05/\")\ndf.filter(\"date='2020-05-24'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200708-095402_1873372446","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}