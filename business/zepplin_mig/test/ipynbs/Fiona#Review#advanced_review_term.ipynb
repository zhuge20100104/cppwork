{"cells":[{"cell_type":"code","execution_count":0,"id":"20191022-014050_1298657998","metadata":{},"outputs":[],"source":["%md\n\n# For QA Test\n## Note book for QA Test\n"]},{"cell_type":"code","execution_count":0,"id":"20200217-104207_749254272","metadata":{},"outputs":[],"source":["\n\n    \nfrom applications.db_check_v1.common.constants import query, citus_settings\n\nimport unittest\nfrom pyspark.sql.functions import count\nimport datetime\n\ndef datetime_to_string(date_time, convert_format=\"%Y-%m-%d\"):\n    return datetime.datetime.strftime(date_time, convert_format)\n\ndef check_date_completeness():\n    numdays=329\n    start_date='2019-01-01'\n    end_date='2019-11-26'\n    base = datetime.date.today()- datetime.timedelta(days=2)\n    expected_date_list = [str(base - datetime.timedelta(days=x)) for x in range(numdays)]\n    # print expected_date_list\n    t = unittest.TestCase('run')\n\n    date_fail = dict()\n    actual_date_list = []\n    country_code_list = ['US', 'GB', 'CA', 'AU', 'IN', 'JP', 'KR']\n    language_list=[\"en\",\"ja\",\"ko\"]\n    device_code='ios-all'\n    for country_code in country_code_list:\n        print country_code\n        for language in language_list:\n            date_fail[device_code] = list()\n            sql = \"SELECT distinct date FROM  advancedreview.{} where country_code='{}' and language='{}' and device_code='ios-all' and date between '{}' and '{}'\".format(\"advancedreview_term_fact_v3\", country_code, language, start_date, end_date)\n            date_list = query(citus_settings(\"aa\"), sql)\n            for item in date_list:\n                actual_date_list.append(datetime_to_string(item[0]))\n            for date in expected_date_list:\n                if str(date) not in actual_date_list:\n                    date_fail[device_code].append(date)\n            if not date_fail[device_code]:\n                date_fail.pop(device_code)\n            t.assertFalse(date_fail, msg='{}'.format(date_fail))\n    print 'pass'\n\ncheck_date_completeness()\n"]},{"cell_type":"code","execution_count":0,"id":"20191118-093918_802819943","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132 -p 5432 -U citus_bdp_usage_qa -d aa_citus_db << EOF\nSET search_path=advancedreview;\n--EXPLAIN ANALYZE\nselect date, country_code, count(1) from advancedreview_term_fact_v3 where date between '2020-12-04' and '2020-12-05' group by date, country_code order by date, country_code desc\n--COMMIT;\nEOF \n\n"]},{"cell_type":"code","execution_count":0,"id":"20191228-020522_2070232181","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132 -p 5432 -U citus_bdp_usage_qa -d aa_citus_db << EOF\nSET search_path=advancedreview;\nselect date,country_code,count(*) from advancedreview_term_fact_v3 where date between '2019-12-24' and '2019-12-26' group by date,country_code order by date;\nEOF \n"]},{"cell_type":"code","execution_count":0,"id":"20191021-062226_1473254185","metadata":{},"outputs":[],"source":["%%sh\n# PGPASSWORD='@7EFiOthPf96' psql -h 10.2.10.132 -p 5432 -Ucitus1_datapipeline_advance -d aa_citus_db << EOF\n# SET search_path=advancedreview;\n# BEGIN;\n# SELECT  * , array_length(term_ids,1)  FROM   advancedreview_term_fact_v1 where product_id='431946152' ;\n# COMMIT;\n# EOF \n\n\n# #485624 444924\n\n# SELECT *  FROM   advancedreview_topic_fact_v1 where date='2019-07-08' and country_code='AU' limit 4;\n\n# PGPASSWORD='@7EFiOthPf96' psql -h 10.2.10.132 -p 5432 -Ucitus1_datapipeline_advance -d aa_citus_db << EOF\n# SET search_path=advancedreview;\n# BEGIN;\n# SELECT * FROM  advancedreview_topic_fact_v1 where language='ko' order by date desc limit 3; \n# COMMIT;\n# EOF\n\n# PGPASSWORD='@7EFiOthPf96' psql -h 10.2.10.132 -p 5432 -Ucitus1_datapipeline_advance -d aa_citus_db << EOF\n# SET search_path=advancedreview;\n# BEGIN;\n# SELECT count(*)  FROM   advancedreview_topic_fact_v1 where date='2019-07-02' and country_code='US' and  language='en';\n# COMMIT;\n# EOF\n#  and 0.0 > ANY(scores) limit 4;\n# SELECT review_id FROM advancedreview_term_fact_v1_p_201812 LIMIT 10;\n# SELECT DISTINCT product_id FROM advancedreview_term_fact_v1_p_201812 WHERE review_id='3596923019' LIMIT 10;\n\n\nPGPASSWORD='@7EFiOthPf96' psql -h 10.2.10.132 -p 5432 -Ucitus1_datapipeline_advance -d aa_citus_db << EOF\nSET search_path=advancedreview;\n\\timing\n\\d advancedreview_term_fact_v1_p_201812;\n\n\nBEGIN;\nEXPLAIN ANALYZE\n\nselect max(term) as term,\nROUND((pow(sum(edf), 2) * (AVG(rating) - 4.5) * AVG(LN(score)) / count(*))::numeric, 1) as fact_review_phrase_impact_score__aggr,\ncount(*) OVER() AS total\nfrom (\n    select advancedreview_term_fact_v1.review_id as review_id,\n    unnest(advancedreview_term_fact_v1.terms) as term,\n    unnest(advancedreview_term_fact_v1.term_ids) as term_id,\n    unnest(advancedreview_term_fact_v1.scores) as score,\n    unnest(advancedreview_term_fact_v1.is_extracted) as edf,\n    advancedreview_term_fact_v1.rating as rating\n    from advancedreview_term_fact_v1\n    INNER JOIN advancedreview_topic_fact_v1\n    ON advancedreview_topic_fact_v1.review_id = advancedreview_term_fact_v1.review_id\n    and advancedreview_topic_fact_v1.product_id = advancedreview_term_fact_v1.product_id\n    where\n        advancedreview_term_fact_v1.product_id = 284882215\n    AND advancedreview_term_fact_v1.country_code = 'CA'\n    AND advancedreview_topic_fact_v1.product_id = 284882215\n    AND advancedreview_topic_fact_v1.country_code = 'CA'\n    and advancedreview_topic_fact_v1.topic_ids @> ARRAY[10]::smallint[]) as sub_table\nGROUP BY term_id;\n\n\nCOMMIT;\nEOF \n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191023-091803_21870799","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\n\n# result = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/date=2019-09-11/\")\n# result.printSchema()\n# result.show()\n# row = result.collect()\n# array_result= row[0].asDict()[\"terms\"]\n# print len(array_result)\n\n# result=result.withColumn(\"record_date\", result['review_datetime'].cast(DateType()))\n# result=result.groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\"))\n# result.groupBy(\"record_date\").agg(count(\"count_review_id\").alias(\"total_review\")).show()\n\n\nresult = spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.0/platform=2/\")\n# result.filter(\"country=AU and language=en'\" ).select(\"review_id\").count().show()\n# ,result.process_date.between('2019-07-01','2019-07-15')\nx = result.filter(\"country='AU' and language='en' and ( process_date >= '2019-07-01' and  process_date <= '2019-07-15' ) \").withColumn(\"record_date\", result['review_datetime'].cast(DateType())).select(\"record_date\").distinct()\n# result.filter(\"country='AU' and language='en' and ( process_date >= '2019-07-01' and  process_date <= '2019-07-15' )\").groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\"))\n# print x # (658702)\nx.show()\n\nprint '~~~'\n# result = spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.0/platform=2/process_date=2019-07-14/country=AU/language=en/\")\n\n# result.filter(\"review_id='4458376928'\").show()\n\n# result1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/\")\n# y = result1.filter(\"country_code='AU' and language='en' and ( process_date >= '2019-07-01' and  process_date <= '2019-07-15' ) \").select(\"review_id\").distinct().count()\n# print y\n# result1.filter(\"process_date='2019-07-02' and country_code='JP' and language='en' and product_version is null \").show(30,False)\n# and (process_date >= '2019-07-01' and  process_date <= '2019-07-15')\n# review_id='4401037413'\n# result=result.withColumn(\"record_date\", result['review_datetime'].cast(DateType()))\n# result=result.groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\"))\n# result.groupBy(\"record_date\").agg(count(\"count_review_id\").alias(\"total_review\")).show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191029-092807_1459915910","metadata":{},"outputs":[],"source":["%%sh\n# SELECT *  FROM   advancedreview_topic_fact_v1 where date='2019-07-08' and country_code='AU' limit 4;\n\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132 -p 5432 -U citus_bdp_usage_qa -d aa_citus_db << EOF\nSET search_path=advancedreview;\nBEGIN;\nSELECT count(*) FROM  advancedreview_term_fact_v3 where _identifier='120200213104705259';\nCOMMIT;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20191128-103606_30165895","metadata":{},"outputs":[],"source":["\ndf1=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/process_date=2020-02-12\").show()\nprint df1\n# 54392\n# df1=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/process_date=2020-02-03\").filter(\"review_id=5458214386\")\n\n# df2=spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/process_date=2020-02-03\").filter(\"review_id='5458214386'\").show(20,False)\n\n#  5197363743 | 1331794412 | 1.9.2           | US           | en       |      1 | 2019-11-26 | {3985275,330954,16940738,452239,1794980} | {\"12 year\",\"age rating 4\",\"cheating boyfriends\",\"sex relationships\",\"sexual ads\"} | {4,9,4,4,3} | {1,1,1,1,1}  | ios-all     | 220191128034836095 |                    0\n"]},{"cell_type":"code","execution_count":0,"id":"20191119-100206_2106292121","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/process_date=2019-12-03/country=US/language=en/\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20191119-101102_985216725","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/process_date=2020-02-03/\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/process_date=2020-02-04/\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/process_date=2020-02-02/\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/process_date=2020-02-03/\naws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/process_date=2020-02-04/\n\n\n "]},{"cell_type":"code","execution_count":0,"id":"20191021-062935_1668909281","metadata":{},"outputs":[],"source":["\n\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import DateType\n\n# term_df =  spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.0/platform=2/process_date=2019-10-17/country=US/language=en\")\n# term_df.createOrReplaceTempView(\"term_table\")\n# results = spark.sql(\"select * from term_table where review_id='4956824744' \")\n\n# print type(results)\n\n# result_data=results.rdd.collect()\n# for x in result_data:\n#     print x\n    \n\n\n# term_df =  spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.0/platform=2/process_date=2019-10-17/country=US/language=en/\")\n# term_df.createOrReplaceTempView(\"term_table\")\n# # results = spark.sql(\"select term_table.term,term_table.value,term_table.is_extracted from term_table where review_id='4956824744' \")\n# results = spark.sql(\"select count(is_extracted) from term_table where review_id='4956824744' \")\n# results.show()\n# print results.collect()[0][0]\n# print results[\"is_extracted\"].values[0]\n# print type(results)\n\n# result1=results.rdd.collect()\n# for x in result_data:\n#     print x\n#     print x[\"is_extracted\"]\n\n\ndf=spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.0/platform=2/\").filter(\"review_id='4997107774'\").select(\"term\",\"value\",\"is_extracted\")\n# df=df.withColumn(\"record_date\", df['review_datetime'].cast(DateType()))\n# # result1.show()\n# df.createOrReplaceTempView(\"term_table\")\n# result1 = spark.sql(\"select term from term_table where review_id='4993373879' and record_date='2019-10-21' \")\n# result1.printSchema()\n# result1.show()\nresult1 = df.collect()\n# array_result= row[0].asDict()[\"terms\"]\n# print len(array_result)\n\n#  4993373879 |  870161082 | 6.4             | US           | en       |      1 | 2019-10-21 | {158,6759038}       | {\"latest version\",\"open check-ins\"}           | {3.25,3.5} | {1,1}        | ios-all     |                    0\n\n\n# result1=result1.collect()\n# print result1[1][0]\n# spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.0/platform=2/process_date=2019-10-17/country=US/language=en/\").filter(\"term is null\").show()\n\nresult2=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/\").filter(\"review_id='4997107774'\").select(\"terms\",\"scores\",\"is_extracted\")\n# df2.createOrReplaceTempView(\"term_table_unified\")\n# result2 = spark.sql(\"select terms from term_table_unified where review_id='4993373879' and date='2019-10-21'\")\n# result2.printSchema()\n# result2.show()\n\nresult2 = result2.collect()\n# array_result= row[0].asDict()[\"terms\"]\n\n# print len(array_result)\n\n\n# print result2[0][0]\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/date=2019-10-17/\").filter(\"product_id='407690035'\").show(20,False)\n\n\n# spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/process_date=2019-06-30/country=US/language=en/\").filter(\"review_id='4389282703'\").show()\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/date=2019-07-08/part-20190708.c000.gz.parquet\").filter(\"country_code='US' and review_id='4427349889'\").show()\n\n\ndef map_list_sequences(row_rdd1_original, row_rdd2_to_list):\n    # print row_rdd1_original\n    # print row_rdd2_to_list\n    # print \n    original_line_length=len(row_rdd1_original)\n    \n    print original_line_length\n    if 1:\n        # print 'current to_list_index is ' , to_list_index\n        range_length=len(row_rdd2_to_list[0])\n        for line in range(0,original_line_length):\n            target_value = row_rdd1_original[line][0]\n            # print 'range_length: ',range_length\n\n            for x in range(0,range_length):\n                to_list_index=row_rdd2_to_list[0][0].index(target_value)\n                print 'to list index: ', to_list_index\n\n                print row_rdd1_original[line][x]\n                print row_rdd2_to_list[0][x][to_list_index]\n\n                # print row_rdd1_original[line][x]\n                # print row_rdd2_to_list[0][x][to_list_index]\n \n                # print row_rdd1_original[line][x]\n                # print row_rdd2_to_list[0][x][to_list_index]\n\n\nmap_list_sequences(result1,result2)"]},{"cell_type":"code","execution_count":0,"id":"20191023-064945_2052808312","metadata":{},"outputs":[],"source":["_\nfrom datetime import datetime,timedelta\nfrom dateutil.relativedelta import relativedelta\n\ndef get_date_list(start_date, end_date):\n    r = []\n    start = string_to_datetime(start_date)\n    delta = get_total_days(start_date, end_date)\n    for i in range(delta):\n        r.append(datetime_to_string(start + timedelta(days=i)))\n    return r\n\n\n\ndef get_date_list_from_granularity(start_date, end_date, granularity):\n    date_list = get_date_list(start_date, end_date)\n    if granularity == 'weekly':\n        _date_list = []\n        for date in date_list:\n            if string_to_datetime(date).isoweekday() % 7 == 6:\n                _date_list.append(date)\n        date_list = _date_list\n    elif granularity == 'monthly':\n        _date_list = set()\n        for date in date_list:\n            _, last_day = get_month_start_end_date(date)\n            _date_list.add(last_day)\n        date_list = list(_date_list)\n    return date_list\n\n\ndef get_date_list_test():\n    start_date='2019-01-01'\n    end_date='2019-10-21'\n    date_list = get_date_list_from_granularity(\n    start_date, end_date, 'daily')\n    print date_list\n\ndef string_to_datetime(date_str, convert_format=None):\n    return datetime.strptime(date_str, convert_format if convert_format else \"%Y-%m-%d\")\n\n\ndef datetime_to_string(date_time, convert_format=\"%Y-%m-%d\"):\n    return datetime.strftime(date_time, convert_format)\n\n\ndef get_total_days(start_date, end_date):\n    start = string_to_datetime(start_date)\n    end = string_to_datetime(end_date)\n    delta = end - start\n    return delta.days + 1\n\ndate_time_list=get_date_list_test()\n"]},{"cell_type":"code","execution_count":0,"id":"20191021-062232_1598807548","metadata":{},"outputs":[],"source":["%%sh\n\n# aws s3 ls s3://b2c-prod-data-pipeline-raw-advancedreview/raw/app-int.advancedreview.term.v1/insert/_default_partition=2019-10/ | sort -n\n# aws s3 ls s3://b2c-prod-data-pipeline-raw-advancedreview/raw/app-int.advancedreview.topic.v1/\n\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/date=2019-10-23/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.topic.v1/fact/\n\n\n# python  ~/bdp/application/aaintdatapipeline/core/pipeline/doctor.py readavro -k raw/app-int.advancedreview.topic.v1/insert/_default_partition=2019-10/aidp6ca09cc6d5e34736fc96af1e4da34db1.avro\n\n# aws s3 ls s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/process_date=2019-12-03/country=US/\n# aws s3 ls s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/process_date=2019-12-25/country=KR/language=en/\naws s3 ls --recursive s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/process_date=2019-12-26/| grep -v \"_SUCCESS\"\n# aws s3 ls s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/process_date=2019-12-26/country=US/language=en/ \n\n "]},{"cell_type":"code","execution_count":0,"id":"20191021-062651_1447335143","metadata":{},"outputs":[],"source":["%%sh\nexport PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH\n\n# python  ~/bdp/application/aaintdatapipeline/core/pipeline/doctor.py readavro -k raw/app-int.advancedreview.topic.v1/insert/_default_partition=2019-10/aidp6ca09cc6d5e34736fc96af1e4da34db1.avro\n# python  ~/bdp/application/aaintdatapipeline/core/pipeline/doctor.py readavro -k raw/app-int.advancedreview.topic.v1/insert/_default_partition=2019-10/aidp2ba2f19739e3616c96bdfd846f7a0aa3.avro\n# python ~/bdp/application/aaintdatapipeline/core/pipeline/doctor.py listdynamoitems -t B2C-PROD-DATA-PIPELINE_key_id_mapping  -g app-int.advancedreview -id \"auto-ingest-app-int.advancedreview.term.vv1_term_travel constantly\"\n\nvar=0\nwhile read line; do\necho $var \nif [ $var -gt 289 ] \nthen\npython  ~/bdp/application/aaintdatapipeline/core/pipeline/doctor.py readavro -k raw/app-int.advancedreview.term.v1/insert/_default_partition=2019-10/$line   >> /tmp/notification_date_3\nfi\nvar=$((var + 1))\n\ndone < '/tmp/avro_file'\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191117-034204_1605757935","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-raw-advancedreview/raw/app-int.advancedreview.term.v1/insert/_default_partition=2019-12/| sort\n\n\n "]},{"cell_type":"code","execution_count":0,"id":"20191119-085250_1743908724","metadata":{},"outputs":[],"source":["%%sh\nexport PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH\npython  ~/bdp/application/aaintdatapipeline/core/pipeline/doctor.py readavro -k raw/app-int.advancedreview.term.v1/insert/_default_partition=2019-12/aidpb3a57c8c6fe625ec1ec6a51cb27dfb9a.avro\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191021-074022_2074612128","metadata":{},"outputs":[],"source":["\n\n# aws s3 ls s3://b2c-prod-advanced-review/oss/ADVRVW_TOPIC_EFFECTIVE_PREDICTIONS/version=1.0.0/platform=2/\n# spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.0/platform=2/\").show()\n\n# df = spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/\").printSchema()\n\ncolumns = ['version','platform','process_date','country','language','review_id', 'app_id', 'app_version', 'rating', 'review_datetime' ,'term','value','is_extracted' ]\nvals = [(\"1.0.0\",\"2\",\"2019-10-23\",\"AU\",\"en\",\"4996479491\",\"542090992\" ,\"3.1.3\" ,\"1\", \"2019-10-23T00:00:00.000000Z\" ,\"abc def\",\"8.333333333333334\",\"1\")]\n\n(\n    spark.createDataFrame(vals, columns)\n    .write.mode(\"append\")\n    .parquet(\n        's3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS',\n        partitionBy=['version','platform','process_date','country','language'],\n        compression=\"gzip\"\n    )\n)\n    \npath = \"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.0/platform=2/process_date=2019-10-23/country=AU/language=en\"    \n# appended = df.union(df_new)\n# appended.show()\n\n\n# newRow = spark.createDataFrame([(4,5,7)], columns)\n# appended = df.union(newRow)\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/\").filter(\"country_code='GB' and language='en' and product_id='1441365703' and review_id='4874637878'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20191028-090824_36628073","metadata":{},"outputs":[],"source":["%%sh\nexport PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH\npython  ~/bdp/application/aaintdatapipeline/core/pipeline/doctor.py readavro -k raw/app-tech.aso.keyword-trafficshare.v1/insert/_default_partition=2019-11/aidp6be578525c2e54dd11f1bc7d2dafc9aa.avro\n\n# 2019-11-15 14:58:58       2490 aidp84d3d61c90d502ce9989d0ad9d66c822.avro\n# 2019-11-17 10:10:20       2491 aidpdd88a27a39fdf65e28bd0d00267b2dcc.avro\n# 2019-11-18 01:51:57       2491 aidp6be578525c2e54dd11f1bc7d2dafc9aa.avro\n"]},{"cell_type":"code","execution_count":0,"id":"20191028-091018_944160592","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import array_contains\ndf=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/\")\n# df.printSchema()\n# df=df.withColumn(\"contain_stop_word_data\",df.select(array_contains(df.terms,\"quality\")))\ndf.groupBy(\"review_id\").agg(count(\"review_id\")).alias(\"count_review_id\").sort(\"count_review_id.count(review_id)\",ascending=False).show()\n# df.filter(\"array_contains(terms, quality)==true\").show()\n# df.filter(\"review_id='3730118274'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20191030-020834_756968040","metadata":{},"outputs":[],"source":["\nimport unittest\nraw_process_date_path = \"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.0/platform=2\" \\\n                        \"/process_date=2019-07-13/\"\nunified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview\" \\\n                            \".term.v1/\"\n                            \n\ndef check_data_in_process_date_group_by():\n    t = unittest.TestCase('run')\n    raw_result = spark.read.parquet(raw_process_date_path).filter(\"country='AU' and language='en'\")\n    raw_result = raw_result.withColumn(\"record_date\", raw_result['review_datetime'].cast(DateType()))\n    raw_result = raw_result.groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\"))\n    raw_data_row = sorted(raw_result.groupBy(\"record_date\").agg(count(\"count_review_id\").alias(\"total_review\")).collect())\n\n    unified_result = spark.read.parquet(unified_process_date_path)\n    unified_result = unified_result.withColumn(\"record_date\", unified_result['date'].cast(DateType()))\n    unified_data_row = sorted(unified_result.filter(\n    \"process_date='2019-07-13' and country_code='AU' and language='en'\").groupBy(\n    \"date\").count().collect())\n\n    range_len = len(raw_data_row)\n    print range_len\n    for x in range(0, range_len):\n        print raw_data_row[x][0]\n        print \n        t.assertEqual(raw_data_row[x][0], unified_data_row[x][0],\n                     'date is not equals {} ~ {}, debug info {} \\n {} '.format(raw_data_row[x][0],\n                                                                               unified_data_row[x][0],\n                                                                               raw_process_date_path,\n                                                                               unified_process_date_path))\n        t.assertEqual(raw_data_row[x][1], unified_data_row[x][1],\n                     'number is not equals {} ~ {}, debug info {} \\n {} '.format(raw_data_row[x][1],\n                                                                               unified_data_row[x][1],\n                                                                               raw_process_date_path,\n                                                                               unified_process_date_path))\n    print 'no error'\n                                                                                   \n                                                                                   \ncheck_data_in_process_date_group_by()"]},{"cell_type":"code","execution_count":0,"id":"20191031-055545_1146946062","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.functions import count\nfrom pyspark.sql.functions import col\n\ndef count_review_id_test_term():\n    t = unittest.TestCase('run')\n    country_list=[\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n    language_list=[\"en\",\"ja\",\"ko\"]\n\n    result = spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/\")\n    for language in language_list:\n\n        for country in country_list:\n            print language, country\n            raw_count = result.filter(\"country='{}' and language='{}' and ( process_date >= '2019-01-01' and  process_date <= '2019-11-25' )\".format(country,language)).withColumnRenamed(\"review_id\", \"raw_review_id\").withColumnRenamed(\"process_date\",\"raw_process_date\").select(\"raw_review_id\").distinct().count()\n            result1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/\")\n            unified_count = result1.filter(\"country_code='{}' and language='{}' and ( process_date >= '2019-01-01' and  process_date <= '2019-11-25' ) \".format(country,language)).select(\"review_id\").distinct().count()\n            # df=unified_count.join(raw_count, [raw_count.raw_review_id==unified_count.review_id], \"outer\")\n            # df.filter(col(\"raw_review_id\").isNull()).show() #groupBy(\"process_date\").agg({\"review_id\":\"count\"}).show()\n            print 'raw count is : ', raw_count\n            print 'unified_count is : ', unified_count\n            t.assertEqual(raw_count,unified_count, \"raw data is not equals with unifed data {} - {} \".format(raw_count,unified_count))\n    \ncount_review_id_test_term()"]},{"cell_type":"code","execution_count":0,"id":"20191127-061243_1220809670","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='@7EFiOthPf96' psql -h 10.2.10.132 -p 5432 -Ucitus1_datapipeline_advance -d aa_citus_db << EOF\nSET search_path=advancedreview;\nselect * from advancedreview_term_fact_v3 where review_id='5189725805';\nCOMMIT;\nEOF \n\n# %pyspark\n# df=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/\").filter(\"country_code='US'\").where(\" review_id='5189725805' \").orderBy(\"review_id\")\n# df.show()\n\n\nSET\n review_id  | product_id | product_version | country_code | language | rating |    date    | term_ids  |      terms      | scores | is_extracted | device_code |    _identifier     | _disable_idx_4_query \n------------+------------+-----------------+--------------+----------+--------+------------+-----------+-----------------+--------+--------------+-------------+--------------------+----------------------\n 5189725805 | 1016814119 |                 | US           | en       |      5 | 2019-11-24 | {4274061} | {\"cute mascot\"} | {4}    | {1}          | ios-all     | 220191128034836095 |                    0\n(1 row)\n\n\n+------------------+------------+----------+-----------+--------------------+------------+--------+----------+---------------+------+----------+------+---------+-------------+------------+\n|       _identifier|country_code|      date|device_code|              doc_id|is_extracted|language|product_id|product_version|rating| review_id|scores| term_ids|        terms|process_date|\n+------------------+------------+----------+-----------+--------------------+------------+--------+----------+---------------+------+----------+------+---------+-------------+------------+\n|220191128034836095|          US|2019-11-24|    ios-all|2019-11-25-518972...|         [1]|      en|1016814119|           null|     5|5189725805| [4.0]|[4274061]|[cute mascot]|  2019-11-25|\n+------------------+------------+----------+-----------+--------------------+------------+--------+----------+---------------+------+----------+------+---------+-------------+------------+\n\n\n# %pyspark\n# from pyspark.sql.functions import array_contains\n# df=spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/\").filter(\"review_id='5189725805'\").show()\n\n+------------------+------------+----------+-----------+--------------------+------------+--------+----------+---------------+------+----------+------+---------+-------------+------------+\n|       _identifier|country_code|      date|device_code|              doc_id|is_extracted|language|product_id|product_version|rating| review_id|scores| term_ids|        terms|process_date|\n+------------------+------------+----------+-----------+--------------------+------------+--------+----------+---------------+------+----------+------+---------+-------------+------------+\n|220191128034836095|          US|2019-11-24|    ios-all|2019-11-25-518972...|         [1]|      en|1016814119|           null|     5|5189725805| [4.0]|[4274061]|[cute mascot]|  2019-11-25|\n+------------------+------------+----------+-----------+--------------------+------------+--------+----------+---------------+------+----------+------+---------+-------------+------------+\n\n"]},{"cell_type":"code","execution_count":0,"id":"20191024-091246_1981987573","metadata":{},"outputs":[],"source":["%%sh\nexport PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH\npython ~/bdp/application/aaintdatapipeline/core/pipeline/doctor.py listdynamoitems -t B2C-PROD-DATA-PIPELINE_key_id_mapping  -g app-int.advancedreview -id \"auto-ingest-app-int.advancedreview.term.vv1_term_cute mascot\"\n"]},{"cell_type":"code","execution_count":0,"id":"20191030-044433_56686705","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\nfrom datetime import datetime\n\ntest_date = ['2019-08-0'+ str(a) for a in xrange(1,10)] + ['2019-08-1'+str(a) for a in xrange(0,10)] +  ['2019-08-2'+str(a) for a in xrange(0,10)]\n\n                            \ndef check_data_in_process_date_group_by(pd):\n    print pd\n    raw_process_date_path = \"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2\" \\\n                        \"/process_date={}/\".format(pd)\n    unified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview\" \\\n                            \".term.v1/fact/process_date={}/\".format(pd)\n\n    t = unittest.TestCase('run')\n    raw_result = spark.read.parquet(raw_process_date_path).filter(\"country='US' and language='en'\")\n    raw_result = raw_result.withColumn(\"record_date\", raw_result['review_datetime'].cast(DateType()))\n    raw_result = raw_result.groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\"))\n    raw_data_row = raw_result.groupBy(\"record_date\", \"review_id\").agg(count(\"review_id\").alias(\"count_review_id\")).collect()\n\n    # raw_data_row = raw_result.groupBy(\"record_date\").agg(count(\"count_review_id\").alias(\"total_review\")).collect()\n\n    unified_result = spark.read.parquet(unified_process_date_path)\n    unified_result = unified_result.withColumn(\"record_date\", unified_result['date'].cast(DateType()))\n    unified_data_row = unified_result.filter(\n    \"country_code='US' and language='en'\".format(pd)).collect()\n\n    range_len = len(raw_data_row)\n    range_len_unified=len(unified_data_row)\n    review_id_list = []\n    for x in range(0,range_len):\n        # print raw_data_row[x][1]\n        review_id_list.append(raw_data_row[x][1])\n    print len(review_id_list)\n\n    print '~~~~~~~~~~~'\n    review_id_list_unified = []\n    print range_len_unified\n    for x in range(0,range_len_unified):\n        review_id_list_unified.append(unified_data_row[x][10])\n    print len(review_id_list_unified)\n    l= [x for x  in review_id_list if x not in review_id_list_unified]\n    print '!!!', len(l) , \"different is : \" , l \n    for x in l:\n        row=unified_result.filter(\"review_id={}\".format(x)).select(\"process_date\").collect()[0]\n        _date = datetime.strptime(pd, \"%Y-%m-%d\").date()\n        if(row[0]<_date):\n            print 'compare current process date' , pd , ' ~~~ unified process date',  row\n         \n    print 'no error'\n                                                                                   \nfor x in test_date:\n    check_data_in_process_date_group_by(x)"]},{"cell_type":"code","execution_count":0,"id":"20191030-021013_1751858826","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\n\nunified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v2/fact/\"\ndef check_rating_value():\n    t = unittest.TestCase('run')\n    unified_result = spark.read.parquet(unified_process_date_path)\n    unified_result = unified_result.withColumn(\"record_date\", unified_result['date'].cast(DateType()))\n    unified_data_row = unified_result.filter(\n        \"process_date='2020-02-03' and country_code='US' and language='en'\").collect()\n\n    range_len = len(unified_data_row)\n    for x in range(0, range_len):\n        array_result= unified_data_row[x].asDict()[\"rating\"]\n        t.assertTrue(1<=array_result<=5, \"rating is not correct\")\n    print 'pass'\n        \ncheck_rating_value()\n"]},{"cell_type":"code","execution_count":0,"id":"20191030-084735_1591294699","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\n\nunified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v2/fact/\"\ndef check_score_is_larger_than_zero():\n    unittest.TextTestRunner(verbosity=2)\n    t = unittest.TestCase('run')\n    unified_result = spark.read.parquet(unified_process_date_path)\n    unified_result = unified_result.withColumn(\"record_date\", unified_result['date'].cast(DateType()))\n    unified_data_row = unified_result.filter(\n        \"process_date='2020-02-03' and country_code='US' and language='en'\").collect()\n\n    range_len = len(unified_data_row)\n    for x in range(0, range_len):\n        array_result= unified_data_row[x].asDict()[\"scores\"]\n        for i in array_result:\n            t.assertTrue(i>0, \"score is not correct\")\n    print 'pass'\ncheck_score_is_larger_than_zero()\n"]},{"cell_type":"code","execution_count":0,"id":"20200206-025821_826763444","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/"]},{"cell_type":"code","execution_count":0,"id":"20191106-024732_1825061428","metadata":{},"outputs":[],"source":["\n# from aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n#     get_aso_citus_connection)\n# from aaintdatapipeline.application.app_qa.data_validation_v1.constants import urn\n# from aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n#     get_citus_connection)\nimport unittest\nfrom pyspark.sql.functions import count\n\n\ndef test_count():\n    t = unittest.TestCase('run')\n\n    \n    sql=\"SELECT distinct count(*) FROM  advancedreview.advancedreview_term_fact_v3  where device_code='ios-all' and country_code='{}' and language='{}' ;\"\n\n    country_list=[\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n    language_list=[\"en\",\"ja\",\"ko\"]\n\n    result = spark.read.parquet(\"s3://b2c-prod-advanced-review/oss/ADVRVW_RELEVANT_TERMS/version=1.0.1/platform=2/\")\n    for language in language_list:\n\n        for country in country_list:\n            print language, country\n            raw_count = result.filter(\" process_date ='2020-02-03' \").filter(\"country='{}' and language='{}' \".format(country,language)).select(\"review_id\").distinct().count()\n            # db_result = get_citus_connection(urn, sql.format(country,language))\n            unified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/\"\n            unified_data_count=spark.read.parquet(unified_process_date_path).filter(\"process_date='2020-02-03'\").filter(\"country_code='{}' and language='{}' \".format(country,language)).select(\"review_id\").distinct().count()\n            print 'raw count is : ', raw_count\n            # print 'db is : ', db_result[0][0]\n            print 'unified data' ,unified_data_count\n            t.assertEqual(raw_count,unified_data_count, \"raw data is not equals with unifed data {} - {} \".format(raw_count,unified_data_count))\n            # t.assertEqual(raw_count,db_result[0][0], \"raw data is not equals with DB data {} - {} \".format(raw_count,db_result[0][0]))\n\n\n\ntest_count()"]},{"cell_type":"code","execution_count":0,"id":"20191031-060214_1757587240","metadata":{},"outputs":[],"source":["\nimport unittest\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import count, avg\n\nunified_process_date_path = \"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/advancedreview.term.v1/fact/\"\ndef check_is_extracted_is_zero_or_one():\n    unittest.TextTestRunner(verbosity=2)\n    t = unittest.TestCase('run')\n    unified_result = spark.read.parquet(unified_process_date_path)\n    unified_result = unified_result.withColumn(\"record_date\", unified_result['date'].cast(DateType()))\n    unified_data_row = unified_result.filter(\n        \"process_date='2020-02-03' and country_code='US' and language='en'\").collect()\n\n    range_len = len(unified_data_row)\n    for x in range(0, range_len):\n        array_result= unified_data_row[x].asDict()[\"is_extracted\"]\n        for i in array_result:\n            t.assertTrue(i in [0 ,1], \"is_extracted value is not correct \")\n    print 'pass'\ncheck_is_extracted_is_zero_or_one()\n"]},{"cell_type":"code","execution_count":0,"id":"20191119-080605_796499939","metadata":{},"outputs":[],"source":["\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_aso_citus_connection)\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.constants import urn\nfrom aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n    get_citus_connection)\nimport unittest\nfrom pyspark.sql.functions import count\n\ndef test_count_for_term():\n    t = unittest.TestCase('run')\n    country_list=[\"US\", \"GB\", \"CA\", \"AU\", \"IN\", \"JP\", \"KR\"]\n    language_list=[\"en\",\"ja\",\"ko\"]\n    sql=\"SELECT count(distinct review_id) FROM  advancedreview.advancedreview_term_fact_v3  where country_code='{}' and language='{}' and device_code='ios-all';\"\n    raw_path=\"s3://b2c-prod-data-pipeline-unified-advancedreview/unified/app-int.advancedreview.term.v1/fact/\"\n    for language in language_list:\n        for country in country_list:\n            result = spark.read.parquet(raw_path).filter(\"country_code='{}' and language='{}'\".format(country,language)).select(\"review_id\").distinct().count()\n            print language, country\n            db_result = get_citus_connection(urn, sql.format(country,language))\n            print 'unified count is : ', result\n            print 'db is : ', db_result[0][0]\n            t.assertEqual(result,db_result[0][0], \"unified data is not equals with unifed data {} - {} \".format(result,db_result[0][0]))\n\n\ntest_count_for_term()"]},{"cell_type":"code","execution_count":0,"id":"20191119-075821_66941503","metadata":{},"outputs":[],"source":["\n# from aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n#     get_aso_citus_connection)\n# from aaintdatapipeline.application.app_qa.data_validation_v1.constants import urn\n# from aaintdatapipeline.application.app_qa.data_validation_v1.utils import (\n#     get_citus_connection)\n    \nfrom applications.db_check_v1.common.constants import query, citus_settings\n\nimport unittest\nfrom pyspark.sql.functions import count\nimport datetime\n\ndef datetime_to_string(date_time, convert_format=\"%Y-%m-%d\"):\n    return datetime.datetime.strftime(date_time, convert_format)\n\ndef check_date_completeness():\n    numdays=329\n    start_date='2019-01-01'\n    end_date='2019-11-26'\n    base = datetime.date.today()- datetime.timedelta(days=2)\n    expected_date_list = [str(base - datetime.timedelta(days=x)) for x in range(numdays)]\n    # print expected_date_list\n    t = unittest.TestCase('run')\n\n    date_fail = dict()\n    actual_date_list = []\n    country_code_list = ['US', 'GB', 'CA', 'AU', 'IN', 'JP', 'KR']\n    language_list=[\"en\",\"ja\",\"ko\"]\n    device_code='ios-all'\n    for country_code in country_code_list:\n        print country_code\n        for language in language_list:\n            date_fail[device_code] = list()\n            sql = \"SELECT distinct date FROM  advancedreview.{} where country_code='{}' and language='{}' and device_code='ios-all' and date between '{}' and '{}'\".format(\"advancedreview_term_fact_v3\", country_code, language, start_date, end_date)\n            date_list = query(citus_settings(\"aa\"), sql)\n            for item in date_list:\n                actual_date_list.append(datetime_to_string(item[0]))\n            for date in expected_date_list:\n                if str(date) not in actual_date_list:\n                    date_fail[device_code].append(date)\n            if not date_fail[device_code]:\n                date_fail.pop(device_code)\n            t.assertFalse(date_fail, msg='{}'.format(date_fail))\n    print 'pass'\n\ncheck_date_completeness()\n"]},{"cell_type":"code","execution_count":0,"id":"20200214-013308_1758202522","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}