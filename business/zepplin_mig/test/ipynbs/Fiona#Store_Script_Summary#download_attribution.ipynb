{"cells":[{"cell_type":"code","execution_count":0,"id":"20200621-082345_1025659131","metadata":{},"outputs":[],"source":["\nimport datetime\nstart = \"2016-08-28\"\nend = \"2020-03-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append(str(real_date1 + datetime.timedelta(days)))\n\nfor x in sar_list:\n    print x\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200621-082657_576904185","metadata":{},"outputs":[],"source":["\nimport datetime\nstart = \"2016-08-28\"\nend = \"2020-03-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\ndates.sort(reverse=True)\n"]},{"cell_type":"code","execution_count":0,"id":"20200621-082707_276971542","metadata":{},"outputs":[],"source":["%%sh\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3\n\n\n\n\n\n\n\n# aws s3 ls s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2020-04-04/AU/\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v4/fact/granularity=daily/date=\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/download-attribution.v1/fact/month=2020-07/\n"]},{"cell_type":"code","execution_count":0,"id":"20200728-011141_1381737594","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2020-06-13/US/\").where(\"product_id=284882215\").show()\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/download-attribution.v1/fact/month=2020-06/date=2020-06-13/device_id=2001/store_id=143441/\").where(\"app_id=284882215\").show()\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v4/fact/granularity=daily/date=2020-07-02/\").where(\"country_code='US' and device_code='ios-phone'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200728-013432_1697230977","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nselect app_id, store_id, device_id, est_organic_download_share from plproxy.execute_select_nestloop(\\$proxy\\$ \nselect distinct app_id,store_id, device_id, est_organic_download_share\n    from da.app_da_daily_estimate_2001 where date = '2020-03-28' and app_id = 284882215 and store_id = 143441\n     limit 5 \\$proxy\\$) tbl \n      (app_id BIGINT, store_id INT, device_id SMALLINT, est_organic_download_share FLOAT )  ;\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200704-065803_1059237091","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log.v1/fact/granularity=daily/date=2020-01-01/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/granularity=weekly/ --recursive\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-category-load.v3/fact/granularity=daily/\n"]},{"cell_type":"code","execution_count":0,"id":"20200817-140004_547852318","metadata":{},"outputs":[],"source":["\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-category-load.v3/fact/\").createOrReplaceTempView(\"temp\")\nspark.sql('''\n            SELECT Sum(est_paid_download)                                  AS total_est_paid_download\n        FROM   (\n                        SELECT   Max(est_paid_download) AS est_paid_download,\n                                 device_code,\n                                 country_code,\n                                 date,\n                                 app_id\n                        FROM     temp\n                        WHERE    granularity = 'daily'\n                        AND      date between '2020-08-02' and '2020-08-03'\n                        GROUP BY device_code,\n                                 country_code,\n                                 date,\n                                 app_id) AS prod\n\n    \n   ''' ).show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200818-021830_2070273602","metadata":{},"outputs":[],"source":["\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log-pre-aggr-pubrange.v1/fact/\").createOrReplaceTempView(\"temp\")\nspark.sql('''\n            SELECT Sum(est_paid_download)                                  AS total_est_paid_download,\n                    Sum(est_free_app_download)                                  AS est_free_app_download,\n                    Sum(est_paid_app_download)                                  AS est_paid_app_download,\n                    Sum(est_revenue)                                  AS est_revenue,\n                    Sum(est_organic_download)                                  AS est_organic_download\n            \n        FROM   (\n                        SELECT   Max(est_paid_download) AS est_paid_download,\n                                Max(est_free_app_download) AS est_free_app_download,\n                                 Max(est_paid_app_download) AS est_paid_app_download,\n                                Max(est_revenue) AS est_revenue,\n                                 Max(est_organic_download) AS est_organic_download,\n\n                                 device_code,\n                                 country_code,\n                                 date,\n                                 app_id\n                        FROM     temp\n                        WHERE    granularity = 'weekly'\n                        AND      month = '2019-10'\n                        AND       pub_range=999\n                        AND       device_code='ios-phone'\n                        GROUP BY device_code,\n                                 country_code,\n                                 date,\n                                 app_id) AS prod\n\n    \n   ''' ).show()\n\n\n\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log-pre-aggr-pubrange.v2/fact/\").createOrReplaceTempView(\"temp_2\")\nspark.sql('''\n            SELECT Sum(est_paid_download)                                  AS total_est_paid_download,\n                    Sum(est_free_app_download)                                  AS est_free_app_download,\n                    Sum(est_paid_app_download)                                  AS est_paid_app_download,\n                    Sum(est_revenue)                                  AS est_revenue,\n                    Sum(est_organic_download)                                  AS est_organic_download\n        FROM   (\n                        SELECT   Max(est_paid_download) AS est_paid_download,\n                                Max(est_free_app_download) AS est_free_app_download,\n                                 Max(est_paid_app_download) AS est_paid_app_download,\n                                Max(est_revenue) AS est_revenue,\n                                 Max(est_organic_download) AS est_organic_download,\n                                 device_code,\n                                 country_code,\n                                 date,\n                                 app_id\n                        FROM     temp_2\n                        WHERE    granularity = 'weekly'\n                        AND      month = '2019-10'\n                        AND       pub_range=999\n                        AND       device_code='ios-phone'\n                        GROUP BY device_code,\n                                 country_code,\n                                 date,\n                                 app_id) AS prod\n\n    \n   ''' ).show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200818-052930_370660256","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/granularity=weekly/\n \n "]},{"cell_type":"code","execution_count":0,"id":"20200818-052908_343252176","metadata":{},"outputs":[],"source":["\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/\").createOrReplaceTempView(\"temp_agg\")\nspark.sql(\"select * from temp_agg where granularity='weekly' and date between '2019-09-28' and '2019-11-02' and app_id = 1459475003 and date='2019-10-05' and country_code='KR' order by date asc\").show()\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200818-051251_1890382029","metadata":{},"outputs":[],"source":["\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log-pre-aggr-pubrange.v1/fact/\").createOrReplaceTempView(\"temp_1\")\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log-pre-aggr-pubrange.v2/fact/\").createOrReplaceTempView(\"temp_2\")\n\nprint spark.sql(\"select distinct app_id from temp_1 WHERE granularity = 'weekly' AND month = '2019-10' except all select distinct app_id from temp_2 WHERE granularity = 'weekly'  AND  month = '2019-10'\").collect()\nprint spark.sql(\"select distinct app_id from temp_2 WHERE granularity = 'weekly' AND month = '2019-10' except all select distinct app_id from temp_1 WHERE granularity = 'weekly'  AND  month = '2019-10'\").collect()\n\n# spark.sql(''' select * from temp_1   WHERE granularity = 'weekly'\n#                         AND      month = '2019-10'\n#                         AND       pub_range=999\n#                         AND       device_code='ios-phone'\n#                         AND app_id = 1440025446\n                        \n#  ''').show()\n \n \n \n# spark.sql(''' select * from temp_2   WHERE    granularity = 'weekly'\n#                         AND      month = '2019-10'\n#                         AND       pub_range=999\n#                         AND       device_code='ios-phone'\n#                         AND app_id = 1440025446\n\n#  ''').show()"]},{"cell_type":"code","execution_count":0,"id":"20200818-061823_1052685553","metadata":{},"outputs":[],"source":["\n# 1459475003, 20600011643145, 20600012536233, 1300096411, 20600012556110, 1457824642, 1306783602, 1476812245\nspark.sql(\"select * from temp_2 where granularity = 'weekly'  AND  month = '2019-10' and app_id in (1459475003 )  \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200818-052333_772532591","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-update.v1/fact/granularity=weekly/year=2019/_update_date=2020-07-07/\").createOrReplaceTempView(\"pub\")\nspark.sql(\"select distinct app_id , publisher_id_mod from ( select *, mod(publisher_id, 1000) AS publisher_id_mod from pub ) \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200704-082807_1037593606","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log-pubrange.v1/fact/granularity=daily/month=2020-08/pub_range=252/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log-pre-aggr-pubrange.v1/fact/granularity=weekly/month=2019-10/pub_range=103/device_code=ios-phone/\n \n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log-pre-aggr-pubrange.v2/fact/granularity=weekly/month=2019-10/pub_range=103/device_code=ios-phone/\n\n \n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date=2020-05\n"]},{"cell_type":"code","execution_count":0,"id":"20200818-050930_1693676612","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-update.v1/fact/granularity=daily/year=2019/_update_date=2020-07-07/\n\n \n \n"]},{"cell_type":"code","execution_count":0,"id":"20200704-084454_71632523","metadata":{},"outputs":[],"source":["\n\n\ndf_daily = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date=2020-05-{24,25,26,27,28,29,30}\").cache()\n# df_daily = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date=2020-03*/\").cache()\ndf_daily.createOrReplaceTempView(\"dr_daily\")\ndf_weekly = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/granularity=weekly/date=2020-05-30/\").cache()\ndf_weekly.createOrReplaceTempView(\"dr_weekly\")\n\nspark.sql(\"select app_id, country_code, device_code, publisher_id, company_id, parent_company_id , sum(est_free_app_download) as est_free_app_download, sum(est_paid_app_download) as est_paid_app_download, sum(est_revenue) as est_revenue, sum(est_paid_download) as est_paid_download, sum(est_organic_download) as est_organic_download from dr_daily group by app_id, country_code, device_code,  publisher_id, company_id, parent_company_id except select app_id, country_code, device_code, publisher_id, company_id, parent_company_id, est_free_app_download, est_paid_app_download, est_revenue, est_paid_download, est_organic_download  from dr_weekly\").show()\nspark.sql(\"select app_id, country_code, device_code, publisher_id, company_id, parent_company_id, est_free_app_download, est_paid_app_download, est_revenue, est_paid_download, est_organic_download from dr_weekly except select app_id, country_code, device_code, publisher_id, company_id, parent_company_id, sum(est_free_app_download) as est_free_app_download, sum(est_paid_app_download) as est_paid_app_download, sum(est_revenue) as est_revenue, sum(est_paid_download) as est_paid_download, sum(est_organic_download) as est_organic_download from dr_daily group by app_id, country_code, device_code,  publisher_id, company_id, parent_company_id\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200704-065723_277654062","metadata":{},"outputs":[],"source":["\n\n\ndaily_est = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log.v1/fact/granularity=daily/date=2020-02*/\").cache()\ndaily_est.createOrReplaceTempView(\"daily_est\")\n# spark.sql(\"select count(1), sum(est_paid_download), sum(est_organic_download), sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue) from daily_est\").show()\n\n\n# daily = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log.v1/fact/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log.v1/fact/granularity=daily/date=2020-03*/\").cache()\n# daily.createOrReplaceTempView(\"daily\")\n\n# spark.sql(\"select count(1), sum(est_paid_download), sum(est_organic_download), sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue) from daily\").show()\n\n\n# daily_download_attr_preload = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='monthly' and date between '2020-03-01' and '2020-03-31'\").cache()\n# daily_download_attr_preload.createOrReplaceTempView(\"daily_download_attr_preload\")\n\n# spark.sql(\"select count(1), sum(est_paid_download), sum(est_organic_download), sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue) from daily_download_attr_preload\").show()\n\n\n# monthly = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log-pre-aggr.v1/fact/granularity=monthly/date=2020-02-29/\").cache()\n# monthly.createOrReplaceTempView(\"pre_agg\")\n# spark.sql(\"select count(1), sum(est_paid_download), sum(est_organic_download), sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue) from pre_agg \").show()\n# 20600012931082\n# 20600010012490\n\ndd = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-dna-log-pre-aggr.v1/fact/granularity=monthly/date=2020-02-29\").cache()\ndd.createOrReplaceTempView(\"pre_agg_est\")\nspark.sql(\"select * from  pre_agg_est  where app_id = 20600012931082 and country_code='US'\").show()\nspark.sql(\"select * from  pre_agg  where app_id = 20600012931082 and country_code='US' \").show()\nspark.sql(\"select * from daily_est where app_id = 20600012931082 and country_code='US' order by date desc\").show(2000)\n\n+------------------+--------------+----------------+----------+-----------------+--------------+----------+\n|       _identifier|        app_id|      company_id|  end_date|parent_company_id|  publisher_id|start_date|\n+------------------+--------------+----------------+----------+-----------------+--------------+----------+\n|120200630134627330|20600012931082|1000200000103558|2015-09-15|                0|20200001825692|1970-01-01|\n|120200630134627330|20600012931082|1000200000103558|2020-02-07|                0|20200001825692|2015-09-16|\n|120200630134627330|20600012931082|               0|2020-02-15|                0|20200003444285|2020-02-08|\n|120200630134627330|20600012931082|               0|2020-02-25|                0|20200003452891|2020-02-16|\n|120200630134627330|20600012931082|1000200000449440|2020-03-18| 1000200000044132|20200003465209|2020-02-26|\n|120200630134627330|20600012931082|1000200000449440|      null| 1000200000044132|20200003490885|2020-03-19|\n+------------------+--------------+----------------+----------+-----------------+--------------+----------+\n"]},{"cell_type":"code","execution_count":0,"id":"20200704-080424_1806947570","metadata":{},"outputs":[],"source":["\nspark.sql(\"select count (distinct app_id) from daily_est\").show()\n\nspark.sql(\"select count (distinct app_id) from daily\").show()\nspark.sql(\"select * from pre_agg\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200704-080023_1162618558","metadata":{},"outputs":[],"source":["\nspark.sql(\"select app_id, count(app_id) as count_app, publisher_id, company_id from pre_agg where country_code='AU' and device_code='ios-phone' group by app_id, publisher_id, company_id, parent_company_id, device_code order by count_app desc \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200704-074538_1932061334","metadata":{},"outputs":[],"source":["\nspark.sql(\"select sum(est_paid_download), sum(est_organic_download), sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue)  from daily_est where app_id = 284862083 and country_code='US' and device_code='ios-phone'  \").show(50)\n\n\nspark.sql(\"select * from daily where app_id = 284862083 and country_code='AU' and device_code='ios-phone' order by date \").show()\nspark.sql(\"select * from pre_agg where app_id = 284862083 and country_code='AU' and device_code='ios-phone'  \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200704-070446_457152372","metadata":{},"outputs":[],"source":["\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-dna-log.v1/fact/granularity=daily/date=2020-03-01/\").createOrReplaceTempView(\"temp\")\n# spark.sql(\"select count(*) from temp\").show()\nspark.sql(\"select * from temp where app_id = 20600012931082 and country_code in ( 'US') \").show()\n\nspark.sql(\"select * from daily_est where app_id = 20600012931082 and country_code in ('US')  \").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200704-075833_414187658","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\ndf_dna = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.mapping-log.v1/dimension/update_date=2020-06-30/\").cache()\ndf_event = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-dna/unified/dna.app-publisher-event.v1/dimension/update_date=2020-06-30/\").cache()\n\ndf_dna.createOrReplaceTempView(\"dna_logging\")\nspark.sql(\"select * from dna_logging where app_id=20600012931082 \").show(10)\n"]},{"cell_type":"code","execution_count":0,"id":"20200621-082924_230806949","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\n\ndate='2020-02-23'\n\ndef plproxy_row(date):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n\n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n    sql = '''select app_id from plproxy.execute_select_nestloop($$ \n                select app_id from (\n                    select distinct app_id,store_id ,device_id, date, est_organic_download_share\n                        from da.app_da_daily_estimate_1001 where date='{}' and store_id=10 )\n                        as prod $$) tbl \n              (app_id bigint ) order by app_id  ;\n            '''.format(\n            date\n    )\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [Row(app_id=r[0]) for r in rows]\n\n\ndef citus_row(date):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select app_id from store.store_est_fact_v6 where date = '{}' and country_code='US' and est_paid_download !=0  and device_code='android-all' order by app_id\".format(\n            date)\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n\n    result = get_data_in_citus(date)\n    return [Row(app_id=r[0]) for r in result]\n\n\nplproxy_result = plproxy_row(date)\ncitus_result = citus_row(date)\n\ndf_plproxy = spark.createDataFrame(plproxy_result).cache()\ndf_plproxy.createOrReplaceTempView(\"temp_plproxy\")\n\ncitus_df = spark.createDataFrame(citus_result).cache()\ncitus_df.createOrReplaceTempView(\"temp_citus\")\nspark.sql(\"select * from temp_citus except select * from temp_plproxy \").show()\ndiff_df_list = spark.sql(\"select * from temp_plproxy except select * from temp_citus \").collect()\nprint 'not equal count', len(diff_df_list)\ndiff_list = [str(x[0]) for x in diff_df_list]\ndf = spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={}\".format(date)).cache()\ndf.createOrReplaceTempView(\"temp\")\nspark.sql(\"select * from temp where id in ({}) and store_id=10 and feed in (0,1)\".format(\",\".join(diff_list))).show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200621-083241_391237855","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nselect distinct app_id,store_id,device_id, date from plproxy.execute_select_nestloop(\\$proxy\\$ \nselect  distinct app_id,store_id ,device_id, date\n    from da.app_da_daily_estimate_1001 where date='2020-01-01' and app_id = 20600003289877 and store_id=10\n     limit 5 \\$proxy\\$) tbl \n      (app_id BIGINT, store_id INT, device_id SMALLINT, date Date )  ;\n\nEOF\n\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# select sum(cnt) as sum from plproxy.execute_select_nestloop(\\$proxy\\$ \n# select count(1) as cnt from (\n# select distinct app_id,store_id ,device_id, date, est_organic_download_share\n#     from da.app_da_daily_estimate_1001 where ( date  between '2020-01-01' and '2020-01-01') and store_id=5 )\n#     as prod\n#  \\$proxy\\$) tbl \n#       (cnt bigint )  ;\n\n# EOF\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# select app_id from plproxy.execute_select_nestloop(\\$proxy\\$ \n# select app_id from (\n# select distinct app_id,store_id ,device_id, date, est_organic_download_share\n#     from da.app_da_daily_estimate_1001 where ( date  between '2020-01-01' and '2020-01-01') and store_id=5 )\n#     as prod\n#  \\$proxy\\$) tbl \n#       (app_id bigint ) order by app_id  ;\n\n# EOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200624-073729_702560066","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n# a week data\nstart = \"2018-03-28\"\nend = \"2018-04-01\"\n\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\n# for x in sar_list:\n#     for key,item in x.items():\n#         test_path.append(\n#             (\n#                 [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n#                 [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v2/fact/granularity=daily/date={}\".format(i) for i in item], \n#                 [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n#             )\n#         )\n        \nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append(\n            (\n                [\"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/{}/*/\".format(key)] , \n                [i.strftime(\"%Y-%m-%d\") for i in item], \n                [\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}\".format(i) for i in item ]\n            )\n        )\nprint test_path\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n\n\n# [(['s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2020-01-11/*/'], ['2020-01-11', '2020-01-10', '2020-01-09', '2020-01-08', '2020-01-07', '2020-01-06', '2020-01-05'], ['s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-11', 's3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-10', 's3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-09', 's3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-08', 's3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-07', 's3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-06', 's3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-01-05'])]\n\n\ndef test_download_attribution(test_data):\n    print test_data\n    spark.read.option(\"basePath\",\n                      \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/\").parquet(\n        test_data[0][0]).createOrReplaceTempView(\n        \"download_attribution\")\n\n    # spark.read.option(\"basePath\",\n    #                   \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily\").parquet(\n    #     \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}\" % \",\".join(test_path[0][1])).createOrReplaceTempView(\n    #     \"store_unified\")\n\n    # spark.read.option(\"basePath\",\n    #                   \"s3://b2c-mktint-prod-dca-kpi/download_attribution/week_and_month_routine/v1.0.0/WEEK/2016-09-03/\").parquet(\n    #     test_data[0][0]).createOrReplaceTempView(\n    #     \"download_attribution\")\n    \n    print test_data[1]\n    spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\"granularity='daily' and  date in ('{}') \".format(\"','\".join(\n        test_data[1]))).createOrReplaceTempView(\"test_unified_download_attribution\")\n\n    sql_text = \"\"\"\n    \n    WITH download_attribution AS (\n        select *, CAST(est_non_organic_download_share as decimal(36,20)) as new_est_non_organic_download_share from download_attribution\n    );\n\n    WITH download_attribution_1 AS (\n    \n     select device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share, \n        case when device_code='android-phone' THEN 'android-all' \n             when device_code='ios-phone' THEN 'ios-tablet' END AS new_device_code from download_attribution\n    );\n\n    WITH download_attribution_2\n    AS (\n        select distinct * from ( select new_device_code as device_code, country_code, granularity, date, product_id, new_est_non_organic_download_share as est_non_organic_download_share from download_attribution_1 \n        union all\n        select device_code, country_code, granularity, date, product_id, est_non_organic_download_share from download_attribution ) as t1 where device_code!='android-phone'\n\n    );\n    \n    WITH union_data AS (\n    select *, store_unified.device_code as unified_device_code , store_unified.country_code as unified_country_code\n    from store_unified full outer join download_attribution_2\n    on store_unified.device_code=download_attribution_2.device_code and\n    store_unified.country_code=UPPER(download_attribution_2.country_code) and\n    store_unified.app_id=download_attribution_2.product_id\n    where est_non_organic_download_share is not null\n    );\n    \n    \n    WITH calculate_data_prepare AS (\n    select app_id, coalesce(free_app_download, 0) as free_app_download,  coalesce(paid_app_download, 0 )  as paid_app_download, revenue, unified_device_code, unified_country_code, est_non_organic_download_share from union_data where not (free_app_download is null and paid_app_download is null) \n    );\n    \n    \n\n    \n    WITH caculate_data AS (\n    select app_id,free_app_download,paid_app_download,revenue, unified_device_code, unified_country_code, est_non_organic_download_share from calculate_data_prepare\n    );\n    \n    \n    \n    WITH compare_data_raw AS (\n    select app_id,free_app_download, paid_app_download, unified_device_code as device_code, unified_country_code as country_code, (1 - est_non_organic_download_share) AS organic_download_share from caculate_data\n    );\n    \n    \n    WITH compare_data_unified AS (\n    select app_id,coalesce(free_app_download, 0 ) as free_app_download,  coalesce(paid_app_download, 0 ) as paid_app_download,  device_code,  country_code, organic_download_share from test_unified_download_attribution\n    );\n    \n    \n\n    \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n                \"data_encoding\": \"parquet\",\n                \"compression\": \"gzip\",\n                \"name\": \"store_unified\",\n                \"path\": test_data[2]\n            }\n            # ,\n            # {\n            #     \"data_encoding\": \"parquet\",\n            #     \"compression\": \"gzip\",\n            #     \"name\": \"test_unified_download_attribution\",\n            #     \"path\":test_data[1]\n            # }\n    \n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    spark.sql(\"select * from compare_data_raw where app_id = 284882215 and country_code='US'\").show()\n    # spark.sql(\"select * from compare_data_raw where app_id is not null except all select * from compare_data_unified where app_id is not null\").show()\n    # spark.sql(\"select * from compare_data_unified except all select * from compare_data_raw\").show()\n    # count_1 = spark.sql(\"select count(*) from compare_data_raw where app_id is not null\").take(1)\n    # count_2 = spark.sql(\"select count(*) from compare_data_unified \").take(1)\n    # if count_1[0][0] != count_2[0][0]:\n    #     print 'failed!!!!!!!!!!!!!'\n\n\n    eject_all_caches(spark)\n\n\nsc.parallelize(map(test_download_attribution, test_path), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200702-074503_1629595252","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from compare_data_raw\").show()\nspark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/\").where(\"granularity='daily' and  date ='{}' \".format(\"2018-05-10\")).createOrReplaceTempView(\"test_unified_download_attribution\")\n\nspark.sql(\"select * from test_unified_download_attribution where app_id = 284882215 and country_code='US'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200702-075605_707126999","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution.v3/fact/granularity=daily/date=2020-05-30/ --recursive | sort -n \naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-05-30/ --recursive  | sort -n \n"]},{"cell_type":"code","execution_count":0,"id":"20200621-083328_1687358286","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\n\n-- est_free_app_download + est_paid_app_download = organic_download + paid_download\nselect distinct date from store_est_fact_v6 where  country_code='US' and est_paid_download !=0  and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30' order by date ;\nselect distinct date from store_est_category_fact_v7 where  country_code='US' and est_paid_download !=0  and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30' order by date ;\n\n-- select * from store_est_fact_v6 where date = '2020-03-05' and country_code='AU' and device_code='ios-phone' and est_paid_download is not null order by est_paid_download desc limit 5\n-- select * from store_est_category_fact_v7 where date = '2020-01-01' and country_code='US'  and device_code='android-all' and app_id = 20600006409047;\n\n-- select (sum(est_free_app_download) + sum(est_paid_app_download)) as sum_free_paid, (sum(est_paid_download) + sum(est_organic_download)) as sum_orgainc_paid from store_est_fact_v6 where date between '2019-08-29' and '2020-05-30' limit 5 ;\n-- select * from store_est_category_fact_v7 where app_id=446366839 and country_code='AU'  and device_code='ios-phone' and date between '2017-06-05' and '2017-06-05' limit 5 ;\n\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200621-083437_1667214568","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\n\n-- Sample test - compare with UI\n-- select * from store_est_t_w_fact_v6 where device_code='ios-tablet' and country_code='GB' and date ='2019-06-15' and est_paid_download!=0 limit 3;\n-- select * from store_est_t_m_fact_v6 where device_code='android-all' and country_code='JP' and date ='2019-07-31'  and est_paid_download!=0  limit 3;\n--select * from store_est_t_y_fact_v6 where device_code='ios-phone' and country_code='WW' and date ='2019-12-31' and est_paid_download!=0 limit 3;\n--select * from store_est_t_y_fact_v6 where device_code='android-all' and country_code='WW' and date ='2019-12-31' and est_paid_download!=0 limit 3;\n\n-- select * from store_est_category_t_q_fact_v7 limit 3;\n-- select * from store_est_category_t_y_fact_v7 limit 3;\n\n-- Completeness test, date dimension\n--select count(distinct date) from store_est_category_t_w_fact_v7 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30'  ;\n--select distinct date from store_est_t_m_fact_v6 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30' order by date \n-- select distinct date from store_est_category_t_m_fact_v7 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30' order by date;\n\n\n-- select * from store_est_category_fact_v7 where date = '2020-01-01' and country_code='US'  and device_code='android-all' and app_id = 20600006409047;\n\n-- select (sum(est_free_app_download) + sum(est_paid_app_download)) as sum_free_paid, (sum(est_paid_download) + sum(est_organic_download)) as sum_orgainc_paid from store_est_fact_v6 where date between '2019-08-29' and '2020-05-30' limit 5 ;\n\n\n-- select * from store_est_category_fact_v7 where app_id=446366839 and country_code='AU'  and device_code='ios-phone' and date between '2017-06-05' and '2017-06-05' limit 5 ;\n\n\n\n-- select sum(est_organic_download), sum(est_paid_download) from store_est_fact_v6 where date between '2019-10-01' and '2019-10-31' and device_code='ios-phone' and country_code='US' and app_id=1225683141 limit 3;\n-- select * from store_est_t_m_fact_v6 where device_code='ios-phone' and country_code='US' and date ='2019-10-31' and app_id=1225683141 limit 3;\n--select sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_fact_v6 where date between '2019-10-01' and '2019-12-31' and --device_code='ios-phone' and country_code='US' and app_id=1225683141 limit 3;\n--select * from store_est_t_q_fact_v6 where device_code='ios-phone' and country_code='US' and date ='2019-12-31' and app_id=1225683141 limit 3;\n\n\n\n--select sum(est_free_app_download) as sum_download, sum(est_revenue) as sum_revenue, sum(est_organic_download) as sum_organic_download, sum(est_paid_download) as sum_paid_download from store_est_fact_v6 where date between '2018-01-01' and '2018-12-31' and -- device_code='ios-phone' and country_code='US' and app_id=1225683141 limit 3;\n\nselect * from store_est_t_w_fact_v1 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-05-30' limit 5;\nselect * from store_est_t_m_fact_v1 where country_code='US' and est_paid_download !=0 and est_paid_download is not null and device_code='ios-phone' and date between '2016-08-26' and '2020-01-30' limit 5;\n\n\n\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200626-082126_957403942","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom datetime import timedelta\nfrom dateutil.relativedelta import relativedelta\n\n\n# value = ['2016-12-31','2015-06-30', '2012-09-30', '2013-03-31', '2018-03-31'] \n# value = ['2012-12-01','2016-10-22', '2013-05-11', '2013-06-08', '2015-05-02', '2012-07-14', '2013-03-30', '2017-02-04' ,'2018-01-20']\n# value = ['2012-12-31','2016-10-31', '2013-05-31', '2013-06-30', '2015-05-31', '2012-07-31', '2013-03-31', '2017-02-28' ,'2018-01-31']\n\ngranularity = 'quarterly'\n\nstart = \"2019-01-01\"\nend = \"2019-04-01\"\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_date_range(date_list, granularity):\n    result = []\n    start = datetime.datetime.strptime(date_list[0], '%Y-%m-%d')\n    end = datetime.datetime.strptime(date_list[1], '%Y-%m-%d')\n    if granularity == 'weekly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(weeks=1)\n    elif granularity == 'monthly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=1)\n    elif granularity == 'quarterly':\n        while start <= end:\n            start += relativedelta(months=2)\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            # start += relativedelta(months=3)\n    elif granularity == 'yearly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=12)\n    # print result\n    return result\n\nagg_date = get_date_range([ start,end ], granularity)\n\nagg_date.sort()\nprint agg_date\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nprint date_range.days\nfor days in xrange(date_range.days):\n    dates.append(str(real_date1 + datetime.timedelta(days)))\n    \n\ntemp_list = list()\ni = 0 \nfor x in dates:\n    temp_list.append(x)\n    \n    if i < len(agg_date) and x == agg_date[i]  :\n        print x\n        sar_list.append((agg_date[i], temp_list))\n        i = i+1\n        temp_list=[]\n\nprint sar_list"]},{"cell_type":"code","execution_count":0,"id":"20200625-071237_868335361","metadata":{},"outputs":[],"source":["\nimport datetime\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\n\ngranularity = 'quarterly'\n\nstore_db = \"store_est_fact_v1\"\n\ngranularity_table_dict = {\n                        'weekly': ['store.app-est.v3', store_db],\n                         'monthly': ['store.app-est.v3', store_db],\n                         'quarterly' : ['store.app-est-pre-aggr.v3', store_db],\n                         'yearly' : ['store.app-est-pre-aggr.v3', store_db]\n                        }\n\n\n\ndownload_attribution_bucket='store.download-attribution-pre-aggr.v3'\n\n\ndef citus_row(date, granularity):\n    def get_data_in_citus(date):\n        print 'get data in citus ', date\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = '''SELECT  \n                    cast(sum(est_free_app_download) as bigint) as est_free_app_download, \n                    cast(sum(est_paid_app_download) as bigint) as est_paid_app_download, \n                    cast(sum(est_revenue) as bigint) as est_revenue, \n                    cast(sum(est_organic_download) as bigint) as est_organic_download ,  \n                    cast(sum(est_paid_download) as bigint) as est_paid_download\n                FROM store.{}\n                WHERE date in ('{}') '''.format(granularity_table_dict[granularity][1], \"','\".join(date[1:]))\n        print sql\n        db_data = query(citus_dsn_, sql)\n        return db_data\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    result = get_data_in_citus(date)\n    return [Row(sum_est_free_app_download=r[0], sum_est_paid_app_download=r[1], sum_est_revenue=r[2], sum_est_organic_download=r[3], sum_est_paid_download=r[4] ) for r in result]\n\n\nfor d in sar_list:\n    print d[0]\n\n    daily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/{}/fact/\".format(download_attribution_bucket)).where(\"granularity='%s' and date in ('%s')\" % (granularity, d[0]))\n    daily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    est_weekly_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/{}/fact/\".format(granularity_table_dict[granularity][0])).where(\"granularity='{}' and date in ('{}')\".format(granularity,d[0] ))\n    est_weekly_df.createOrReplaceTempView(\"est_weekly_df\")\n\n\n    citus_result = citus_row(d[1], granularity)\n\n    schema = StructType([\n    StructField(\"sum_est_free_app_download\", LongType(), True),\n    StructField(\"sum_est_paid_app_download\", LongType(), True),\n    StructField(\"sum_est_revenue\", LongType(), True),\n    StructField(\"sum_est_organic_download\", LongType(), True),\n    StructField(\"sum_est_paid_download\", LongType(), True)])\n\n    df_3 = spark.createDataFrame(citus_result, schema)\n    df_3.createOrReplaceTempView(\"citus_data\")\n\n    \n    # compare est paid download by DB and download attribution\n    spark.sql(\"select sum(est_paid_download) as sum_est_paid_download from download_attribution_weekly_unified except all select sum_est_paid_download from citus_data\").show()\n    spark.sql(\"select sum_est_paid_download from citus_data except all select sum(est_paid_download) as sum_est_paid_download from download_attribution_weekly_unified\").show()\n\n    # compare est weekly data with DB metric sum(free_app_download), sum(paid_app_download), sum(revenue) \n    spark.sql(\"select sum(free_app_download), sum(paid_app_download), sum(revenue) from est_weekly_df except all select sum_est_free_app_download, sum_est_paid_app_download, sum_est_revenue from citus_data \").show()\n    spark.sql(\"select sum_est_free_app_download, sum_est_paid_app_download, sum_est_revenue from citus_data except all select sum(free_app_download), sum(paid_app_download), sum(revenue) from est_weekly_df \").show()\n\n\n    # compare organic + paid = free_app + paid_app in DB\n    list_result =  spark.sql(\"select sum_est_free_app_download + sum_est_paid_app_download,  sum_est_organic_download + sum_est_paid_download from citus_data \").collect()\n    if list_result[0][0] != list_result[0][1]:\n        print 'compare compare organic + paid = free_app + paid_app failed!! , organic + paid= {}, free_app + paid_app = {}'.format(list_result[0][0], list_result[0][1])\n    else:\n        print 'pass!'"]},{"cell_type":"code","execution_count":0,"id":"20200628-073546_293955495","metadata":{},"outputs":[],"source":["\nspark.sql(\"select sum_est_paid_download from citus_data\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200628-072716_307323096","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\nselect sum(est_paid_download) from store_est_t_q_fact_v1 where date between '2019-01-01' and '2019-03-31' ;\nselect sum(est_paid_download) from store_est_fact_v1 where date between '2019-01-01' and '2019-03-31' ;\n-- select count(1) from store_est_t_w_fact_v1 where date between '2012-08-12' and '2012-08-18' ;\n-- select  count(1) from store_est_category_t_w_fact_v1 where date between '2012-08-12' and '2012-08-18' ;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200625-093715_778923544","metadata":{},"outputs":[],"source":["\nimport datetime\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nstart = \"2019-01-01\"\nend = \"2019-04-01\"\n\n\n\n# value = ['2016-12-31','2015-06-30', '2012-09-30', '2013-03-31', '2018-03-31'] \n# value = ['2012-12-01','2016-10-22', '2013-05-11', '2013-06-08', '2015-05-02', '2012-07-14', '2013-03-30', '2017-02-04' ,'2018-01-20']\n# value = ['2012-12-31','2016-10-31', '2013-05-31', '2013-06-30', '2015-05-31', '2012-07-31', '2013-03-31', '2017-02-28' ,'2018-01-31']\n\ngranularity = 'daily'\n\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    # if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n    #     temp=list()\n    #     while dates:\n    #         temp.append(str(dates.pop()))\n    #     sar_list.append((str(real_date1 + datetime.timedelta(days)), temp))\n\n\n\n\ndef citus_row(date, granularity):\n    def get_data_in_citus(date):\n        print 'get data in citus ', date\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = '''SELECT  \n                    cast(sum(est_free_app_download) as bigint) as est_free_app_download, \n                    cast(sum(est_paid_app_download) as bigint) as est_paid_app_download, \n                    cast(sum(est_revenue) as bigint) as est_revenue, \n                    cast(sum(est_organic_download) as bigint) as est_organic_download ,  \n                    cast(sum(est_paid_download) as bigint) as est_paid_download\n                FROM store.{}\n                WHERE date in ('{}') '''.format(granularity_table_dict[granularity][1],date)\n        db_data = query(citus_dsn_, sql)\n        return db_data\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    result = get_data_in_citus(date)\n    return [Row(sum_est_free_app_download=r[0], sum_est_paid_app_download=r[1], sum_est_revenue=r[2], sum_est_organic_download=r[3], sum_est_paid_download=r[4] ) for r in result]\n\nstore_db = \"store_est_fact_v1\"\ndownload_attribution_bucket = \"store.download-attribution-load.v3\"\ngranularity_table_dict = {\n                        'daily' : ['store.app-est-load.v3', store_db],\n                        'weekly': ['store.app-est.v3', store_db],\n                         'monthly': ['store.app-est.v3', store_db],\n                         'quarterly' : ['store.app-est-pre-aggr.v3', store_db],\n                         'yearly' : ['store.app-est-pre-aggr.v3', store_db]\n                        }\n\n\nfor d in dates:\n\n    daily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/{}/fact/\".format(download_attribution_bucket)).where(\"granularity='%s' and date in ('%s')\" % (granularity, d))\n    daily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    est_weekly_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/{}/fact/\".format(granularity_table_dict[granularity][0])).where(\"granularity='{}' and date in ('{}')\".format(granularity,d ))\n    est_weekly_df.createOrReplaceTempView(\"est_weekly_df\")\n\n\n    citus_result = citus_row(d, 'daily')\n\n    schema = StructType([\n    StructField(\"sum_est_free_app_download\", LongType(), True),\n    StructField(\"sum_est_paid_app_download\", LongType(), True),\n    StructField(\"sum_est_revenue\", LongType(), True),\n    StructField(\"sum_est_organic_download\", LongType(), True),\n    StructField(\"sum_est_paid_download\", LongType(), True)])\n\n    df_3 = spark.createDataFrame(citus_result, schema)\n    df_3.createOrReplaceTempView(\"citus_data\")\n\n    \n    # compare est paid download by DB and download attribution\n    spark.sql(\"select sum(est_paid_download) as sum_est_paid_download from download_attribution_weekly_unified except all select sum_est_paid_download from citus_data\").show()\n    spark.sql(\"select sum_est_paid_download from citus_data except all select sum(est_paid_download) as sum_est_paid_download from download_attribution_weekly_unified\").show()\n\n    # compare est weekly data with DB metric sum(free_app_download), sum(paid_app_download), sum(revenue) \n    spark.sql(\"select sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue) from est_weekly_df except all select sum_est_free_app_download, sum_est_paid_app_download, sum_est_revenue from citus_data \").show()\n    spark.sql(\"select sum_est_free_app_download, sum_est_paid_app_download, sum_est_revenue from citus_data except all select sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue) from est_weekly_df \").show()\n\n\n    # compare organic + paid = free_app + paid_app in DB\n    list_result =  spark.sql(\"select sum_est_free_app_download + sum_est_paid_app_download,  sum_est_organic_download + sum_est_paid_download from citus_data \").collect()\n    if list_result[0][0] != list_result[0][1]:\n        print 'compare compare organic + paid = free_app + paid_app failed!! , organic + paid= {}, free_app + paid_app = {}'.format(list_result[0][0], list_result[0][1])\n    else:\n        print 'pass!'"]},{"cell_type":"code","execution_count":0,"id":"20200625-084612_753610188","metadata":{},"outputs":[],"source":["\nimport datetime\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nstart = \"2016-08-28\"\nend = \"2020-03-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append([str(real_date1 + datetime.timedelta(days)),temp])\n\n\ngranularity = 'weekly'\nstore_db = \"store_est_fact_v1\"\n\ngranularity_table_dict = {\n                        'weekly': ['store.app-est.v3', store_db],\n                         'monthly': ['store.app-est.v3', store_db],\n                         'quarterly' : ['store.app-est-pre-aggr.v3', store_db],\n                         'yearly' : ['store.app-est-pre-aggr.v3', store_db]\n                        }\n\n\ndownload_attribution_bucket='store.download-attribution-pre-aggr.v3'\n\n\ndef citus_row(date, granularity):\n    def get_data_in_citus(date):\n        print 'get data in citus ', date\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = '''SELECT  \n                    cast(sum(est_free_app_download) as bigint) as est_free_app_download, \n                    cast(sum(est_paid_app_download) as bigint) as est_paid_app_download, \n                    cast(sum(est_revenue) as bigint) as est_revenue, \n                    cast(sum(est_organic_download) as bigint) as est_organic_download ,  \n                    cast(sum(est_paid_download) as bigint) as est_paid_download\n                FROM store.{}\n                WHERE date between '{}' and '{}' '''.format(granularity_table_dict[granularity][1], date[1][-1], date[1][0])\n        db_data = query(citus_dsn_, sql)\n        return db_data\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    result = get_data_in_citus(date)\n    return [Row(sum_est_free_app_download=r[0], sum_est_paid_app_download=r[1], sum_est_revenue=r[2], sum_est_organic_download=r[3], sum_est_paid_download=r[4] ) for r in result]\n\n\nfor d in sar_list:\n    print 'd[0][0] is ', d[0]\n    print d\n    daily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/{}/fact/\".format(download_attribution_bucket)).where(\"granularity='%s' and date in ('%s')\" % (granularity, d[0]))\n    daily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    est_weekly_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/{}/fact/\".format(granularity_table_dict[granularity][0])).where(\"granularity='{}' and date in ('{}')\".format(granularity,d[0] ))\n    est_weekly_df.createOrReplaceTempView(\"est_weekly_df\")\n\n\n    citus_result = citus_row(d, 'weekly')\n\n    schema = StructType([\n    StructField(\"sum_est_free_app_download\", LongType(), True),\n    StructField(\"sum_est_paid_app_download\", LongType(), True),\n    StructField(\"sum_est_revenue\", LongType(), True),\n    StructField(\"sum_est_organic_download\", LongType(), True),\n    StructField(\"sum_est_paid_download\", LongType(), True)])\n\n    df_3 = spark.createDataFrame(citus_result, schema)\n    df_3.createOrReplaceTempView(\"citus_data\")\n\n    \n    # compare est paid download by DB and download attribution\n    spark.sql(\"select sum(est_paid_download) as sum_est_paid_download from download_attribution_weekly_unified except all select sum_est_paid_download from citus_data\").show()\n    spark.sql(\"select sum_est_paid_download from citus_data except all select sum(est_paid_download) as sum_est_paid_download from download_attribution_weekly_unified\").show()\n\n    # compare est weekly data with DB metric sum(free_app_download), sum(paid_app_download), sum(revenue) \n    spark.sql(\"select sum(free_app_download), sum(paid_app_download), sum(revenue) from est_weekly_df except all select sum_est_free_app_download, sum_est_paid_app_download, sum_est_revenue from citus_data \").show()\n    spark.sql(\"select sum_est_free_app_download, sum_est_paid_app_download, sum_est_revenue from citus_data except all select sum(free_app_download), sum(paid_app_download), sum(revenue) from est_weekly_df \").show()\n\n\n    # compare organic + paid = free_app + paid_app in DB\n    list_result =  spark.sql(\"select sum_est_free_app_download + sum_est_paid_app_download,  sum_est_organic_download + sum_est_paid_download from citus_data \").collect()\n    if list_result[0][0] != list_result[0][1]:\n        print 'compare compare organic + paid = free_app + paid_app failed!! , organic + paid= {}, free_app + paid_app = {}'.format(list_result[0][0], list_result[0][1])\n    else:\n        print 'pass!'"]},{"cell_type":"code","execution_count":0,"id":"20200628-080918_195337749","metadata":{},"outputs":[],"source":["\n\nimport datetime\nfrom datetime import timedelta\nfrom dateutil.relativedelta import relativedelta\n\n\n# value = ['2016-12-31','2015-06-30', '2012-09-30', '2013-03-31', '2018-03-31'] \n# value = ['2012-12-01','2016-10-22', '2013-05-11', '2013-06-08', '2015-05-02', '2012-07-14', '2013-03-30', '2017-02-04' ,'2018-01-20']\n# value = ['2012-12-31','2016-10-31', '2013-05-31', '2013-06-30', '2015-05-31', '2012-07-31', '2013-03-31', '2017-02-28' ,'2018-01-31']\n\ngranularity = 'quarterly'\n\nstart = \"2011-01-01\"\nend = \"2020-01-01\"\n\ndef last_day_of_month(check_month):\n    next_month = check_month.replace(day=28) + datetime.timedelta(days=4)\n    return next_month - datetime.timedelta(days=next_month.day)\n\n\ndef get_date_range(date_list, granularity):\n    result = []\n    start = datetime.datetime.strptime(date_list[0], '%Y-%m-%d')\n    end = datetime.datetime.strptime(date_list[1], '%Y-%m-%d')\n    if granularity == 'weekly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(weeks=1)\n    elif granularity == 'monthly':\n        while start <= end:\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=1)\n    elif granularity == 'quarterly':\n        while start < end:\n            if start.month in [1, 4, 7, 10]:\n                start += relativedelta(months=2)\n            elif start.month in [2, 5, 8, 11]:\n                start += relativedelta(months=1)\n            else:\n                start += relativedelta(months=3)\n            start = last_day_of_month(start)\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n    elif granularity == 'yearly':\n        while start <= end:\n            date_row = datetime.datetime.strftime(start, '%Y-%m-%d')\n            result.append(date_row)\n            start += relativedelta(months=12)\n    # print result\n    return result\n\nagg_date = get_date_range([ start,end ], granularity)\n\nagg_date.sort()\nprint agg_date\n\n\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nprint date_range.days\nfor days in xrange(date_range.days):\n    dates.append(str(real_date1 + datetime.timedelta(days)))\n    \n\ntemp_list = list()\ni = 0 \nfor x in dates:\n    temp_list.append(x)\n    \n    if i < len(agg_date) and x == agg_date[i]  :\n        print x\n        sar_list.append((agg_date[i], temp_list))\n        i = i+1\n        temp_list=[]\n\nprint sar_list"]},{"cell_type":"code","execution_count":0,"id":"20200628-080108_678344520","metadata":{},"outputs":[],"source":["\nimport datetime\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\n\ngranularity = 'quarterly'\n\nstore_db = \"store_est_fact_v1\"\n\ngranularity_table_dict = {\n                        'weekly': ['store_est_t_w_fact_v1', store_db],\n                         'monthly': ['store_est_t_m_fact_v1', store_db],\n                         'quarterly' : ['store_est_q_w_fact_v1', store_db],\n                         'yearly' : ['store_est_t_y_fact_v1', store_db]\n                        }\n\n\n\n\n\ndef citus_row(date, granularity):\n    def get_data_in_citus(date):\n        # print 'get data in citus ', date\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = '''SELECT  \n                    cast(sum(est_free_app_download) as bigint) as est_free_app_download, \n                    cast(sum(est_paid_app_download) as bigint) as est_paid_app_download, \n                    cast(sum(est_revenue) as bigint) as est_revenue, \n                    cast(sum(est_organic_download) as bigint) as est_organic_download ,  \n                    cast(sum(est_paid_download) as bigint) as est_paid_download\n                FROM store.{}\n                WHERE date in ('{}') '''.format(granularity_table_dict[granularity][1], \"','\".join(date))\n        db_data = query(citus_dsn_, sql)\n        return db_data\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    result = get_data_in_citus(date)\n    return [Row(sum_est_free_app_download=r[0], sum_est_paid_app_download=r[1], sum_est_revenue=r[2], sum_est_organic_download=r[3], sum_est_paid_download=r[4] ) for r in result]\n\n\nfor d in sar_list:\n    citus_result_daily = citus_row(d[1], granularity)\n\n    schema = StructType([\n    StructField(\"sum_est_free_app_download\", LongType(), True),\n    StructField(\"sum_est_paid_app_download\", LongType(), True),\n    StructField(\"sum_est_revenue\", LongType(), True),\n    StructField(\"sum_est_organic_download\", LongType(), True),\n    StructField(\"sum_est_paid_download\", LongType(), True)])\n\n    df_3 = spark.createDataFrame(citus_result_daily, schema)\n    df_3.createOrReplaceTempView(\"citus_data_daily\")\n\n\n    citus_pre_agg_result = citus_row(d[1], granularity)\n\n    schema = StructType([\n    StructField(\"sum_est_free_app_download\", LongType(), True),\n    StructField(\"sum_est_paid_app_download\", LongType(), True),\n    StructField(\"sum_est_revenue\", LongType(), True),\n    StructField(\"sum_est_organic_download\", LongType(), True),\n    StructField(\"sum_est_paid_download\", LongType(), True)])\n\n    df_3 = spark.createDataFrame(citus_pre_agg_result, schema)\n    df_3.createOrReplaceTempView(\"citus_data_pre_agg\")\n\n    \n    # compare est paid download by DB and download attribution\n    spark.sql(\"select * from citus_data_daily except all select * from citus_data_pre_agg\").show()\n    spark.sql(\"select * from citus_data_pre_agg except all select * from citus_data_daily\").show()\n\n\n\n    # compare organic + paid = free_app + paid_app in DB\n    list_result =  spark.sql(\"select sum_est_free_app_download + sum_est_paid_app_download,  sum_est_organic_download + sum_est_paid_download from citus_data_pre_agg \").collect()\n    if list_result[0][0] != list_result[0][1]:\n        print 'compare compare organic + paid = free_app + paid_app failed!! , organic + paid= {}, free_app + paid_app = {}'.format(list_result[0][0], list_result[0][1])\n    else:\n        print 'pass!'"]},{"cell_type":"code","execution_count":0,"id":"20200621-084130_492904243","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2018-05-06\"\nend = \"2018-05-12\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(str(dates.pop()))\n        sar_list.append((str(real_date1 + datetime.timedelta(days)), temp))\nprint sar_list\nfor d in sar_list:\n    daily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(\"granularity='daily' and date in ('%s')\" % (\"','\".join(sar_list[0][1])))\n    daily_download_attr.createOrReplaceTempView(\"daily_pre_load_data\")\n    \n    daily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='weekly' and date in ('%s')\" % (sar_list[0][0]))\n    daily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    \n\n    spark.sql(\"select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified except all select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code\").show()\n\n    spark.sql(\"select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code except all select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified\").show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200621-084217_1785964890","metadata":{},"outputs":[],"source":["\nimport datetime\n\nstart = \"2020-02-01\"\nend = \"2020-04-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\n# sar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(str(real_date1 + datetime.timedelta(days)))\n#     if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n#         temp=list()\n#         while dates:\n#             temp.append(str(dates.pop()))\n#         sar_list.append((str(real_date1 + datetime.timedelta(days)), temp))\n# print sar_list\n\nprint dates\nprint dates[-1]\n# sql_where = \"granularity='daily' and date in ('%s')\" % (\"','\".join(dates))\nsql_where = \"granularity='daily' and date between '2019-03-01' and '2019-03-31'\"\n\nprint sql_where\ndaily_download_attr = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/\").where(sql_where)\ndaily_download_attr.createOrReplaceTempView(\"daily_pre_load_data\")\n\ndaily_download_attr_preload_unified = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-pre-aggr.v3/fact/\").where(\"granularity='monthly' and date='2019-03-31' \")\ndaily_download_attr_preload_unified.createOrReplaceTempView(\"download_attribution_weekly_unified\")\n    \n    \nspark.sql(\"select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified except all select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code\").show()\n\nspark.sql(\"select app_id, country_code, device_code, sum(est_organic_download) as est_organic_download, sum(est_paid_download) as est_paid_download from daily_pre_load_data group by app_id, country_code, device_code except all select app_id, country_code, device_code, est_organic_download, est_paid_download from download_attribution_weekly_unified\").show()\n\n\nspark.sql(\"select * from download_attribution_weekly_unified\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200702-013001_1445975983","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/granularity=daily/date=2018-05-10/device_code=ios-phone/\n\n\n \n"]},{"cell_type":"code","execution_count":0,"id":"20200624-073119_408801264","metadata":{},"outputs":[],"source":["\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.download-attribution-load.v3/fact/granularity=daily/date=2018-05-10/device_code=ios-phone/\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200702-013059_1193087877","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}