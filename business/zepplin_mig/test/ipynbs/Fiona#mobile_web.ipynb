{"cells":[{"cell_type":"code","execution_count":0,"id":"20200325-122334_238470191","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":0,"id":"20200310-090045_219262647","metadata":{},"outputs":[],"source":["%%sh\n# [\"app-cross-domain\", \"basic\", \"cross-app\", \"cross-domain\", \"deduplicated\", \"domain-browser\", \"domain-outbound\", \"domain-referral\", \"domain-retention\", \"ranking\"]\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v1/fact/granularity=m/month=201901/date=2019-01-31/device_code=ap/country_code=de/\n \n \n# aws s3 cp s3://b2c-prod-data-pipeline-qa/aa.mobile.web/part-00000-d60e1650-58f6-4ebd-9384-16a0aa5bd621-c000.snappy.parquet - | head\n# aws s3 cp s3://b2c-prod-data-pipeline-qa/aa.mobile.web/part-00000-6249f5d9-12b6-4ded-babc-30e9ee19281f-c000.snappy.parquet - | head\n# aws s3 ls s3://b2c-prod-data-pipeline-qa/aa.mobile.web/ | wc -l\n\n\n# aws s3 cp s3://b2c-prod-data-pipeline-qa/aa.mobile.web/_delta_log/._last_checkpoint.74f5dde8-976f-4074-a59d-4f0771417991.tmp - | head\n# aws s3 ls s3://b2c-prod-dca-mobile-web-to-int/oss/\n\n\n# --recursive --human --summarize | tail -30\n# basic\n# deduplicated\n# ranking\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.app-cross-domain.v3/fact/granularity=m/month=202002/date=2020-02-29/device_code=ip/country_code=au/\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v3/fact/granularity=m/month=202002/date=2020-02-29/device_code=ip/country_code=au/\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v3/fact/granularity=m/month=202002/date=2020-02-29/device_code=ap/country_code=au/part-00000-231f5366-7613-44db-b5dc-f9f2196e651d.c000.gz.parquet\n \n \n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v1/dimension/date=2019-03-15/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v2/dimension/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/mobileweb.domain.v4/dimension/date=2019-03-15/\n\n# aws s3 cp s3://b2c-prod-data-pipeline-unified-mobileweb-free/unified/temp/2018-11-13.deflate - | head\naws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v3/fact/granularity=d/\naws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobileweb.ranking.v4/fact/granularity=d/\n\n \n"]},{"cell_type":"code","execution_count":0,"id":"20200319-060537_1617575786","metadata":{},"outputs":[],"source":["\n# do not save version column, its not raw data value\n\n# raw data:\nprint spark.read.parquet(\"s3://b2c-prod-dca-mobile-web-to-int/oss/MOBILE_WEB_DOMAIN_BASIC_METRICS/version=v1.0.0/granularity=DAY/date=2019-05-31/\").show(1)\n\n# unified data:\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v1/fact/granularity=d/month=201908/date=2019-08-01/device_code=ap/\").printSchema()\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v1/fact/\").filter(\"version is not null\").cache().groupBy(\"version\").agg({\"*\":\"count\"}).show()\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v1/fact/granularity=d/month=201908/date=2019-08-23/device_code=ap/\").printSchema() # have version\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v1/fact/granularity=d/month=201908/date=2019-08-25/device_code=ap/\").printSchema()  # don't have version\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200320-123535_1334630417","metadata":{},"outputs":[],"source":["\nddf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.mobile.web/\").cache()\nnew_df =  ddf.filter(\"count1 != count2 and date != '2020-01-31' and date !='2020-02-29' \").orderBy(\"pack\",\"granularity\",\"date\").select(\"pack\",\"granularity\",\"date\",\"device_code\").distinct()\n# ddf.filter(\"device_code='ap' and pack='app-cross-domain'\").show()\nprint [row.asDict() for row in new_df.collect()]\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200319-060735_432918246","metadata":{},"outputs":[],"source":["\nfrom datetime import datetime\nfrom calendar import monthrange\nfrom pyspark.sql.utils import AnalysisException\n\n# date_list=set()\n# for m in range(1,13):\n#     month=str('%02d' % m)\n#     date_list.add(\"2019-%s-%s\" % (month ,monthrange(2019, m)[1]))\n\ndays=\"2020-02-29\"\nip_packs_single_day = [\"app-cross-domain\", \"basic\", \"cross-app\", \"cross-domain\", \"deduplicated\", \"domain-browser\", \"domain-outbound\", \"domain-referral\", \"domain-retention\", \"ranking\"]\nap_packs_single_day = [\"basic\", \"cross-domain\", \"deduplicated\", \"domain-browser\", \"domain-outbound\", \"domain-referral\", \"domain-retention\", \"ranking\"]\nup_packs_single_day = [\"ranking\"]\npartitions_single_day = [(pack, 'm','ap', days) for pack in ap_packs_single_day]\npartitions_single_day.extend([(pack, 'm','ip',days) for pack in ip_packs_single_day])\npartitions_single_day.extend([(pack, 'm','up',days) for pack in up_packs_single_day])\n# print partitions\n# mobile-web.deduplicated.v1 w\n# mobile-web.ranking.v1 w d\n# mobile-web.basic.v1 w,d\n\nprint partitions_single_day[0]\n\nfor pack, granularity, device_code, date in partitions_single_day:\n    # print '========================================== %s ========================================== start to test' % pack \n    month = datetime.strptime(date, \"%Y-%m-%d\").strftime('%Y%m')\n    df1 = None\n    df2 = None\n    try:\n        # print \"%s, %s, %s, %s, %s started\" % (pack, granularity, month, date, device_code)\n        df2 = spark.read.option(\"basePath\", 's3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.%s.v1/fact' % pack).parquet('s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.%s.v1/fact/granularity=%s/month=%s/date=%s/device_code=%s' % (pack, granularity, month, date, device_code))\n        df1 = spark.read.format('delta').load('s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.%s.v000/fact' % pack).where(\"granularity='%s' and month='%s' and date='%s' and device_code='%s'\" % ( granularity, month, date, device_code)).withColumnRenamed(\"est_browser_traffic_share\",\"TS\").withColumnRenamed(\"est_outbound_traffic_share\",\"TS\").withColumnRenamed(\"est_referral_traffic_share\",\"TS\")\n        if sorted(df1.columns) != sorted(df2.columns):\n            print \"columns are different in %s, %s\"  % (granularity, date)\n            print sorted(df1.columns)\n            print sorted(df2.columns)\n            continue\n        df1 = df1.select(df1.columns).drop('_identifier').cache()\n        df2 = df2.select(df1.columns).drop('_identifier').cache()\n        count1 = df1.exceptAll(df2).count()\n        count2 = df2.exceptAll(df1).count()\n        if count1 != 0 or count2 !=0:\n            print \"Count1 in %s, %s, %s, %s, %s: %s\"  % (pack, granularity, month, date, device_code, count1)\n            print df1.exceptAll(df2).take(1)\n            print \"Count2 in %s, %s, %s, %s, %s: %s\"  % (pack, granularity, month, date, device_code, count2)\n            print df2.exceptAll(df1).take(1)\n            print \"df1: %s df2: %s\" % (df1.count(), df2.count())\n            continue\n        # result_df = spark.createDataFrame([(pack, granularity, month, date, device_code, df1.count())], schema=[\"pack\",\"granularity\",\"month\",\"date\",\"device_code\",\"count\"])\n        # result_df.coalesce(1).write.format(\"delta\").mode(\"append\").partitionBy(\"event_month\").save(\"s3://b2c-prod-data-pipeline-qa/aa.mobile.web/\")\n        print \"%s, %s, %s, %s, %s pass\" % (pack, granularity, month, date, device_code)\n    except AnalysisException as e:\n        print e\n    finally:\n        if df1:\n            df1.unpersist()\n        if df2:\n            df2.unpersist()\n"]},{"cell_type":"code","execution_count":0,"id":"20200323-105049_1739024967","metadata":{},"outputs":[],"source":["%%sh\n# \"\\nSELECT domain_id\\nFROM plproxy.execute_select_nestloop($proxy$\\n    SELECT domain_id, MAX(est_usage_penetration)::REAL AS est_usage_penetration\\n    FROM mw.category_m_up_us\\n    WHERE date BETWEEN '2020-02-01' AND '2020-02-29' AND\\n        category_id = 800019 AND\\n        rank_est_usage_penetration <= 3000\\n    GROUP BY domain_id\\n    ORDER BY MAX(est_usage_penetration) DESC\\n    LIMIT 3000\\n$proxy$) t (domain_id BIGINT, est_usage_penetration REAL)\\nGROUP BY domain_id\\nORDER BY MAX(est_usage_penetration) DESC, domain_id ASC\\nLIMIT 3000\\n\"\n\n\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n-- select domain_id,est_usage_penetration from plproxy.execute_select_nestloop(\\$proxy\\$SELECT domain_id, MAX(est_usage_penetration)::REAL AS est_usage_penetration  FROM mw.category_m_up_us WHERE date BETWEEN '2020-02-01' -- AND '2020-02-29' and category_id = 800000 and rank_est_usage_penetration <= 3000 GROUP BY domain_id  ORDER BY MAX(est_usage_penetration) asc limit 3000 \\$proxy\\$)t (domain_id bigint , est_usage_penetration real) --limit 3000;\n\n-- select sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$SELECT count(1) as cnt  FROM mw.category_m_up_us WHERE date BETWEEN '2020-02-01' AND '2020-02-29' and category_id = 800000 and rank_est_usage_penetration <= -- 3000 GROUP BY domain_id  ORDER BY MAX(est_usage_penetration) asc \\$proxy\\$)t (cnt bigint) ;\n\n\nselect rank_est_usage_penetration, device_code, category_id,est_usage_penetration from plproxy.execute_select_nestloop(\\$proxy\\$SELECT device_code, category_id , category_id, est_usage_penetration from mw.category_m where domain_id = 70010000002035 and date = '2020-02-29' and category_id='800019' and country_code='us' limit 10  \\$proxy\\$)t (device_code char, rank_est_usage_penetration int,  category_id int  , est_usage_penetration real)  limit 10;\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200324-083425_1516241814","metadata":{},"outputs":[],"source":["%%sh\n\n# aws s3 cp s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v000/fact/granularity=m/month=202002/date=2020-02-29/  s3://b2c-prod-data-pipeline-qa/mobile_web/rank_v1/ --recursive\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n#  select rank_est_usage_penetration , device_code ,category_id, est_usage_penetration from plproxy.execute_select_nestloop(\\$proxy\\$select device_code , rank_est_usage_penetration, category_id , est_usage_penetration\n#                     from mw.category_m where  domain_id = 70010000004276 and date = '2020-02-29' and category_id= '800000' and country_code='us' limit 15 \\$proxy\\$)t (\n#                     device_code char , rank_est_usage_penetration int, category_id int, est_usage_penetration real)  limit 10;\n# EOF\n\n# aws s3 cp s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v000/fact/granularity=m/month=202002/date=2020-02-29/  s3://b2c-prod-data-pipeline-qa/mobile_web/rank_v1/ --recursive\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v000/fact/granularity=m/month=202002/date=2020-02-29/device_code=up/country_code=au/\n \n\n\n\n\n \n \napp-cross-domain\", \"basic\", \"cross-app\", \"cross-domain\", \"deduplicated\", \"domain-browser\", \"domain-outbound\", \"domain-referral\", \"domain-retention\", \"ranking\n"]},{"cell_type":"code","execution_count":0,"id":"20200324-080241_273011476","metadata":{},"outputs":[],"source":["\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.app-cross-domain.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.cross.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.cross-domain.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.deduplicated.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.domain-browser.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.domain-outbound.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.domain-referral.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.domain-retention.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v3/fact/granularity=m/month=202002/date=2020-02-29/\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200324-094801_431735981","metadata":{},"outputs":[],"source":["\n\ndf_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v1/fact/granularity=m/month=202002/date=2020-02-29/\").filter(\" country_code='us' and device_code='up' and category_id ='800000' \").withColumnRenamed(\"domain_id\",\"wrong_domain_id\").withColumnRenamed(\"est_usage_penetration\",\"wrong_est_usage_penetration\").withColumnRenamed(\"rank_est_usage_penetration\",\"df1_rank_est_usage_penetration\").select(\"df1_rank_est_usage_penetration\",\"wrong_domain_id\",\"wrong_est_usage_penetration\")\n\ndf_2 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v3/fact/\").where(\"granularity='m' and month='202002' and date='2020-02-29' and _identifier like '120200325%' \").filter(\" category_id ='800000' and  country_code='us' and device_code='up'\")\n\njoin_df = df_1.join(df_2, df_1.df1_rank_est_usage_penetration == df_2.rank_est_usage_penetration, \"outer\")\njoin_df.drop(\"_identifier\").select(\"df1_rank_est_usage_penetration\", \"rank_est_usage_penetration\",\"wrong_est_usage_penetration\",\"est_usage_penetration\",\"wrong_domain_id\",\"domain_id\", \"category_id\").orderBy(\"rank_est_usage_penetration\").show(100000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200323-070309_2136317605","metadata":{},"outputs":[],"source":["\n\n# \"SELECT domain_id FROM plproxy.execute_select_nestloop($proxy$SELECT domain_id, MAX(est_usage_penetration)::REAL AS est_usage_penetration FROM mw.category_m_up_us WHERE date BETWEEN '2020-02-01' AND '2020-02-29' AND category_id = 800019 AND       rank_est_usage_penetration <= 3000  GROUP BY domain_id ORDER BY MAX(est_usage_penetration) DESC LIMIT 3000$proxy$) t (domain_id BIGINT, est_usage_penetration REAL)GROUP BY domain_id ORDER BY MAX(est_usage_penetration) DESC, domain_id ASC LIMIT 3000\"\ndf_1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v1/fact/granularity=m/month=202002/date=2020-02-29/\").filter(\" country_code='us' and device_code='up' and category_id = '800000'\").withColumnRenamed(\"domain_id\",\"wrong_domain_id\").withColumnRenamed(\"est_usage_penetration\",\"wrong_est_usage_penetration\").withColumnRenamed(\"rank_est_usage_penetration\",\"df1_rank_est_usage_penetration\").select(\"df1_rank_est_usage_penetration\",\"wrong_domain_id\",\"wrong_est_usage_penetration\")\n\ndf_2 = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v3/fact/\").where(\"granularity='m' and month='202002' and date='2020-02-29' and _identifier like '120200320%' \").filter(\"  country_code='us' and device_code='up' and category_id = '800000'\")\n\n\njoin_df = df_1.join(df_2, df_1.wrong_domain_id == df_2.domain_id, \"outer\")\n# join_df.show()\njoin_df.drop(\"_identifier\").select(\"df1_rank_est_usage_penetration\", \"rank_est_usage_penetration\",\"wrong_est_usage_penetration\",\"est_usage_penetration\",\"wrong_domain_id\",\"domain_id\", \"category_id\").filter(\"wrong_domain_id is null or domain_id is null\").orderBy(\"rank_est_usage_penetration\",\"df1_rank_est_usage_penetration\", ascending=True).show(100000)\nd1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v1/fact/granularity=m/month=202002/date=2020-02-29/\").filter(\"category_id = '800019' and domain_id='70010000000232' and country_code='us'\").show(5)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200324-021554_442843255","metadata":{},"outputs":[],"source":["\n\nchart_df = spark.read.option('basePath','s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v1/fact/granularity=d/').parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v1/fact/granularity=m/month=*/date=*/device_code=up/country_code=us/\").filter(\"category_id = '800019' and domain_id in ('70010000332425' ) \")\n\n\n# chart_correct_df = spark.read.format(\"delta\").load('s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v3/fact/').where(\"granularity = 'm' and device_code='up' and country_code='us'\").filter(\"category_id = '800019' and domain_id in ('70010000003567' ) \")\n\n\n\nl1=chart_df.collect()\n\n\nnew_list = [{ \"date\":x[\"date\"], \"rank\":x[\"rank_est_usage_penetration\"]} for x in l1 ]\nprint new_list[0]\nprint new_list[0][\"date\"]\nprint new_list[0][\"rank\"]\n\n# print l[2]\n# print \"%table {}\\t{}\\t\".format( \"rank_est_usage_penetration\" , \"date\" )\n# for rank in l :\n#     print \"{}\\t{}\\t\".format(rank[\"rank_est_usage_penetration\"], rank[\"date\"])\nnew_list.sort(reverse=True)\nprint \"%table {}\\t{}\\t\".format( \"date\" , \"est_rank\" )\nfor record in new_list :\n    print \"{}\\t{}\\t\".format(record[\"date\"],record[\"rank\"])\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200310-095518_336291539","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.log import logger\n\nimport unittest\nfrom pyspark.sql.utils import AnalysisException\nfrom aadatapipelinecore.core.fs.device import s3\nfrom aadatapipelinecore.core.fs import Conf\n\n# s3_bucket_list = s3.S3Bucket(Conf(bucket_name='b2c-prod-data-pipeline-unified-mobileweb-paid'))\n# path_list = s3_bucket_list.all(prefix=\"unified/mobile-web.basic.v1/fact/granularity=d/month=202002/\", depth_is_1=True)\n# print path_list[0]\n\n[(\"app-cross-domain\",\"MOBILE_WEB_APP_CROSS_DOMAIN_METRICS\"), (\"basic\",\"MOBILE_WEB_DOMAIN_BASIC_METRICS\"), (\"cross-app\",\"MOBILE_WEB_CROSS_APP_METRICS\"), (\"cross-domain\",\"MOBILE_WEB_CROSS_DOMAIN_METRICS\"), (\"deduplicated\",\"MOBILE_WEB_UNIFIED_PRODUCT_METRICS\"), (\"domain-browser\",\"MOBILE_WEB_DOMAIN_BROWSER_METRICS\"), (\"domain-outbound\",\"MOBILE_WEB_DOMAIN_OUTBOUND_METRICS\"), (\"domain-referral\",\"MOBILE_WEB_DOMAIN_REFERRAL_METRICS\"), (\"domain-retention\",\"MOBILE_WEB_DOMAIN_RETENTION_METRICS\")]\n\ndata_source={\"mobile-web.basic\", \"mobile-web.ranking\",\"mobile-web.deduplicated\"}\n# data_source={\"mobile-web.cross-app\",\"mobile-web.deduplicated\",\"mobile-web.domain-browser\",\"mobile-web.domain-outbound\",\"mobile-web.domain-referral\",\"mobile-web.domain-retention\",\"mobile-web.ranking\"}\n# data_source={\"mobile-web.basic\"}\n# \"mobile-web.app-cross-domain\",\"mobile-web.basic\"\n\ndef compare_count(granularity,namespace,date):\n    try:\n        print granularity,namespace,date\n        df_old = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/{}.v000/fact/granularity={}/month={}/date={}/\".format(namespace,granularity, date.replace(\"-\",\"\")[:-2], date))\n        df_new = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/{}.v3/fact/\".format(namespace)).where( \"_identifier like '120200325%'\").where(\"granularity='{}' and month='{}' and date='{}'\".format(granularity, date.replace(\"-\",\"\")[:-2], date)).cache()\n        df_old = df_old.select(df_old.columns).drop('_identifier').cache()\n        df_new = df_new.select(df_new.columns).drop('_identifier').drop(\"date\").drop(\"granularity\").drop(\"month\").cache()\n        c1 = df_new.count()\n        c2 = df_old.count()\n        # df_new.createOrReplaceTempView(\"new\")\n        # df_old.createOrReplaceTempView(\"old\")\n        # spark.sql(\"select app_id, cross_domain_id from (select app_id, cross_domain_id from old EXCEPT select app_id, cross_domain_id from new) as prod\").show()\n        if c1 != c2 :\n            print \"not equal!!! namespace: {} , new count is {} , old count is {} diff is {} \".format(namespace,  c1, c2 , c1-c2)\n        else:\n            print \"namespace {} pass\".format(namespace)\n            \n    except AnalysisException as e:\n        print e\n    finally:\n        if df_old:\n            df_old.unpersist()\n        if df_new:\n            df_new.unpersist()\n\nfor ds in data_source:\n    compare_count(\"m\", ds, \"2020-02-29\")"]},{"cell_type":"code","execution_count":0,"id":"20200325-122753_263261068","metadata":{},"outputs":[],"source":["%%sh\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v1/fact/granularity=d/month=202003/date=2020-03-08/device_code=ap/country_code=us/\n\n \n \n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-2-31838298.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d cohort -p 7432 << EOF \n\n-- select sum(cnt) from plproxy.execute_select(\\$proxy\\$select count(1) as cnt \n--                    from mw.domain_basic_d where device_code = 'ap' and\n--                         country_code='us' and date='2020-03-08' limit 5 \\$proxy\\$)t (cnt bigint)  limit 3;\n\n-- select sum(cnt) from plproxy.execute_select(\\$proxy\\$select count(1) as cnt \n--                   from mw.domain_basic_d where  date='2020-03-08' limit 5 \\$proxy\\$)t (cnt bigint)  limit 3;\n                   \n\n-- select sum(cnt) from plproxy.execute_select(\\$proxy\\$select count(1) as cnt \n--                   from mw.category_d where  date='2020-03-08' limit 5 \\$proxy\\$)t (cnt bigint)  limit 3;\n\nselect domain_id, category_id, est_usage_penetration , rank_est_usage_penetration, device_code, country_code\n from plproxy.execute_select(\\$proxy\\$select domain_id, category_id, est_usage_penetration , rank_est_usage_penetration ,device_code,country_code\n                  from mw.category_d where  date='2020-03-08' and device_code='up' limit 5 \\$proxy\\$)t (domain_id bigint , category_id int , est_usage_penetration real, rank_est_usage_penetration int , device_code char, country_code char)  limit 3;\n\n\n\nselect domain_id, device_code, country_code, est_average_active_users, est_usage_penetration, est_total_time, est_average_session_per_user, est_average_session_duration, est_average_time_per_user\n from plproxy.execute_select(\\$proxy\\$select domain_id ,device_code,country_code, est_average_active_users, est_usage_penetration, est_total_time, est_average_session_per_user, est_average_session_duration, est_average_time_per_user  from mw.domain_basic_d where  date='2020-03-08' and device_code='ap' limit 5 \\$proxy\\$)t (domain_id bigint , device_code char, country_code char, est_average_active_users real, est_usage_penetration real, est_total_time real, est_average_session_per_user real , est_average_session_duration real, est_average_time_per_user real)  limit 3;\n\n\n\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200325-063046_1710658595","metadata":{},"outputs":[],"source":["\n\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v1/fact/granularity=d/month=202003/date=2020-03-08/\").filter(\"domain_id='70010000099289' and country_code='de' and device_code='ap' \").show()\nprint 'test'\nprint spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v3/fact/\").where(\"granularity='d' and month='202003' and date='2020-03-08'\").filter(\"domain_id='70010000099289' and country_code='de' and device_code='ap' \").show()\n\n\n# print spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v1/fact/granularity=d/month=202003/date=2020-03-08/\").filter(\"category_id='800000' and domain_id='70010000000004' and device_code='up' and country_code='de'\").show()\n# print 'test'\n# print spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v3/fact/\").where(\"granularity='d' and month='202003' and date='2020-03-08'\").filter(\"category_id='800000' and domain_id='70010000000004' and device_code='up' and country_code='de' \").show()"]},{"cell_type":"code","execution_count":0,"id":"20200321-103314_57880315","metadata":{},"outputs":[],"source":["\n\n# cross_domain_id like '70010000001654%' and\n# filter(\"app_id='100' and cross_domain_id like '70010000016986%' \").show()\nd1 = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v000/fact/granularity=m/month=202002/date=2020-02-29/\").drop(\"_identifier\").orderBy(\"category_id\",\"domain_id\",\"rank_est_usage_penetration\")\n\nd2= spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.ranking.v3/fact/\").where(\"granularity='m' and month='202002' and date='2020-02-29' and _identifier like '120200320%' \").drop(\"granularity\",\"month\",\"date\",\"device_code\",\"country_code\").drop(\"_identifier\").orderBy(\"category_id\",\"domain_id\",\"rank_est_usage_penetration\")\n\nprint d1.count()\nprint d2.count()\n# d2.subtract(d1).show()\n# d1.subtract(d2).show()\nd1 = d1.withColumnRenamed(\"category_id\", \"d1_category_id\").withColumnRenamed(\"domain_id\", \"d1_domain_id\")\nd3 = d1.join(d2, [d1.d1_category_id == d2.category_id, d1.d1_domain_id==d2.domain_id], how = 'outer')\n\n\nd3.filter(\"d1_category_id is null\").groupBy(\"category_id\").agg({\"category_id\":\"count\"}).show()\nd3.filter(\"category_id is null\").groupBy(\"device_code\").agg({\"device_code\":\"count\"}).show()\n\n# df3 = df1.join(df2, [df1.name == df2.name] , how = 'inner' )\n# df3.filter(df3.df1_count != df3.df2_count).show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200320-021408_1881797511","metadata":{},"outputs":[],"source":["\nfrom datetime import datetime\nfrom calendar import monthrange\nfrom pyspark.sql.utils import AnalysisException\n\ndate_list=set()\nfor m in range(1,3):\n    month=str('%02d' % m)\n    date_list.add(\"2020-%s-%s\" % (month ,monthrange(2020, m)[1]))\n\nip_packs = [\"basic\"]\nap_packs = [\"basic\"]\nup_packs = [\"basic\"]\npartitions = [(pack, 'w','ap', days) for pack in ap_packs for days in date_list]\npartitions.extend([(pack, 'w','ip',days) for pack in ip_packs for days in date_list])\npartitions.extend([(pack, 'w','up',days) for pack in up_packs for days in date_list])\n# print partitions\n# mobile-web.deduplicated.v1 w\n# mobile-web.ranking.v1 w d\n# mobile-web.basic.v1 w,d\n\n\n\nfor pack, granularity, device_code, date in partitions:\n    # print '========================================== %s ========================================== start to test' % pack \n    month = datetime.strptime(date, \"%Y-%m-%d\").strftime('%Y%m')\n    df1 = None\n    df2 = None\n    try:\n        # print \"%s, %s, %s, %s, %s started\" % (pack, granularity, month, date, device_code)\n        df2 = spark.read.option(\"basePath\", 's3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.%s.v1/fact' % pack).parquet('s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.%s.v1/fact/granularity=%s/month=%s/date=%s/device_code=%s' % (pack, granularity, month, date, device_code))\n        df1 = spark.read.format('delta').load('s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.%s.v3/fact' % pack).where(\"granularity='%s' and month='%s' and date='%s' and device_code='%s'\" % ( granularity, month, date, device_code)).withColumnRenamed(\"est_browser_traffic_share\",\"TS\").withColumnRenamed(\"est_outbound_traffic_share\",\"TS\").withColumnRenamed(\"est_referral_traffic_share\",\"TS\")\n        if sorted(df1.columns) != sorted(df2.columns):\n            print \"columns are different in %s, %s\"  % (granularity, date)\n            print sorted(df1.columns)\n            print sorted(df2.columns)\n            continue\n        df1 = df1.select(df1.columns).drop('_identifier').cache()\n        df2 = df2.select(df1.columns).drop('_identifier').cache()\n        count1 = df1.exceptAll(df2).count()\n        count2 = df2.exceptAll(df1).count()\n        if count1 != 0 or count2 !=0:\n            print \"Count1 in %s, %s, %s, %s, %s: %s\"  % (pack, granularity, month, date, device_code, count1)\n            print df1.exceptAll(df2).take(1)\n            print \"Count2 in %s, %s, %s, %s, %s: %s\"  % (pack, granularity, month, date, device_code, count2)\n            print df2.exceptAll(df1).take(1)\n            print \"df1: %s df2: %s\" % (df1.count(), df2.count())\n            continue\n        result_df = spark.createDataFrame([(pack, granularity, month, date, device_code, df1.count())], schema=[\"pack\",\"granularity\",\"month\",\"date\",\"device_code\",\"count\"])\n        result_df.coalesce(1).write.format(\"delta\").mode(\"append\").partitionBy(\"event_month\").save(\"s3://b2c-prod-data-pipeline-qa/aa.mobile.web/\")\n        print \"%s, %s, %s, %s, %s pass\" % (pack, granularity, month, date, device_code)\n    except AnalysisException as e:\n        print e\n    finally:\n        if df1:\n            df1.unpersist()\n        if df2:\n            df2.unpersist()\n"]},{"cell_type":"code","execution_count":0,"id":"20200321-141012_1581712851","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h 10.2.26.136 -U app_bdp_usage_qa -d aa -p 6432 << EOF \n--select * from plproxy.execute_select(\\$proxy\\$select date, app_id, city_id, est_average_active_users ,est_average_bytes_per_user\n--               from mu.city_app_w where city_id='31248510' and app_id='20600000304614'  and country_code='ae' and date='2020-02-29'\n--                \\$proxy\\$) t (date date, \n--               app_id bigint, city_id bigint, est_average_active_users real , est_average_bytes_per_user real) limit 3;\n-- SELECT domain_id AS domain_id, name AS domain_name FROM aa_domain_metadata WHERE is_disabled IN ('f') and sensitive_status IN (0);\n-- \\d aa_domain_metadata\nSELECT  domain_id , update_time  FROM aa_domain_metadata WHERE is_disabled IN ('f') and sensitive_status IN (0) and domain_id in ( '70010000208830' , '70010000210592', '70010000002208' , '70010000021237', '70010000000321', '70010000211341', '70010000001238');\nSELECT  *  FROM aa_domain_metadata WHERE is_disabled IN ('f') and sensitive_status IN (0) limit 3\n\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200317-090057_1998499806","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.utils import AnalysisException\nimport datetime\nfrom calendar import monthrange\nfrom datetime import date, timedelta\nfrom datetime import datetime as dt\n\n# date_list_month=set()\n# for m in range(1,4):\n#     month=str('%02d' % m)\n#     date_list_month.add(\"2020-%s-%s\" % (month ,monthrange(2020, m)[1]))\n# for m in range(1,13):\n#     month=str('%02d' % m)\n#     date_list_month.add(\"2019-%s-%s\" % (month ,monthrange(2019, m)[1]))\n\n# sorted(date_list_month)\n# ip_packs = [\"app-cross-domain\", \"basic\", \"cross-app\", \"cross-domain\", \"deduplicated\", \"domain-browser\", \"domain-outbound\", \"domain-referral\", \"domain-retention\", \"ranking\"]\n# ap_packs = [\"app-cross-domain\", \"basic\", \"cross-app\", \"cross-domain\", \"deduplicated\", \"domain-browser\", \"domain-outbound\", \"domain-referral\", \"domain-retention\", \"ranking\"]\n# up_packs = [\"ranking\"]\n# partitions = [(pack, 'm','ap', days) for pack in ap_packs for days in date_list_month]\n# partitions.extend([(pack, 'm','ip',days) for pack in ip_packs for days in date_list_month])\n# partitions.extend([(pack, 'm','up',days) for pack in up_packs for days in date_list_month])\n# print partitions\n# special_month=set()\n# for m in range(1,13):\n#     month=str('%02d' % m)\n#     special_month.add(\"2018-%s-%s\" % (month ,monthrange(2019, m)[1]))\n# new_partitions = [('cross-app', 'm','ip', days) for days in special_month]    \n\n\npartitions=[]\nstart = '2020-03-01'\nend = '2020-03-20'\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range =  real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\ndates.sort()\nip_packs_d = [\"ranking\", \"basic\"]\nap_packs_d = [\"ranking\", \"basic\"]\nup_packs_d = [\"ranking\", \"basic\"]\npartitions.extend([(pack, 'd','ap', str(days)) for pack in ip_packs_d for days in dates])\npartitions.extend([(pack, 'd','ip',str(days)) for pack in ap_packs_d for days in dates])\npartitions.extend([(pack, 'd','up',str(days)) for pack in up_packs_d for days in dates])\n\n\ndstart = date(2019,03,01)\ndend = date(2020,03,20)\n# this will return all sundays between start-end dates.\nweek_days = [dstart + timedelta(days=x) for x in range((dend-dstart).days + 1) if (dstart + timedelta(days=x)).weekday() == 5]\nsorted(week_days)\nip_packs_d = [\"deduplicated\", \"basic\",\"ranking\"]\nap_packs_d = [\"deduplicated\", \"basic\",\"ranking\"]\nup_packs_d = [\"deduplicated\", \"basic\",\"ranking\"]\npartitions.extend([(pack, 'w','ap', str(days)) for pack in ip_packs_d for days in week_days])\npartitions.extend([(pack, 'w','ip',str(days)) for pack in ap_packs_d for days in week_days])\npartitions.extend([(pack, 'w','up',str(days)) for pack in up_packs_d for days in week_days])\n\npartitions.sort()\n# basic\n# deduplicated\n# ranking\n\n\nfor pack, granularity, device_code, date in partitions:\n    # print '========================================== %s ========================================== start to test' % pack \n    month = dt.strptime(date, \"%Y-%m-%d\").strftime('%Y%m')\n    df1 = None\n    df2 = None\n    try:\n        # print \"%s, %s, %s, %s, %s started\" % (pack, granularity, month, date, device_code)\n        df2 = spark.read.option(\"basePath\", 's3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.%s.v1/fact' % pack).parquet('s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.%s.v1/fact/granularity=%s/month=%s/date=%s/device_code=%s' % (pack, granularity, month, date, device_code))\n        df1 = spark.read.format('delta').load('s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.%s.v3/fact' % pack).where(\"granularity='%s' and month='%s' and date='%s' and device_code='%s'\" % ( granularity, month, date, device_code)).withColumnRenamed(\"est_browser_traffic_share\",\"TS\").withColumnRenamed(\"est_outbound_traffic_share\",\"TS\").withColumnRenamed(\"est_referral_traffic_share\",\"TS\")\n        df_columns = df2.columns\n        if \"version\" in df_columns:\n            df_columns.remove(\"version\") \n        \n        if sorted(df_columns) != sorted(df1.columns):\n            print \"columns are different in %s, %s\"  % (granularity, date)\n            print sorted(df1.columns)\n            print sorted(df2.columns)\n            continue\n        df1 = df1.select(df1.columns).drop('_identifier').cache()\n        df2 = df2.select(df1.columns).drop('_identifier').cache()\n        count1 = df1.count()\n        count2 = df2.count()\n        count1_diff = df1.exceptAll(df2).count()\n        count2_diff = df2.exceptAll(df1).count()\n        if count1_diff != 0 or count1_diff !=0:\n            print \"Count1 in %s, %s, %s, %s, %s: %s\"  % (pack, granularity, month, date, device_code, count1_diff)\n            print df1.exceptAll(df2).take(1)\n            print \"Count2 in %s, %s, %s, %s, %s: %s\"  % (pack, granularity, month, date, device_code, count2_diff)\n            print df2.exceptAll(df1).take(1)\n            print \"df1: %s df2: %s\" % (count1, count2)\n            continue\n        result_df = spark.createDataFrame([(pack, granularity, month, date, device_code,count1, count2)], schema=[\"pack\",\"granularity\",\"month\",\"date\",\"device_code\",\"count1\", \"count2\"])\n        result_df.coalesce(1).write.format(\"delta\").mode(\"append\").partitionBy(\"event_month\").save(\"s3://b2c-prod-data-pipeline-qa/aa.mobile.web/\")\n        # print \"%s, %s, %s, %s, %s pass\" % (pack, granularity, month, date, device_code)\n    except AnalysisException as e:\n        if \"v3\" in str(e):\n            result_df = spark.createDataFrame([(pack, granularity, month, date, device_code,0, 0)], schema=[\"pack\",\"granularity\",\"month\",\"date\",\"device_code\",\"count1\", \"count2\"])\n            result_df.coalesce(1).write.format(\"delta\").mode(\"append\").partitionBy(\"event_month\").save(\"s3://b2c-prod-data-pipeline-qa/aa.mobile.web/\")\n            print 'data is not exist !!!! {},{}, {}, {} , {}, {}'.format(e, pack, granularity, month, date, device_code)\n    finally:\n        if df1:\n            df1.unpersist()\n        if df2:\n            df2.unpersist()\n"]},{"cell_type":"code","execution_count":0,"id":"20200322-060858_1002697381","metadata":{},"outputs":[],"source":["%%sh\necho `date`\n\nPGPASSWORD='2mHdFW6%#REu' psql -h 10.2.26.136 -U app_bdp_usage_qa -d dna -p 6432 << EOF \n\nSELECT * FROM in_domain_category_mapping order by last_updated desc limit 20\nEOF "]},{"cell_type":"code","execution_count":0,"id":"20200310-090431_752611782","metadata":{},"outputs":[],"source":["\nimport pyspark.sql.functions as F\n# ip_packs_single_day = [\"app-cross-domain\",  \"cross-app\", \"cross-domain\", \"deduplicated\", \"domain-browser\", \"domain-outbound\", \"domain-referral\", \"domain-retention\", \"ranking\", \"basic\"]\n\ndef get_compared_data(date):\n    df_new = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v3/fact/\").where(\"granularity='m' and month='{}' and date='{}' \".format(date.replace(\"-\",\"\")[:-2], date))\n    df_old = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-mobileweb-paid/unified/mobile-web.basic.v000/fact/granularity=m/month=202002/date={}/\".format(date))\n\n    list1= df_new.sample(False, 0.001, seed=0).limit(20).take(20)\n    return list1, df_old\n\ntest_list, df_old = get_compared_data('2020-02-29')\n\n\n\ndef compare(test_list, df_old):\n    columns_list = df_old.columns\n    print columns_list\n    columns_list.remove(\"_identifier\")\n    # columns_list.remove(\"TS\")\n    print len(test_list)\n    for x in range(20):\n        # list2 = df_old.filter(\"app_id='{}' and cross_domain_id='{}' \".format(test_list[x][\"app_id\"], test_list[x][\"cross_domain_id\"])).collect()\n        # list2 = df_old.filter(\"domain_id='{}' \".format(test_list[x][\"domain_id\"])).collect()\n        # list2 = df_old.filter(\"domain_id='{}' and cross_app_id='{}' \".format(test_list[x][\"domain_id\"],test_list[x][\"cross_app_id\"])).collect()\n        # list2 = df_old.filter(\"domain_id='{}' and browser_app_id='{}' \".format(test_list[x][\"domain_id\"],test_list[x][\"browser_app_id\"])).collect()\n        # list2 = df_old.filter(\"domain_id='{}' and outbound_domain_id='{}' \".format(test_list[x][\"domain_id\"],test_list[x][\"outbound_domain_id\"])).collect()\n        # list2 = df_old.filter(\"domain_id='{}' and referral_domain_id='{}' \".format(test_list[x][\"domain_id\"],test_list[x][\"referral_domain_id\"])).collect()\n        # list2 = df_old.filter(\"domain_id='{}'and retention_days='{}' \".format(test_list[x][\"domain_id\"],test_list[x][\"retention_days\"] )).collect()\n        # list2 = df_old.filter(\"category_id='{}'and domain_id='{}' and device_code='{}' \".format(test_list[x][\"category_id\"],test_list[x][\"domain_id\"],test_list[x][\"device_code\"] )).collect()\n        list2 = df_old.filter(\"domain_id='{}'  \".format(test_list[x][\"domain_id\"])).collect()\n\n        for column in columns_list:\n            # print column\n            if test_list[x][column] != list2[0][column]:\n                print 'not equal',column, test_list[x][column], list2[0][column]\n    print 'Finished'           \n\ncompare(test_list, df_old)\n\n# print spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-usage/unified/city-level-usage.v2/fact/\").where(\"granularity='m' and date='2020-02-29'\").filter(\"device_code='ap'\").count()"]},{"cell_type":"code","execution_count":0,"id":"20200325-031713_80535227","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}