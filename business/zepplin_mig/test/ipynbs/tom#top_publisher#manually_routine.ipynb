{"cells":[{"cell_type":"code","execution_count":0,"id":"20200208-025344_1682238072","metadata":{},"outputs":[],"source":["\n\nimport datetime\nimport psycopg2\nfrom contextlib import closing\ndbinfo = {\n    'NAME': 'dailyest',\n    'USER': 'app_bdp_usage_qa',\n    'PASSWORD': '2mHdFW6%#REu',\n    'HOST': 'internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com',\n    'PORT': 7432,\n}\nconn = psycopg2.connect(\"\"\"\n            dbname={dbname} user={user} host={host} port={port} password={passwd}\n            \"\"\".format(dbname=dbinfo['NAME'], user=dbinfo['USER'], passwd=dbinfo['PASSWORD'],\n                       host=dbinfo['HOST'], port=dbinfo['PORT']))\ndef select(stmt, params):\n    with closing(conn.cursor()) as cur:\n        cur.execute(stmt, params)\n        rows = cur.fetchall()\n        cnt = cur.rowcount\n    return rows, cnt\n\n\nsql = \"\"\"select device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate from plproxy.execute_select_nestloop($proxy$ select device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate from ms.market_size_daily_estimate_1000 where date = '2020-01-10' and purchase_type_id=12 and app_type_id=2 and store_id=14 and category_id in (65) $proxy$) tbl (device_id SMALLINT, date DATE, store_id INT, kpi SMALLINT, category_id INT, app_type_id SMALLINT, purchase_type_id SMALLINT, estimate BIGINT);\"\"\"\nrows, _ = select(sql, None)\nprint len(rows)\n\nfor row in rows:\n    print row\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200210-041014_582896329","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \nselect date, sum(count_a) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select date, count(pub_id) as counta\n    from pp.pub_store_daily_estimate_0\n    where \n        DATE between '2010-07-04' and '2010-07-04'\n    group by date\n\\$proxy\\$) tbl (date DATE, count_a BIGINT) group by date order by date asc;\n\n--\\$proxy\\$) tbl (device_id SMALLINT, date DATE, store_id INT, kpi SMALLINT, category_id INT, app_type_id SMALLINT, purchase_type_id SMALLINT, estimate BIGINT);\n--2010-07-04, 10575875, 10575875, 0\n--------------+----------\n-- 2010-07-04 | 10575875\n\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-023007_1224286779","metadata":{},"outputs":[],"source":["%%sh\n\ntime PGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nEXPLAIN ANALYZE\nSELECT sum(est_revenue) FROM store.store_est_publisher_fact_v2 WHERE date BETWEEN '2017-05-01' AND '2017-05-31';\nEOF\n\ntime PGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nEXPLAIN ANALYZE\nSELECT sum(est_revenue) FROM store.store_est_publisher_fact_v2 WHERE granularity='daily' AND date in ('2018-01-01', '2018-01-31');\nEOF\n\n\n\n#category_id=400070\n# device_code=\"android-all\"\n# country_code=\"ZA\"\n# app_price_type_id=2\n# purchase_type_id=12\n"]},{"cell_type":"code","execution_count":0,"id":"20200819-031851_1228582716","metadata":{},"outputs":[],"source":["%%sh\n\ntime PGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \n\\x\nselect * from pg_indexes where tablename='store_est_publisher_fact_v2';\nEOF\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-024638_419356960","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import lit, udf\nfrom pyspark.sql.types import (\n    ArrayType, BooleanType, LongType, IntegerType,\n    StringType, StructType, StructField\n)\nschema = StructType([\n    StructField(\"store_id\", IntegerType(), False),\n    StructField(\"date\", StringType(), False),\n    StructField(\"platform_id\", IntegerType(), False),\n    StructField(\"device\", StringType(), False),\n    StructField(\"data_type\", StringType(), False),\n    StructField(\"price_type\", IntegerType(), False),\n    StructField(\"purchase_type\", IntegerType(), False),\n    StructField(\"category_id\", IntegerType(), False),\n    StructField(\"estimate\", LongType(), False)\n])\n# sample unified data:\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/date=2020-01-10/\").filter(\"country_code='US' and app_price_type_id=0 and purchase_type_id=10 and device_code='android-all' and category_id=400002\").show()\n# raw data\nprint spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-01-10/ios/market_size/*\", schema=schema, sep='\\t').show(1)\n# print spark.read.csv([\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-01-10/android/market_size/*\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-01-10/ios/market_size/*\"], schema=schema, sep='\\t').count()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200621-025222_1302843844","metadata":{},"outputs":[],"source":["\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher.v1/fact/granularity=daily/date=2020-01-01\").show(10)\n# spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-01/\").select('store_id').distinct().show(9999)\n# spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-01/\").select('category_id').distinct().show(9999)\n# spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-01/\").select('feed').distinct().show(9999)\n\nspark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-01/\").select('platform').distinct().show(9999)\nspark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-01/\").select('platform_id').distinct().show(9999)\n\n\n# print spark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-01/\").filter(\"store_id in (10, 143441)\").count()\n# df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher.v1/fact/granularity=daily/date=2020-01-01\").filter(\"country_code = 'US'\")\n# metrics_count = 0\n# for metric in [\"free_app_download\", \"paid_app_download\", \"revenue\"]:\n#     metrics_count += df.filter(\"{} is not null\".format(metric)).count()\n# print metrics_count\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-025517_1199195064","metadata":{},"outputs":[],"source":["%%sh\n\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher.v1/fact/granularity=daily/date=2020-01-01/device_code=ios-all/\n# aws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-01/platform=ios/\n# aws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-01/platform=android/\n\naws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-100727_2100263628","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport pandas as pd\npd.set_option('expand_frame_repr', False)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-075305_1430324376","metadata":{},"outputs":[],"source":["\n\n# Copyright (c) 2018 App Annie Inc. All rights reserved.\n# pylint: disable=E1101,C0412,C1801\n\n\"\"\"\nDB Check modules\n\"\"\"\n\nimport datetime\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df, etl_skip\nfrom applications.db_check_v1.common.constants import COUNTRY_CODE_MAPPING_BY_MARKET_CODE as COUNTRY_CODE_MAPPING, \\\n    CATEGORY_ID_MAPPING_BY_MARLKET_AND_DEVICE_CODE as CATEGORY_ID_MAPPING\nfrom applications.db_check_v1.common.utils import get_week_start_end_date, get_date_list\n#from applications.db_check_v1.cases.store.publisher_est_v1.constants import MARKET_SIZE_DSN\n\n\nclass PublisherEstRawData(object):\n    raw_s3_path = \"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY\" \\\n                  \"/date={}/\"\n    device_code_mapping = {\n        \"00\": \"android-all\",\n        \"01\": \"android-all\",\n        \"02\": \"android-all\",\n        \"10\": \"ios-phone\",\n        \"11\": \"ios-phone\",\n        \"12\": \"ios-phone\",\n        \"1100\": \"ios-tablet\",\n        \"1101\": \"ios-tablet\",\n        \"1102\": \"ios-tablet\",\n        \"21000\": \"ios-all\",\n        \"21001\": \"ios-all\",\n        \"21002\": \"ios-all\",\n    }\n\n    metric_mapping = {\n        0: \"free_app_download\",\n        1: \"paid_app_download\",\n        2: \"revenue\",\n        101: \"free_app_download\",\n        100: \"paid_app_download\",\n        102: \"revenue\",\n        1000: \"free_app_download\",\n        1001: \"paid_app_download\",\n        1002: \"revenue\"\n    }\n\n    dimension_mapping = {\n        \"id\": \"publisher_id\",\n    }\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code):\n        df = self._get_raw_data_by_date_country(date, country_code)\n        df = self._parse_mapping(df)\n        df = self._parse_unified_format(df)\n        df = self._data_clean_up(df)\n        return df\n\n    def _data_clean_up(self, df):\n        # clean unknown mapping\n        category_id_list = list(set(CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].values() +\n                                    CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].values()))\n\n        country_code_list = list(set(COUNTRY_CODE_MAPPING[\"apple-store\"].values() +\n                                     COUNTRY_CODE_MAPPING[\"google-play\"].values()))\n\n        df = df[(df['category_id'].isin(category_id_list)) & (df['country_code'].isin(country_code_list))]\n        return df\n\n    def _parse_mapping(self, df):\n        # country_code mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"google-play\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"apple-store\"]})\n        df = df.rename(columns={'store_id': 'country_code'})\n\n        # category_id mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"]})\n\n        # device_code mapping\n        df[\"device_code\"] = df[\"platform_id\"].astype(str) + df[\"feed\"].astype(str)\n        df = df.replace({\"device_code\": self.device_code_mapping})\n\n        # granularity\n        df[\"granularity\"] = \"daily\"\n\n        # metrics mapping (from feed)\n        df = df.replace({\"feed\": self.metric_mapping})\n        return df\n\n    def _parse_unified_format(self, df):\n        df = df.rename(columns=self.dimension_mapping)\n        df = df.pivot_table(index=[\"publisher_id\", \"category_id\", \"device_code\", \"country_code\", \"granularity\"],\n                            columns='feed', values='est')\n        df.reset_index(inplace=True)\n        df.columns.name = None\n        return df\n\n    def _get_raw_data_by_date_country(self, date, country_code):\n        \"\"\"\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        |        id|store_id|category_id|platform_id|vertical|rank|feed|  est|platform|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        | 284417353|       0|       6006|          1|       1|   1|1002|45235|     ios|\n        | 349554266|       0|       6006|          1|       1|   2|1002|20732|     ios|\n        |1316153435|       0|       6006|          1|       1|   3|1002|15136|     ios|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        \"\"\"\n        ios_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"apple-store\"].items() if v == country_code]\n        gp_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"google-play\"].items() if v == country_code]\n        raw_df = self.spark.read.parquet(self.raw_s3_path.format(date)).\\\n            filter('store_id in ({})'.format(\",\".join(ios_store_ids + gp_store_ids))).toPandas()\n        return raw_df\n\n    def get_metrics_count(self, date):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n        # ios_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].keys()]\n        # gp_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].keys()]\n\n        fillter_sql = \"platform_id = {} and store_id in ({})\"\n        df = self.spark.read.parquet(self.raw_s3_path.format(date))\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id))).count() + \\\n                    df.filter(fillter_sql.format(0, \",\".join(gp_store_id))).count()\n        return count_all\n\n    def get_v1_raw_metrics_count(self, date):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n\n        df = self.spark.read.option(\"delimiter\", \"\\t\").csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/*/sbe_est_publisher/*/*.csv.gz\".format(date))\n        fillter_sql = \"_c2 = {} and _c0 in ({})\"  # _c2 > platform_id, _c0 > store_id\n\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id))).count() + \\\n                    df.filter(fillter_sql.format(0, \",\".join(gp_store_id))).count()\n        return count_all\n\n\nclass PublisherEstUnifiedData(object):\n    unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher.v1/fact/\" \\\n                      \"granularity=daily/date={}/\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code):\n        unified_df = self.spark.read.format(\"delta\").\\\n            load(self.unified_s3_path.format(date)).filter(\"country_code = '{}'\".format(country_code)).toPandas()\n        unified_df = unified_df.drop([\"_identifier\", \"revenue_iap\", \"revenue_non_iap\", \"date\"], axis=1)\n        return unified_df\n\n    def get_metrics_count(self, date):\n        df = self.spark.read.format(\"delta\").load(self.unified_s3_path.format(date))\n        metrics_count = 0\n        for metric in [\"free_app_download\", \"paid_app_download\", \"revenue\"]:\n            metrics_count += df.filter(\"{} is not null\".format(metric)).count()\n        return metrics_count\n\n\n\nclass PublisherEstDBData(object):\n    def get(self, date):\n        sql = \"SELECT * FROM store.store_publisher_est_fact_v1 WHERE date='{}'\".format(date)\n        return query_df(MARKET_SIZE_DSN, sql)\n\n    def get_led(self):\n        sql = \"SELECT * FROM store.store_publisher_est_latest_date_v1\"\n        return query_df(MARKET_SIZE_DSN, sql)\n\n\nclass TestPublisherEstWeekly(PipelineTest):\n    # Every Tuesday 16:00 UTC time will refresh the data of last Full Week.\n    trigger_date_config = ('* 16 * * 2', 3)\n\n    def _compare_df(self, df1, df2, log=''):\n        for diff_type in [\"left\", \"right\"]:\n            diff_df = df1.merge(df2, indicator=True, how=diff_type)  # .loc[lambda x : x['_merge']!='both']\n            diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n            if len(diff_df) != 0:\n                print diff_type\n                print \"dataframe overview of df1 and df2\"\n                print df1\n                print df2\n                print \"dimension overview of diff df\"\n                print diff_df.country_code.unique()\n                print diff_df.category_id.unique()\n                print diff_df.device_code.unique()\n            self.assertEqual(len(diff_df), 0,\n                             msg=\"found mismatch when compare the raw, unified, db.\"\n                                 \" diff count is \\n {}, logs:{}\".format(len(diff_df), log))\n\n    @etl_skip()\n    def test_publisher_est_etl_accuracy(self):\n        # Every Tuesday 16:00 UTC time will refresh the data of last Full Week.\n        country_code = 'US'\n        start_date, end_date = get_week_start_end_date(self.check_date_str)\n        date_list = get_date_list(start_date, end_date)\n        for date in date_list:\n            raw_df = PublisherEstRawData(self.spark).get(date, country_code)\n            unified_df = PublisherEstUnifiedData(self.spark).get(date, country_code)\n            # db_df = PublisherEstDBData().get(date)\n\n            self._compare_df(raw_df, unified_df, log=\"raw / unified - {}\".format(date))\n            # self._compare_df(unified_df, db_df, log=\"unified / db - {}\".format(date))\n\n    @etl_skip()\n    def test_publisher_Est_etl_completeness(self):\n        start_date, end_date = get_week_start_end_date(self.check_date_str)\n        date_list = get_date_list(start_date, end_date)\n        for date in date_list:\n            raw_count = PublisherEstRawData(self.spark).get(date)\n            unified_count = PublisherEstUnifiedData(self.spark).get(date)\n            self.assertEqual(raw_count, unified_count)\n\n    def test_publisher_est_etl_timelines(self):\n        # Every Tuesday 16:00 UTC time will refresh the data of last Full Week.\n        # E.g. 2020-02-11 17:00 the data of 2020-02-02 ~ 2020-02-08 will be ready\n        trigger_datetime = datetime.datetime.strptime(\"2020-02-11 17:00:00\", '%Y-%m-%d %H:%M:%S')\n        check_date_str_actual = self._get_check_date_from_routing_config(trigger_datetime).strftime(\"%Y-%m-%d\")\n        self.assertEqual(\"2020-02-08\", check_date_str_actual)"]},{"cell_type":"code","execution_count":0,"id":"20200211-061600_1445661098","metadata":{},"outputs":[],"source":["\n\nfrom datetime import datetime\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\nbegin_date = datetime(2019, 07, 14)\nend_date = datetime(2020, 06, 06)\n\nDATE_GRANULARITY_MAPPINGLIST = {\n    \"daily\": get_date_list(begin_date, end_date, \"D\"),\n    \"weekly\": get_date_list(begin_date, end_date, \"W-SAT\"),\n    \"monthly\": get_date_list(begin_date, end_date, \"M\")\n}\n\n\ndef debug(case_list):\n    \"\"\"\n    method of debug cases in zepplin, result will be print in standard output\n\n    e.g. test case list example\n    TestMarketSizeWeekly(trigger_datetime=t_date, methodName='test_market_size_etl_accuracy_and_completeness')\n    \"\"\"\n    std_out_origin = sys.stdout\n    std_err_origin = sys.stderr\n    try:\n        suite = unittest.TestSuite()\n        for case in case_list:\n            suite.addTest(case)\n        runner = unittest.TextTestRunner(verbosity=2, buffer=True)\n        runner.run(suite)\n    except Exception as ex:\n        print dir(ex)\n        print ex.message\n        traceback.print_exception(type(ex), ex, ex.__traceback__)\n    finally:\n        sys.stdout = std_out_origin\n        sys.stderr = std_err_origin\n\n\ndef load_context_zepplin(spark):\n    \"\"\"\n    load context in zepplin\n    \"\"\"\n    spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n    spark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/code.zip\")\n\ndef _compare_df(self, df1, df2, log=''):\n    for diff_type in [\"left\", \"right\"]:\n        diff_df = df1.merge(df2, indicator=True, how=diff_type)  # .loc[lambda x : x['_merge']!='both']\n        diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n        if len(diff_df) != 0:\n            print diff_type\n            print \"dataframe overview of df1 and df2\"\n            print df1\n            print df2\n            print \"dimension overview of diff df\"\n            print diff_df.country_code.unique()\n            print diff_df.category_id.unique()\n            print diff_df.device_code.unique()\n        self.assertEqual(len(diff_df), 0,\n                         msg=\"found mismatch when compare the raw, unified, db.\"\n                             \" diff count is \\n {}, logs:{}\".format(len(diff_df), log))\n        print \"PASS - {}\".format(log)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200218-025142_1844292398","metadata":{},"outputs":[],"source":["\n\nbegin_date = datetime.datetime(2010, 07, 04)\nend_date = datetime.datetime(2019, 07, 13)\n\n\ndate_list = get_date_list(begin_date, end_date, \"D\")\n\nfor date in date_list:\n    # count1 = PublisherEstRawData(spark).get_metrics_count(date)\n    count1 = PublisherEstRawData(spark).get_v1_raw_metrics_count(date)\n    \n    count2 = PublisherEstUnifiedData(spark).get_metrics_count(date)\n    print \"{}, {}, {}, {}\".format(date, count1, count2, count1-count2)\n"]},{"cell_type":"code","execution_count":0,"id":"20200218-025316_522492770","metadata":{},"outputs":[],"source":["\n\nraw_df.        \ncategory_id_list = list(set(CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].values() +\n                                    CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].values()))\n\n        country_code_list = list(set(COUNTRY_CODE_MAPPING[\"apple-store\"].values() +\n                                     COUNTRY_CODE_MAPPING[\"google-play\"].values()))\n\n        df = df[(df['category_id'].isin(category_id_list)) & (df['country_code'].isin(country_code_list))]\n"]},{"cell_type":"code","execution_count":0,"id":"20200211-082622_57536812","metadata":{},"outputs":[],"source":["\n\n# legacy_raw_df = MarketSizeRawData(spark)._get_raw_data_by_date(\"2020-01-01\")\n# print legacy_raw_df.loc[(legacy_raw_df[\"category_id\"]==36) & (legacy_raw_df[\"device\"]!=\"google-play\")]\nprint raw_df.loc[(raw_df[\"category_id\"]==100000) & (raw_df[\"device_code\"]!=\"android-all\")]\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-101218_1638908096","metadata":{},"outputs":[],"source":["\n\nfrom applications.db_check_v1.cases.store.app_rank_v1.constants import citus_dsn\nfrom applications.db_check_v1.common.db_check_utils import query_df\nfrom datetime import datetime\ndate_list = DATE_GRANULARITY_MAPPINGLIST[\"daily\"]\ndate_list = [\"2020-01-09\", \"2020-01-10\", \"2020-01-29\"]\n\n\nfor date in date_list:\n    print '*'*100\n    print date\n    start=datetime.today()\n    # legacy_raw_df = MarketSizeRawData(spark)._get_raw_data_by_date(date)\n    raw_df = MarketSizeRawData(spark).get(date)\n\n    # unified_df =  spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/date={}/\".format(date)).toPandas()\n    # unified_df = unified_df.drop([\"_identifier\"], axis=1)\n    \n    # legacy_unified_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/date={}/\".format(date)).toPandas()\n    # legacy_unified_df = legacy_unified_df.drop([\"_identifier\"], axis=1)\n    # legacy_unified_df[\"category_id\"] = pd.to_numeric(legacy_unified_df[\"category_id\"])\n    \n    sql = \"SELECT * FROM store.store_market_size_fact_v1  WHERE date='{}'\".format(date)\n    db_df = query_df(citus_dsn, sql)\n\n    print datetime.today() - start\n\n\n    df1= raw_df\n    # df2= unified_df\n    # df2= legacy_unified_df\n    df2= db_df\n    for diff_type in [\"left\", \"right\"]:\n        diff_df = df1.merge(df2,indicator = True, how=diff_type)#.loc[lambda x : x['_merge']!='both']\n        diff_df = diff_df.loc[ diff_df[\"_merge\"]!=\"both\"]\n        if len(diff_df)!=0:\n            print len(df1)\n            print len(df2)\n            print diff_type\n            print len(diff_df)\n            print diff_df\n            print diff_df.country_code.unique()\n            print diff_df.category_id.unique()\n            print diff_df.device_code.unique()\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-130042_812589505","metadata":{},"outputs":[],"source":["\n\n\n# print raw_df.loc[raw_df[\"est_market_size_revenue\"]==15017778.5]\n# print unified_df.loc[unified_df[\"est_market_size_revenue\"]==15017778.5]\n# print legacy_raw_df\n\ndate = \"2020-01-01\"\n\n# legacy_raw_df = MarketSizeRawData(spark)._get_raw_data_by_date(date)\nraw_df = MarketSizeRawData(spark).get(date)\n\nunified_df =  spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/date={}/\".format(date)).toPandas()\n# unified_df = unified_df.drop([\"_identifier\"], axis=1)\n\n# legacy_unified_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/date={}/\".format(date)).toPandas()\n# legacy_unified_df = legacy_unified_df.drop([\"_identifier\"], axis=1)\n# legacy_unified_df[\"category_id\"] = pd.to_numeric(legacy_unified_df[\"category_id\"])\n\n\n#        app_price_type_id  purchase_type_id  category_id  device_code country_code  est_market_size_download  est_market_size_revenue        date granularity  _disable_idx_4_query      _merge\n# 183842                  1                11       400007  android-all           CN                      69.0                      NaN  2020-01-09       daily                     0  right_only\n\ncategory_id=400007\ndevice_code=\"android-all\"\ncountry_code=\"CN\"\napp_price_type_id=1\npurchase_type_id=12\n\n# print legacy_raw_df.loc[(legacy_raw_df[\"category_id\"]==44) & (legacy_raw_df[\"device\"]==\"google-play\")  &  (legacy_raw_df[\"store_id\"]==3)  & (legacy_raw_df[\"price_type\"]==app_price_type_id)  & (legacy_raw_df[\"purchase_type\"]==purchase_type_id)]\nprint legacy_raw_df.loc[(legacy_raw_df[\"category_id\"]==44) & (legacy_raw_df[\"store_id\"]==3)]\n\n\nprint raw_df.loc[(raw_df[\"category_id\"]==category_id) & (raw_df[\"device_code\"]==device_code) & (raw_df[\"country_code\"]==country_code) & (raw_df[\"app_price_type_id\"]==app_price_type_id) & (raw_df[\"purchase_type_id\"]==purchase_type_id) ]\n\n\n# print unified_df.loc[(unified_df[\"category_id\"]==category_id) & (unified_df[\"device_code\"]==device_code) & (unified_df[\"country_code\"]==country_code) & (unified_df[\"app_price_type_id\"]==app_price_type_id) & (unified_df[\"purchase_type_id\"]==purchase_type_id) ]\n\n\n# print legacy_unified_df.loc[(legacy_unified_df[\"category_id\"]==category_id) & (legacy_unified_df[\"device_code\"]==device_code) & (legacy_unified_df[\"country_code\"]==country_code) & (legacy_unified_df[\"app_price_type_id\"]==app_price_type_id) & (legacy_unified_df[\"purchase_type_id\"]==purchase_type_id) ]\n\n\nprint \"(1000, datetime.date(2020, 1, 10), 14, 1, 65, 2, 12, 188L)\"\n# print legacy_unified_df\n# print raw_df\n# print diff_df.country_code.unique()\n# print diff_df.category_id.unique()\n# print diff_df.device_code.unique()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200215-043343_601649260","metadata":{},"outputs":[],"source":["\n\nprint raw_df"]},{"cell_type":"code","execution_count":0,"id":"20200214-044820_1257269720","metadata":{},"outputs":[],"source":["%%sh\n# SELECT * FROM store.store_market_size_fact_v1  WHERE category_id=400007 AND app_price_type_id=1 AND purchase_type_id=12 AND country_code='CN' AND device_code='android-all' AND date = '2020-01-09'\n\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132 -U citus_bdp_usage_qa -d aa_citus_db -p 5432 << EOF \nSELECT count(*) FROM store.store_market_size_fact_v1  WHERE date = '2020-01-30'\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-102150_1616331121","metadata":{},"outputs":[],"source":["\n# tom_df = tom_df.drop([\"date\"], axis=1)\n# unified_df = unified_df.drop([\"_identifier\"], axis=1)\n# legacy_unified_df = legacy_unified_df.drop([\"_identifier\"], axis=1)\npd.set_option('expand_frame_repr', False)\n\n# print raw_df.dtypes\n# print unified_df.dtypes\n# print legacy_unified_df.dtypes\n\n\n# print raw_df\n# print unified_df\n# print legacy_unified_df\n\ndf1= raw_df\ndf2= unified_df\n# df2= legacy_unified_df\n\ndiff_df = df1.merge(df2,indicator = True, how='left', on=[\"app_price_type_id\",  \"purchase_type_id\",  \"category_id\",  \"device_code\", \"country_code\", \"est_market_size_download\", \"est_market_size_revenue\" ])#.loc[lambda x : x['_merge']!='both']\n# print df2[ ~df2.isin(df1)].dropna()\n\ndiff_df = diff_df.loc[ diff_df[\"_merge\"]!=\"both\"]\n\nprint diff_df\nprint diff_df.country_code.unique()\nprint diff_df.category_id.unique()\nprint diff_df.device_code.unique()"]},{"cell_type":"code","execution_count":0,"id":"20200207-132458_525033164","metadata":{},"outputs":[],"source":["%%sh\n\n\naws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-01-10/ios/market_size/143443/\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/date=2020-01-10/\n"]},{"cell_type":"code","execution_count":0,"id":"20200207-024355_1951106438","metadata":{},"outputs":[],"source":["%%sh\n# aws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-02-01/ios/market_size/143441/\n# aws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-02-08/android/market_size/3/\n# aws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/MARKET_SIZE_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-01-13/platform=android/\naws s3 ls s3://b2c-prod-dca-store-estimates/store_estv2/MARKET_SIZE_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-02-15/\n\naws s3 ls s3://b2c-prod-data-pipeline-raw-store-paid/\necho '***************************************************************************************'\n# aws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-02-01/android/market_size/1/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/\n# aws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/\n"]},{"cell_type":"code","execution_count":0,"id":"20200208-020656_120777918","metadata":{},"outputs":[],"source":["\n\n\"\"\"\n+--------+----------+-----------+-----------+---------+----------+-------------+-----------+--------+\n|store_id|      date|platform_id|     device|data_type|price_type|purchase_type|category_id|estimate|\n+--------+----------+-----------+-----------+---------+----------+-------------+-----------+--------+\n|      10|2020-01-10|          0|google-play|downloads|         1|           10|          1|11981138|\n+--------+----------+-----------+-----------+---------+----------+-------------+-----------+--------+\n    \"\"\"\nschema = StructType([\n    StructField(\"store_id\", IntegerType(), False),\n    StructField(\"date\", StringType(), False),\n    StructField(\"platform_id\", IntegerType(), False),\n    StructField(\"device\", StringType(), False),\n    StructField(\"data_type\", StringType(), False),\n    StructField(\"price_type\", IntegerType(), False),\n    StructField(\"purchase_type\", IntegerType(), False),\n    StructField(\"category_id\", IntegerType(), False),\n    StructField(\"estimate\", LongType(), False)\n])\ns3_path = \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-01-09/android/market_size/3/\"\ndebug_raw_df = spark.read.csv(s3_path, schema=schema, sep='\\t').toPandas()\n\nprint debug_raw_df.loc[ (debug_raw_df[\"category_id\"]==44)]\n\nprint  debug_raw_df"]},{"cell_type":"code","execution_count":0,"id":"20200208-020919_171935550","metadata":{},"outputs":[],"source":["\nfrom aadatapipelinecore.core.fs import Conf\nfrom aadatapipelinecore.core.fs.device import S3Bucket, specified_bucket\n\nfrom applications.db_check_v1.common.constants import COUNTRY_CODE_MAPPING_BY_MARKET_CODE as COUNTRY_CODE_MAPPING, \\\n    CATEGORY_ID_MAPPING_BY_MARLKET_AND_DEVICE_CODE as CATEGORY_ID_MAPPING\n\n\nconf = Conf(\n    bucket_name=\"b2c-prod-dca-store-estimates\",\n    bucket_class=S3Bucket\n)\nbucket = specified_bucket(conf)\n\ndate_list = [\"2020-01-10\", \"2019-01-10\", \"2018-01-10\"]\n\nfor store in [\"ios\", \"android\"]:\n    for date in date_list:\n        path = \"store_est/v_final/DAY/{date}/{store}/market_size/\".format(date=date, store=store)\n        print \"*\" * 100\n        print path\n        for filepath in bucket.list(path, depth_is_1=True):\n            # print filepath\n            filename = filepath.replace(path, '').replace(\"/\", '')\n            country_id_list = COUNTRY_CODE_MAPPING[\"apple-store\"].keys() if store==\"ios\" else COUNTRY_CODE_MAPPING[\"google-play\"].keys()\n            country_id_list = [str(country) for country in country_id_list]\n            if filename not in country_id_list:\n                print \"{filename} should not in {path}\".format(filename=filename, path=path)\n                \n    \n \n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200208-023653_1800154508","metadata":{},"outputs":[],"source":["\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/date=2020-01-10/\").printSchema()\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/date=2020-01-10/\").printSchema()"]},{"cell_type":"code","execution_count":0,"id":"20200210-040711_1922762321","metadata":{},"outputs":[],"source":["\n\n       app_price_type_id  category_id country_code  device_code  est_market_size_download  est_market_size_revenue  purchase_type_id\n56749                  2       400070           ZA  android-all                     191.0                      NaN                12"]},{"cell_type":"code","execution_count":0,"id":"20200214-065004_621186325","metadata":{},"outputs":[],"source":["%%sh\n\n%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \nselect device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate\n    from ms.market_size_daily_estimate_1000 \n    where \n        date = '2020-01-10' and \n        purchase_type_id=12 and \n        app_type_id=2 and \n        store_id=14 and \n        category_id in (65) \n\\$proxy\\$) tbl (device_id SMALLINT, date DATE, store_id INT, kpi SMALLINT, category_id INT, app_type_id SMALLINT, purchase_type_id SMALLINT, estimate BIGINT);\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200622-101225_809647011","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-qa/tom/top_publisher/\n\naws s3 cp s3://b2c-prod-data-pipeline-qa/tom/top_publisher/regression1.txt /tmp/regression1.txt\naws s3 cp s3://b2c-prod-data-pipeline-qa/tom/top_publisher/regression2.txt /tmp/regression2.txt\naws s3 cp s3://b2c-prod-data-pipeline-qa/tom/top_publisher/regression3.txt /tmp/regression3.txt\naws s3 cp s3://b2c-prod-data-pipeline-qa/tom/top_publisher/regression4.txt /tmp/regression4.txt\naws s3 cp s3://b2c-prod-data-pipeline-qa/tom/top_publisher/regression5.txt /tmp/regression5.txt\n\ncat /tmp/regression*.txt\n"]},{"cell_type":"code","execution_count":0,"id":"20200622-101229_56282400","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}