{"cells":[{"cell_type":"code","execution_count":0,"id":"20200615-042103_705957101","metadata":{},"outputs":[],"source":["\nimport datetime\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.transform import _view\nfrom aadatapipelinecore.core.utils.spark import eject_all_caches\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\n\n\"\"\"\nget date:  [ [month, [days]], [month, [days]], [month, [days]], ....... ]\n\"\"\"\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\n# start = \"2010-07-31\"  # prod\nstart = \"2018-09-30\"    # test\nend = start\n\nmonthly = get_date_list(start, end, freq='M')\nprint monthly\n\n\ndate_list = []\nfor m in monthly:\n    d = get_date_list(m[:8]+'01', m, freq='D')  # start = the first day of each month; end = each month\n    month_and_day = [m, d]\n    date_list.append(month_and_day)\n\nprint date_list\n\n\n\nclass DryRunSqlExecutor(SqlExecutor):\n    def _verify_tasks(self):\n        pass\n\n\ndef run(spark, raw_data, sql_text, dry_run=True):\n    urn = Urn(namespace=raw_data[\"namespace\"])\n    source_data_list = raw_data.pop(\"source\")\n    raw_data.update(raw_data.pop(\"options\"))\n    _view(spark, sql_text, None, source_data_list)\n    context = raw_data\n    tasks = SqlParser(spark, sql_text, context).parse()\n    if dry_run:\n        sql_executor = DryRunSqlExecutor\n    else:\n        sql_executor = SqlExecutor\n    sql_executor(urn, spark, tasks, type_.EventType.TRANSFORM, context).run()\n\n# CSV schema\nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\n\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)\n\ndef test_monthkly_data(test_data):\n    month_indicator = test_data[0]\n    '''\n    ### 1. only csv, but date range is '2010-07-04' to '2010-07-31' ###\n    if month_indicator == '2010-07-31':\n        temp_date_range = get_date_list('2010-07-04', '2010-07-31', freq='D')\n        df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv( \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        \n\n    ### 2. only csv\n    elif month_indicator > '2010-08-01' and month_indicator < '2019-07-01':\n        df_ios = spark.read.option(\"basePath\",\n                                   \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\" % (','.join(test_data[1])),\n            sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id',\n                                                                  'platform_id', 'vertical', 'rank', 'feed',\n                                                                  'est', 'date', 'platform').cache()\n        df_android = spark.read.option(\"basePath\",\n                                       \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(\n            csv_schema).csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/android/sbe_est_app/*/\" % (\n                ','.join(test_data[1])), sep=\"\\t\").withColumn(\"platform\", F.lit(\"android\")).select('id', 'store_id',\n                                                                                      'category_id',\n                                                                                      'platform_id',\n                                                                                      'vertical', 'rank',\n                                                                                      'feed', 'est', 'date',\n                                                                                      'platform').cache()\n        df_1 = df_ios.union(df_android)\n        \n\n    ### 3. half is csv, half is parquet ###\n    elif month_indicator == '2019-07-31':\n        # First half of 2019-07\n        temp_date_range = get_date_list('2019-07-01', '2019-07-14')\n        first_half_month_df = spark.read.option(\"basePath\", \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(temp_date_range), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        # Second half of 2019-07\n        temp_date_range = get_date_list('2019-07-15', '2019-07-31')\n        second_half_month = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(temp_date_range)).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\n        df_1 = first_half_month_df.union(second_half_month)\n        \n\n    ### 4. only parquet ###\n    else:  # month_indicator >= '2019-08-31'\n    '''\n    temp_date_range = get_date_list('2019-10-01', '2019-12-31')\n    df_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(temp_date_range)) .cache()\n\n    df_1.createOrReplaceTempView(\"daily_data\")\n    \n    \n    \n    quarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/\").where(\"granularity='quarterly' and date='{}' and data_stage='final'\".format('2019-12-31')).cache()\n    quarterly_df_ho.createOrReplaceTempView(\"unified_quarterly\")\n\n    sql_text = \"\"\"\n    \n    WITH filter_top_N_raw_data AS(\n    SELECT\n     distinct\n      id,\n      Sum(est) AS est,\n      store_id,\n      platform_id,\n      feed,\n      vertical,\n      platform\n    FROM\n      (\n        SELECT\n          DISTINCT d1.id,\n          d1.est,\n          d1.store_id,\n          d1.date,\n          d1.feed,\n          d1.vertical,\n          d1.platform_id,\n          d1.platform\n        FROM\n          daily_data AS d1\n          JOIN daily_data AS d2 \n          ON d1.id = d2.id\n          AND d1.store_id = d2.store_id\n          AND d1.feed = d2.feed\n          AND d1.vertical = d2.vertical\n          AND d1.platform_id = d2.platform_id\n        WHERE (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 0 and d1.platform = 'ios' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 0 and d1.platform = 'ios' )\n            OR  (d1.rank <= 4000 and d2.rank<=4000 and d1.store_id == 1000 and d1.platform = 'android' ) \n            OR (d1.rank <= 1000 and d2.rank<=1000 and  d1.store_id != 1000 and d1.platform = 'android' )\n      ) AS t\n    WHERE\n      feed IN (\n        0,\n        1,\n        2,\n        101,\n        100,\n        102\n      )\n    GROUP BY\n      id,\n      store_id,\n      platform_id,\n      vertical,\n      feed,\n      platform);\n      \n     WITH replace_metric AS (\n     SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'free_app_download'\n        when feed='1' and platform='ios' then 'paid_app_download'\n        when feed='2' and platform='ios' then 'revenue' \n        when feed='101' and platform='ios' then 'free_app_download' \n        when feed='100' and platform='ios' then 'paid_app_download' \n        when feed='102' and platform='ios' then 'revenue' \n        when feed='0' and platform='android' then 'free_app_download' \n        when feed='1' and platform='android' then 'paid_app_download' \n        when feed='2' and platform='android' then 'revenue' \n        end as metric from filter_top_N_raw_data);\n      \n      \n         WITH replace_metric_device_code AS (\n        SELECT * ,\n         case \n        when feed='0' and platform='ios' then 'ios-phone'\n        when feed='1' and platform='ios' then 'ios-phone'\n        when feed='2' and platform='ios' then 'ios-phone' \n        when feed='101' and platform='ios' then 'ios-tablet' \n        when feed='100' and platform='ios' then 'ios-tablet' \n        when feed='102' and platform='ios' then 'ios-tablet' \n        when feed='0' and platform='android' then 'android-all' \n        when feed='1' and platform='android' then 'android-all' \n        when feed='2' and platform='android' then 'android-all' \n        end as device_code from replace_metric);\n\n\n    WITH group_by_metric_1 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in (3,4,5,6) and device_code in ('ios-phone' ,'ios-tablet' ) and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n        );\n        \n    WITH group_by_metric_2 AS (\n        SELECT max(est) as est, id, metric,device_code, store_id, platform from replace_metric_device_code where store_id not in ( 1003, 1005, 1006,1007) and device_code='android-all' and feed not in (1000, 1001, 1002) group by id, store_id, metric,device_code, platform\n    );\n    \n    WITH group_by_metric AS(\n        SELECT * FROM group_by_metric_1\n        UNION ALL\n        SELECT * FROM group_by_metric_2\n        );\n\n      -- pivot metric column\n    WITH pivot_metric_raw AS (\n\n    SELECT \n        distinct id as app_id, store_id, platform, device_code, free_app_download,revenue, paid_app_download\n    FROM\n          group_by_metric\n     PIVOT (\n        max(est) \n    \tFOR metric IN ('free_app_download','revenue', 'paid_app_download')\n      )\n    );\n    \n    \n    -- union all platform with country_code mapping\n\n    WITH country_code_mapping AS (\n    select *, 'android' as market_code from android_country_mapping \n    UNION ALL select *, 'ios' market_code from ios_country_mapping\n    UNION ALL select 143502, 'VE', 'VESA', 'ios'\n    UNION ALL select 0, 'WW', 'worldwide', 'ios'\n    UNION ALL select 36, 'CZ', 'CZ', 'android'\n    UNION ALL select 5, 'ES', 'ES', 'android'\n\n    );\n\n\n\n    -- map raw with country_code\n\n    WITH country_category_mapping_raw AS (\n    select app_id, country_code, device_code, free_app_download, paid_app_download, revenue \n     from country_code_mapping \n     inner join \n         pivot_metric_raw \n     on \n         country_code_mapping.store_id=pivot_metric_raw.store_id \n     and \n         country_code_mapping.market_code=pivot_metric_raw.platform\n    where country_name!='Global'\n    );\n\n\n      \"\"\"\n    \n    # store_unified , rank_unified\n    namespace = \"aa.store.market-size.v1\"\n    ingest_msg = {\n        \"namespace\": \"aa.store.market-size.v1\",\n        \"job_type\": \"routine\",\n        \"options\": {},\n        \"source\": [\n            {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"ios_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/IOS_COUNTRY_MAPPING\"],\n        },   \n        {\n            \"data_encoding\": \"csv\",\n            \"compression\": \"gzip\",\n            \"name\":\"android_country_mapping\",\n            \"data_schema\": [\n                            {\"name\":\"store_id\",\"type\":\"int\",\"nullable\": False},\n                            {\"name\":\"country_code\",\"type\":\"string\",\"nullable\": False},\n                            {\"name\":\"country_name\",\"type\":\"string\",\"nullable\": False}\n                            ],\n             \"csv_options\": {\n          'header': True,\n          'sep': '\\t',\n          'quote': '',\n          'encoding': 'utf-8',\n          'escape': ''\n          },\n\n            \"path\": [\"s3://b2c-prod-dca-store-estimates/store_back/dimension/ANDROID_COUNTRY_MAPPING\"],\n        }\n        ]\n    }\n    \n    run(spark, ingest_msg, sql_text)\n    eject_all_caches(spark)\n\n\nsc.parallelize(map(test_monthkly_data, date_list), 1)\ndiff_df1 = spark.sql(\"select * from country_category_mapping_raw except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_quarterly \")\ndiff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from unified_quarterly  except all select * from country_category_mapping_raw\")\n\ndiff_df1.show()\ndiff_df2.show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200618-035954_284777594","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \n\nstart = '2019-10-01'\nend = '2019-12-31'\n\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df = daily_df.na.fill(0)\n\nagg_df = daily_df.groupBy('app_id', 'country_code', 'device_code').agg(sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), sum('est_revenue').alias('revenue'))\n# print agg_df.count()\n# agg_df.orderBy(desc('free_app_download')).show(1000)\nagg_df.createOrReplaceTempView(\"agg_df\")\ntop_df = spark.sql(\"select * from agg_df where country_code='WW' order by free_app_download desc, app_id desc\")\ntop_df.show(1000)\n\n\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/\").where(\"granularity='quarterly' and date='{}' and data_stage='final'\".format(end))\n# print quarterly_df_ho.count()\n# quarterly_df_ho.show()\nquarterly_df_ho = quarterly_df_ho.na.fill(0)\n\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\n\ndiff_df1 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df\")\nprint diff_df1.count(), diff_df2.count()\n# diff_df1.show()\n# diff_df2.show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200622-045217_450134907","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \n\nstart = '2013-07-01'\nend = '2013-09-30'\n\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df = daily_df.na.fill(0)\n\nagg_df = daily_df.groupBy('app_id', 'country_code', 'device_code').agg(sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), sum('est_revenue').alias('revenue'))\n# print agg_df.count()\n# agg_df.orderBy(desc('free_app_download')).show(1000)\nagg_df.createOrReplaceTempView(\"agg_df\")\ntop_df = spark.sql(\"select * from agg_df where country_code='WW' order by free_app_download desc, app_id desc\")\ntop_df.show(1000)\n\n\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/\").where(\"granularity='quarterly' and date='{}' and data_stage='final'\".format(end))\n# print quarterly_df_ho.count()\n# quarterly_df_ho.show()\nquarterly_df_ho = quarterly_df_ho.na.fill(0)\n\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\n\ndiff_df1 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df\")\nprint diff_df1.count(), diff_df2.count()\n# diff_df1.show()\n# diff_df2.show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200622-070619_1109178056","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \n\nstart = '2019-07-01'\nend = '2019-09-30'\n\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df = daily_df.na.fill(0)\n\nagg_df = daily_df.groupBy('app_id', 'country_code', 'device_code').agg(sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), sum('est_revenue').alias('revenue'))\n# print agg_df.count()\n# agg_df.orderBy(desc('free_app_download')).show(1000)\nagg_df.createOrReplaceTempView(\"agg_df\")\ntop_df = spark.sql(\"select * from agg_df where country_code='WW' order by free_app_download desc, app_id desc\")\ntop_df.show(1000)\n\n\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/\").where(\"granularity='quarterly' and date='{}' and data_stage='final'\".format(end))\n# print quarterly_df_ho.count()\n# quarterly_df_ho.show()\nquarterly_df_ho = quarterly_df_ho.na.fill(0)\n\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\n\ndiff_df1 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df\")\nprint diff_df1.count(), diff_df2.count()\n# diff_df1.show()\n# diff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200623-021124_2068563373","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \n\nstart = '2018-04-01'\nend = '2018-06-30'\n\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df = daily_df.na.fill(0)\n\nagg_df = daily_df.groupBy('app_id', 'country_code', 'device_code').agg(sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), sum('est_revenue').alias('revenue'))\n# print agg_df.count()\n# agg_df.orderBy(desc('free_app_download')).show(1000)\nagg_df.createOrReplaceTempView(\"agg_df\")\ntop_df = spark.sql(\"select * from agg_df where country_code='WW' order by free_app_download desc, app_id desc\")\ntop_df.show(1000)\n\n\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/\").where(\"granularity='quarterly' and date='{}' and data_stage='final'\".format(end))\n# print quarterly_df_ho.count()\n# quarterly_df_ho.show()\nquarterly_df_ho = quarterly_df_ho.na.fill(0)\n\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\n\ndiff_df1 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df\")\nprint diff_df1.count(), diff_df2.count()\n# diff_df1.show()\n# diff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200622-075656_341090674","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \n\nstart = '2019-01-01'\nend = '2019-12-31'\n\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df = daily_df.na.fill(0)\n\nagg_df = daily_df.groupBy('app_id', 'country_code', 'device_code').agg(sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), sum('est_revenue').alias('revenue'))\n# print agg_df.count()\n# agg_df.orderBy(desc('free_app_download')).show(1000)\nagg_df.createOrReplaceTempView(\"agg_df\")\ntop_df = spark.sql(\"select * from agg_df where country_code='WW' order by free_app_download desc, app_id desc\")\ntop_df.show(1000)\n\n\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/\").where(\"granularity='yearly' and date='{}' and data_stage='final'\".format(end))\n# print quarterly_df_ho.count()\n# quarterly_df_ho.show()\nquarterly_df_ho = quarterly_df_ho.na.fill(0)\n\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\n\ndiff_df1 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df\")\nprint diff_df1.count(), diff_df2.count()\n# diff_df1.show()\n# diff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200623-021826_153073656","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \n\nstart = '2018-01-01'\nend = '2018-12-31'\n\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df = daily_df.na.fill(0)\n\nagg_df = daily_df.groupBy('app_id', 'country_code', 'device_code').agg(sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), sum('est_revenue').alias('revenue'))\n# print agg_df.count()\n# agg_df.orderBy(desc('free_app_download')).show(1000)\nagg_df.createOrReplaceTempView(\"agg_df\")\ntop_df = spark.sql(\"select * from agg_df where country_code='WW' order by free_app_download desc, app_id desc\")\ntop_df.show(1000)\n\n\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/\").where(\"granularity='yearly' and date='{}' and data_stage='final'\".format(end))\n# print quarterly_df_ho.count()\n# quarterly_df_ho.show()\nquarterly_df_ho = quarterly_df_ho.na.fill(0)\n\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\n\ndiff_df1 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df\")\nprint diff_df1.count(), diff_df2.count()\n# diff_df1.show()\n# diff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200622-082451_611404350","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\n\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \n\nstart = '2017-01-01'\nend = '2017-12-31'\n\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df = daily_df.na.fill(0)\n\nagg_df = daily_df.groupBy('app_id', 'country_code', 'device_code').agg(sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), sum('est_revenue').alias('revenue'))\n# print agg_df.count()\n# agg_df.orderBy(desc('free_app_download')).show(1000)\nagg_df.createOrReplaceTempView(\"agg_df\")\ntop_df = spark.sql(\"select * from agg_df where country_code='WW' order by free_app_download desc, app_id desc\")\ntop_df.show(1000)\n\n\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-pre-aggr.v3/fact/\").where(\"granularity='yearly' and date='{}' and data_stage='final'\".format(end))\n# print quarterly_df_ho.count()\n# quarterly_df_ho.show()\nquarterly_df_ho = quarterly_df_ho.na.fill(0)\n\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\n\ndiff_df1 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df except all select app_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df\")\nprint diff_df1.count(), diff_df2.count()\n# diff_df1.show()\n# diff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200619-085833_1298800361","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\n\nstart = '2018-05-06'\nend = '2018-05-12'\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ncategory_id_df = category_daily_df.select('app_id', 'country_code', 'device_code', 'category_id').filter(\"est_free_app_download is not null\").distinct()\ncategory_id_df.createOrReplaceTempView(\"category_id_df\")\n\nest_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\nest_daily_df = est_daily_df.na.fill(0)\nest_agg_df = est_daily_df.groupBy('app_id', 'country_code', 'device_code').agg(sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), sum('est_revenue').alias('revenue'))\nest_agg_df.createOrReplaceTempView(\"est_agg_df\")\n\nagg_df = spark.sql(\"\"\"select\n                        d1.app_id,\n                        d1.device_code,\n                        d1.country_code,\n                        d1.category_id,\n                        d2.free_app_download,\n                        d2.paid_app_download,\n                        d2.revenue\n                      FROM category_id_df as d1 \n                        JOIN est_agg_df as d2 \n                        ON d1.app_id=d2.app_id \n                        AND d1.device_code=d2.device_code \n                        AND d1.country_code=d2.country_code\"\"\")\nagg_df = agg_df.na.fill(0)\nagg_df.createOrReplaceTempView(\"agg_df\")\ntop_df = spark.sql(\"select * from agg_df where country_code='WW' order by free_app_download desc, app_id desc\")\ntop_df.show(1000)\n\n\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='weekly' and date='{}' and data_stage='final'\".format(end)).filter(\"free_app_download is not null\")\n# print quarterly_df_ho.count()\n# quarterly_df_ho.show()\nquarterly_df_ho = quarterly_df_ho.na.fill(0)\n\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\n\ndiff_df1 = spark.sql(\"select app_id, category_id, country_code, device_code, free_app_download from agg_df except all select app_id, category_id, country_code, device_code, free_app_download from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, category_id, country_code, device_code, free_app_download from quarterly_df except all select app_id, category_id, country_code, device_code, free_app_download from agg_df\")\nprint agg_df.count(), quarterly_df_ho.count()\nprint diff_df1.count(), diff_df2.count()\ndiff_df1.show()\ndiff_df2.show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200623-033626_1277352739","metadata":{},"outputs":[],"source":["\nstart = '2018-05-01'\nend = '2018-05-31'\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df.createOrReplaceTempView(\"daily_df\")\nagg_df = spark.sql(\"select app_id, device_code, country_code, category_id, sum(est_free_app_download) as free_app_download, sum(est_paid_app_download) as paid_app_download, sum(est_revenue) as revenue from daily_df group by app_id, device_code, country_code, category_id\")\nagg_df.createOrReplaceTempView(\"agg_df\")\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='monthly' and date='{}' and data_stage='final'\".format(end))\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\n\ndiff_df1 = spark.sql(\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df except all select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df except all select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df\")\nprint agg_df.count(), quarterly_df_ho.count()\nprint diff_df1.count(), diff_df2.count()\ndiff_df1.show()\ndiff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200622-104822_224161937","metadata":{},"outputs":[],"source":["\nstart = '2018-05-06'\nend = '2018-05-12'\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ncategory_daily_df.createOrReplaceTempView(\"category_daily_df\")\nspark.sql(\"\"\" select date, app_id, country_code, device_code, category_id,\n                    stack(3, 'free_app_download', est_free_app_download, 'paid_app_download', est_paid_app_download, 'revenue', est_revenue) as (metric, est)\n                from  category_daily_df\"\"\").createOrReplaceTempView(\"unpivot_category_daily_df\")\nspark.sql(\"\"\"select app_id, country_code, device_code, category_id, metric, sum(est) as est\n                from unpivot_category_daily_df\n                where est is not null\n                group by\n                app_id,\n                country_code,\n                device_code,\n                category_id,\n                metric\n\"\"\").createOrReplaceTempView(\"unpivot_category_agg_df\")\nest_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\nest_daily_df.createOrReplaceTempView(\"est_daily_df\")\nspark.sql(\"\"\" select app_id, country_code, device_code, \n                stack(3, 'free_app_download', free_app_download, 'paid_app_download', paid_app_download, 'revenue', revenue) as (metric, est)\n            FROM(\n                select app_id, country_code, device_code, sum(est_free_app_download) as free_app_download, sum(est_paid_app_download) as paid_app_download, sum(est_revenue) as revenue from est_daily_df group by app_id, country_code, device_code)\"\"\").createOrReplaceTempView(\"unpivot_est_agg_df\")\nspark.sql(\"\"\"select d1.app_id, d1.country_code, d1.device_code, d1.category_id, d1.metric, d2.est\n                from unpivot_category_agg_df as d1 \n                join unpivot_est_agg_df as d2 \n                on\n                d1.app_id=d2.app_id\n                and d1.country_code=d2.country_code\n                and d1.device_code=d2.device_code\n                and d1.metric=d2.metric\"\"\").createOrReplaceTempView(\"unpivot_agg_df\")\nagg_df = spark.sql(\"\"\" select * FROM unpivot_agg_df\n                pivot(\n                    max(est) FOR metric IN ('free_app_download','revenue', 'paid_app_download')\n                )\n\"\"\")\nagg_df.createOrReplaceTempView(\"agg_df\")\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='weekly' and date='{}' and data_stage='final'\".format(end))\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\ndiff_df1 = spark.sql(\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df except all select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df\")\ndiff_df2 = spark.sql(\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue from quarterly_df except all select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue from agg_df\")\nprint agg_df.count(), quarterly_df_ho.count()\nprint diff_df1.count(), diff_df2.count()\ndiff_df1.show()\ndiff_df2.show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200623-082841_1931302050","metadata":{},"outputs":[],"source":["\nstart = '2018-01-01'\nend = '2018-12-31'\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v3/fact/\").where(\"granularity='yearly' and date='{}' and data_stage='final'\".format(end))\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df_ho\")\n# spark.sql(\"select app_id, device_code, country_code, category_id, free_app_download, revenue, paid_app_download from quarterly_df_ho where app_id=20600005203185 and category_id=400026 and country_code='WW'\").show()\nspark.sql(\"select app_id, device_code, country_code, category_id, free_app_download, revenue, paid_app_download from quarterly_df_ho where app_id=20600000013820 and category_id=400026 and country_code='WW'\").show()\nspark.sql(\"select app_id, device_code, country_code, category_id, free_app_download, revenue, paid_app_download from quarterly_df_ho where category_id=400026 and country_code='WW' order by free_app_download desc, app_id desc limit 1000\").show(1000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200623-123710_1597928222","metadata":{},"outputs":[],"source":["\nstart = '2018-01-01'\nend = '2018-04-30'\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='monthly' and date='{}' and data_stage='final'\".format(end))\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df_ho\")\n# spark.sql(\"select app_id, device_code, country_code, category_id, free_app_download, revenue, paid_app_download from quarterly_df_ho where app_id=20600005203185 and category_id=400026 and country_code='WW'\").show()\n# spark.sql(\"select app_id, device_code, country_code, category_id, free_app_download, revenue, paid_app_download from quarterly_df_ho where app_id=20600000013820 and category_id=400026 and country_code='WW'\").show()\nspark.sql(\"select app_id, device_code, country_code, category_id, free_app_download, revenue, paid_app_download from quarterly_df_ho where category_id=400026 and country_code='WW' order by free_app_download desc, app_id desc limit 1000\").show(1000)"]},{"cell_type":"code","execution_count":0,"id":"20200624-022710_338667701","metadata":{},"outputs":[],"source":["\nstart = '2018-05-06'\nend = '2018-05-12'\ndaily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ndaily_df.createOrReplaceTempView(\"daily_df\")\nquarterly_df_ho = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='weekly' and date='{}' and data_stage='final'\".format(end))\nquarterly_df_ho.createOrReplaceTempView(\"quarterly_df\")\nspark.sql(\"select app_id,device_code,country_code,category_id,free_app_download from quarterly_df where app_id=1009836739 and country_code='AM' and device_code='ios-phone'\").show()\nspark.sql(\"select app_id,device_code,country_code,category_id,est_free_app_download from daily_df where app_id=1009836739 and country_code='AM' and device_code='ios-phone' order by category_id\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200624-022718_1039675720","metadata":{},"outputs":[],"source":["\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \n    \nfrom pyspark.sql import types as T\nfrom pyspark.sql import functions as F\ncsv_schema = T.StructType(\n    [\n        T.StructField(\"store_id\", T.IntegerType(), True),\n        T.StructField(\"date\", T.DateType(), True),\n        T.StructField(\"platform_id\", T.IntegerType(), True),\n        T.StructField(\"vertical\", T.IntegerType(), True),\n        T.StructField(\"feed\", T.IntegerType(), True),\n        T.StructField(\"id\", T.LongType(), True),\n        T.StructField(\"est\", T.IntegerType(), True),\n        T.StructField(\"category_id\", T.IntegerType(), True),\n        T.StructField(\"rank\", T.IntegerType(), True)\n    ]\n)    \nstart = '2018-05-06'\nend = '2018-05-12'    \ntest_data = get_date_list(start, end)\nprint test_data\ndf_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/\").schema(csv_schema).csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{%s}/ios/sbe_est_app/*/\"%\",\".join(test_data), sep=\"\\t\").withColumn(\"platform\", F.lit(\"ios\")).select('id', 'store_id', 'category_id', 'platform_id', 'vertical', 'rank', 'feed', 'est', 'date', 'platform').cache()\ndf_1.filter(\"id=1009836739 and platform='ios' and feed='0' and store_id=143524 and rank<=1000\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200624-090124_1795694856","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\nstart = '2019-12-29'\nend = '2020-01-04'\ngranularity = 'weekly'\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\n\n\nagg_df = category_daily_df.groupBy('app_id', 'country_code', 'device_code', 'category_id').agg(\n    sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), \n        sum('est_revenue').alias('revenue'))\nagg_df.createOrReplaceTempView(\"agg_df\")\n\nif granularity == 'weekly' or granularity == 'monthly':\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\nelse:\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n\npre_agg_df.createOrReplaceTempView(\"pre_agg_df\")\n\ndiff_df1 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df\"\"\")\ndiff_df2 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df\"\"\")\nprint agg_df.count(), pre_agg_df.count()\nprint diff_df1.count(), diff_df2.count()\ndiff_df1.show()\ndiff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200624-101135_920210130","metadata":{},"outputs":[],"source":["\nstart = '2019-12-29'\nend = '2020-01-04'\ngranularity = 'weekly'\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ncategory_daily_df.createOrReplaceTempView(\"category_daily_df\")\nspark.sql(\"\"\" select date, app_id, country_code, device_code, category_id,\n                    stack(3, 'free_app_download', est_free_app_download, 'paid_app_download', est_paid_app_download, 'revenue', est_revenue) \n                    as (metric, est)\n                from  category_daily_df\"\"\").createOrReplaceTempView(\"unpivot_category_daily_df\")\nspark.sql(\"\"\"select app_id, country_code, device_code, category_id, metric, sum(est) as est\n                from unpivot_category_daily_df\n                where est is not null\n                group by\n                app_id,\n                country_code,\n                device_code,\n                category_id,\n                metric\n\"\"\").createOrReplaceTempView(\"unpivot_category_agg_df\")\n\nest_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\nest_daily_df.createOrReplaceTempView(\"est_daily_df\")\nspark.sql(\"\"\" select app_id, country_code, device_code, \n                stack(3, 'free_app_download', free_app_download, 'paid_app_download', paid_app_download, 'revenue', revenue) as (metric, est)\n            FROM(\n                select app_id, country_code, device_code, \n                sum(est_free_app_download) as free_app_download, \n                sum(est_paid_app_download) as paid_app_download, \n                sum(est_revenue) as revenue \n                from est_daily_df \n                group by app_id, country_code, device_code)\"\"\").createOrReplaceTempView(\"unpivot_est_agg_df\")\nspark.sql(\"\"\"select d1.app_id, d1.country_code, d1.device_code, d1.category_id, d1.metric, d2.est\n                from unpivot_category_agg_df as d1 \n                join unpivot_est_agg_df as d2 \n                on\n                d1.app_id=d2.app_id\n                and d1.country_code=d2.country_code\n                and d1.device_code=d2.device_code\n                and d1.metric=d2.metric\"\"\").createOrReplaceTempView(\"unpivot_agg_df\")\nspark.sql(\"\"\" select * FROM unpivot_agg_df\n                pivot(\n                    max(est) FOR metric IN ('free_app_download','revenue', 'paid_app_download')\n                )\n\"\"\").createOrReplaceTempView(\"agg_df\")\n\nspark.sql(\"\"\"select * from agg_df where category_id=400026 and country_code='WW' and device_code='android-all' order by free_app_download desc, app_id desc limit 1000\"\"\").show(1000)"]},{"cell_type":"code","execution_count":0,"id":"20200624-091856_1928755464","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\nstart = '2020-01-01'\nend = '2020-01-31'\ngranularity = 'monthly'\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\n\n\nagg_df = category_daily_df.groupBy('app_id', 'country_code', 'device_code', 'category_id').agg(\n    sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), \n        sum('est_revenue').alias('revenue'))\nagg_df.createOrReplaceTempView(\"agg_df\")\n\nif granularity == 'weekly' or granularity == 'monthly':\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\nelse:\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n\npre_agg_df.createOrReplaceTempView(\"pre_agg_df\")\n\ndiff_df1 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df\"\"\")\ndiff_df2 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df\"\"\")\nprint agg_df.count(), pre_agg_df.count()\nprint diff_df1.count(), diff_df2.count()\ndiff_df1.show()\ndiff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200624-102517_2138306877","metadata":{},"outputs":[],"source":["\nstart = '2020-01-01'\nend = '2020-01-31'\ngranularity = 'monthly'\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ncategory_daily_df.createOrReplaceTempView(\"category_daily_df\")\nspark.sql(\"\"\" select date, app_id, country_code, device_code, category_id,\n                    stack(3, 'free_app_download', est_free_app_download, 'paid_app_download', est_paid_app_download, 'revenue', est_revenue) \n                    as (metric, est)\n                from  category_daily_df\"\"\").createOrReplaceTempView(\"unpivot_category_daily_df\")\nspark.sql(\"\"\"select app_id, country_code, device_code, category_id, metric, sum(est) as est\n                from unpivot_category_daily_df\n                where est is not null\n                group by\n                app_id,\n                country_code,\n                device_code,\n                category_id,\n                metric\n\"\"\").createOrReplaceTempView(\"unpivot_category_agg_df\")\n\nest_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\nest_daily_df.createOrReplaceTempView(\"est_daily_df\")\nspark.sql(\"\"\" select app_id, country_code, device_code, \n                stack(3, 'free_app_download', free_app_download, 'paid_app_download', paid_app_download, 'revenue', revenue) as (metric, est)\n            FROM(\n                select app_id, country_code, device_code, \n                sum(est_free_app_download) as free_app_download, \n                sum(est_paid_app_download) as paid_app_download, \n                sum(est_revenue) as revenue \n                from est_daily_df \n                group by app_id, country_code, device_code)\"\"\").createOrReplaceTempView(\"unpivot_est_agg_df\")\nspark.sql(\"\"\"select d1.app_id, d1.country_code, d1.device_code, d1.category_id, d1.metric, d2.est\n                from unpivot_category_agg_df as d1 \n                join unpivot_est_agg_df as d2 \n                on\n                d1.app_id=d2.app_id\n                and d1.country_code=d2.country_code\n                and d1.device_code=d2.device_code\n                and d1.metric=d2.metric\"\"\").createOrReplaceTempView(\"unpivot_agg_df\")\nspark.sql(\"\"\" select * FROM unpivot_agg_df\n                pivot(\n                    max(est) FOR metric IN ('free_app_download','revenue', 'paid_app_download')\n                )\n\"\"\").createOrReplaceTempView(\"agg_df\")\n\nspark.sql(\"\"\"select * from agg_df where category_id=400026 and country_code='WW' and device_code='android-all' order by free_app_download desc, app_id desc limit 1000\"\"\").show(1000)"]},{"cell_type":"code","execution_count":0,"id":"20200625-013005_422479048","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql.functions import desc\n\nstart = '2019-01-01'\nend = '2019-12-31'\ngranularity = 'yearly'\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\n\n\nagg_df = category_daily_df.groupBy('app_id', 'country_code', 'device_code', 'category_id').agg(\n    sum('est_free_app_download').alias('free_app_download'), sum('est_paid_app_download').alias('paid_app_download'), \n        sum('est_revenue').alias('revenue'))\nagg_df.createOrReplaceTempView(\"agg_df\")\n\nif granularity == 'weekly' or granularity == 'monthly':\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\nelse:\n    pre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\n\npre_agg_df.createOrReplaceTempView(\"pre_agg_df\")\n\ndiff_df1 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df\"\"\")\ndiff_df2 = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from pre_agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df\"\"\")\nprint agg_df.count(), pre_agg_df.count()\nprint diff_df1.count(), diff_df2.count()\ndiff_df1.show()\ndiff_df2.show()"]},{"cell_type":"code","execution_count":0,"id":"20200625-025611_360889906","metadata":{},"outputs":[],"source":["\nstart = '2019-01-01'\nend = '2019-12-31'\ngranularity = 'yearly'\n\nquarterly_category_bucket =\"store.app-est-category-pre-aggr.v3\"\nquarterly_est_bucket= \"store.app-est-pre-aggr.v3\"\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"{}/fact/\".format(quarterly_category_bucket)).where(\"granularity='{}' and date between '{}' and '{}'\".format(granularity, start, end))\ncategory_daily_df.createOrReplaceTempView(\"category_daily_df1\")\nspark.sql(\"\"\" select date, app_id, country_code, device_code, category_id,\n                    free_app_download,paid_app_download, revenue\n                from  category_daily_df1\"\"\").createOrReplaceTempView(\"unpivot_category_daily_df1\")\nest_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"{}/fact/\".format(quarterly_est_bucket)).where(\"granularity='{}' and date between '{}' and '{}'\".format(granularity, start, end))\nest_daily_df.createOrReplaceTempView(\"est_daily_df1\")\nspark.sql(\"\"\" select app_id, country_code, device_code, \n                 free_app_download, paid_app_download, revenue\n            FROM est_daily_df1 \"\"\").createOrReplaceTempView(\"unpivot_est_agg_df1\")\nspark.sql(\"\"\"select d1.app_id, d1.country_code, d1.device_code, d1.category_id, d2.free_app_download, d2.paid_app_download, d2.revenue\n                from unpivot_category_daily_df1 as d1 \n                join unpivot_est_agg_df1 as d2 \n                on\n                d1.app_id=d2.app_id\n                and d1.country_code=d2.country_code\n                and d1.device_code=d2.device_code\n             \"\"\").cache().createOrReplaceTempView(\"unpivot_agg_df1\")\nspark.sql(\"\"\"select d1.app_id, d1.country_code, d1.device_code, d1.category_id, \n                case when d1.free_app_download is null then d1.free_app_download else d2.free_app_download end free_app_download, \n                case when d1.paid_app_download is null then d1.paid_app_download else d2.paid_app_download end paid_app_download, \n                case when d1.revenue is null then d1.revenue else d2.revenue end revenue\n                from unpivot_category_daily_df1 as d1 \n                join unpivot_est_agg_df1 as d2 \n                on\n                d1.app_id=d2.app_id\n                and d1.country_code=d2.country_code\n                and d1.device_code=d2.device_code\n             \"\"\").cache().createOrReplaceTempView(\"unpivot_agg_df2\")\n\nspark.sql(\"\"\"select * from unpivot_agg_df2 where category_id=400026 and country_code='WW' and device_code='android-all' order by free_app_download desc, app_id desc limit 1000\"\"\").show(1000)"]},{"cell_type":"code","execution_count":0,"id":"20200624-091555_68410844","metadata":{},"outputs":[],"source":["\nstart = '2019-12-29'\nend = '2020-01-04'\ngranularity = 'weekly'\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ncategory_daily_df.filter(\"app_id=1217351254 and category_id=100071 and country_code='AO' and device_code='ios-phone'\").show()\npre_agg_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/\").where(\"granularity='{}' and date='{}' and data_stage='final'\".format(granularity, end))\npre_agg_df.filter(\"app_id=1217351254 and category_id=100071 and country_code='AO' and device_code='ios-phone'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200618-031456_1643007229","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-pre-aggr.v3/fact/granularity=yearly/date=2019-12-31/ --recursive | sort -n"]},{"cell_type":"code","execution_count":0,"id":"20200616-095328_60562974","metadata":{},"outputs":[],"source":["\ndef get_date_list(start_date, end_date, freq=\"D\"):\n    import pandas as pd\n    \"\"\"\n    freq:   D: calendar day frequency\n            M: month end frequency\n            MS: month start frequency\n            A, Y: year end frequency\n            AS, YS: year start frequency\n    \"\"\"\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n    \n\ntest_data = get_date_list('2019-09-08', '2019-09-14')\ndf_1 = spark.read.option(\"basePath\",\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/\").parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date={%s}/platform=*/*/\" %  \",\".join(test_data))\ndf1.filter(\"\")"]},{"cell_type":"code","execution_count":0,"id":"20200624-151937_576094724","metadata":{},"outputs":[],"source":["\nstart = '2019-12-29'\nend = '2020-01-04'\ngranularity = 'weekly'\nquarterly_category_bucket =\"store.app-est-category.v3\"\nquarterly_est_bucket= \"store.app-est.v3\"\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"{}/fact/\".format(quarterly_category_bucket)).where(\"granularity='{}' and date between '{}' and '{}'\".format(granularity, start, end))\ncategory_daily_df.createOrReplaceTempView(\"category_daily_df1\")\nspark.sql(\"\"\" select date, app_id, country_code, device_code, category_id,\n                    free_app_download,paid_app_download, revenue\n                from  category_daily_df1\"\"\").createOrReplaceTempView(\"unpivot_category_daily_df1\")\nest_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"{}/fact/\".format(quarterly_est_bucket)).where(\"granularity='{}' and date between '{}' and '{}'\".format(granularity, start, end))\nest_daily_df.createOrReplaceTempView(\"est_daily_df1\")\nspark.sql(\"\"\" select app_id, country_code, device_code, \n                 free_app_download, paid_app_download, revenue\n            FROM est_daily_df1 \"\"\").createOrReplaceTempView(\"unpivot_est_agg_df1\")\nspark.sql(\"\"\"select d1.app_id, d1.country_code, d1.device_code, d1.category_id, d2.free_app_download, d2.paid_app_download, d2.revenue\n                from unpivot_category_daily_df1 as d1 \n                join unpivot_est_agg_df1 as d2 \n                on\n                d1.app_id=d2.app_id\n                and d1.country_code=d2.country_code\n                and d1.device_code=d2.device_code\n             \"\"\").cache().createOrReplaceTempView(\"unpivot_agg_df1\")\nspark.sql(\"\"\"select d1.app_id, d1.country_code, d1.device_code, d1.category_id, \n                case when d1.free_app_download is null then d1.free_app_download else d2.free_app_download end free_app_download, \n                case when d1.paid_app_download is null then d1.paid_app_download else d2.paid_app_download end paid_app_download, \n                case when d1.revenue is null then d1.revenue else d2.revenue end revenue\n                from unpivot_category_daily_df1 as d1 \n                join unpivot_est_agg_df1 as d2 \n                on\n                d1.app_id=d2.app_id\n                and d1.country_code=d2.country_code\n                and d1.device_code=d2.device_code\n             \"\"\").cache().createOrReplaceTempView(\"unpivot_agg_df2\")"]},{"cell_type":"code","execution_count":0,"id":"20200624-152319_713991593","metadata":{},"outputs":[],"source":["\nstart = '2019-12-29'\nend = '2020-01-04'\ngranularity = 'weekly'\n\ncategory_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"store.app-est-category-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\ncategory_daily_df.createOrReplaceTempView(\"category_daily_df\")\nspark.sql(\"\"\" select date, app_id, country_code, device_code, category_id,\n                    stack(3, 'free_app_download', est_free_app_download, 'paid_app_download', est_paid_app_download, 'revenue', est_revenue) \n                    as (metric, est)\n                from  category_daily_df\"\"\").createOrReplaceTempView(\"unpivot_category_daily_df\")\nspark.sql(\"\"\"select app_id, country_code, device_code, category_id, metric, sum(est) as est\n                from unpivot_category_daily_df\n                where est is not null\n                group by\n                app_id,\n                country_code,\n                device_code,\n                category_id,\n                metric\n\"\"\").createOrReplaceTempView(\"unpivot_category_agg_df\")\n\nest_daily_df = spark.read.format(\"delta\").load(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/\" \\\n    \"store.app-est-load.v3/fact/\").where(\"granularity='daily' and date between '{}' and '{}'\".format(start, end))\nest_daily_df.createOrReplaceTempView(\"est_daily_df\")\nspark.sql(\"\"\" select app_id, country_code, device_code, \n                stack(3, 'free_app_download', free_app_download, 'paid_app_download', paid_app_download, 'revenue', revenue) as (metric, est)\n            FROM(\n                select app_id, country_code, device_code, \n                sum(est_free_app_download) as free_app_download, \n                sum(est_paid_app_download) as paid_app_download, \n                sum(est_revenue) as revenue \n                from est_daily_df \n                group by app_id, country_code, device_code)\"\"\").createOrReplaceTempView(\"unpivot_est_agg_df\")\nspark.sql(\"\"\"select d1.app_id, d1.country_code, d1.device_code, d1.category_id, d1.metric, d2.est\n                from unpivot_category_agg_df as d1 \n                join unpivot_est_agg_df as d2 \n                on\n                d1.app_id=d2.app_id\n                and d1.country_code=d2.country_code\n                and d1.device_code=d2.device_code\n                and d1.metric=d2.metric\"\"\").createOrReplaceTempView(\"unpivot_agg_df\")\nspark.sql(\"\"\" select * FROM unpivot_agg_df\n                pivot(\n                    max(est) FOR metric IN ('free_app_download','revenue', 'paid_app_download')\n                )\n\"\"\").cache().createOrReplaceTempView(\"agg_df\")\n"]},{"cell_type":"code","execution_count":0,"id":"20200624-152645_1747793647","metadata":{},"outputs":[],"source":["\ndiff_df = spark.sql(\"\"\"select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from agg_df \n                        except all \n                        select app_id, category_id, country_code, device_code, free_app_download, paid_app_download, revenue \n                            from unpivot_agg_df2\n                        order by free_app_download desc\"\"\")\ndiff_df.show(999)"]},{"cell_type":"code","execution_count":0,"id":"20200624-155736_405277590","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from unpivot_est_agg_df where app_id=20600011926298 and country_code='WW' and device_code='android-all'\").show()\nspark.sql(\"select * from unpivot_category_agg_df where app_id=20600011926298 and country_code='WW' and device_code='android-all'\").show()\nspark.sql(\"select * from unpivot_agg_df where app_id=20600011926298 and country_code='WW' and device_code='android-all'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200624-152808_573762822","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from unpivot_agg_df2 where app_id=20600011926298 and category_id=400000 and country_code='WW' and device_code='android-all'\").show()\n\nspark.sql(\"select * from agg_df where app_id=20600011926298 and country_code='WW' and device_code='android-all'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200624-153133_1353163831","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from category_daily_df where app_id=20600011926298 and category_id=400000 and country_code='WW' and device_code='android-all'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200714-022556_1408241412","metadata":{},"outputs":[],"source":["\nfrom bdce.common.utils import update_application_code\nupdate_application_code(spark, role=\"BDP-PROD-APP-INT-QA\", application_name=\"zidong-application-autopipeline\")\n\nspark.sparkContext.addPyFile(\"/tmp/zeppelin_application_code/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom applications.auto_pipeline.temp_script.utils.base_test import PipelineTest"]},{"cell_type":"code","execution_count":0,"id":"20200625-011719_1353271256","metadata":{},"outputs":[],"source":["\nimport aaplproxy\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom aadatapipelinecore.core.urn import Urn\nfrom aadatapipelinecore.core.pipeline import type_\nfrom applications.common.parser import SqlParser\nfrom applications.common.executor import SqlExecutor\nfrom applications.auto_pipeline.temp_script.utils.base_test import PipelineTest\nfrom applications.auto_pipeline.transform import _view"]},{"cell_type":"code","execution_count":0,"id":"20200714-022551_379136893","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}