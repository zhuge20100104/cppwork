{"cells":[{"cell_type":"code","execution_count":0,"id":"20200114-021553_859837874","metadata":{},"outputs":[],"source":["    \n\nimport datetime\nimport psycopg2\nfrom contextlib import closing\ndbinfo = {\n    'NAME': 'dailyest',\n    'USER': 'app_bdp_usage_qa',\n    'PASSWORD': '2mHdFW6%#REu',\n    'HOST': 'internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com',\n    'PORT': 7432,\n}\nconn = psycopg2.connect(\"\"\"\n            dbname={dbname} user={user} host={host} port={port} password={passwd}\n            \"\"\".format(dbname=dbinfo['NAME'], user=dbinfo['USER'], passwd=dbinfo['PASSWORD'],\n                       host=dbinfo['HOST'], port=dbinfo['PORT']))\ndef select(stmt, params):\n    with closing(conn.cursor()) as cur:\n        cur.execute(stmt, params)\n        rows = cur.fetchall()\n        cnt = cur.rowcount\n    return rows, cnt\n\n\nsql = \"\"\"select device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate from plproxy.execute_select_nestloop($proxy$ select device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate from ms.market_size_daily_estimate_1000 where date = '2019-12-01' and category_id in (801) $proxy$) tbl (device_id SMALLINT, date DATE, store_id INT, kpi SMALLINT, category_id INT, app_type_id SMALLINT, purchase_type_id SMALLINT, estimate BIGINT);\"\"\"\nrows, _ = select(sql, None)\nprint len(rows)\nprint rows\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200114-021758_1363690934","metadata":{},"outputs":[],"source":["\n\nimport datetime\nimport psycopg2\nfrom contextlib import closing\ndbinfo = {\n    'NAME': 'dailyest',\n    'USER': 'app_bdp_usage_qa',\n    'PASSWORD': '2mHdFW6%#REu',\n    'HOST': 'internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com',\n    'PORT': 7432,\n}\nconn = psycopg2.connect(\"\"\"\n            dbname={dbname} user={user} host={host} port={port} password={passwd}\n            \"\"\".format(dbname=dbinfo['NAME'], user=dbinfo['USER'], passwd=dbinfo['PASSWORD'],\n                       host=dbinfo['HOST'], port=dbinfo['PORT']))\ndef select(stmt, params):\n    with closing(conn.cursor()) as cur:\n        cur.execute(stmt, params)\n        rows = cur.fetchall()\n        cnt = cur.rowcount\n    return rows, cnt\n\n\nsql_1 = \"\"\"select device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate from plproxy.execute_select_nestloop($proxy$ select device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate from ms.market_size_daily_estimate_1000 where date = '2019-12-01' and category_id = 801$proxy$) tbl (device_id SMALLINT, date DATE, store_id INT, kpi SMALLINT, category_id INT, app_type_id SMALLINT, purchase_type_id SMALLINT, estimate BIGINT) GROUP BY device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate;\"\"\"\n\n\n# sql = \"\"\"select * from plproxy.execute_select_nestloop($proxy$ select device_id,count(distinct store_id) from ms.market_size_daily_estimate_1000 where date='2019-12-01' GROUP BY device_id $proxy$) tbl (device_id SMALLINT, date DATE, store_id INT, kpi SMALLINT, category_id INT, app_type_id SMALLINT, purchase_type_id SMALLINT, estimate BIGINT);\"\"\"\n# sql = \"\"\"select * from plproxy.execute_select_nestloop($proxy$ select device_id,count(distinct store_id) as store_id_count from ms.market_size_daily_estimate_1000 where date='2019-12-01' AND app_type_id = 0 AND purchase_type_id = 12 AND kpi = 1 AND category_id=600  GROUP BY device_id $proxy$) tbl (device_id SMALLINT, store_id_count BIGINT);\"\"\"\n\nsql = \"\"\"select SUM(estimate) as estimate from plproxy.execute_select_nestloop($proxy$ select estimate from ms.market_size_daily_estimate_1000 where date BETWEEN '2019-12-01' and '2019-12-01' AND category_id=1 AND app_type_id=0 AND purchase_type_id=10 AND kpi=1 AND store_id=10$proxy$) tbl (estimate BIGINT);\"\"\"\n\n\n# actual_sql = \"\"\"select * from plproxy.execute_select_nestloop($proxy$ select estimate,device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id from ms.market_size_daily_estimate_1000 where date BETWEEN '2019-12-01' and '2019-12-07' AND category_id=1 AND app_type_id=0 AND purchase_type_id=10 AND kpi=1 AND store_id=10 $proxy$) tbl ( estimate BIGINT,device_id SMALLINT, date DATE, store_id INT, kpi SMALLINT, category_id INT, app_type_id SMALLINT, purchase_type_id SMALLINT);\"\"\"\n\n\n\n\nled_test = \"select device_id, date from plproxy.execute_select_nestloop($proxy$ select device_id, date from ms.latest_date $proxy$, 0) t (device_id INT, date DATE);\"\n\nrows, _ = select(led_test, None)\n# rows, _ = select(sql_1, None)\n\n# print len(rows)\nprint rows\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200114-021630_1987436878","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import lit, udf\nfrom pyspark.sql.types import (\n    ArrayType, BooleanType, LongType, IntegerType,\n    StringType, StructType, StructField\n)\nschema = StructType([\n    StructField(\"store_id\", IntegerType(), False),\n    StructField(\"date\", StringType(), False),\n    StructField(\"platform_id\", IntegerType(), False),\n    StructField(\"device\", StringType(), False),\n    StructField(\"data_type\", StringType(), False),\n    StructField(\"price_type\", IntegerType(), False),\n    StructField(\"purchase_type\", IntegerType(), False),\n    StructField(\"category_id\", IntegerType(), False),\n    StructField(\"estimate\", LongType(), False)\n])\nspark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2019-12-01/android/market_size/10\", schema=schema, sep='\\t').filter(\"price_type=0 and purchase_type=10\").createOrReplaceTempView(\"raw_data\")\n\nres = spark.sql(\"select SUM(revenue) AS total from raw_data PIVOT (AVG(estimate) for data_type in ('downloads', 'revenue'))\").first()\nprint res[0]\n"]},{"cell_type":"code","execution_count":0,"id":"20200115-013034_1605633385","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import lit, udf\nfrom pyspark.sql.types import (\n    ArrayType, BooleanType, LongType, IntegerType,\n    StringType, StructType, StructField\n)\nschema = StructType([\n    StructField(\"store_id\", IntegerType(), False),\n    StructField(\"date\", StringType(), False),\n    StructField(\"platform_id\", IntegerType(), False),\n    StructField(\"device\", StringType(), False),\n    StructField(\"data_type\", StringType(), False),\n    StructField(\"price_type\", IntegerType(), False),\n    StructField(\"purchase_type\", IntegerType(), False),\n    StructField(\"category_id\", IntegerType(), False),\n    StructField(\"estimate\", LongType(), False)\n])\nspark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2019-12-01/ios/\", schema=schema, sep='\\t').show()\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200114-072725_1428123455","metadata":{},"outputs":[],"source":["\n\nimport datetime\nimport psycopg2\nfrom contextlib import closing\ndbinfo = {\n    'NAME': 'dailyest',\n    'USER': 'app_bdp_usage_qa',\n    'PASSWORD': '2mHdFW6%#REu',\n    'HOST': 'internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com',\n    'PORT': 7432,\n}\nconn = psycopg2.connect(\"\"\"\n            dbname={dbname} user={user} host={host} port={port} password={passwd}\n            \"\"\".format(dbname=dbinfo['NAME'], user=dbinfo['USER'], passwd=dbinfo['PASSWORD'],\n                       host=dbinfo['HOST'], port=dbinfo['PORT']))\ndef select(stmt, params):\n    with closing(conn.cursor()) as cur:\n        cur.execute(stmt, params)\n        rows = cur.fetchall()\n        cnt = cur.rowcount\n    return rows, cnt\n\n# should return 65\ngp_country_count = \"\"\"select COUNT( store_id)from plproxy.execute_select_nestloop($proxy$ select store_id from ms.market_size_daily_estimate where date='2019-12-01' AND device_id=1000$proxy$) tbl (store_id INT);\"\"\"\n\n# should return 156\nios_country_count =\"\"\"select COUNT(distinct store_id)from plproxy.execute_select_nestloop($proxy$ select store_id from ms.market_size_daily_estimate where date='2019-12-01' AND device_id in(2000)$proxy$) tbl (store_id INT);\"\"\"\n\n#86\nios_category_count =\"\"\"select SUM(cnt) from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt from ms.market_size_daily_estimate_2000 where date='2019-12-01' AND store_id=143441$proxy$) tbl (cnt BIGINT);\"\"\"\n\n#103\ngp_category_count =\"\"\"select SUM(cnt)from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt from ms.market_size_daily_estimate_1000 where date='2019-12-01' AND store_id=10$proxy$) tbl (cnt BIGINT);\"\"\"\n\nrows, _ = select(gp_category_count, None)\nprint rows[0][0]"]},{"cell_type":"code","execution_count":0,"id":"20200114-073126_1414589292","metadata":{},"outputs":[],"source":["\n\nspark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2019-12-01/android/market_size/10\", schema=schema, sep='\\t').filter(\"category_id='1' and purchase_type='10' and price_type='0'\").createOrReplaceTempView(\"raw_data\")\nspark.sql(\"select * from raw_data PIVOT (AVG(estimate) for data_type in ('downloads', 'revenue'))\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200114-075216_853373048","metadata":{},"outputs":[],"source":["\n\nimport datetime\nimport psycopg2\nfrom contextlib import closing\ndbinfo = {\n    'NAME': 'dailyest',\n    'USER': 'app_bdp_usage_qa',\n    'PASSWORD': '2mHdFW6%#REu',\n    'HOST': 'internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com',\n    'PORT': 7432,\n}\nconn = psycopg2.connect(\"\"\"\n            dbname={dbname} user={user} host={host} port={port} password={passwd}\n            \"\"\".format(dbname=dbinfo['NAME'], user=dbinfo['USER'], passwd=dbinfo['PASSWORD'],\n                       host=dbinfo['HOST'], port=dbinfo['PORT']))\ndef select(stmt, params):\n    with closing(conn.cursor()) as cur:\n        cur.execute(stmt, params)\n        rows = cur.fetchall()\n        cnt = cur.rowcount\n    return rows, cnt\n\n# should return 65\ngp_country_count = \"\"\"select COUNT(distinct store_id)from plproxy.execute_select_nestloop($proxy$ select store_id from ms.market_size_daily_estimate_1000 where date='2019-12-01'$proxy$) tbl (store_id INT);\"\"\"\n\n# should return 156\nios_country_count =\"\"\"select COUNT(distinct store_id)from plproxy.execute_select_nestloop($proxy$ select store_id from ms.market_size_daily_estimate where date='2019-12-01' AND device_id in(2000)$proxy$) tbl (store_id INT);\"\"\"\n\n#86\nios_category_count =\"\"\"select COUNT(distinct category_id)from plproxy.execute_select_nestloop($proxy$ select category_id from ms.market_size_daily_estimate_2000 where date='2019-12-01' AND store_id=143441$proxy$) tbl (category_id INT);\"\"\"\n\n#103\ngp_category_count =\"\"\"select COUNT(distinct category_id)from plproxy.execute_select_nestloop($proxy$ select category_id from ms.market_size_daily_estimate_1000 where date='2019-12-01' AND store_id=10$proxy$) tbl (category_id INT);\"\"\"\n\n\n# Test Metric Values\n\n#PLproxy this have category filter\ndownloads = \"\"\"select SUM(estimate) as estimate from plproxy.execute_select_nestloop($proxy$ select estimate from ms.market_size_daily_estimate_1000 where date BETWEEN '2019-12-01' and '2019-12-01' AND app_type_id=0 AND purchase_type_id=10 AND kpi=1 AND store_id=10$proxy$) tbl (estimate BIGINT);\"\"\"\nrevenue = \"\"\"select SUM(estimate) as estimate from plproxy.execute_select_nestloop($proxy$ select estimate from ms.market_size_daily_estimate_1000 where date BETWEEN '2019-12-01' and '2019-12-01'  AND app_type_id=0 AND purchase_type_id=10 AND kpi=2 AND store_id=10$proxy$) tbl (estimate BIGINT);\"\"\"\nrows, _ = select(revenue, None)\nprint rows[0][0]\n\nrows, _ = select(downloads, None)\nprint rows[0][0]\n# print len(rows)\n\n#should be the same value with below\nfrom pyspark.sql.functions import lit, udf\nfrom pyspark.sql.types import (\n    ArrayType, BooleanType, LongType, IntegerType,\n    StringType, StructType, StructField\n)\nschema = StructType([\n    StructField(\"store_id\", IntegerType(), False),\n    StructField(\"date\", StringType(), False),\n    StructField(\"platform_id\", IntegerType(), False),\n    StructField(\"device\", StringType(), False),\n    StructField(\"data_type\", StringType(), False),\n    StructField(\"price_type\", IntegerType(), False),\n    StructField(\"purchase_type\", IntegerType(), False),\n    StructField(\"category_id\", IntegerType(), False),\n    StructField(\"estimate\", LongType(), False)\n])\nspark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2019-12-01/android/market_size/10\", schema=schema, sep='\\t').filter(\"price_type=0 and purchase_type=10\").createOrReplaceTempView(\"raw_data\")\nrevenue = spark.sql(\"select SUM(revenue) AS total from raw_data PIVOT (AVG(estimate) for data_type in ('downloads', 'revenue'))\").first()\nprint revenue[0]\ndownload = spark.sql(\"select SUM(downloads) AS total from raw_data PIVOT (AVG(estimate) for data_type in ('downloads', 'revenue'))\").first()\nprint download[0]\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200114-081419_1541124145","metadata":{},"outputs":[],"source":["%%sh\n\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/date=2019-12-01/"]},{"cell_type":"code","execution_count":0,"id":"20200115-033738_427989151","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import sum as spark_sum\ndf = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact/month=2019-12/device_id=2000/\").filter(\"store_id=0 and app_type_id=0 and purchase_type_id=10 and kpi=1 and date='2019-12-01' and category_id in(36)\").select(\"estimate\").groupBy().sum().collect()\nprint(df)"]},{"cell_type":"code","execution_count":0,"id":"20200115-034748_1536870137","metadata":{},"outputs":[],"source":["\n\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\n\n\n\n\ndef pg_settings(urn, sql, granularity, schema):\n    result = {}\n    db_settings = build_db_settings(urn, schema)\n    connection = ClusterConnection(db_settings)\n    master_runner = connection.master_runner\n    rows, _ = master_runner.select(sql)\n    result[granularity] = rows\n    return result\n\nurn = Urn(\n    namespace='app-qa.db-check.v1',\n    owner='app_qa'\n)\n\n\n\n\n\n# test country count\ngp_country_count = \"\"\"select sum(cnt) from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt from ms.market_size_daily_estimate_1000 where date='2019-12-08'$proxy$) tbl (cnt BIGINT);\"\"\"\nres = pg_settings(urn, gp_country_count, '2019-12-01', 'daily_est')\nprint res['2019-12-01'][0][0]\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact//month=2019-12/device_id=1000/\").filter(\"date='2019-12-01'\").count()\n\n# test category count\n# gp_category_count =\"\"\"select SUM(cnt)from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt from ms.market_size_daily_estimate_1000 where date='2019-12-01' AND store_id=10$proxy$) tbl (cnt BIGINT);\"\"\"\n# res = pg_settings(urn, gp_category_count, '2019-12-01', 'daily_est')\n# print res['2019-12-01'][0][0]\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact//month=2019-12/device_id=1000/\").filter(\"date='2019-12-01' and store_id=10\").count()\n\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200115-055953_837368254","metadata":{},"outputs":[],"source":["\n\n\nfrom aadatapipelinecore.core.urn import Urn\nimport psycopg2\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nfrom aaplproxy.connection import ClusterConnection\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\n\n\n\n\ndef pg_settings(urn, sql, granularity, schema):\n    result = {}\n    db_settings = build_db_settings(urn, schema)\n    connection = ClusterConnection(db_settings)\n    master_runner = connection.master_runner\n    rows, _ = master_runner.select(sql)\n    result[granularity] = rows\n    return result\n\nurn = Urn(\n    namespace='app-qa.db-check.v1',\n    owner='app_qa'\n)\n# test country count\ngp_country_count = \"\"\"select sum(cnt) from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt from ms.market_size_daily_estimate_1000 where date='2019-12-01'$proxy$) tbl (cnt BIGINT);\"\"\"\n\n# res = pg_settings(urn, gp_country_count, '2019-12-01', 'daily_est')\n# print res['2019-12-01'][0][0]\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact//month=2019-12/device_id=1000/\").filter(\"date='2019-12-01'\").count()\n\n# test category count\n# gp_category_count =\"\"\"select SUM(cnt)from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt from ms.market_size_daily_estimate_1000 where date='2019-12-01' AND store_id=10$proxy$) tbl (cnt BIGINT);\"\"\"\n# res = pg_settings(urn, gp_category_count, '2019-12-01', 'daily_est')\n# print res['2019-12-01'][0][0]\nUC = [600, 601, 801, 802, 803, 808, 804, 805, 806, 816, 807, 809, 810, 811, 812, 813, 814, 815, 602, 701, 702, 703, 704, 705, 721, 706, 708, 722, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 711, 723, 707, 901, 902, 903]\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact//month=2019-12/device_id=1000/\").filter(\"app_type_id=0 and purchase_type_id=10 and date='2019-12-01' and kpi=1 and store_id=10 and category_id not in (600, 601, 801, 802, 803, 808, 804, 805, 806, 816, 807, 809, 810, 811, 812, 813, 814, 815, 602, 701, 702, 703, 704, 705, 721, 706, 708, 722, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 711, 723, 707, 901, 902, 903)\").select(\"category_id\",\"store_id\").collect().\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/date=2019-12-01/\").filter(\"country_code='WW' and app_price_type_id=0 and purchase_type_id=10 and device_code='android-all' \").show()\n\n\nnew = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/date=2019-12-01/\").filter(\"country_code='WW' and app_price_type_id=0 and purchase_type_id=10 and device_code='ios-all' \").select(\"est_market_size_download\").groupBy().sum().collect()\nprint(new)\n\n\n\nold=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact/month=2019-12/device_id=2000/\").filter(\"store_id=0 and app_type_id=0 and purchase_type_id=10 and kpi=1 and date='2019-12-01' and category_id not in(600, 601, 801, 802, 803, 808, 804, 805, 806, 816, 807, 809, 810, 811, 812, 813, 814, 815, 602, 701, 702, 703, 704, 705, 721, 706, 708, 722, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 711, 723, 707, 901, 902, 903)\").select(\"estimate\").groupBy().sum().collect()\nprint(old)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200115-092339_1293773224","metadata":{},"outputs":[],"source":["\nimport os\nimport shutil\nimport unittest\n\nfrom conf.settings import ROOT\nfrom aadatapipelinecore.core.fs.device import meta_bucket, raw_bucket, unified_bucket\nfrom aadatapipelinecore.core.fs.device.bucket import unified_data_system_config_bucket\nfrom aadatapipelinecore.core.utils.commandline import env\nfrom aadatapipelinecore.core.utils.encode import activate_system_utf8\nfrom aadatapipelinecore.core.utils.spark import create_spark, eject_all_caches\n\n\nclass PySparkTest(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"Setup a basic Spark context for testing\"\"\"\n        activate_system_utf8()\n        env(PYTHONIOENCODING='utf8')\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext\n        cls._release_resource()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls._release_resource()\n        cls.sc = None\n        # comment below code for performance\n        # cls.spark.stop()\n\n    @classmethod\n    def _release_resource(cls):\n        # clear spark cached and persisted files\n        eject_all_caches(cls.spark)\n        # clear physical files\n        cls._empty_bucket(meta_bucket())\n        cls._empty_bucket(raw_bucket())\n        cls._empty_bucket(unified_bucket())\n        cls._empty_bucket(unified_data_system_config_bucket())\n        cls._empty_spark_db_dir(\"{}/metastore_db\".format(ROOT))\n        cls._empty_spark_db_dir(\"{}/spark-warehouse\".format(ROOT))\n\n    @classmethod\n    def _empty_bucket(cls, bucket):\n        try:\n            if os.environ.get('EMPTY_BUCKET', \"true\").lower() != \"false\":\n                bucket.empty()\n        except OSError:\n            pass\n\n    @classmethod\n    def _empty_spark_db_dir(cls, db_dir):\n        try:\n            shutil.rmtree(db_dir, ignore_errors=False)\n        except OSError:\n            pass\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport random\nfrom applications.db_check_v1.common.constants import pg_settings, COUNTRY_MAPPING_BY_MARKET\nfrom applications.db_check_v1.common.table_common_info import urn\nfrom applications.db_check_v1.common.utils import get_date_list, string_to_datetime\n\nDUMP_FILE_PATH = \\\n    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact/\" \\\n    \"month={}-{}/device_id={}/\"\nAPP_PRICE_TYPE_ID = {'ALL': 0, 'FREE': 1, 'PAID': 2}\nPURCHASE_TYPE_ID = {'ALL': 10, 'TRUE': 11, 'FALSE': 12}\nUNIFIED_FILE_PATH = \"\"\ndef check_country_count(date, device_id):\n        year = string_to_datetime(date).year\n        \n        month = string_to_datetime(date).month\n        if month<10:\n            month = '0'+str(month)\n        sql = \"\"\"select SUM(cnt)from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt \n        from ms.market_size_daily_estimate \n        where date='{date}' AND device_id={device_id}$proxy$) tbl (cnt BIGINT);\"\"\".format(date=date,\n                                                                                          device_id=device_id)\n        pg_count = pg_settings(urn, sql, date, 'daily_est')[date][0][0]\n\n        parquet_count = spark.read.parquet(DUMP_FILE_PATH.format(year, month, device_id)).filter(\n            \"date='{}'\".format(date)).count()\n        assert(pg_count==parquet_count),\"can not match {} - {}\".format(date, device_id)\ndef test_gp_country_count():\n    for date in random.sample(get_date_list(\"2013-01-01\", \"2020-01-01\"), 50):\n        print \"Running:\"+str(date)\n        check_country_count(date, 1000)\n\n\ndef check_category_count(date, device_id, store_id):\n        year = string_to_datetime(date).year\n        month = string_to_datetime(date).month\n        if month<10:\n            month = '0'+str(month)\n        sql = \"\"\"select SUM(cnt)from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt from \n        ms.market_size_daily_estimate_{device_id} \n        where date='{date}' AND store_id={store_id}$proxy$) tbl (cnt BIGINT);\"\"\".format(device_id=device_id,\n                                                                                        date=date,\n                                                                                        store_id=store_id)\n        pg_count = pg_settings(urn, sql, date, \"daily_est\")[date][0][0]\n\n        parquet_count = spark.read.parquet(DUMP_FILE_PATH.format(year, month, device_id)).filter(\n            \"store_id={} and date='{}'\".format(store_id, date)).count()\n\n        assert(pg_count==parquet_count),\"can not match {} - {} - {}\".format(date, device_id, store_id)\n\ndef test_gp_category_count():\n    for date in random.sample(get_date_list(\"2013-01-01\", \"2020-01-01\"), 50):\n        for store_id in COUNTRY_MAPPING_BY_MARKET['gp']:\n            print \"{}-{}\".format(date, store_id)\n            check_category_count(date, 1000, store_id)\n\ntest_gp_category_count()\n"]},{"cell_type":"code","execution_count":0,"id":"20200115-093956_1707553068","metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":0,"id":"20200115-094235_566333225","metadata":{},"outputs":[],"source":["\nimport os\nimport shutil\nimport unittest\n\nfrom conf.settings import ROOT\nfrom aadatapipelinecore.core.fs.device import meta_bucket, raw_bucket, unified_bucket\nfrom aadatapipelinecore.core.fs.device.bucket import unified_data_system_config_bucket\nfrom aadatapipelinecore.core.utils.commandline import env\nfrom aadatapipelinecore.core.utils.encode import activate_system_utf8\nfrom aadatapipelinecore.core.utils.spark import create_spark, eject_all_caches\n\n\nclass PySparkTest(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"Setup a basic Spark context for testing\"\"\"\n        activate_system_utf8()\n        env(PYTHONIOENCODING='utf8')\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext\n        cls._release_resource()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls._release_resource()\n        cls.sc = None\n        # comment below code for performance\n        # cls.spark.stop()\n\n    @classmethod\n    def _release_resource(cls):\n        # clear spark cached and persisted files\n        eject_all_caches(cls.spark)\n        # clear physical files\n        cls._empty_bucket(meta_bucket())\n        cls._empty_bucket(raw_bucket())\n        cls._empty_bucket(unified_bucket())\n        cls._empty_bucket(unified_data_system_config_bucket())\n        cls._empty_spark_db_dir(\"{}/metastore_db\".format(ROOT))\n        cls._empty_spark_db_dir(\"{}/spark-warehouse\".format(ROOT))\n\n    @classmethod\n    def _empty_bucket(cls, bucket):\n        try:\n            if os.environ.get('EMPTY_BUCKET', \"true\").lower() != \"false\":\n                bucket.empty()\n        except OSError:\n            pass\n\n    @classmethod\n    def _empty_spark_db_dir(cls, db_dir):\n        try:\n            shutil.rmtree(db_dir, ignore_errors=False)\n        except OSError:\n            pass\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport random\nfrom applications.db_check_v1.common.constants import pg_settings, COUNTRY_MAPPING_BY_MARKET\nfrom applications.db_check_v1.common.table_common_info import urn\nfrom applications.db_check_v1.common.utils import get_date_list, string_to_datetime\n\nDUMP_FILE_PATH = \\\n    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact/\" \\\n    \"month={}-{}/device_id={}/\"\nAPP_PRICE_TYPE_ID = {'ALL': 0, 'FREE': 1, 'PAID': 2}\nPURCHASE_TYPE_ID = {'ALL': 10, 'TRUE': 11, 'FALSE': 12}\nUNIFIED_FILE_PATH = \"\"\ndef check_country_count(date, device_id):\n        year = string_to_datetime(date).year\n        \n        month = string_to_datetime(date).month\n        if month<10:\n            month = '0'+str(month)\n        sql = \"\"\"select SUM(cnt)from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt \n        from ms.market_size_daily_estimate \n        where date='{date}' AND device_id={device_id}$proxy$) tbl (cnt BIGINT);\"\"\".format(date=date,\n                                                                                          device_id=device_id)\n        pg_count = pg_settings(urn, sql, date, 'daily_est')[date][0][0]\n\n        parquet_count = spark.read.parquet(DUMP_FILE_PATH.format(year, month, device_id)).filter(\n            \"date='{}'\".format(date)).count()\n        assert(pg_count==parquet_count),\"can not match {} - {}\".format(date, device_id)\ndef test_gp_country_count():\n    for date in random.sample(get_date_list(\"2013-01-01\", \"2020-01-01\"), 50):\n        print \"Running:\"+str(date)\n        check_country_count(date, 1000)\n\n\ndef check_category_count(date, device_id, store_id):\n        year = string_to_datetime(date).year\n        month = string_to_datetime(date).month\n        if month<10:\n            month = '0'+str(month)\n        sql = \"\"\"select SUM(cnt)from plproxy.execute_select_nestloop($proxy$ select count(1) as cnt from \n        ms.market_size_daily_estimate_{device_id} \n        where date='{date}' AND store_id={store_id}$proxy$) tbl (cnt BIGINT);\"\"\".format(device_id=device_id,\n                                                                                        date=date,\n                                                                                        store_id=store_id)\n        pg_count = pg_settings(urn, sql, date, \"daily_est\")[date][0][0]\n\n        parquet_count = spark.read.parquet(DUMP_FILE_PATH.format(year, month, device_id)).filter(\n            \"store_id={} and date='{}'\".format(store_id, date)).count()\n\n        assert(pg_count==parquet_count),\"can not match {} - {} - {}\".format(date, device_id, store_id)\n\ndef test_gp_category_count():\n    for date in random.sample(get_date_list(\"2013-01-01\", \"2020-01-01\"), 50):\n        for store_id in COUNTRY_MAPPING_BY_MARKET['gp']:\n            print \"{}-{}\".format(date, store_id)\n            check_category_count(date, 1000, store_id)\n            \ndef check_metric_value(date, device_id, store_id, kpi):\n\n        year = string_to_datetime(date).year\n        month = string_to_datetime(date).month\n        if month<10:\n            month = '0'+str(month)\n        sql = \"\"\"select SUM(estimate) as estimate from plproxy.execute_select_nestloop\n        ($proxy$ select estimate from ms.market_size_daily_estimate_{device_id} \n        where date='{date}' AND app_type_id={app_price_type_id}\n        AND purchase_type_id={purchase_type_id} AND kpi={kpi} AND store_id={store_id}$proxy$) tbl (estimate BIGINT);\"\"\".format(\n            device_id=device_id, date=date, app_price_type_id=APP_PRICE_TYPE_ID['ALL'],\n            purchase_type_id=PURCHASE_TYPE_ID['ALL'], kpi=kpi, store_id=store_id)\n\n        pg_metric_value = pg_settings(urn, sql, date, \"daily_est\")[date][0][0]\n        parquet_metric_value = spark.read.parquet(DUMP_FILE_PATH.format(year, month, device_id)).filter(\n            \"store_id={store_id} and app_type_id={app_type_id} \"\n            \"and purchase_type_id={purchase_type_id} and kpi={kpi} and date='{date}'\".format(store_id=store_id,\n                                                                                             app_type_id=APP_PRICE_TYPE_ID['ALL'],\n                                                                                             purchase_type_id=PURCHASE_TYPE_ID['ALL'],\n                                                                                             kpi=kpi,\n                                                                                             date=date)).select(\n            \"estimate\").groupBy().sum().collect()[0][0]\n        assert(pg_metric_value==parquet_metric_value),\"can not match {}-{}-{}-{}\".format(date, device_id, store_id, kpi)           \ndef test_ios_metric_data_accuracy():\n        KPI = [1, 2]\n        for date in random.sample(get_date_list(\"2013-01-01\", \"2020-01-01\"), 50):\n            for store_id in COUNTRY_MAPPING_BY_MARKET['ios']:\n                print \"{}-{}\".format(date, store_id)\n                kpi = random.choice(KPI)\n                check_metric_value(date, 2000, store_id, kpi)\n\ntest_ios_metric_data_accuracy()\n"]},{"cell_type":"code","execution_count":0,"id":"20200115-102304_1529369424","metadata":{},"outputs":[],"source":["\nimport os\nimport shutil\nimport unittest\nimport random\nimport time\nfrom conf.settings import ROOT\nfrom aadatapipelinecore.core.fs.device import meta_bucket, raw_bucket, unified_bucket\nfrom aadatapipelinecore.core.fs.device.bucket import unified_data_system_config_bucket\nfrom aadatapipelinecore.core.utils.commandline import env\nfrom aadatapipelinecore.core.utils.encode import activate_system_utf8\nfrom aadatapipelinecore.core.utils.spark import create_spark, eject_all_caches\n\nDUMP_FILE_PATH = \\\n    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact/\" \\\n    \"month={}-{}/device_id={}/\"\n\nUNIFIED_FILE_PATH = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/\" \\\n                    \"granularity=daily/date={}/\"\nclass PySparkTest(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"Setup a basic Spark context for testing\"\"\"\n        activate_system_utf8()\n        env(PYTHONIOENCODING='utf8')\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext\n        cls._release_resource()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls._release_resource()\n        cls.sc = None\n        # comment below code for performance\n        # cls.spark.stop()\n\n    @classmethod\n    def _release_resource(cls):\n        # clear spark cached and persisted files\n        eject_all_caches(cls.spark)\n        # clear physical files\n        cls._empty_bucket(meta_bucket())\n        cls._empty_bucket(raw_bucket())\n        cls._empty_bucket(unified_bucket())\n        cls._empty_bucket(unified_data_system_config_bucket())\n        cls._empty_spark_db_dir(\"{}/metastore_db\".format(ROOT))\n        cls._empty_spark_db_dir(\"{}/spark-warehouse\".format(ROOT))\n\n    @classmethod\n    def _empty_bucket(cls, bucket):\n        try:\n            if os.environ.get('EMPTY_BUCKET', \"true\").lower() != \"false\":\n                bucket.empty()\n        except OSError:\n            pass\n\n    @classmethod\n    def _empty_spark_db_dir(cls, db_dir):\n        try:\n            shutil.rmtree(db_dir, ignore_errors=False)\n        except OSError:\n            pass\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nCOUNTRY_MAPPING_BY_MARKET = {\n    'gp': {1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB',\n           8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA',\n           15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID',\n           22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL',\n           29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT',\n           36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO',\n           43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE',\n           50: 'KW', 51: 'SA', 52: 'CO', 53: 'KZ', 54: 'PK', 56: 'PE', 61: 'AZ',\n           62: 'EC', 64: 'CR', 65: 'LB', 73: 'QA', 74: 'NG', 78: 'LT', 80: 'HR',\n           86: 'LV', 95: 'KE', 1000: 'WW'},\n    'ios': {143441: 'US', 143442: 'FR', 143443: 'DE', 143444: 'GB', 143445: 'AT',\n            143446: 'BE', 143447: 'FI', 143448: 'GR', 143449: 'IE', 143450: 'IT',\n            143451: 'LU', 143452: 'NL', 143453: 'PT', 143454: 'ES', 143455: 'CA',\n            143456: 'SE', 143457: 'NO', 143458: 'DK', 143459: 'CH', 143460: 'AU',\n            143461: 'NZ', 143462: 'JP', 143463: 'HK', 143464: 'SG', 143465: 'CN',\n            143466: 'KR', 143467: 'IN', 143468: 'MX', 143469: 'RU', 143470: 'TW',\n            143471: 'VN', 143472: 'ZA', 143473: 'MY', 143474: 'PH', 143475: 'TH',\n            143476: 'ID', 143477: 'PK', 143478: 'PL', 143479: 'SA', 143480: 'TR',\n            143481: 'AE', 143482: 'HU', 143483: 'CL', 143484: 'NP', 143485: 'PA',\n            143486: 'LK', 143487: 'RO', 143489: 'CZ', 143491: 'IL', 143492: 'UA',\n            143493: 'KW', 143494: 'HR', 143495: 'CR', 143496: 'SK', 143497: 'LB',\n            143498: 'QA', 143499: 'SI', 143501: 'CO', 143502: 'VE', 143503: 'BR',\n            143504: 'GT', 143505: 'AR', 143506: 'SV', 143507: 'PE', 143508: 'DO',\n            143509: 'EC', 143510: 'HN', 143511: 'JM', 143512: 'NI', 143513: 'PY',\n            143514: 'UY', 143515: 'MO', 143516: 'EG', 143517: 'KZ', 143518: 'EE',\n            143519: 'LV', 143520: 'LT', 143521: 'MT', 143523: 'MD', 143524: 'AM',\n            143525: 'BW', 143526: 'BG', 143528: 'JO', 143529: 'KE', 143530: 'MK',\n            143531: 'MG', 143532: 'ML', 143533: 'MU', 143534: 'NE', 143535: 'SN',\n            143536: 'TN', 143537: 'UG', 143538: 'AI', 143539: 'BS', 143540: 'AG',\n            143541: 'BB', 143542: 'BM', 143543: 'VG', 143544: 'KY', 143545: 'DM',\n            143546: 'GD', 143547: 'MS', 143548: 'KN', 143549: 'LC', 143550: 'VC',\n            143551: 'TT', 143552: 'TC', 143553: 'GY', 143554: 'SR', 143555: 'BZ',\n            143556: 'BO', 143557: 'CY', 143558: 'IS', 143559: 'BH', 143560: 'BN',\n            143561: 'NG', 143562: 'OM', 143563: 'DZ', 143564: 'AO', 143565: 'BY',\n            143566: 'UZ', 143568: 'AZ', 143571: 'YE', 143572: 'TZ', 143573: 'GH',\n            143575: 'AL', 143576: 'BJ', 143577: 'BT', 143578: 'BF', 143579: 'KH',\n            143580: 'CV', 143581: 'TD', 143582: 'CG', 143583: 'FJ', 143584: 'GM',\n            143585: 'GW', 143586: 'KG', 143587: 'LA', 143588: 'LR', 143589: 'MW',\n            143590: 'MR', 143591: 'FM', 143592: 'MN', 143593: 'MZ', 143594: 'NA',\n            143595: 'PW', 143597: 'PG', 143598: 'ST', 143599: 'SC', 143600: 'SL',\n            143601: 'SB', 143602: 'SZ', 143603: 'TJ', 143604: 'TM', 143605: 'ZW',\n            0: 'WW'},\n}\nDEVICE_ID_TO_DEVICE_CODE_MAPPING = {\n    1000: 'android-all',\n    2000: 'ios-all',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\nDEVICE_ID_TYPE_MAPPING = {\n    1000: 'gp',\n    2000: 'ios',\n    2001: 'ios',\n    2002: 'ios'\n}\nUC = [600, 601, 801, 802, 803, 808, 804, 805, 806, 816, 807, 809, 810, 811, 812, 813, 814, 815, 602, 701, 702, 703, 704, 705, 721, 706, 708, 722, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 711, 723, 707, 901, 902, 903]\nAPP_PRICE_TYPE_ID = {'ALL': 0, 'FREE': 1, 'PAID': 2}\nPURCHASE_TYPE_ID = {'ALL': 10, 'TRUE': 11, 'FALSE': 12}\ndef _parse_date(date):\n        year = string_to_datetime(date).year\n        month = string_to_datetime(date).month\n        if month < 10:\n            month = '0' + str(month)\n\n        return year, month\n        \n\n\n\n\n# new = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/date=2019-12-01/\").filter(\"country_code='WW' and app_price_type_id=0 and purchase_type_id=10 and device_code='ios-all' \").select(\"est_market_size_download\").groupBy().sum().collect()\n# print(new)\n\n\n\n# old=spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact/month=2019-12/device_id=2000/\").filter(\"store_id=0 and app_type_id=0 and purchase_type_id=10 and kpi=1 and date='2019-12-01' and category_id not in(600, 601, 801, 802, 803, 808, 804, 805, 806, 816, 807, 809, 810, 811, 812, 813, 814, 815, 602, 701, 702, 703, 704, 705, 721, 706, 708, 722, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 711, 723, 707, 901, 902, 903)\").select(\"estimate\").groupBy().sum().collect()\n# print(old)\n\n\n\n    \ndef get_dump_estimate(date, device_id, store_id, kpi):\n        year, month = _parse_date(date)\n        dump_estimate = spark.read.parquet(DUMP_FILE_PATH.format(year, month, device_id)).filter(\n            \"store_id={store_id} and app_type_id={app_type_id} \"\n            \"and purchase_type_id={purchase_type_id} and kpi={kpi} and date='{date}' and category_id not in(600, 601, 801, 802, 803, 808, 804, 805, 806, 816, 807, 809, 810, 811, 812, 813, 814, 815, 602, 701, 702, 703, 704, 705, 721, 706, 708, 722, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 711, 723, 707, 901, 902, 903)\".format(store_id=store_id,\n                                                                                             app_type_id=\n                                                                                             APP_PRICE_TYPE_ID['ALL'],\n                                                                                             purchase_type_id=\n                                                                                             PURCHASE_TYPE_ID['ALL'],\n                                                                                             kpi=kpi,\n                                                                                             date=date)).select(\n            \"estimate\").groupBy().sum().collect()[0][0]\n        return dump_estimate\n\ndef get_unified_estimate(date, device_code, country_code, metric):\n        unified_estimate = spark.read.parquet(UNIFIED_FILE_PATH.format(date)).filter(\n            \"country_code='{country_code}' and app_price_type_id={app_price_type_id} \"\n            \"and purchase_type_id={purchase_type_id} and device_code='{device_code}' \".format(country_code=country_code,\n                                                                                              app_price_type_id=\n                                                                                              APP_PRICE_TYPE_ID['ALL'],\n                                                                                              purchase_type_id=\n                                                                                              PURCHASE_TYPE_ID['ALL'],\n                                                                                              device_code=device_code,\n                                                                                              )).select(\n            \"{metric}\".format(metric=metric)).groupBy().sum().collect()[0][0]\n        return unified_estimate\n\ndef check_metric_value(date, device_id, store_id, kpi):\n        KPI_TO_METRIC = {1: 'est_market_size_download', 2: 'est_market_size_revenue'}\n        dump_metric_value = get_dump_estimate(date, device_id, store_id, kpi)\n        device_code = DEVICE_ID_TO_DEVICE_CODE_MAPPING[device_id]\n        # ios or gp\n        device_type = DEVICE_ID_TYPE_MAPPING[device_id]\n        country_code = COUNTRY_MAPPING_BY_MARKET[device_type][store_id]\n        unified_metric_value = get_unified_estimate(date, device_code, country_code, KPI_TO_METRIC[kpi])\n        assert(dump_metric_value==unified_metric_value),\"can not match {}-{}-{}-{}\".format(date, device_id, store_id, kpi)\n\n\n\n\n\n\ndef test_ios_data_accuracy():\n        KPI = [1, 2]\n        for date in random.sample(get_date_list(\"2013-01-01\", \"2020-01-01\"), 200):\n            start_time = time.time()\n            for store_id in COUNTRY_MAPPING_BY_MARKET['ios']:\n                print \"{} - {}\".format(date, store_id)\n                kpi = random.choice(KPI)\n                check_metric_value(date, 2002, store_id, kpi)\n            end_time = time.time()\n            print \"############################# one day cost :{}\".format(end_time - start_time)\ntest_ios_data_accuracy()\n"]},{"cell_type":"code","execution_count":0,"id":"20200116-103208_662511315","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import lit, udf\nfrom pyspark.sql.types import (\n    ArrayType, BooleanType, LongType, IntegerType,\n    StringType, StructType, StructField\n)\nschema = StructType([\n    StructField(\"store_id\", IntegerType(), False),\n    StructField(\"date\", StringType(), False),\n    StructField(\"platform_id\", IntegerType(), False),\n    StructField(\"device\", StringType(), False),\n    StructField(\"data_type\", StringType(), False),\n    StructField(\"price_type\", IntegerType(), False),\n    StructField(\"purchase_type\", IntegerType(), False),\n    StructField(\"category_id\", IntegerType(), False),\n    StructField(\"estimate\", LongType(), False)\n])\n\n# Rountine unified data\n# new = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/date=2019-12-01/\").filter(\"country_code='US' and app_price_type_id=0 and purchase_type_id=10 and device_code='ios-all' \").select(\"est_market_size_download\").groupBy().sum().collect()\n# print new\n# # raw data \n\n\n\n\n\n# # Rountine RAW Data\n# # store_id|      date|platform_id|     device|data_type|price_type|purchase_type|category_id|estimate\n# spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2019-12-01/ios/market_size/143441\", schema=schema, sep='\\t').filter(\"price_type=0 and purchase_type=10 and data_type='downloads' and category_id not in (600, 601, 801, 802, 803, 808, 804, 805, 806, 816, 807, 809, 810, 811, 812, 813, 814, 815, 602, 701, 702, 703, 704, 705, 721, 706, 708, 722, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 711, 723, 707, 901, 902, 903) and device='ios'\").select(\"estimate\").groupBy().sum().collect()\n\n\n\n\n\n\n\n\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/date=2020-01-10/\").filter(\"country_code='US' and app_price_type_id=0 and purchase_type_id=10 and device_code='android-all' and category_id=400002\").show()\nspark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-01-10/android/market_size/10\", schema=schema, sep='\\t').filter(\"price_type=0 and purchase_type=10 and data_type='downloads' and category_id=38 and device='google-play'\").show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200119-062236_708952472","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import lit, udf\nfrom pyspark.sql.types import (\n    ArrayType, BooleanType, LongType, IntegerType,\n    StringType, StructType, StructField\n)\nschema = StructType([\n    StructField(\"store_id\", IntegerType(), False),\n    StructField(\"date\", StringType(), False),\n    StructField(\"platform_id\", IntegerType(), False),\n    StructField(\"device\", StringType(), False),\n    StructField(\"data_type\", StringType(), False),\n    StructField(\"price_type\", IntegerType(), False),\n    StructField(\"purchase_type\", IntegerType(), False),\n    StructField(\"category_id\", IntegerType(), False),\n    StructField(\"estimate\", LongType(), False)\n])\n\n\n# Rountine unified data\n# new = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/date=2020-01-10/\").filter(\"country_code='US' and app_price_type_id=0 and purchase_type_id=10 and device_code='android-all' and category_id=400001\").show()\n# print new\n# raw data \n\n# Rountine RAW Data\n# store_id|      date|platform_id|     device|data_type|price_type|purchase_type|category_id|estimate\n# spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-01-10/android/market_size/10\", schema=schema, sep='\\t').filter(\"price_type=0 and purchase_type=10 and data_type='downloads' and category_id=2 and device='google-play'\").show()\n\n\n\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/date=2020-01-10/\").filter(\"country_code='US' and app_price_type_id=0 and purchase_type_id=10 and device_code='android-all' and category_id=400002\").show()\n\n\n\n\n\nspark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2020-01-10/android/market_size/10\", schema=schema, sep='\\t').filter(\"price_type=0 and purchase_type=10 and data_type='downloads' and category_id=38 and device='google-play'\").show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200120-035507_778776505","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132  -U citus_bdp_usage_qa -d aa_citus_db -p 5432 << EOF  \nselect * from store.store_market_size_fact_v1 where date='2020-01-11' and country_code='US' and app_price_type_id=0 and purchase_type_id=10 and device_code='android-all' and category_id=400001;\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200120-065548_63193073","metadata":{},"outputs":[],"source":["\n\n# spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.market-size.v1/fact/granularity=daily/date=2020-01-11/\").filter(\"country_code='US' and app_price_type_id=0 and purchase_type_id=10 and device_code='android-all'\").select(\"est_market_size_download\").groupBy().sum().collect()\n\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/date=2020-01-11/\").filter(\"country_code='US' and app_price_type_id=0 and purchase_type_id=10 and device_code='android-all'\").select(\"est_market_size_download\").groupBy().sum().collect()\n\n\n\n\n\n\n\n\n\n\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200206-074022_1580683261","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/granularity=daily/"]},{"cell_type":"code","execution_count":0,"id":"20200206-080410_996414759","metadata":{},"outputs":[],"source":["\nimport psycopg2\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nfrom aaplproxy.connection import ClusterConnection\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\ndef citus_settings(schema):\n    template = \"CITUS_\" + schema.upper() + \"_CITUS_DB_{property}\"\n    host, port = getattr(settings, template.format(property='HOSTS'))[0]\n    return (\n        \"dbname='{db}' user='{user}' password='{password}' \"\n        \"host='{host}' port='{port}'\".format(\n            db=getattr(settings, template.format(property='NAME')),\n            user=getattr(settings, template.format(property='ACCESS_ID')),\n            host=host,\n            password=getattr(settings, template.format(property='SECRET_KEY')),\n            port=port\n        )\n    )\n\nsql = \"select sum(est_market_size_download) from store.store_market_size_fact_v1 where date='2020-01-11' and country_code='US' and app_price_type_id=0 and purchase_type_id=10 and device_code='android-all' and category_id=400001;\"\nresult = query(citus_settings(\"aa\"), sql)\nprint result[0][0]"]},{"cell_type":"code","execution_count":0,"id":"20200206-083611_229207287","metadata":{},"outputs":[],"source":["\nimport os\nimport shutil\nimport unittest\nimport random\nimport time\nfrom conf.settings import ROOT\nfrom aadatapipelinecore.core.fs.device import meta_bucket, raw_bucket, unified_bucket\nfrom aadatapipelinecore.core.fs.device.bucket import unified_data_system_config_bucket\nfrom aadatapipelinecore.core.utils.commandline import env\nfrom aadatapipelinecore.core.utils.encode import activate_system_utf8\nfrom aadatapipelinecore.core.utils.spark import create_spark, eject_all_caches\nimport psycopg2\nfrom applications.db_check_v1.common.utils import get_date_list, string_to_datetime\n\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\n\nfrom conf import settings\nfrom aadatapipelinecore.core.loader.plproxy import build_db_settings\nfrom aaplproxy.connection import ClusterConnection\ndef query(dsn, sql):\n    with psycopg2.connect(dsn) as conn:\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql)\n            result = cur.fetchall()\n            conn.commit()\n    return result\ndef citus_settings(schema):\n    template = \"CITUS_\" + schema.upper() + \"_CITUS_DB_{property}\"\n    host, port = getattr(settings, template.format(property='HOSTS'))[0]\n    return (\n        \"dbname='{db}' user='{user}' password='{password}' \"\n        \"host='{host}' port='{port}'\".format(\n            db=getattr(settings, template.format(property='NAME')),\n            user=getattr(settings, template.format(property='ACCESS_ID')),\n            host=host,\n            password=getattr(settings, template.format(property='SECRET_KEY')),\n            port=port\n        )\n    )\n    \nDUMP_FILE_PATH = \\\n    \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.legacy-market_size_daily_estimate.v2/fact/\" \\\n    \"month={}-{}/device_id={}/\"\n\nUNIFIED_FILE_PATH = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/app-tech.store.market-size.v1/fact/\" \\\n                    \"granularity=daily/date={}/\"\nclass PySparkTest(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"Setup a basic Spark context for testing\"\"\"\n        activate_system_utf8()\n        env(PYTHONIOENCODING='utf8')\n        cls.spark = create_spark()\n        cls.sc = cls.spark.sparkContext\n        cls._release_resource()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls._release_resource()\n        cls.sc = None\n        # comment below code for performance\n        # cls.spark.stop()\n\n    @classmethod\n    def _release_resource(cls):\n        # clear spark cached and persisted files\n        eject_all_caches(cls.spark)\n        # clear physical files\n        cls._empty_bucket(meta_bucket())\n        cls._empty_bucket(raw_bucket())\n        cls._empty_bucket(unified_bucket())\n        cls._empty_bucket(unified_data_system_config_bucket())\n        cls._empty_spark_db_dir(\"{}/metastore_db\".format(ROOT))\n        cls._empty_spark_db_dir(\"{}/spark-warehouse\".format(ROOT))\n\n    @classmethod\n    def _empty_bucket(cls, bucket):\n        try:\n            if os.environ.get('EMPTY_BUCKET', \"true\").lower() != \"false\":\n                bucket.empty()\n        except OSError:\n            pass\n\n    @classmethod\n    def _empty_spark_db_dir(cls, db_dir):\n        try:\n            shutil.rmtree(db_dir, ignore_errors=False)\n        except OSError:\n            pass\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nCOUNTRY_MAPPING_BY_MARKET = {\n    'gp': {1: 'AU', 2: 'CA', 3: 'CN', 4: 'DE', 5: 'ES', 6: 'FR', 7: 'GB',\n           8: 'IT', 9: 'JP', 10: 'US', 11: 'BE', 12: 'CH', 13: 'CL', 14: 'ZA',\n           15: 'VN', 16: 'HK', 17: 'AR', 18: 'BR', 19: 'IN', 20: 'FI', 21: 'ID',\n           22: 'RU', 23: 'NL', 24: 'MY', 25: 'TR', 26: 'MX', 27: 'KR', 28: 'PL',\n           29: 'TH', 30: 'TW', 31: 'PH', 32: 'SG', 33: 'EG', 34: 'SE', 35: 'AT',\n           36: 'CZ', 37: 'HU', 38: 'DK', 39: 'IE', 40: 'IL', 41: 'NZ', 42: 'NO',\n           43: 'PT', 44: 'RO', 45: 'SK', 46: 'GR', 47: 'BG', 48: 'UA', 49: 'AE',\n           50: 'KW', 51: 'SA', 52: 'CO', 53: 'KZ', 54: 'PK', 56: 'PE', 61: 'AZ',\n           62: 'EC', 64: 'CR', 65: 'LB', 73: 'QA', 74: 'NG', 78: 'LT', 80: 'HR',\n           86: 'LV', 95: 'KE', 1000: 'WW'},\n    'ios': {143441: 'US', 143442: 'FR', 143443: 'DE', 143444: 'GB', 143445: 'AT',\n            143446: 'BE', 143447: 'FI', 143448: 'GR', 143449: 'IE', 143450: 'IT',\n            143451: 'LU', 143452: 'NL', 143453: 'PT', 143454: 'ES', 143455: 'CA',\n            143456: 'SE', 143457: 'NO', 143458: 'DK', 143459: 'CH', 143460: 'AU',\n            143461: 'NZ', 143462: 'JP', 143463: 'HK', 143464: 'SG', 143465: 'CN',\n            143466: 'KR', 143467: 'IN', 143468: 'MX', 143469: 'RU', 143470: 'TW',\n            143471: 'VN', 143472: 'ZA', 143473: 'MY', 143474: 'PH', 143475: 'TH',\n            143476: 'ID', 143477: 'PK', 143478: 'PL', 143479: 'SA', 143480: 'TR',\n            143481: 'AE', 143482: 'HU', 143483: 'CL', 143484: 'NP', 143485: 'PA',\n            143486: 'LK', 143487: 'RO', 143489: 'CZ', 143491: 'IL', 143492: 'UA',\n            143493: 'KW', 143494: 'HR', 143495: 'CR', 143496: 'SK', 143497: 'LB',\n            143498: 'QA', 143499: 'SI', 143501: 'CO', 143502: 'VE', 143503: 'BR',\n            143504: 'GT', 143505: 'AR', 143506: 'SV', 143507: 'PE', 143508: 'DO',\n            143509: 'EC', 143510: 'HN', 143511: 'JM', 143512: 'NI', 143513: 'PY',\n            143514: 'UY', 143515: 'MO', 143516: 'EG', 143517: 'KZ', 143518: 'EE',\n            143519: 'LV', 143520: 'LT', 143521: 'MT', 143523: 'MD', 143524: 'AM',\n            143525: 'BW', 143526: 'BG', 143528: 'JO', 143529: 'KE', 143530: 'MK',\n            143531: 'MG', 143532: 'ML', 143533: 'MU', 143534: 'NE', 143535: 'SN',\n            143536: 'TN', 143537: 'UG', 143538: 'AI', 143539: 'BS', 143540: 'AG',\n            143541: 'BB', 143542: 'BM', 143543: 'VG', 143544: 'KY', 143545: 'DM',\n            143546: 'GD', 143547: 'MS', 143548: 'KN', 143549: 'LC', 143550: 'VC',\n            143551: 'TT', 143552: 'TC', 143553: 'GY', 143554: 'SR', 143555: 'BZ',\n            143556: 'BO', 143557: 'CY', 143558: 'IS', 143559: 'BH', 143560: 'BN',\n            143561: 'NG', 143562: 'OM', 143563: 'DZ', 143564: 'AO', 143565: 'BY',\n            143566: 'UZ', 143568: 'AZ', 143571: 'YE', 143572: 'TZ', 143573: 'GH',\n            143575: 'AL', 143576: 'BJ', 143577: 'BT', 143578: 'BF', 143579: 'KH',\n            143580: 'CV', 143581: 'TD', 143582: 'CG', 143583: 'FJ', 143584: 'GM',\n            143585: 'GW', 143586: 'KG', 143587: 'LA', 143588: 'LR', 143589: 'MW',\n            143590: 'MR', 143591: 'FM', 143592: 'MN', 143593: 'MZ', 143594: 'NA',\n            143595: 'PW', 143597: 'PG', 143598: 'ST', 143599: 'SC', 143600: 'SL',\n            143601: 'SB', 143602: 'SZ', 143603: 'TJ', 143604: 'TM', 143605: 'ZW',\n            0: 'WW'},\n}\nDEVICE_ID_TO_DEVICE_CODE_MAPPING = {\n    1000: 'android-all',\n    2000: 'ios-all',\n    2001: 'ios-phone',\n    2002: 'ios-tablet'\n}\nDEVICE_ID_TYPE_MAPPING = {\n    1000: 'gp',\n    2000: 'ios',\n    2001: 'ios',\n    2002: 'ios'\n}\nUC = [600, 601, 801, 802, 803, 808, 804, 805, 806, 816, 807, 809, 810, 811, 812, 813, 814, 815, 602, 701, 702, 703, 704, 705, 721, 706, 708, 722, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 711, 723, 707, 901, 902, 903]\nAPP_PRICE_TYPE_ID = {'ALL': 0, 'FREE': 1, 'PAID': 2}\nPURCHASE_TYPE_ID = {'ALL': 10, 'TRUE': 11, 'FALSE': 12}\ndef _parse_date(date):\n        year = string_to_datetime(date).year\n        month = string_to_datetime(date).month\n        if month < 10:\n            month = '0' + str(month)\n\n        return year, month\n        \n\ndef get_estimate_from_citus(date, device_code, country_code, metric):\n    sql = \"select sum({metric}) from store.store_market_size_fact_v1 where date='{date}' and country_code='{country_code}' and app_price_type_id=0 and purchase_type_id=10 and device_code='{device_code}';\".format(metric=metric, date=date,country_code=country_code,device_code=device_code)\n    result = query(citus_settings(\"aa\"), sql)\n    return result[0][0]\n    \n\n\n\ndef get_unified_estimate(date, device_code, country_code, metric):\n        unified_estimate = spark.read.parquet(UNIFIED_FILE_PATH.format(date)).filter(\n            \"country_code='{country_code}' and app_price_type_id={app_price_type_id} \"\n            \"and purchase_type_id={purchase_type_id} and device_code='{device_code}' \".format(country_code=country_code,\n                                                                                              app_price_type_id=\n                                                                                              APP_PRICE_TYPE_ID['ALL'],\n                                                                                              purchase_type_id=\n                                                                                              PURCHASE_TYPE_ID['ALL'],\n                                                                                              device_code=device_code,\n                                                                                              )).select(\n            \"{metric}\".format(metric=metric)).groupBy().sum().collect()[0][0]\n        return unified_estimate\n\ndef check_metric_value(date, device_id, store_id, kpi):\n        KPI_TO_METRIC = {1: 'est_market_size_download', 2: 'est_market_size_revenue'}\n        device_code = DEVICE_ID_TO_DEVICE_CODE_MAPPING[device_id]\n        # ios or gp\n        device_type = DEVICE_ID_TYPE_MAPPING[device_id]\n        country_code = COUNTRY_MAPPING_BY_MARKET[device_type][store_id]\n        citus_value = get_estimate_from_citus(date, device_code, country_code, KPI_TO_METRIC[kpi])\n        unified_metric_value = get_unified_estimate(date, device_code, country_code, KPI_TO_METRIC[kpi])\n        #  print(citus_value, unified_metric_value)\n        assert(citus_value==unified_metric_value),\"can not match {}-{}-{}-{}\".format(date, device_id, store_id, kpi)\n\n\n\n\n\n\ndef test_ios_data_accuracy():\n        KPI = [1, 2]\n        for date in get_date_list(\"2010-07-04\", \"2020-01-11\"):\n            start_time = time.time()\n            for store_id in COUNTRY_MAPPING_BY_MARKET['ios']:\n                print \"{} - {}\".format(date, store_id)\n                kpi = random.choice(KPI)\n                check_metric_value(date, 2002, store_id, kpi)\n            end_time = time.time()\n            print \"############################# one day cost :{}\".format(end_time - start_time)\ntest_ios_data_accuracy()\n"]},{"cell_type":"code","execution_count":0,"id":"20200206-093350_1473966124","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \nselect sum(estimate) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate\n    from ms.market_size_daily_estimate_1000 \n    where \n        date = '2019-12-08' and\n        purchase_type_id=10 and \n        app_type_id=0 and \n        store_id  in(8, 80, 16, 54, 47, 46, 48, 28, 36, 15, 4, 30, 95, 56, 40, 53, 23, 20, 44, 25, 1, 26, 86, 49, 13, 22, 45, 27, 21, 5, 43, 11, 39, 3, 61, 14, 17, 50, 33, 19, 51, 31, 65, 35, 52, 37, 34, 32, 12, 10, 42, 18, 78, 9, 24, 64, 84, 38, 74, 6, 29, 2, 41, 7) and \n        category_id in (1) and\n        device_id in(1000) and\n        kpi=1\n\\$proxy\\$) tbl (device_id SMALLINT, date DATE, store_id INT, kpi SMALLINT, category_id INT, app_type_id SMALLINT, purchase_type_id SMALLINT, estimate BIGINT);\nEOF\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \nselect distinct(store_id) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select device_id, date, store_id, kpi, category_id, app_type_id, purchase_type_id, estimate\n    from ms.market_size_daily_estimate_1000 \n    where \n        date = '2019-12-08' and\n        purchase_type_id=10 and \n        app_type_id=0 and \n        category_id in (1) and\n        device_id in(1000) and\n        kpi=1\n\\$proxy\\$) tbl (device_id SMALLINT, date DATE, store_id INT, kpi SMALLINT, category_id INT, app_type_id SMALLINT, purchase_type_id SMALLINT, estimate BIGINT);\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200218-023949_1431716969","metadata":{},"outputs":[],"source":["%%sh\n\n\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132  -U citus_bdp_usage_qa -d aa_citus_db -p 5432 << EOF  \nSELECT sum(est_market_size_download) AS est_market_size_download, sum(est_market_size_revenue) as est_market_size_revenue FROM store.store_market_size_fact_v1 WHERE app_price_type_id=0 AND purchase_type_id=10 AND device_code in ('android-all') AND country_code not in ('WW') AND category_id in (400000) AND date between '2019-12-08' AND '2019-12-14' ORDER BY est_market_size_download desc;\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200218-024059_1827479776","metadata":{},"outputs":[],"source":["%%sh\n\n\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132  -U citus_bdp_usage_qa -d aa_citus_db -p 5432 << EOF  \nSELECT sum(est_market_size_download) AS est_market_size_download, sum(est_market_size_revenue) as est_market_size_revenue,country_code FROM store.store_market_size_fact_v1 WHERE app_price_type_id=0 AND purchase_type_id=10 AND device_code in ('android-all') AND country_code not in ('WW') AND category_id in (400000) AND date between '2019-12-08' AND '2019-12-14' group by country_code;\nEOF\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200218-025605_797137778","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='dNzWtSV3pKTx' psql -h 10.2.10.132  -U citus_bdp_usage_qa -d aa_citus_db -p 5432 << EOF  \nSELECT distinct(country_code) FROM store.store_market_size_fact_v1 WHERE app_price_type_id=0 AND purchase_type_id=10 AND device_code in ('android-all') AND country_code not in ('WW') AND category_id in (400000) AND date between '2019-12-08' AND '2019-12-14';\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200218-035656_1777764716","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.functions import lit, udf\nfrom pyspark.sql.types import (\n    ArrayType, BooleanType, LongType, IntegerType,\n    StringType, StructType, StructField\n)\nschema = StructType([\n    StructField(\"store_id\", IntegerType(), False),\n    StructField(\"date\", StringType(), False),\n    StructField(\"platform_id\", IntegerType(), False),\n    StructField(\"device\", StringType(), False),\n    StructField(\"data_type\", StringType(), False),\n    StructField(\"price_type\", IntegerType(), False),\n    StructField(\"purchase_type\", IntegerType(), False),\n    StructField(\"category_id\", IntegerType(), False),\n    StructField(\"estimate\", LongType(), False)\n])\nww_res = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2019-12-08/android/market_size/1000\", schema=schema, sep='\\t').filter(\"price_type=0 and purchase_type=10 and category_id=1\").collect()\nstore_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 61,62, 64, 65, 73, 74, 78, 80, 86, 95]\npl_s=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 61, 64, 65, 74, 78, 80, 84, 86, 95]\npl_ww = [1000]\n\ns3_store_list = [1,10,1003,1004,1005,1006,1007,11,12,13,14,15,16,17,18,19,2,20,21,22,23,24,25,26,27,28,29,3,30,31,32,33,34,35,36,37,38,39,4,40,41,42,43,44,45,46,47,48,49,5,50,51,52,53,54,56,6,61,64,65,7,74,78,8,80,84,86,9,95\n]\nprint(ww_res)\n# Row(store_id=1000, date=u'2019-12-08', platform_id=0, device=u'google-play', data_type=u'revenue', price_type=0, purchase_type=10, category_id=1, estimate=57864205)\n# Row(store_id=1000, date=u'2019-12-08', platform_id=0, device=u'google-play', data_type=u'downloads', price_type=0, purchase_type=10, category_id=1, estimate=207141484)\ntotal_estimate = []\nfor store_id in pl_s:\n    res = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2019-12-08/android/market_size/{}\".format(store_id), schema=schema, sep='\\t').filter(\"price_type=0 and purchase_type=10 and category_id=1\").collect()\n    for _r in res:\n        if _r[4]=='downloads':\n            total_estimate.append(_r[8])\nprint sum(total_estimate)"]},{"cell_type":"code","execution_count":0,"id":"20200218-041017_1131156170","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2019-12-08/android/market_size/"]},{"cell_type":"code","execution_count":0,"id":"20200218-041703_804568357","metadata":{},"outputs":[],"source":["\nios_store = [143441, 143442, 143443, 143444, 143445, 143446, 143447, 143448, 143449, 143450, 143451, 143452, 143453, 143454, 143455, 143456, 143457, 143458, 143459, 143460, 143461, 143462, 143463, 143464, 143465, 143466, 143467, 143468, 143469, 143470, 143471, 143472, 143473, 143474, 143475, 143476, 143477, 143478, 143479, 143480, 143481, 143482, 143483, 143484, 143485, 143486, 143487, 143489, 143491, 143492, 143493, 143494, 143495, 143496, 143497, 143498, 143499, 143501, 143502, 143503, 143504, 143505, 143506, 143507, 143508, 143509, 143510, 143511, 143512, 143513, 143514, 143515, 143516, 143517, 143518, 143519, 143520, 143521, 143523, 143524, 143525, 143526, 143528, 143529, 143530, 143531, 143532, 143533, 143534, 143535, 143536, 143537, 143538, 143539, 143540, 143541, 143542, 143543, 143544, 143545, 143546, 143547, 143548, 143549, 143550, 143551, 143552, 143553, 143554, 143555, 143556, 143557, 143558, 143559, 143560, 143561, 143562, 143563, 143564, 143565, 143566, 143568, 143571, 143572, 143573, 143575, 143576, 143577, 143578, 143579, 143580, 143581, 143582, 143583, 143584, 143585, 143586, 143587, 143588, 143589, 143590, 143591, 143592, 143593, 143594, 143595, 143597, 143598, 143599, 143600, 143601, 143602, 143603, 143604, 143605]\n\ntotal_estimate = []\nfor store_id in [0]:\n    res = spark.read.csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/2019-12-08/ios/market_size/{}\".format(store_id), schema=schema, sep='\\t').filter(\"price_type=0 and purchase_type=10 and category_id=36\").collect()\n    for _r in res:\n        if _r[4]=='downloads':\n            total_estimate.append(_r[8])\nprint sum(total_estimate)\n# 183239466"]},{"cell_type":"code","execution_count":0,"id":"20200218-081338_1089572140","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}