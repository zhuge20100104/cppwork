{"cells":[{"cell_type":"code","execution_count":0,"id":"20200522-023333_1038952147","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category.v3/fact/granularity=monthly/date=2020-04-30/data_stage=final/device_code=android-all/\n \n\n\n \n\n"]},{"cell_type":"code","execution_count":0,"id":"20200420-130835_1627109010","metadata":{},"outputs":[],"source":["\nimport datetime\nstart = \"2015-08-01\"\nend = \"2015-09-01\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\n# sar_list=list()\n# for days in xrange(date_range.days):\n#     dates.append(real_date1 + datetime.timedelta(days))\n#     if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n#         temp=list()\n#         while dates:\n#             temp.append(dates.pop())\n#         sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\nmonth_day=list()\nfor days in xrange(date_range.days):\n    month_day.append(real_date1 + datetime.timedelta(days))\n\n\ntest_list= sorted(list(set([ d.strftime(\"%Y-%m-%d\")[:7] for d in month_day ])))\n\nprint test_list"]},{"cell_type":"code","execution_count":0,"id":"20200420-130855_622204288","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\n\ntest_list=[\"2019-09\"]\n\ndef check_diff(month):\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}-*/\".format(month))\n    df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=monthly/date={}-*/\".format(month))\n\n    app_id_daily = df_1.createOrReplaceTempView(\"daily\")\n    app_id_monthly= df_3.createOrReplaceTempView(\"monthly\")\n    \n    print \"compare app id: \" , month\n    # spark.sql(\"select distinct app_id from monthly except all select distinct app_id from  daily \").show(2)\n    # spark.sql(\"select distinct app_id from daily except all select distinct app_id from  monthly \").show(2)\n    spark.sql(\"select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download , sum(revenue) as revenue, device_code from daily group by country_code,device_code,app_id \").createOrReplaceTempView(\"sum_daily\")\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from sum_daily except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from monthly\").show(2)\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from monthly except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from sum_daily\").show(2)\n\n    except_1 = spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from sum_daily except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from monthly\").withColumn(\"date\", F.lit(month) ).withColumn(\"type\",F.lit(\"daily_monthly\"))\n    except_2 = spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from monthly except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from sum_daily\").withColumn(\"date\", F.lit(month)).withColumn(\"type\",F.lit(\"monthly_daily\"))\n\n    except_1.show()\n    except_2.show()\n    # df_write_result = except_2.union(except_1)\n    # if df_write_result.rdd.isEmpty():\n    #     print 'pass'\n    # else:\n    #     print 'failed!!!!!!!' , month\n    # from aadatapipelinecore.core.utils.retry import retry\n    # def write_test_result(df_write_result):\n    #     df_write_result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_monthly_count/\",\n    #                                       mode=\"append\",\n    #                                       partitionBy=[\"date\"])\n    # retry(write_test_result,(df_write_result,),{},interval=10)\n\nsc.parallelize(map(check_diff, test_list), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200420-131512_1903292483","metadata":{},"outputs":[],"source":["%python\nimport datetime\nstart = \"2011-03-01\"\nend = \"2020-04-09\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\n# sar_list=list()\n# for days in xrange(date_range.days):\n#     dates.append(real_date1 + datetime.timedelta(days))\n#     if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n#         temp=list()\n#         while dates:\n#             temp.append(dates.pop())\n#         sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\nmonth_day=list()\nfor days in xrange(date_range.days):\n    month_day.append(real_date1 + datetime.timedelta(days))\n\n\ntest_list= sorted(list(set([ d.strftime(\"%Y-%m-%d\")[:7] for d in month_day ])))\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200420-152844_448640891","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import functions as F\nimport datetime\nstart_week = \"2019-09-08\"\nend_week = \"2019-09-15\"\n\n# start_week = \"2019-10-20\"\n# end_week = \"2019-10-27\"\n# end = \"2012-05-01\"\nreal_date1 = datetime.date(*[int(x) for x in start_week.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end_week.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nsar_list=list()\nfor days in xrange(date_range.days):\n    dates.append(real_date1 + datetime.timedelta(days))\n    if (real_date1 + datetime.timedelta(days)).weekday() == 5:\n        temp=list()\n        while dates:\n            temp.append(dates.pop())\n        sar_list.append({real_date1 + datetime.timedelta(days):temp})\n\ntest_path=list()\nfor x in sar_list:\n    for key,item in x.items():\n        test_path.append((key.strftime(\"%Y-%m-%d\"),[d.strftime(\"%Y-%m-%d\") for d in item]))\n\n\n\ndef check_diff(test_data):\n    df_1 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={%s}/\"%(\",\".join(test_data[1])))\n    df_3 = spark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=weekly/date={}/\".format(test_data[0]))\n\n    app_id_daily = df_1.createOrReplaceTempView(\"daily\")\n    app_id_monthly= df_3.createOrReplaceTempView(\"weekly\")\n    \n    print \"compare app id: \" , test_data[0]\n    # spark.sql(\"select distinct app_id from monthly except all select distinct app_id from  daily \").show(2)\n    # spark.sql(\"select distinct app_id from daily except all select distinct app_id from  monthly \").show(2)\n    spark.sql(\"select app_id, country_code, sum(free_app_download) as free_app_download , sum(paid_app_download) as paid_app_download , sum(revenue) as revenue, device_code from daily group by country_code,device_code,app_id \").createOrReplaceTempView(\"sum_daily\")\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from sum_daily except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from monthly\").show(2)\n    # spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from monthly except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from sum_daily\").show(2)\n\n    except_1 = spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from sum_daily except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from weekly\").withColumn(\"date\", F.lit(test_data[0]) ).withColumn(\"type\",F.lit(\"daily_weekly\"))\n    except_2 = spark.sql(\"select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from weekly except all select app_id,country_code,free_app_download,paid_app_download,revenue,device_code from sum_daily\").withColumn(\"date\", F.lit(test_data[0])).withColumn(\"type\",F.lit(\"weekly_daily\"))\n\n    df_write_result = except_2.union(except_1)\n    if df_write_result.rdd.isEmpty():\n        print 'pass'\n    else:\n        print 'failed!!!!!!!' , test_data[0]\n    # from aadatapipelinecore.core.utils.retry import retry\n    # def write_test_result(df_write_result):\n    #     df_write_result.write.format(\"delta\").save(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_weekly_count/\",\n    #                                       mode=\"append\",\n    #                                       partitionBy=[\"date\"])\n    # retry(write_test_result,(df_write_result,),{},interval=10)\n\nsc.parallelize(map(check_diff, test_path), 1)"]},{"cell_type":"code","execution_count":0,"id":"20200426-115614_2011202203","metadata":{},"outputs":[],"source":["\nspark.sql(\"select * from sum_daily where app_id=373998688 and country_code='US'\").show()\nspark.sql(\"select * from weekly where app_id= and country_code='US'\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200420-152920_1028123710","metadata":{},"outputs":[],"source":["\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_weekly_count\").select(\"date\").orderBy(\"date\").distinct().show(20)"]},{"cell_type":"code","execution_count":0,"id":"20200421-001226_525170214","metadata":{},"outputs":[],"source":["\nprint spark.read.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_store_unified_monthly_count\").select(\"date\").orderBy(\"date\").distinct().show(20)"]},{"cell_type":"code","execution_count":0,"id":"20200422-091859_333022444","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-category-rank.v1/fact/granularity=monthly/date=2019-10-31/"]},{"cell_type":"code","execution_count":0,"id":"20200422-094404_908360569","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/qa/temp/monthly/date=2019-09-30/device_code=android-all/\n \naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/_obselete/qa/temp/weekly/date=2019-09-28/\n \n"]},{"cell_type":"code","execution_count":0,"id":"20200422-100833_858562881","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}