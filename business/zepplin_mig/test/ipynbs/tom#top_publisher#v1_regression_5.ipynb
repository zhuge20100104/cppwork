{"cells":[{"cell_type":"code","execution_count":0,"id":"20200622-095518_516320632","metadata":{},"outputs":[],"source":["\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport pandas as pd\npd.set_option('expand_frame_repr', False)\nimport datetime\nfrom applications.db_check_v1.common.base_test import PipelineTest\nfrom applications.db_check_v1.common.db_check_utils import query_df, etl_skip\nfrom applications.db_check_v1.common.constants import COUNTRY_CODE_MAPPING_BY_MARKET_CODE as COUNTRY_CODE_MAPPING, \\\n    CATEGORY_ID_MAPPING_BY_MARLKET_AND_DEVICE_CODE as CATEGORY_ID_MAPPING\nfrom applications.db_check_v1.common.utils import get_week_start_end_date, get_date_list\n#from applications.db_check_v1.cases.store.publisher_est_v1.constants import MARKET_SIZE_DSN\n\ndef get_date_list(begin_date, end_date, freq):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=begin_date, end=end_date, freq=freq))]\n    return date_list\n\nimport boto3\ns3 = boto3.resource('s3')\ns3object = s3.Object('b2c-prod-data-pipeline-qa', 'tom/top_publisher/regression5.txt')\n\ndef write_log(strobj, s3obj):\n    s3obj.put(Body=str(strobj))\n\n\nclass PublisherEstRawData(object):\n    raw_s3_path = \"s3://b2c-prod-dca-store-estimates/store_estv2/PUB_ESTIMATES_FINAL/version=2.0.0/range_type=DAY\" \\\n                  \"/date={}/\"\n    device_code_mapping = {\n        \"00\": \"android-all\",\n        \"01\": \"android-all\",\n        \"02\": \"android-all\",\n        \"10\": \"ios-phone\",\n        \"11\": \"ios-phone\",\n        \"12\": \"ios-phone\",\n        \"1100\": \"ios-tablet\",\n        \"1101\": \"ios-tablet\",\n        \"1102\": \"ios-tablet\",\n        \"11000\": \"ios-all\",\n        \"11001\": \"ios-all\",\n        \"11002\": \"ios-all\",\n    }\n\n    metric_mapping = {\n        0: \"free_app_download\",\n        1: \"paid_app_download\",\n        2: \"revenue\",\n        101: \"free_app_download\",\n        100: \"paid_app_download\",\n        102: \"revenue\",\n        1000: \"free_app_download\",\n        1001: \"paid_app_download\",\n        1002: \"revenue\"\n    }\n\n    dimension_mapping = {\n        \"id\": \"publisher_id\",\n    }\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code):\n        df = self._get_raw_data_by_date_country(date, country_code)\n        df = self._parse_mapping(df)\n        df = self._parse_unified_format(df)\n        df = self._data_clean_up(df)\n        return df\n\n    def _data_clean_up(self, df):\n        # clean unknown mapping\n        category_id_list = list(set(CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].values() +\n                                    CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].values()))\n\n        country_code_list = list(set(COUNTRY_CODE_MAPPING[\"apple-store\"].values() +\n                                     COUNTRY_CODE_MAPPING[\"google-play\"].values()))\n\n        df = df[(df['category_id'].isin(category_id_list)) & (df['country_code'].isin(country_code_list))]\n        return df\n\n    def _parse_mapping(self, df):\n        # country_code mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"google-play\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"store_id\": COUNTRY_CODE_MAPPING[\"apple-store\"]})\n        df = df.rename(columns={'store_id': 'country_code'})\n\n        # category_id mapping\n        df.loc[df[\"platform_id\"] == 0] = df.loc[df[\"platform_id\"] == 0].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"]})\n        df.loc[df[\"platform_id\"] == 1] = df.loc[df[\"platform_id\"] == 1].\\\n            replace({\"category_id\": CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"]})\n\n        # device_code mapping\n        df[\"device_code\"] = df[\"platform_id\"].astype(str) + df[\"feed\"].astype(str)\n        df = df.replace({\"device_code\": self.device_code_mapping})\n\n        # granularity\n        df[\"granularity\"] = \"daily\"\n\n        # metrics mapping (from feed)\n        df = df.replace({\"feed\": self.metric_mapping})\n        return df\n\n    def _parse_unified_format(self, df):\n        df = df.rename(columns=self.dimension_mapping)\n        df = df.pivot_table(index=[\"publisher_id\", \"category_id\", \"device_code\", \"country_code\", \"granularity\"],\n                            columns='feed', values='est')\n        df.reset_index(inplace=True)\n        df.columns.name = None\n        return df\n\n    def _get_raw_data_by_date_country(self, date, country_code):\n        \"\"\"\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        |        id|store_id|category_id|platform_id|vertical|rank|feed|  est|platform|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        | 284417353|       0|       6006|          1|       1|   1|1002|45235|     ios|\n        | 349554266|       0|       6006|          1|       1|   2|1002|20732|     ios|\n        |1316153435|       0|       6006|          1|       1|   3|1002|15136|     ios|\n        +----------+--------+-----------+-----------+--------+----+----+-----+--------+\n        \"\"\"\n        ios_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"apple-store\"].items() if v == country_code]\n        gp_store_ids = [str(k) for k, v in COUNTRY_CODE_MAPPING[\"google-play\"].items() if v == country_code]\n        raw_df = self.spark.read.parquet(self.raw_s3_path.format(date)).\\\n            filter('store_id in ({})'.format(\",\".join(ios_store_ids + gp_store_ids))).toPandas()\n        return raw_df\n\n    def get_metrics_count(self, date):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n        # ios_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].keys()]\n        # gp_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].keys()]\n\n        fillter_sql = \"platform_id = {} and store_id in ({})\"\n        df = self.spark.read.parquet(self.raw_s3_path.format(date))\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id))).count() + \\\n                    df.filter(fillter_sql.format(0, \",\".join(gp_store_id))).count()\n        print fillter_sql.format(1, \",\".join(ios_store_id))\n        print fillter_sql.format(0, \",\".join(gp_store_id))\n        return count_all\n\n    def get_v1_raw_metrics_count(self, date):\n        ios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\n        gp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n\n        df = self.spark.read.option(\"delimiter\", \"\\t\").csv(\n            \"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/*/sbe_est_publisher/*/*\".format(date))\n        fillter_sql = \"_c2 = {} and _c0 in ({})\"  # _c2 > platform_id, _c0 > store_id\n\n        count_all = df.filter(fillter_sql.format(1, \",\".join(ios_store_id))).count() + \\\n                    df.filter(fillter_sql.format(0, \",\".join(gp_store_id))).count()\n                    \n        return count_all\n\n\nclass PublisherEstUnifiedData(object):\n    unified_s3_path = \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-publisher.v1/fact/\" \\\n                      \"granularity=daily/date={}/\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def get(self, date, country_code):\n        unified_df = self.spark.read.format(\"delta\").\\\n            load(self.unified_s3_path.format(date)).filter(\"country_code = '{}'\".format(country_code)).toPandas()\n        unified_df = unified_df.drop([\"_identifier\", \"revenue_iap\", \"revenue_non_iap\", \"date\"], axis=1)\n        return unified_df\n\n    def get_metrics_count(self, date):\n        df = self.spark.read.format(\"delta\").load(self.unified_s3_path.format(date))\n        metrics_count = 0\n        for metric in [\"free_app_download\", \"paid_app_download\", \"revenue\"]:\n            metrics_count += df.filter(\"{} is not null\".format(metric)).count()\n        return metrics_count\n\n\n\nclass PublisherEstDBData(object):\n    def get(self, date):\n        sql = \"SELECT * FROM store.store_publisher_est_fact_v1 WHERE date='{}'\".format(date)\n        return query_df(MARKET_SIZE_DSN, sql)\n\n    def get_led(self):\n        sql = \"SELECT * FROM store.store_publisher_est_latest_date_v1\"\n        return query_df(MARKET_SIZE_DSN, sql)\n\n\nclass TestPublisherEstWeekly(PipelineTest):\n    # Every Tuesday 16:00 UTC time will refresh the data of last Full Week.\n    trigger_date_config = ('* 16 * * 2', 3)\n\n    def _compare_df(self, df1, df2, log=''):\n        for diff_type in [\"left\", \"right\"]:\n            diff_df = df1.merge(df2, indicator=True, how=diff_type)  # .loc[lambda x : x['_merge']!='both']\n            diff_df = diff_df.loc[diff_df[\"_merge\"] != \"both\"]\n            if len(diff_df) != 0:\n                print diff_type\n                print \"dataframe overview of df1 and df2\"\n                print df1\n                print df2\n                print \"dimension overview of diff df\"\n                print diff_df.country_code.unique()\n                print diff_df.category_id.unique()\n                print diff_df.device_code.unique()\n            self.assertEqual(len(diff_df), 0,\n                             msg=\"found mismatch when compare the raw, unified, db.\"\n                                 \" diff count is \\n {}, logs:{}\".format(len(diff_df), log))\n\n    @etl_skip()\n    def test_publisher_est_etl_accuracy(self):\n        # Every Tuesday 16:00 UTC time will refresh the data of last Full Week.\n        country_code = 'US'\n        start_date, end_date = get_week_start_end_date(self.check_date_str)\n        date_list = get_date_list(start_date, end_date)\n        for date in date_list:\n            raw_df = PublisherEstRawData(self.spark).get(date, country_code)\n            unified_df = PublisherEstUnifiedData(self.spark).get(date, country_code)\n            # db_df = PublisherEstDBData().get(date)\n\n            self._compare_df(raw_df, unified_df, log=\"raw / unified - {}\".format(date))\n            # self._compare_df(unified_df, db_df, log=\"unified / db - {}\".format(date))\n\n    @etl_skip()\n    def test_publisher_Est_etl_completeness(self):\n        start_date, end_date = get_week_start_end_date(self.check_date_str)\n        date_list = get_date_list(start_date, end_date)\n        for date in date_list:\n            raw_count = PublisherEstRawData(self.spark).get(date)\n            unified_count = PublisherEstUnifiedData(self.spark).get(date)\n            self.assertEqual(raw_count, unified_count)\n\n    def test_publisher_est_etl_timelines(self):\n        # Every Tuesday 16:00 UTC time will refresh the data of last Full Week.\n        # E.g. 2020-02-11 17:00 the data of 2020-02-02 ~ 2020-02-08 will be ready\n        trigger_datetime = datetime.datetime.strptime(\"2020-02-11 17:00:00\", '%Y-%m-%d %H:%M:%S')\n        check_date_str_actual = self._get_check_date_from_routing_config(trigger_datetime).strftime(\"%Y-%m-%d\")\n        self.assertEqual(\"2020-02-08\", check_date_str_actual)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200622-095610_234589399","metadata":{},"outputs":[],"source":["\n\nbegin_date = datetime.datetime(2018, 07, 04)\nend_date = datetime.datetime(2019, 07, 13)\n\ndate_list = get_date_list(begin_date, end_date, \"D\")\n\ndate_list = [\"2013-03-24\"]\nlog = \"\"\n\nfor date in date_list:\n    # count1 = PublisherEstRawData(spark).get_metrics_count(date)\n    # count1 = PublisherEstRawData(spark).get_v1_raw_metrics_count(date)\n    \n    count2 = PublisherEstUnifiedData(spark).get_metrics_count(date)\n    log = log + \"{}, {}, {}, {} \\n\".format(date, count1, count2, count1-count2)\n    print log\n    write_log(log, s3object)\n"]},{"cell_type":"code","execution_count":0,"id":"20200628-051523_1792749244","metadata":{},"outputs":[],"source":["\n\nios_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"apple-store\"].keys()]\ngp_store_id = [str(s_id) for s_id in COUNTRY_CODE_MAPPING[\"google-play\"].keys()]\n# ios_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"apple-store\"][\"ios-all\"].keys()]\n# gp_category_id = [str(c_id) for c_id in CATEGORY_ID_MAPPING[\"google-play\"][\"android-all\"].keys()]\n\nfillter_sql = \"platform_id = {} and store_id in ({})\"\n\nprint fillter_sql.format(1, \",\".join(ios_store_id))\nprint fillter_sql.format(0, \",\".join(gp_store_id))\n"]},{"cell_type":"code","execution_count":0,"id":"20200622-100133_296964280","metadata":{},"outputs":[],"source":["%%sh\n\naws s3 cp s3://b2c-prod-data-pipeline-qa/tom/top_publisher/regression5.txt /tmp/regression5.txt\ncat /tmp/regression5.txt\n"]},{"cell_type":"code","execution_count":0,"id":"20200622-100340_1401317392","metadata":{},"outputs":[],"source":["%%sh\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}