{"cells":[{"cell_type":"code","execution_count":0,"id":"20200617-084843_1300328053","metadata":{},"outputs":[],"source":["%%sh\naws s3 ls s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est-load.v3/fact/granularity=daily/\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200612-091955_1074172572","metadata":{},"outputs":[],"source":["%%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select count(1) as cnt \n    from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_1\n    where date = '2020-04-11'  and isunique='t' ) as prod\n\\$proxy\\$) tbl (cnt bigint) ;\n\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select count(1) as cnt \n    from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_0\n    where date = '2020-04-11'  and isunique='t' ) as prod\n\\$proxy\\$) tbl (cnt bigint) \n\nEOF\n\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# select feed_id, sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n#     select feed_id, sum(estimate) as cnt \n#     from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_1\n#     where date = '2019-12-05'  and  feed_id in (0,1,2,100,101,102) and isunique='t' ) as prod group by feed_id\n# \\$proxy\\$) tbl (feed_id smallint, cnt bigint) group by feed_id\n\n\n# EOF\n\n\n\n# PGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\n# select  app_id,feed_id,store_id,estimate,category_id ,device_id, rank, date from plproxy.execute_select_nestloop(\\$proxy\\$ \n# select  distinct app_id,feed_id,store_id,estimate,category_id ,device_id, rank, date\n#     from aa.app_store_daily_estimate_1\n#     where \n#       date between '2020-05-10' and '2020-05-10' and feed_id=0  limit 500 \\$proxy\\$) tbl \n#       (app_id BIGINT,feed_id SMALLINT,store_id INT, estimate INT, category_id INT, device_id SMALLINT, rank INT, date Date ) order by category_id  asc, date desc limit 500 ;\n\n# EOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200613-065357_875169159","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\nselect * from store.store_est_fact_v6 where date='2020-05-10' and app_id=331975235 and country_code='PK' \nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200615-053944_1419618329","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\nselect * from store.store_est_t_w_fact_v6 where est_free_app_download=0 limit 5\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200615-131917_907449477","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h 10.2.6.141  -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\nselect * from store.store_est_t_w_fact_v6 where est_paid_download != 0 limit 5\nEOF"]},{"cell_type":"code","execution_count":0,"id":"20200613-065917_517509011","metadata":{},"outputs":[],"source":["\n\nspark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-02-01/platform=android\").createOrReplaceTempView(\"t\")\nspark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-02-01/device_code=android-all/\").createOrReplaceTempView(\"q\")\nspark.sql(\"select * from t where feed=0 and id=20600009210972 and store_id=40\").show()\n# spark.sql(\"select sum(free_app_download) from q where free_app_download is not null\").show()"]},{"cell_type":"code","execution_count":0,"id":"20200612-115158_1571019805","metadata":{},"outputs":[],"source":["%%sh\nPGPASSWORD='wZw8cfBuuklIskVG' psql -h   -U citus_bdp_prod_app_int_qa -d aa_store_db -p 5432 << EOF \nset search_path=store;\nselect device_code, sum(est_free_app_download), sum(est_paid_app_download), sum(est_revenue),  sum(est_organic_download),  sum(est_paid_download) from store.store_est_fact_v6 where date='2020-01-01' group by device_code\n"]},{"cell_type":"code","execution_count":0,"id":"20200617-083758_835035355","metadata":{},"outputs":[],"source":["\nios_feed={1:\"0,1,2,100,101,102\"}\nandroid_feed={0:\"0,1,2\"}\n\nprint type(ios_feed.keys()[0])\n\nif ios_feed.keys() == 1:\n    store_id = 1\nelse:\n    store_id = 1001\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200612-091514_1289097252","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\n\n\nstart = \"2014-11-12\"\nend = \"2014-11-13\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\ndates.sort(reverse=True)\n\n\nios_feed={1:\"0,1,2,100,101,102\"}\nandroid_feed={0:\"0,1,2\"}\n\ndef plproxy_row(date, device_feed_dict):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n    \n    \n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n\n    if device_feed_dict.keys()[0] == 1:\n        store_id = 1\n    else:\n        store_id = 1001\n\n    sql = '''select feed_id, Cast(sum(cnt) as int) as metric_sum from plproxy.execute_select_nestloop($$\n    select feed_id, sum(estimate)  as cnt\n    from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_{}\n        where date = '{}'  and  feed_id in ({}) and isunique='t' and store_id not in ({}) ) as prod group by feed_id \n        $$) tbl (feed_id smallint, cnt bigint) group by feed_id '''.format(\n     device_feed_dict.keys()[0], date, ','.join(device_feed_dict.values()), store_id\n    )\n\n    print sql\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [ Row(feed_id=r[0], metric_sum=r[1]) for r in rows ]\n\n\n\ndef citus_row(date):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select device_code, cast(sum(est_free_app_download) as int) as est_free_app_download, cast(sum(est_paid_app_download) as int) as est_paid_app_download, cast(sum(est_revenue) as int) as est_revenue from store.store_est_fact_v6 where date='{}' group by device_code\".format(date )\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    \n    result = get_data_in_citus(date)\n    return [ Row(device_code=r[0], sum_est_free_app_download=r[1], sum_est_paid_app_download=r[2], sum_est_revenue=r[3]) for r in result ]\n\n\nfor d in dates:\n    print d\n    r1 = plproxy_row(d,ios_feed)\n    r2 = plproxy_row(d,android_feed)\n    citus_reseult = citus_row(d)\n\n\n    d1 = generate_plploxy_result(r1,r2)\n    d2 = generate_citus_result(citus_reseult)\n\n    # d1.show()\n    # d2.show()\n\n    \n    d1.createOrReplaceTempView(\"plploxy_r\")\n    d2.createOrReplaceTempView(\"citus_r\")\n    spark.sql(\"select * from plploxy_r except all select * from citus_r\").show()\n    spark.sql(\"select * from citus_r except all select * from plploxy_r\").show()\n\n\n# 2020-02-23\n# +-----------+-------------------------+-------------------------+---------------+\n# |device_code|sum_est_free_app_download|sum_est_paid_app_download|sum_est_revenue|\n# +-----------+-------------------------+-------------------------+---------------+\n# |  ios-phone|                175958534|                  1957879|      192484890|\n# +-----------+-------------------------+-------------------------+---------------+\n\n# +-----------+-------------------------+-------------------------+---------------+\n# |device_code|sum_est_free_app_download|sum_est_paid_app_download|sum_est_revenue|\n# +-----------+-------------------------+-------------------------+---------------+\n# |  ios-phone|                175936729|                  1957879|      192484890|\n# +-----------+-------------------------+-------------------------+---------------+\n"]},{"cell_type":"code","execution_count":0,"id":"20200613-081657_1920823397","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\n\n\nstart = \"2020-02-01\"\nend = \"2020-03-01\"\nreal_date1 = datetime.date(*[int(x) for x in start.split('-')])\nreal_date2 = datetime.date(*[int(x) for x in end.split('-')])\ndate_range = real_date2 - real_date1\ndates = list()\nfor days in xrange(date_range.days):\n    dates.append((str(real_date1 + datetime.timedelta(days))))\ndates.sort(reverse=True)\n\n\nios_feed={1:\"0,1,2,100,101,102\"}\nandroid_feed={0:\"0,1,2\"}\n\ndef plproxy_row(date, device_feed_dict):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n    \n    \n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n    sql = '''select feed_id, Cast(sum(cnt) as int) as metric_sum from plproxy.execute_select_nestloop($$\n    select feed_id, sum(estimate)  as cnt\n    from (select distinct app_id, feed_id, store_id, estimate,category_id, device_id from aa.app_store_daily_estimate_{}\n        where date = '{}'  and  feed_id in ({}) ) as prod group by feed_id \n        $$) tbl (feed_id smallint, cnt bigint) group by feed_id '''.format(\n     device_feed_dict.keys()[0], date, ','.join(device_feed_dict.values())\n    )\n\n\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [ Row(feed_id=r[0], metric_sum=r[1]) for r in rows ]\n\n\n\ndef citus_row(date):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select device_code, cast(sum(est_free_app_download) as int) as est_free_app_download, cast(sum(est_paid_app_download) as int) as est_paid_app_download, cast(sum(est_revenue) as int) as est_revenue from store.store_est_category_fact_v6 where date='{}' group by device_code\".format(date )\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    \n    result = get_data_in_citus(date)\n    return [ Row(device_code=r[0], sum_est_free_app_download=r[1], sum_est_paid_app_download=r[2], sum_est_revenue=r[3]) for r in result ]\n\n\nfor d in dates:\n    print d\n    r1 = plproxy_row(d,ios_feed)\n    r2 = plproxy_row(d,android_feed)\n    citus_reseult = citus_row(d)\n\n\n    d1 = generate_plploxy_result(r1,r2)\n    d2 = generate_citus_result(citus_reseult)\n\n    # d1.show()\n    # d2.show()\n\n    \n    d1.createOrReplaceTempView(\"plploxy_r\")\n    d2.createOrReplaceTempView(\"citus_r\")\n    spark.sql(\"select * from plploxy_r except all select * from citus_r\").show()\n    spark.sql(\"select * from citus_r except all select * from plploxy_r\").show()\n\n\n# 2020-02-23\n# +-----------+-------------------------+-------------------------+---------------+\n# |device_code|sum_est_free_app_download|sum_est_paid_app_download|sum_est_revenue|\n# +-----------+-------------------------+-------------------------+---------------+\n# |  ios-phone|                175958534|                  1957879|      192484890|\n# +-----------+-------------------------+-------------------------+---------------+\n\n# +-----------+-------------------------+-------------------------+---------------+\n# |device_code|sum_est_free_app_download|sum_est_paid_app_download|sum_est_revenue|\n# +-----------+-------------------------+-------------------------+---------------+\n# |  ios-phone|                175936729|                  1957879|      192484890|\n# +-----------+-------------------------+-------------------------+---------------+\n"]},{"cell_type":"code","execution_count":0,"id":"20200612-121804_1184307896","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import *\ndef generate_plploxy_result(r1, r2):\n    schema = StructType([\n    StructField(\"feed_id\", StringType(), True),\n    StructField(\"metric_sum\", IntegerType(), True)])\n    df1 = spark.createDataFrame(r1,schema)\n    df1.createOrReplaceTempView(\"plploxy_ios\")\n    \n    df2 = spark.createDataFrame(r2,schema)\n    df2.createOrReplaceTempView(\"plploxy_android\")\n    \n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='100' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='102' THEN \"sum_est_revenue\"\n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_ios\n    ''').createOrReplaceTempView(\"plploxy_metric\")\n    \n    \n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"sum_est_free_app_download\"\n    WHEN feed_id='1' THEN \"sum_est_paid_app_download\"\n    WHEN feed_id='2' THEN \"sum_est_revenue\"\n    END AS metric from plploxy_android\n    ''').createOrReplaceTempView(\"plploxy_metric_android\")\n    \n    \n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='101' THEN \"ios-tablet\"\n    WHEN feed_id='100' THEN \"ios-tablet\"\n    WHEN feed_id='102' THEN \"ios-tablet\"\n    WHEN feed_id='0' THEN \"ios-phone\"\n    WHEN feed_id='1' THEN \"ios-phone\"\n    WHEN feed_id='2' THEN \"ios-phone\"\n    END AS device_code from plploxy_metric\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code\")\n    \n    \n    spark.sql('''\n    SELECT *,\n    CASE \n    WHEN feed_id='0' THEN \"android-all\"\n    WHEN feed_id='1' THEN \"android-all\"\n    WHEN feed_id='2' THEN \"android-all\"\n    END AS device_code from plploxy_metric_android\n    ''').createOrReplaceTempView(\"plploxy_metric_device_code_android\")\n    \n    \n    spark.sql('''\n    SELECT * FROM plploxy_metric_device_code_android\n    UNION ALL\n    SELECT * FROM plploxy_metric_device_code\n    ''').createOrReplaceTempView(\"all_device\")\n    \n    spark.sql('''\n    SELECT \n        device_code, sum_est_free_app_download , sum_est_paid_app_download , sum_est_revenue\n    FROM\n          all_device\n     PIVOT (\n        max(metric_sum) \n        FOR metric IN ('sum_est_free_app_download','sum_est_paid_app_download', 'sum_est_revenue')\n      )\n    ''').createOrReplaceTempView(\"after_pivot\")\n    \n    spark.sql('''\n    SELECT device_code, sum_est_free_app_download FROM after_pivot\n    WHERE sum_est_free_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_1\")\n\n    spark.sql('''\n    SELECT device_code, sum_est_paid_app_download FROM after_pivot\n    WHERE sum_est_paid_app_download is not null\n    ''').createOrReplaceTempView(\"after_pivot_2\")\n\n\n    spark.sql('''\n    SELECT device_code, sum_est_revenue FROM after_pivot\n    WHERE sum_est_revenue is not null\n    ''').createOrReplaceTempView(\"after_pivot_3\")\n\n    return spark.sql('''\n    SELECT c.device_code, c.sum_est_free_app_download, c.sum_est_paid_app_download, d.sum_est_revenue\n    FROM (\n    SELECT a.device_code, a.sum_est_free_app_download, b.sum_est_paid_app_download FROM after_pivot_1 a\n    JOIN after_pivot_2 b\n    ON a.device_code=b.device_code\n    ) AS c\n    JOIN after_pivot_3 d\n    on c.device_code=d.device_code\n    order by device_code desc\n    ''')\n\n    \n"]},{"cell_type":"code","execution_count":0,"id":"20200612-121217_706952635","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\n\ndef citus_row(date):\n    def get_data_in_citus(date):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.6.141\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select device_code, cast(sum(est_free_app_download) as int) as est_free_app_download, cast(sum(est_paid_app_download) as int) as est_paid_app_download, cast(sum(est_revenue) as int) as est_revenue from store.store_est_fact_v6 where date='{}' group by device_code\".format(date )\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    \n    result = get_data_in_citus(date)\n    return [ Row(device_code=r[0], sum_est_free_app_download=r[1], sum_est_paid_app_download=r[2], sum_est_revenue=r[3]) for r in result ]\n\n\ncitus_reseult = citus_row(\"2020-01-01\")\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200612-131903_816423849","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql.types import *\ndef generate_citus_result(citus_data):\n    schema = StructType([\n    StructField(\"device_code\", StringType(), True),\n    StructField(\"sum_est_free_app_download\", IntegerType(), True),\n    StructField(\"sum_est_paid_app_download\", IntegerType(), True),\n    StructField(\"sum_est_revenue\", IntegerType(), True)])\n\n    df_3 = spark.createDataFrame(citus_reseult, schema)\n    df_3.createOrReplaceTempView(\"citus_data\")\n    return spark.sql(\"select * from citus_data order by device_code desc\")"]},{"cell_type":"code","execution_count":0,"id":"20200612-120208_2041545669","metadata":{},"outputs":[],"source":["\nios_feed={1:\"0,1,2,100,101,102\"}\nprint ios_feed.keys()[0] , ','.join(ios_feed.values())"]},{"cell_type":"code","execution_count":0,"id":"20200410-115054_1957002163","metadata":{},"outputs":[],"source":["   \nANDROID_CATEGORIES = [\n    (1, 400000), (2, 400001), (3, 400022), (4, 400023), (5, 400024),\n    (6, 400008), (7, 400011), (8, 400014), (9, 400017), (10, 400020),\n    (11, 400025), (12, 400030), (13, 400031), (14, 400032), (15, 400033),\n    (16, 400035), (17, 400036), (18, 400038), (19, 400040), (20, 400042),\n    (21, 400043), (22, 400044), (23, 400058), (24, 400046), (25, 400047),\n    (26, 400048), (27, 400050), (28, 400051), (29, 400052), (30, 400053),\n    (31, 400054), (32, 400055), (33, 400056), (34, 400045), (35, 400057),\n    (36, 400059), (37, 400060), (38, 400002), (39, 400003), (40, 400021),\n    (41, 400004), (42, 400005), (43, 400006), (44, 400007), (46, 400009),\n    (47, 400010), (48, 400012), (49, 400013), (51, 400015), (52, 400016),\n    (54, 400018), (55, 400019), (56, 400061), (57, 400063), (58, 400064),\n    (59, 400065), (60, 400062), (61, 400066), (62, 400067), (63, 400068),\n    (64, 400069), (65, 400070), (66, 400026), (67, 400027), (68, 400041),\n    (69, 400028), (70, 400029), (71, 400034), (72, 400037), (73, 400039),\n    (75, 400049)\n]\n\n\nIOS_CATEGORIES = [\n    (36, 100000), (100, 100021), (360, 100030), (361, 100031), (362, 100032),\n    (363, 100033), (6000, 100023), (6001, 100077), (6002, 100076), (6003, 100075),\n    (6004, 100073), (6005, 100072), (6006, 100070), (6007, 100069), (6008, 100068),\n    (6009, 100067), (6010, 100066), (6011, 100065), (6012, 100034), (6013, 100029),\n    (6014, 100001), (6015, 100027), (6016, 100026), (6017, 100025), (6018, 100022),\n    (6020, 100064), (6021, 100035), (6022, 100024), (6023, 100028), (6024, 100071),\n    (6025, 100074), (7001, 100002), (7002, 100003), (7003, 100004), (7004, 100005),\n    (7005, 100006), (7006, 100007), (7007, 100008), (7008, 100009), (7009, 100010),\n    (7010, 100011), (7011, 100012), (7012, 100013), (7013, 100014), (7014, 100015),\n    (7015, 100016), (7016, 100017), (7017, 100018), (7018, 100019), (7019, 100020),\n    (13001, 100053), (13002, 100046), (13003, 100049), (13004, 100054), (13005, 100060),\n    (13006, 100037), (13007, 100036), (13008, 100038), (13009, 100039), (13010, 100040),\n    (13011, 100041), (13012, 100042), (13013, 100043), (13014, 100044), (13015, 100045),\n    (13017, 100047), (13018, 100048), (13019, 100050), (13020, 100051), (13021, 100052),\n    (13023, 100055), (13024, 100056), (13025, 100057), (13026, 100058), (13027, 100059),\n    (13028, 100061), (13029, 100062), (13030, 100063)\n]\n\n\nIOS_STORE_COUNTRY_MAPPING = [\n    (0, 'WW'), (143575, 'AL'), (143563, 'DZ'), (143564, 'AO'), (143538, 'AI'),\n    (143540, 'AG'), (143505, 'AR'), (143524, 'AM'), (143460, 'AU'), (143445, 'AT'),\n    (143568, 'AZ'), (143539, 'BS'), (143559, 'BH'), (143541, 'BB'), (143565, 'BY'),\n    (143446, 'BE'), (143555, 'BZ'), (143576, 'BJ'), (143542, 'BM'), (143577, 'BT'),\n    (143556, 'BO'), (143525, 'BW'), (143503, 'BR'), (143543, 'VG'), (143560, 'BN'),\n    (143526, 'BG'), (143578, 'BF'), (143579, 'KH'), (143455, 'CA'), (143580, 'CV'),\n    (143544, 'KY'), (143581, 'TD'), (143483, 'CL'), (143465, 'CN'), (143501, 'CO'),\n    (143582, 'CG'), (143495, 'CR'), (143494, 'HR'), (143557, 'CY'), (143489, 'CZ'),\n    (143458, 'DK'), (143545, 'DM'), (143508, 'DO'), (143509, 'EC'), (143516, 'EG'),\n    (143506, 'SV'), (143518, 'EE'), (143583, 'FJ'), (143447, 'FI'), (143442, 'FR'),\n    (143584, 'GM'), (143443, 'DE'), (143573, 'GH'), (143448, 'GR'), (143546, 'GD'),\n    (143504, 'GT'), (143585, 'GW'), (143553, 'GY'), (143510, 'HN'), (143463, 'HK'),\n    (143482, 'HU'), (143558, 'IS'), (143467, 'IN'), (143476, 'ID'), (143449, 'IE'),\n    (143491, 'IL'), (143450, 'IT'), (143511, 'JM'), (143462, 'JP'), (143528, 'JO'),\n    (143517, 'KZ'), (143529, 'KE'), (143493, 'KW'), (143586, 'KG'), (143587, 'LA'),\n    (143519, 'LV'), (143497, 'LB'), (143588, 'LR'), (143520, 'LT'), (143451, 'LU'),\n    (143515, 'MO'), (143530, 'MK'), (143531, 'MG'), (143589, 'MW'), (143473, 'MY'),\n    (143532, 'ML'), (143521, 'MT'), (143590, 'MR'), (143533, 'MU'), (143468, 'MX'),\n    (143591, 'FM'), (143523, 'MD'), (143592, 'MN'), (143547, 'MS'), (143593, 'MZ'),\n    (143594, 'NA'), (143484, 'NP'), (143452, 'NL'), (143461, 'NZ'), (143512, 'NI'),\n    (143534, 'NE'), (143561, 'NG'), (143457, 'NO'), (143562, 'OM'), (143477, 'PK'),\n    (143595, 'PW'), (143485, 'PA'), (143597, 'PG'), (143513, 'PY'), (143507, 'PE'),\n    (143474, 'PH'), (143478, 'PL'), (143453, 'PT'), (143498, 'QA'), (143487, 'RO'),\n    (143469, 'RU'), (143598, 'ST'), (143479, 'SA'), (143535, 'SN'), (143599, 'SC'),\n    (143600, 'SL'), (143464, 'SG'), (143496, 'SK'), (143499, 'SI'), (143601, 'SB'),\n    (143472, 'ZA'), (143466, 'KR'), (143454, 'ES'), (143486, 'LK'), (143548, 'KN'),\n    (143549, 'LC'), (143550, 'VC'), (143554, 'SR'), (143602, 'SZ'), (143456, 'SE'),\n    (143459, 'CH'), (143470, 'TW'), (143603, 'TJ'), (143572, 'TZ'), (143475, 'TH'),\n    (143551, 'TT'), (143536, 'TN'), (143480, 'TR'), (143604, 'TM'), (143552, 'TC'),\n    (143537, 'UG'), (143492, 'UA'), (143481, 'AE'), (143444, 'GB'), (143441, 'US'),\n    (143514, 'UY'), (143566, 'UZ'), (143502, 'VE'), (143471, 'VN'), (143571, 'YE'),\n    (143605, 'ZW')]\n    \n# IOS_STORE_COUNTRY_MAPPING = [\n#     (0, 'WW'), (143441, 'US'), (143465, 'CN'), (143460, 'AU'), (143444, 'GB')\n#     ]\n\nANDROID_STORE_COUNTRY_MAPPING = [\n    (17, 'AR'), (1, 'AU'), (35, 'AT'), (61, 'AZ'), (11, 'BE'), (18, 'BR'), (47, 'BG'),\n    (2, 'CA'), (13, 'CL'), (3, 'CN'), (52, 'CO'), (64, 'CR'), (80, 'HR'), (36, 'CZ'),\n    (38, 'DK'), (62, 'EC'), (33, 'EG'), (20, 'FI'), (6, 'FR'), (4, 'DE'), (46, 'GR'),\n    (16, 'HK'), (37, 'HU'), (19, 'IN'), (21, 'ID'), (39, 'IE'), (40, 'IL'), (8, 'IT'),\n    (9, 'JP'), (53, 'KZ'), (95, 'KE'), (50, 'KW'), (86, 'LV'), (65, 'LB'), (78, 'LT'),\n    (24, 'MY'), (26, 'MX'), (23, 'NL'), (41, 'NZ'), (74, 'NG'), (42, 'NO'), (54, 'PK'),\n    (56, 'PE'), (31, 'PH'), (28, 'PL'), (43, 'PT'), (84, 'PR'), (73, 'QA'), (44, 'RO'),\n    (22, 'RU'), (51, 'SA'), (32, 'SG'), (45, 'SK'), (14, 'ZA'), (27, 'KR'), (5, 'ES'),\n    (34, 'SE'), (12, 'CH'), (30, 'TW'), (29, 'TH'), (25, 'TR'), (48, 'UA'), (49, 'AE'),\n    (7, 'GB'), (10, 'US'), (15, 'VN'), (1000, 'WW')\n]\n# ANDROID_STORE_COUNTRY_MAPPING = [\n#   (12, 'CH'), (30, 'TW'), (25, 'TR'), (48, 'UA'),\n#     (7, 'GB'), (10, 'US'), (1000, 'WW')\n# ]\n\n\n\ndef device_code_to_feed(market_code, device_code, metric_name):\n    mapping = [\n        ['apple-store',0,'ios-phone','est_free_app_download'],\n        ['apple-store',1,'ios-phone','est_paid_app_download'],\n        ['apple-store',2,'ios-phone','est_revenue'],\n        ['apple-store',101,'ios-tablet','est_free_app_download'],\n        ['apple-store',100,'ios-tablet','est_paid_app_download'],\n        ['apple-store',102,'ios-tablet','est_revenue'],\n        ['google-play',0,'android-all','est_free_app_download'],\n        ['google-play',1,'android-all','est_paid_app_download'],\n        ['google-play',2,'android-all','est_revenue'],\n    ]\n    return [x for x in mapping if (x[0], x[2], x[3]) == (market_code, device_code, metric_name)][0][1]\n\n\n\ndef country_code_to_id(market_code, code):\n    if market_code == 'apple-store':\n        ios_mapping = {_code:_id for (_id, _code) in IOS_STORE_COUNTRY_MAPPING}\n        return ios_mapping[code]\n    else:\n        gp_mapping = {_code:_id for (_id, _code) in ANDROID_STORE_COUNTRY_MAPPING}\n        return gp_mapping[code]\n\n\ndef category_to_legacy_category(market_code, legacy):\n    if market_code == 'apple-store':\n        ios_category = {_category:_legacy_category for (_legacy_category,_category) in IOS_CATEGORIES }\n        return ios_category[legacy]\n    else:\n        gp_category =  {_category:_legacy_category for (_legacy_category,_category) in ANDROID_CATEGORIES }\n        return gp_category[legacy]\n\ncountry_code_to_id(\"google\",'WW')\n# 13028, 100061\n# 71, 400034\n# category_to_legacy_category(\"apple-store\",100000)\n"]},{"cell_type":"code","execution_count":0,"id":"20200410-115105_1181134543","metadata":{},"outputs":[],"source":["\nstore_id=country_code_to_id(\"apple-store\",\"BY\")\nprint store_id\nfeed_id=device_code_to_feed(\"apple-store\",\"ios-tablet\",\"est_free_app_download\")\nprint feed_id\nspark.read.parquet(\"s3://b2c-prod-dca-store-estimates/store_estv2/APP_ESTIMATES_FINAL/version=2.0.0/range_type=DAY/date=2020-03-01/\").where(\"store_id={} and feed={}\".format(store_id, feed_id)).show()\nspark.read.option(\"basePath\", \"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/\").parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date=2020-03-01/\").filter(\"country_code='BY' and device_code='ios-tablet'\").show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200410-120525_698147226","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\n\n\n\ndef plproxy_row(date, country_code , market_code , device_code, metric_name):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n    \n    \n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n    sql = '''\n    select distinct app_id,'{}', '{}', date,estimate from plproxy.execute_select($$select\n     distinct app_id,feed_id,store_id,estimate, date\n    from aa.app_store_daily_estimate_0\n        where \n      date = '{}' and store_id='{}' and feed_id = {}\n        $$) tbl (app_id BIGINT,feed_id SMALLINT,store_id INT, estimate INT , date DATE)  '''.format(\n           country_code,\n           device_code,\n        date, \n        country_code_to_id(market_code, country_code), \n        device_code_to_feed(market_code, device_code, metric_name)\n    )\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [ Row(app_id=r[0], country_code=r[1], device_code=r[2], date=r[3], estimate=r[4]) for r in rows ], len(rows)\n\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200410-120528_240017245","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\n\ndef citus_row( date, country_code , market_code , device_code, metric_name):\n    def get_data_in_citus(date, country_code , market_code , device_code, metric_name):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.10.254\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select app_id,country_code,device_code,date,{} as estimate from  store.store_est_fact_v1 where granularity='daily' and date='{}' and device_code='{}' and {} is not null and country_code='{}' \".format(metric_name, date, device_code, metric_name, country_code )\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    \n    result = get_data_in_citus(date, country_code , market_code , device_code, metric_name)\n    return [ Row(app_id=r[0], country_code=r[1], device_code=r[2], date=r[3], estimate=r[4]) for r in result ], len(result)\n\n\n\n# df_plploxy.createOrReplaceTempView(\"old\")\n# df_citus.createOrReplaceTempView(\"new\")\n\n# spark.sql(\"\")\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200410-120733_1456870523","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\n\ndef plproxy_row_category(date, country_code , market_code , device_code, category_id,  metric_name):\n    urn = Urn(\n        namespace='app-qa.db-check.v1',\n        owner='app_qa'\n    )\n\n\n    def build_db_settings(urn, db_conf_name):\n        template = \"PG_\" + db_conf_name.upper() + \"_{property}\"\n        settings = application_settings(urn)\n        host, port = getattr(settings, template.format(property='HOSTS'))[0]\n        return {\n            'HOST': host,\n            'PORT': port,\n            'NAME': getattr(settings, template.format(property='NAME')),\n            'PASSWORD': getattr(settings, template.format(property='SECRET_KEY')),\n            'USER': getattr(settings, template.format(property='ACCESS_ID')),\n            'NODE_NUM': int(getattr(settings, template.format(property='NODE_NUM'))),\n            'CLUSTER': getattr(settings, template.format(property='CLUSTER'))\n        }\n    \n    \n    settings = build_db_settings(urn, \"DAILY_EST\")\n\n    sql = '''\n    select distinct app_id,'{}', '{}', '{}', date, estimate from plproxy.execute_select($$select\n     distinct app_id,feed_id,store_id,estimate, date, category_id\n    from aa.app_store_daily_estimate_1\n        where \n      date = '{}' and store_id='{}' and feed_id = {} and category_id={}\n        $$) tbl (app_id BIGINT,feed_id SMALLINT,store_id INT, estimate INT , date DATE, category_id INT)  '''.format(\n           country_code,\n           device_code,\n           category_id,\n           date, \n           country_code_to_id(market_code, country_code), \n           device_code_to_feed(market_code, device_code, metric_name),\n           category_to_legacy_category(market_code,category_id)\n        )\n\n    runner = LocalSqlRunner(settings)\n    rows, _, columns = runner.select_return_columns(sql)\n    return [ Row(app_id=r[0], country_code=r[1], device_code=r[2], category_id=[3], date=r[4], estimate=r[5]) for r in rows ], len(rows)\n\n# plproxy_row_category('2020-01-02', 'US' , 'apple-store' , 'ios-phone', 100000,  'est_revenue')\n"]},{"cell_type":"code","execution_count":0,"id":"20200410-120736_1880352867","metadata":{},"outputs":[],"source":["\nimport psycopg2\nimport datetime\nspark.sparkContext.addPyFile(\"/home/hadoop/bdp/application/libs/python/dependencies.zip\")\nimport aaplproxy\nfrom aadatapipelinecore.core.urn import Urn\nfrom aaplproxy.da.local_sqlrunner import LocalSqlRunner\nfrom aadatapipelinecore.core.utils.module import application_settings\nfrom pyspark.sql import Row\n\ndef citus_row_category(date, country_code , market_code , device_code, category_id, metric_name):\n    def get_data_in_citus(date, country_code , market_code , device_code, category_id, metric_name):\n        citus_dsn_ = (\n            \"dbname='{db}' user='{user}' password='{password}' \"\n            \"host='{host}' port='{port}'\".format(\n                db=\"aa_store_db\",\n                user=\"citus_bdp_prod_app_int_qa\",\n                host=\"10.2.10.254\",\n                password=\"wZw8cfBuuklIskVG\",\n                port=5432\n            )\n        )\n        sql = \"select app_id,country_code,device_code,category_id, date,{} as estimate from  store.store_est_category_fact_v1 where granularity='daily' and date='{}' and device_code='{}' and {} is not null and country_code='{}' and category_id={} \".format(metric_name, date, device_code, metric_name, country_code, category_id )\n        db_data = query(citus_dsn_, sql)\n        return db_data\n\n    def query(dsn, sql):\n        with psycopg2.connect(dsn) as conn:\n            conn.autocommit = True\n            with conn.cursor() as cur:\n                cur.execute(sql)\n                result = cur.fetchall()\n                conn.commit()\n        return result\n    \n    result = get_data_in_citus(date, country_code , market_code , device_code, category_id, metric_name)\n    return [ Row(app_id=r[0], country_code=r[1], device_code=r[2], category_id=[3], date=r[4], estimate=r[5]) for r in result ] , len(result)\n\n# citus_row_category('2020-03-28', 'US' , 'apple-store' , 'ios-phone', 100000,  'est_revenue')"]},{"cell_type":"code","execution_count":0,"id":"20200410-120751_2096535264","metadata":{},"outputs":[],"source":["\nest_list=[\"est_free_app_download\", \"est_paid_app_download\", \"est_revenue\" ]\ndevice_code_list=['ios-phone','ios-tablet']\ndate_list=['2019-11-11']\nl1 = [[d, x[1], 'apple-store' , device_code, est] for x in IOS_STORE_COUNTRY_MAPPING for est in est_list for device_code in device_code_list for d in date_list]\nl2 = [ [d, x[1], 'google-play' , \"android-all\", est] for x in ANDROID_STORE_COUNTRY_MAPPING for est in est_list for d in date_list]\n\ntest_data_list = l1 + l2\n\n# print len(test_data_list)\n# device_code_to_feed()\n# def device_code_to_feed(market_code, device_code, metric_name):\nprint test_data_list\n"]},{"cell_type":"code","execution_count":0,"id":"20200410-120800_1658395053","metadata":{},"outputs":[],"source":["\nfor test_data in test_data_list[1000:]:\n    print test_data\n    citus_rows, citus_db_size = citus_row(test_data[0],test_data[1],test_data[2],test_data[3],test_data[4]   )\n    plproxy_rows , plproxy_db_size= plproxy_row(test_data[0],test_data[1],test_data[2],test_data[3],test_data[4]  )\n    if plproxy_db_size != 0:\n        spark.createDataFrame(plproxy_rows).createOrReplaceTempView(\"plproxy\")\n        try:\n            spark.createDataFrame(citus_rows).createOrReplaceTempView(\"citus\")\n            except_count_1=spark.sql(\"select * from plproxy EXCEPT ALL select * from citus\").count()\n            db_count_diff=citus_db_size-plproxy_db_size\n            if except_count_1 != 0:\n                except_count_2=spark.sql(\"select * from citus EXCEPT ALL select * from plproxy\").count()\n                spark.createDataFrame([(test_data[0],test_data[1],test_data[2],test_data[3],test_data[4], except_count_1, except_count_2, db_count_diff)],\n                schema=[\"date\", \"country_code\" , \"market_code\" , \"device_code\", \"est\", \"citus_plproxy\" , \"plproxy_citus\" , \"diff\"]).write.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_est_db_diff\", mode=\"append\")\n                print 'failed',test_data[0],test_data[1],test_data[2],test_data[3],test_data[4], db_count_diff\n        except Exception as exp:\n            print exp\n            print 'failed', test_data[0],test_data[1],test_data[2],test_data[3],test_data[4]\n        \n\n"]},{"cell_type":"code","execution_count":0,"id":"20200411-025927_847395114","metadata":{},"outputs":[],"source":["\nest_list=[\"est_free_app_download\", \"est_paid_app_download\", \"est_revenue\" ]\ndevice_code_list=['ios-phone','ios-tablet']\ndate_list=['2020-02-05']\n\nl1 = [[d, x[1], 'apple-store' , device_code,category[1],est ] for x in IOS_STORE_COUNTRY_MAPPING for est in est_list for device_code in device_code_list for d in date_list for category in IOS_CATEGORIES ]\nl2 = [ [d, x[1], 'google-play' , \"android-all\",category[1], est ] for x in ANDROID_STORE_COUNTRY_MAPPING for est in est_list for d in date_list for category in ANDROID_CATEGORIES ]\n\ntest_data_list_category = l1 + l2\n\nprint len(test_data_list_category)\n\n"]},{"cell_type":"code","execution_count":0,"id":"20200410-120815_687296979","metadata":{},"outputs":[],"source":["\nfor test_data in test_data_list_category[43100:]:\n    plproxy_rows, plroxy_db_category_size = plproxy_row_category(test_data[0],test_data[1],test_data[2],test_data[3],test_data[4],test_data[5]  )\n    citus_rows, citus_db_category_size = citus_row_category(test_data[0],test_data[1],test_data[2],test_data[3],test_data[4] ,test_data[5] )\n    db_count_diff=citus_db_category_size-plroxy_db_category_size\n    print test_data , 'diff is:' , db_count_diff\n    if plroxy_db_category_size != 0:\n        spark.createDataFrame(plproxy_rows).createOrReplaceTempView(\"plproxy_category\")\n        try:\n            spark.createDataFrame(citus_rows).createOrReplaceTempView(\"citus_category\")\n            except_count_1=spark.sql(\"select * from citus_category EXCEPT ALL select * from plproxy_category\").count()\n            if except_count_1 != db_count_diff:\n                except_count_2=spark.sql(\"select * from plproxy_category EXCEPT ALL select * from citus_category\").count()\n                spark.createDataFrame([(test_data[0],test_data[1],test_data[2],test_data[3],test_data[4], except_count_1, except_count_2, db_count_diff)],\n                schema=[\"date\", \"country_code\" , \"market_code\" , \"device_code\", \"category_id\",  \"citus_plproxy\" , \"plproxy_citus\" , \"diff\"]).write.parquet(\"s3://b2c-prod-data-pipeline-qa/aa.store/result_rank_db_diff\", mode=\"append\")\n\n                print 'failed', test_data[0],test_data[1],test_data[2],test_data[3],test_data[4] ,test_data[5]\n        except Exception as exp:\n            print exp\n            print 'failed', test_data[0],test_data[1],test_data[2],test_data[3],test_data[4],test_data[5]\n        \n        "]},{"cell_type":"code","execution_count":0,"id":"20200411-114949_1044450355","metadata":{},"outputs":[],"source":["%%sh\n\n\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -p 7432 -U app_bdp_usage_qa -d dailyest << EOF\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select sum(estimate) as cnt \n    from ( select app_id,feed_id,store_id,estimate,category_id,device_id,rank from aa.app_store_daily_estimate_1\n    where date = '2018-04-01'  and  feed_id in (0) ) as prod\n\\$proxy\\$) tbl (cnt bigint);\n\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select sum(estimate) as cnt \n    from ( select app_id,feed_id,store_id,estimate,category_id,device_id,rank from aa.app_store_daily_estimate_1\n    where date = '2018-04-01'  and  feed_id in (1) ) as prod\n\\$proxy\\$) tbl (cnt bigint);\n\n\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select sum(estimate) as cnt \n    from ( select app_id,feed_id,store_id,estimate,category_id,device_id,rank from aa.app_store_daily_estimate_1\n    where date = '2018-04-01'  and  feed_id in (2) ) as prod\n\\$proxy\\$) tbl (cnt bigint);\n\n\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select sum(estimate) as cnt \n    from ( select app_id,feed_id,store_id,estimate,category_id,device_id,rank from aa.app_store_daily_estimate_1\n    where date = '2018-04-01'  and  feed_id in (101) ) as prod\n\\$proxy\\$) tbl (cnt bigint);\n\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select sum(estimate) as cnt \n    from ( select app_id,feed_id,store_id,estimate,category_id,device_id,rank from aa.app_store_daily_estimate_1\n    where date = '2018-04-01'  and  feed_id in (100) ) as prod\n\\$proxy\\$) tbl (cnt bigint);\n\n\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select sum(estimate) as cnt \n    from ( select app_id,feed_id,store_id,estimate,category_id,device_id,rank from aa.app_store_daily_estimate_1\n    where date = '2018-04-01'  and  feed_id in (102)  ) as prod\n\\$proxy\\$) tbl (cnt bigint);\n\n\n\n\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select sum(estimate) as cnt \n    from ( select app_id,feed_id,store_id,estimate,category_id,device_id,rank from aa.app_store_daily_estimate_0\n    where date = '2018-04-01'  and  feed_id in (0) ) as prod\n\\$proxy\\$) tbl (cnt bigint);\n\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select sum(estimate) as cnt \n    from ( select app_id,feed_id,store_id,estimate,category_id,device_id,rank from aa.app_store_daily_estimate_0\n    where date = '2018-04-01'  and  feed_id in (1) ) as prod\n\\$proxy\\$) tbl (cnt bigint);\n\n\nselect sum(cnt) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select sum(estimate) as cnt \n    from ( select app_id,feed_id,store_id,estimate,category_id,device_id,rank from aa.app_store_daily_estimate_0\n    where date = '2018-04-01'  and  feed_id in (2) ) as prod\n\\$proxy\\$) tbl (cnt bigint);\n\n\nEOF\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}