{"cells":[{"cell_type":"code","execution_count":0,"id":"20200224-114221_1778229052","metadata":{},"outputs":[],"source":["    \n\n\ndate='2019-12-11'\nraw_df = spark.read.option(\"delimiter\", \"\\t\").csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/*/sbe_est_app/*/\".format(date)).show()\n"]},{"cell_type":"code","execution_count":0,"id":"20200224-114253_1786943555","metadata":{},"outputs":[],"source":["\n%sh\n\nPGPASSWORD='2mHdFW6%#REu' psql -h internal-aa-prod-plproxy-internal-4-329644124.us-east-1.elb.amazonaws.com -U app_bdp_usage_qa -d dailyest -p 7432 << EOF \n\nselect feed_id,store_id,sum(estimate) from plproxy.execute_select_nestloop(\\$proxy\\$ \n    select feed_id,store_id,sum(estimate)\n    from aa.app_store_daily_estimate_1\n    where \n        date = '2019-09-01' \n    group by store_id,feed_id\n\\$proxy\\$) tbl (feed_id SMALLINT,store_id INT, estimate INT) group by store_id,feed_id;\n\nEOF\n"]},{"cell_type":"code","execution_count":0,"id":"20200224-115215_75697625","metadata":{},"outputs":[],"source":["\nfrom pyspark.sql import types as T\nimport pandas as pd\n\nEST_SCHEMA = T.StructType(\n[\n    T.StructField(\"country_code\", T.StringType(), True),\n    T.StructField(\"date\", T.StringType(), True),\n    T.StructField(\"device_code\", T.StringType(), True),\n    T.StructField(\"granularity\", T.StringType(), True),\n    T.StructField(\"free_app_download\", T.StringType(), True),  \n    T.StructField(\"app_id\", T.StringType(), True),\n    T.StructField(\"paid_app_download\", T.StringType(), True),\n    T.StructField(\"revenue\", T.StringType(), True),\n    T.StructField(\"revenue_iap\", T.StringType(), True)    ]\n)\n    \ndef get_date_list(start_date, end_date, freq=\"D\"):\n    date_list = [x.strftime('%Y-%m-%d') for x in list(pd.date_range(start=start_date, end=end_date, freq=freq))]\n    return date_list\n\ndate_list = get_date_list(\"2018-03-07\", \"2019-07-13\")\n# date_list = get_date_list(\"2010-08-02\", \"2019-07-13\")\n\nfor date in date_list:\n    unified_df = spark.read.parquet(\"s3://b2c-prod-data-pipeline-unified-store-paid/unified/store.app-est.v1/fact/granularity=daily/date={}/\".format(date)).select(\"app_id\").distinct()\n    unifie_count = unified_df.count()\n    # unified_df.write.parquet(\"s3://b2c-prod-data-pipeline-qa/tom/store/unified/date={}/\".format(date))\n    \n\n    raw_df = spark.read.option(\"delimiter\", \"\\t\").csv(\"s3://b2c-prod-dca-store-estimates/store_est/v_final/DAY/{}/*/sbe_est_app/*/\".format(date)).select(\"_c5\").distinct()\n    raw_count= raw_df.count()\n    # raw_df.write.parquet(\"s3://b2c-prod-data-pipeline-qa/tom/store/raw/date={}/\".format(date))\n\n    \n    if unifie_count!=raw_count:\n        print \"{}; Fail; {};{}\".format(date, raw_count, unifie_count)\n    else:\n        print \"{}; Pass\".format(date)"]},{"cell_type":"code","execution_count":0,"id":"20200225-002607_1574120210","metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}